\documentclass{beamer}
 
\usepackage[utf8]{inputenc}
\usepackage{braket}
\usepackage{mathtools} % Needed for \prescript
\usetheme{Boadilla}
\setbeamertemplate{navigation symbols}{}
\usepackage{subfig}
\usepackage{hyperref}

\captionsetup[subfigure]{labelformat=empty}
\captionsetup[figure]{labelformat=empty}

\newcommand{\tot}{\mathrm{tot}}
\DeclareMathOperator{\Tr}{Tr}

%Information to be included in the title page:
\title{LSTM Tagger}
\author{Philipp Windischhofer}
%\institute{ETH ZÃ¼rich}
\date{\today}
  
\begin{document}
 
\frame{\titlepage}

%\begin{frame}
%\frametitle{Table of Contents}
%\tableofcontents
%\end{frame}

\begin{frame}
  \frametitle{The Workflow}
  \framesubtitle{Training}
  \begin{itemize}
  \item match tracks to their associated jets (contained in different ROOT trees)
    \begin{itemize}
      \item done \textsl{ahead} of training
    \end{itemize}
  \item save matched tracks / jets to disk (HDF5)
  \item for each track in the jet, feed the track parameters into the classifier network during training
    \begin{itemize}
      \item (may) use $p_T$ ordering, i.e.~hardest track first
    \end{itemize}
  \item supervised training: provide a binary (0/1) output value for each jet (from MC truth)
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Results so far (work in progress)}
  trained a number of LSTM networks, scanned the hyperparameters:
    \begin{itemize}
    \item number of nodes in each layer
    \item number of layers
    \item number of training epochs
    \end{itemize}
  
  Details of the training:
  \begin{itemize}
  \item training dataset contains 100-150k jets
  \item training/validation split 80:20
  \item batch size: 1k jets (also tried unbatched training)
  \end{itemize}

  Progress w.r.t. last time:
  \begin{itemize}
    \item made code (much) more flexible
    \item ready to efficiently ramp up training \& try different network types
  \end{itemize}  
\end{frame}

\begin{frame}
  \frametitle{different network shapes (unbatched)}
  \begin{itemize}
  \item batch size = 1
    \item labelling: \textsl{number nodes / number layers}
  \end{itemize}
  
    \begin{figure}[htb]
    \centering
    \subfloat[][]{\includegraphics[width =0.7\textwidth]{./1layers.pdf}}	
    \end{figure}

    3-layer networks very similar...
\end{frame}

\begin{frame}
  \frametitle{using less information}
    \begin{figure}[htb]
    \centering
    \subfloat[][]{\includegraphics[width =0.9\textwidth]{./removed_parameters.pdf}}	
  \end{figure}

\end{frame}

\begin{frame}
  \frametitle{batched training}
  \begin{itemize}
    \item batch size = 1k jets
    \item labelling: \textsl{number nodes / number layers}
      \end{itemize}

    \begin{figure}[htb]
    \centering
    \subfloat[][]{\includegraphics[width =0.7\textwidth]{./batchedunbatched.pdf}}	
    \end{figure}
    What is a reasonable batch size to use?
\end{frame}

\begin{frame}
  \frametitle{different network shapes (batched)}
  \begin{itemize}
  \item batch size = 1k jets
    \item labelling: \textsl{number nodes / number layers}
  \end{itemize}
  
    \begin{figure}[htb]
    \centering
    \subfloat[][]{\includegraphics[width =0.7\textwidth]{./batchedcomparison.pdf}}	% add the correct file here!
    \end{figure}
    Again very similar AUC:\\
    128/1 vs 64/3 vs 64/1 vs cMVA = 0.8945 vs 0.8920 vs 0.8924 vs 0.9240
\end{frame}

\begin{frame}
  \frametitle{LSTM64/1: batched training}
  batch size = 1k jets $\rightarrow$ much smoother descent
  \begin{figure}[htb]
    \centering
    \subfloat[][]{\includegraphics[width =0.8\textwidth]{../../RNN_out_external_matching/LSTM64_1layer_large_training_batch/loss-history.pdf}}	
  \end{figure}

  TODO: add callback for terminating the training once saturated!
\end{frame}

\begin{frame}
  \frametitle{LSTM64/1: batched training}
\only<1>{\centering\includegraphics[width =0.9\textwidth]{../../RNN_out_external_matching/LSTM64_1layer_large_training_batch/corrplot_b.pdf}}
\only<2>{\centering\includegraphics[width =0.9\textwidth]{../../RNN_out_external_matching/LSTM64_1layer_large_training_batch/corrplot_non_b.pdf}}\\
(very similar for the others)
\end{frame}

\begin{frame}
  \frametitle{Next steps}

  New results very similar to what we had before!

  What to do next?
  \begin{itemize}
  \item try more epochs (until overtraining becomes visible?)
  \item try more training data (i.e.~multiple 150k sets?)
  \item try conventional RNN instead of LSTM
  \item try different optimizers (up to now used Adam with standard parameters)
  \item ...
  \end{itemize}

\end{frame}

\end{document}
