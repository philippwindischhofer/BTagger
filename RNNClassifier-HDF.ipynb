{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ROOT\n",
    "import root_numpy as rnpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import pickle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build here the keras model\n",
    "def RNN_classifier():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(32, input_shape = (None, 8)))\n",
    "    \n",
    "    # make an output layer with just 1 output -> for a binary classification problem: b-jet / not b-jet\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_training_data(jet_list, label):\n",
    "    # extract the tracks and put them in pt-order, hardest tracks first\n",
    "    jet_tracks = [cur[-1] for cur in jet_list]\n",
    "    jet_tracks = [sorted(cur, key = lambda tracks: tracks[0], reverse = True) for cur in jet_tracks]\n",
    "    \n",
    "    # zero-pad the track dimension, to make sure all jets fed into the network during training have the same length\n",
    "    max_tracks = max([len(cur) for cur in jet_tracks])\n",
    "    padded = [np.vstack([cur, np.full((max_tracks - len(cur), 8), 0, float)]) for cur in jet_tracks]\n",
    "    \n",
    "    batch_size = len(padded)\n",
    "    timestep_size = max_tracks\n",
    "    jet_dim = 8\n",
    "    x_train = np.array(padded).reshape(batch_size, timestep_size, jet_dim)\n",
    "    y_train = np.full((batch_size, 1), label, float) # all are b-jets!\n",
    "    \n",
    "    return x_train, y_train, batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size_jets = 120\n",
    "batch_size_tracks = 520\n",
    "read_pos_jets = 0\n",
    "read_pos_tracks = 0\n",
    "number_chunks = 0\n",
    "chunks_limit = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RNN_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25 samples, validate on 13 samples\n",
      "Epoch 1/5\n",
      "25/25 [==============================] - 0s - loss: 0.7346 - acc: 0.2400 - val_loss: 0.7607 - val_acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "25/25 [==============================] - 0s - loss: 0.7407 - acc: 0.1600 - val_loss: 0.7649 - val_acc: 0.0000e+00\n",
      "Epoch 3/5\n",
      "25/25 [==============================] - 0s - loss: 0.7441 - acc: 0.1600 - val_loss: 0.7667 - val_acc: 0.0000e+00\n",
      "Epoch 4/5\n",
      "25/25 [==============================] - 0s - loss: 0.7453 - acc: 0.1200 - val_loss: 0.7663 - val_acc: 0.0000e+00\n",
      "Epoch 5/5\n",
      "25/25 [==============================] - 0s - loss: 0.7446 - acc: 0.1600 - val_loss: 0.7639 - val_acc: 0.0000e+00\n",
      "Train on 10 samples, validate on 5 samples\n",
      "Epoch 1/5\n",
      "10/10 [==============================] - 0s - loss: 0.5116 - acc: 1.0000 - val_loss: 0.4566 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 0s - loss: 0.5099 - acc: 1.0000 - val_loss: 0.4532 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 0s - loss: 0.5068 - acc: 1.0000 - val_loss: 0.4486 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 0s - loss: 0.5023 - acc: 1.0000 - val_loss: 0.4430 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 0s - loss: 0.4967 - acc: 1.0000 - val_loss: 0.4365 - val_acc: 1.0000\n",
      "Train on 33 samples, validate on 17 samples\n",
      "Epoch 1/5\n",
      "33/33 [==============================] - 0s - loss: 0.6025 - acc: 0.9394 - val_loss: 0.6321 - val_acc: 0.9412\n",
      "Epoch 2/5\n",
      "33/33 [==============================] - 0s - loss: 0.5989 - acc: 0.9394 - val_loss: 0.6279 - val_acc: 0.9412\n",
      "Epoch 3/5\n",
      "33/33 [==============================] - 0s - loss: 0.5942 - acc: 0.9697 - val_loss: 0.6227 - val_acc: 0.9412\n",
      "Epoch 4/5\n",
      "33/33 [==============================] - 0s - loss: 0.5885 - acc: 0.9697 - val_loss: 0.6164 - val_acc: 0.9412\n",
      "Epoch 5/5\n",
      "33/33 [==============================] - 0s - loss: 0.5818 - acc: 0.9697 - val_loss: 0.6092 - val_acc: 0.9412\n",
      "Train on 20 samples, validate on 10 samples\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 0s - loss: 0.8674 - acc: 0.0000e+00 - val_loss: 1.0382 - val_acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 0s - loss: 0.8745 - acc: 0.0000e+00 - val_loss: 1.0421 - val_acc: 0.0000e+00\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 0s - loss: 0.8772 - acc: 0.0000e+00 - val_loss: 1.0421 - val_acc: 0.0000e+00\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 0s - loss: 0.8763 - acc: 0.0000e+00 - val_loss: 1.0388 - val_acc: 0.0000e+00\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 0s - loss: 0.8723 - acc: 0.0000e+00 - val_loss: 1.0327 - val_acc: 0.0000e+00\n",
      "Train on 8 samples, validate on 4 samples\n",
      "Epoch 1/5\n",
      "8/8 [==============================] - 0s - loss: 0.5496 - acc: 1.0000 - val_loss: 0.4772 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "8/8 [==============================] - 0s - loss: 0.5517 - acc: 1.0000 - val_loss: 0.4780 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "8/8 [==============================] - 0s - loss: 0.5522 - acc: 1.0000 - val_loss: 0.4775 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "8/8 [==============================] - 0s - loss: 0.5513 - acc: 1.0000 - val_loss: 0.4758 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "8/8 [==============================] - 0s - loss: 0.5491 - acc: 1.0000 - val_loss: 0.4731 - val_acc: 1.0000\n",
      "Train on 35 samples, validate on 18 samples\n",
      "Epoch 1/5\n",
      "35/35 [==============================] - 0s - loss: 0.6090 - acc: 0.8857 - val_loss: 0.5487 - val_acc: 0.9444\n",
      "Epoch 2/5\n",
      "35/35 [==============================] - 0s - loss: 0.6049 - acc: 0.9143 - val_loss: 0.5440 - val_acc: 0.9444\n",
      "Epoch 3/5\n",
      "35/35 [==============================] - 0s - loss: 0.5998 - acc: 0.9429 - val_loss: 0.5385 - val_acc: 0.9444\n",
      "Epoch 4/5\n",
      "35/35 [==============================] - 0s - loss: 0.5937 - acc: 0.9429 - val_loss: 0.5324 - val_acc: 0.9444\n",
      "Epoch 5/5\n",
      "35/35 [==============================] - 0s - loss: 0.5868 - acc: 0.9429 - val_loss: 0.5257 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "while number_chunks < chunks_limit:\n",
    "    number_chunks += 1\n",
    "    \n",
    "    # read in new chunk of jet and track data\n",
    "    #d1 = pd.DataFrame(rnpy.root2array(\"/mnt/t3nfs01/data01/shome/jpata/btv/gc/TagVarExtractor/GCa08e5e237323/TT_TuneCUETP8M1_13TeV-powheg-pythia8/job_0_out.root\",\n",
    "    #                            treename = \"tagVars/ttree\", start = read_pos_jets, stop = read_pos_jets + batch_size_jets))\n",
    "    #d2 = pd.DataFrame(rnpy.root2array(\"/mnt/t3nfs01/data01/shome/jpata/btv/gc/TagVarExtractor/GCa08e5e237323/TT_TuneCUETP8M1_13TeV-powheg-pythia8/job_0_out.root\",\n",
    "    #                            treename = \"tagVars/ttree_track\", start = read_pos_tracks, stop = read_pos_tracks + batch_size_tracks))\n",
    "    \n",
    "    d1 = pd.read_hdf('../0.h5', key = 'jets', start = read_pos_jets, stop = read_pos_jets + batch_size_jets)\n",
    "    d1 = d1.reset_index(drop=True)\n",
    "    \n",
    "    d2 = pd.read_hdf('../0.h5', key = 'tracks', start = read_pos_tracks, stop = read_pos_tracks + batch_size_tracks)\n",
    "    d2 = d2.reset_index(drop=True)\n",
    "    \n",
    "    # break if we reached the end of the file\n",
    "    if len(d1) < batch_size_jets:\n",
    "        break;\n",
    "        \n",
    "    if len(d2) < batch_size_tracks:\n",
    "        break;\n",
    "    \n",
    "    # figure out where the next chunk should start so that we don't count any jets multiple times\n",
    "    last_tracks = (int)(d2.tail(1)['Track_jetIndex'].iloc[0]-1)\n",
    "    last_jet = (int)(d1.tail(1)['Jet_jetIndex'].iloc[0]-1)\n",
    "    \n",
    "    # find the latest jet index thatis fully contained in this chunk\n",
    "    while(len(d2.loc[d2['Track_jetIndex'] == last_tracks]) == 0):\n",
    "        last_tracks -= 1\n",
    "        \n",
    "    if last_tracks > last_jet:\n",
    "        print(\"Error: have more tracks than jets! Choose different chunk sizes!\")\n",
    "    \n",
    "    read_pos_jets += (d1.loc[d1['Jet_jetIndex'] == last_tracks].index[-1] + 1)\n",
    "    read_pos_tracks += (d2.loc[d2['Track_jetIndex'] == last_tracks].index[-1] + 1)\n",
    "\n",
    "    # add the track data to the jet list\n",
    "    d1['track_data'] = pd.np.empty((len(d1.index),0)).tolist()\n",
    "    \n",
    "    # iterate over the track list to join jets with the tracks belonging to them\n",
    "    for irow, row in d2.iterrows():\n",
    "        # these are the track data of the current track:\n",
    "        tracks = row[[\"Track_pt\", \"Track_eta\", \"Track_phi\", \"Track_dxy\", \"Track_dz\", \"Track_IP\", \"Track_IP2D\", \"Track_length\"]].as_matrix()\n",
    "        jet_index = int(row[\"Track_jetIndex\"])\n",
    "        if jet_index > last_tracks:\n",
    "            break\n",
    "        table_index = d1.loc[d1['Jet_jetIndex'] == jet_index].index[0]\n",
    "\n",
    "        # append the tracks data to the matching jet in the main table\n",
    "        d1['track_data'][table_index].append(tracks)\n",
    "    \n",
    "    # now divide the jets and put them in separate lists, according to their flavour\n",
    "    jets_b = []\n",
    "    jets_l = []\n",
    "    jets_c = []\n",
    "\n",
    "    # iterate over the jet list, with already matched tracks\n",
    "    for irow, row in d1.iterrows():\n",
    "        jet_index = int(row[\"Jet_jetIndex\"])\n",
    "        if jet_index > last_tracks:\n",
    "            break\n",
    "\n",
    "        flavour = int(row[\"Jet_flavour\"])\n",
    "\n",
    "        # select the right list this jet belongs to\n",
    "        if abs(flavour) == 5:\n",
    "            jets = jets_b\n",
    "        elif abs(flavour) == 4:\n",
    "            jets = jets_c\n",
    "        else:\n",
    "            jets = jets_l\n",
    "\n",
    "        # add the new jet to the list, if it contains any resolved tracks\n",
    "        if len(row[\"track_data\"]) > 0:\n",
    "            jets += [(row[\"Jet_pt\"], row[\"Jet_eta\"], row[\"Jet_phi\"], row[\"Jet_mass\"], flavour, row[\"track_data\"])]\n",
    "        \n",
    "    # now, have sorted jets in three lists, can use them directly for training!\n",
    "    x_train_b, y_train_b, batch_size_b = prepare_training_data(jets_b, 1)\n",
    "    x_train_c, y_train_c, batch_size_c = prepare_training_data(jets_c, 0)\n",
    "    x_train_l, y_train_l, batch_size_l = prepare_training_data(jets_l, 0)\n",
    "    history = model.fit(x_train_b, y_train_b, validation_split = 0.33, batch_size = batch_size_b, nb_epoch = 5)\n",
    "    model.fit(x_train_c, y_train_c, validation_split = 0.33, batch_size = batch_size_c, nb_epoch = 5)\n",
    "    model.fit(x_train_l, y_train_l, validation_split = 0.33, batch_size = batch_size_l, nb_epoch = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the model after training here\n",
    "model.save('./RNNtest2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
