{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "require(['codemirror/mode/clike/clike'], function(Clike) { console.log('ROOTaaS - C++ CodeMirror module loaded'); });"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "IPython.CodeCell.config_defaults.highlight_modes['magic_text/x-c++src'] = {'reg':[/^%%cpp/]};"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to ROOTaaS 6.06/08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import ROOT\n",
    "import root_numpy as rnpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import pickle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build here the keras model\n",
    "def RNN_classifier():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(32, input_shape = (None, 8)))\n",
    "    \n",
    "    # make an output layer with just 1 output -> for a binary classification problem: b-jet / not b-jet\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_training_data(jet_list, label):\n",
    "    # extract the tracks and put them in pt-order, hardest tracks first\n",
    "    jet_tracks = [cur[-1] for cur in jet_list]\n",
    "    jet_tracks = [sorted(cur, key = lambda tracks: tracks[0], reverse = True) for cur in jet_tracks]\n",
    "    \n",
    "    # zero-pad the track dimension, to make sure all jets fed into the network during training have the same length\n",
    "    max_tracks = max([len(cur) for cur in jet_tracks])\n",
    "    padded = [np.vstack([cur, np.full((max_tracks - len(cur), 8), 0, float)]) for cur in jet_tracks]\n",
    "    \n",
    "    batch_size = len(padded)\n",
    "    timestep_size = max_tracks\n",
    "    jet_dim = 8\n",
    "    x_train = np.array(padded).reshape(batch_size, timestep_size, jet_dim)\n",
    "    y_train = np.full((batch_size, 1), label, float) # all are b-jets!\n",
    "    \n",
    "    return x_train, y_train, batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size_jets = 100\n",
    "batch_size_tracks = 320\n",
    "read_pos_jets = 0\n",
    "read_pos_tracks = 0\n",
    "number_chunks = 0\n",
    "chunks_limit = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RNN_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15 samples, validate on 8 samples\n",
      "Epoch 1/5\n",
      "15/15 [==============================] - 0s - loss: 0.8615 - acc: 0.0000e+00 - val_loss: 0.8147 - val_acc: 0.2500\n",
      "Epoch 2/5\n",
      "15/15 [==============================] - 0s - loss: 0.8317 - acc: 0.0000e+00 - val_loss: 0.7898 - val_acc: 0.2500\n",
      "Epoch 3/5\n",
      "15/15 [==============================] - 0s - loss: 0.8034 - acc: 0.0000e+00 - val_loss: 0.7661 - val_acc: 0.2500\n",
      "Epoch 4/5\n",
      "15/15 [==============================] - 0s - loss: 0.7765 - acc: 0.0667 - val_loss: 0.7435 - val_acc: 0.2500\n",
      "Epoch 5/5\n",
      "15/15 [==============================] - 0s - loss: 0.7510 - acc: 0.0667 - val_loss: 0.7218 - val_acc: 0.2500\n",
      "Train on 8 samples, validate on 4 samples\n",
      "Epoch 1/5\n",
      "8/8 [==============================] - 0s - loss: 0.6397 - acc: 1.0000 - val_loss: 0.6873 - val_acc: 0.5000\n",
      "Epoch 2/5\n",
      "8/8 [==============================] - 0s - loss: 0.6514 - acc: 0.8750 - val_loss: 0.6914 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "8/8 [==============================] - 0s - loss: 0.6560 - acc: 0.8750 - val_loss: 0.6918 - val_acc: 0.5000\n",
      "Epoch 4/5\n",
      "8/8 [==============================] - 0s - loss: 0.6555 - acc: 0.8750 - val_loss: 0.6895 - val_acc: 0.5000\n",
      "Epoch 5/5\n",
      "8/8 [==============================] - 0s - loss: 0.6512 - acc: 1.0000 - val_loss: 0.6850 - val_acc: 0.5000\n",
      "Train on 22 samples, validate on 11 samples\n",
      "Epoch 1/5\n",
      "22/22 [==============================] - 0s - loss: 0.7002 - acc: 0.3636 - val_loss: 0.7077 - val_acc: 0.2727\n",
      "Epoch 2/5\n",
      "22/22 [==============================] - 0s - loss: 0.6971 - acc: 0.3636 - val_loss: 0.7038 - val_acc: 0.2727\n",
      "Epoch 3/5\n",
      "22/22 [==============================] - 0s - loss: 0.6927 - acc: 0.4545 - val_loss: 0.6988 - val_acc: 0.4545\n",
      "Epoch 4/5\n",
      "22/22 [==============================] - 0s - loss: 0.6872 - acc: 0.5455 - val_loss: 0.6928 - val_acc: 0.5455\n",
      "Epoch 5/5\n",
      "22/22 [==============================] - 0s - loss: 0.6807 - acc: 0.5909 - val_loss: 0.6859 - val_acc: 0.7273\n"
     ]
    }
   ],
   "source": [
    "while number_chunks < chunks_limit:\n",
    "    number_chunks += 1\n",
    "    \n",
    "    # read in new chunk of jet and track data\n",
    "    #d1 = pd.DataFrame(rnpy.root2array(\"/mnt/t3nfs01/data01/shome/jpata/btv/gc/TagVarExtractor/GCa08e5e237323/TT_TuneCUETP8M1_13TeV-powheg-pythia8/job_0_out.root\",\n",
    "    #                            treename = \"tagVars/ttree\", start = read_pos_jets, stop = read_pos_jets + batch_size_jets))\n",
    "    #d2 = pd.DataFrame(rnpy.root2array(\"/mnt/t3nfs01/data01/shome/jpata/btv/gc/TagVarExtractor/GCa08e5e237323/TT_TuneCUETP8M1_13TeV-powheg-pythia8/job_0_out.root\",\n",
    "    #                            treename = \"tagVars/ttree_track\", start = read_pos_tracks, stop = read_pos_tracks + batch_size_tracks))\n",
    "    \n",
    "    d1 = pd.read_hdf('../data/not_matched/1.h5', key = 'jets', start = read_pos_jets, stop = read_pos_jets + batch_size_jets)\n",
    "    d1 = d1.reset_index(drop=True)\n",
    "    \n",
    "    d2 = pd.read_hdf('../data/not_matched/1.h5', key = 'tracks', start = read_pos_tracks, stop = read_pos_tracks + batch_size_tracks)\n",
    "    d2 = d2.reset_index(drop=True)\n",
    "    \n",
    "    # break if we reached the end of the file\n",
    "    if len(d1) < batch_size_jets:\n",
    "        break;\n",
    "        \n",
    "    if len(d2) < batch_size_tracks:\n",
    "        break;\n",
    "    \n",
    "    # figure out where the next chunk should start so that we don't count any jets multiple times\n",
    "    last_tracks = (int)(d2.tail(1)['Track_jetIndex'].iloc[0]-1)\n",
    "    last_jet = (int)(d1.tail(1)['Jet_jetIndex'].iloc[0]-1)\n",
    "    \n",
    "    # find the latest jet index thatis fully contained in this chunk\n",
    "    while(len(d2.loc[d2['Track_jetIndex'] == last_tracks]) == 0):\n",
    "        last_tracks -= 1\n",
    "        \n",
    "    if last_tracks > last_jet:\n",
    "        print(\"Error: have more tracks than jets! Choose different chunk sizes!\")\n",
    "    \n",
    "    read_pos_jets += (d1.loc[d1['Jet_jetIndex'] == last_tracks].index[-1] + 1)\n",
    "    read_pos_tracks += (d2.loc[d2['Track_jetIndex'] == last_tracks].index[-1] + 1)\n",
    "\n",
    "    # add the track data to the jet list\n",
    "    d1['track_data'] = pd.np.empty((len(d1.index),0)).tolist()\n",
    "    \n",
    "    # iterate over the track list to join jets with the tracks belonging to them\n",
    "    for irow, row in d2.iterrows():\n",
    "        # these are the track data of the current track:\n",
    "        tracks = row[[\"Track_pt\", \"Track_eta\", \"Track_phi\", \"Track_dxy\", \"Track_dz\", \"Track_IP\", \"Track_IP2D\", \"Track_length\"]].as_matrix()\n",
    "        jet_index = int(row[\"Track_jetIndex\"])\n",
    "        if jet_index > last_tracks:\n",
    "            break\n",
    "        table_index = d1.loc[d1['Jet_jetIndex'] == jet_index].index[0]\n",
    "\n",
    "        # append the tracks data to the matching jet in the main table\n",
    "        d1['track_data'][table_index].append(tracks)\n",
    "    \n",
    "    # now divide the jets and put them in separate lists, according to their flavour\n",
    "    jets_b = []\n",
    "    jets_l = []\n",
    "    jets_c = []\n",
    "\n",
    "    # iterate over the jet list, with already matched tracks\n",
    "    for irow, row in d1.iterrows():\n",
    "        jet_index = int(row[\"Jet_jetIndex\"])\n",
    "        if jet_index > last_tracks:\n",
    "            break\n",
    "\n",
    "        flavour = int(row[\"Jet_flavour\"])\n",
    "\n",
    "        # select the right list this jet belongs to\n",
    "        if abs(flavour) == 5:\n",
    "            jets = jets_b\n",
    "        elif abs(flavour) == 4:\n",
    "            jets = jets_c\n",
    "        else:\n",
    "            jets = jets_l\n",
    "\n",
    "        # add the new jet to the list, if it contains any resolved tracks\n",
    "        if len(row[\"track_data\"]) > 0:\n",
    "            jets += [(row[\"Jet_pt\"], row[\"Jet_eta\"], row[\"Jet_phi\"], row[\"Jet_mass\"], flavour, row[\"track_data\"])]\n",
    "        \n",
    "    # now, have sorted jets in three lists, can use them directly for training!\n",
    "    x_train_b, y_train_b, batch_size_b = prepare_training_data(jets_b, 1)\n",
    "    x_train_c, y_train_c, batch_size_c = prepare_training_data(jets_c, 0)\n",
    "    x_train_l, y_train_l, batch_size_l = prepare_training_data(jets_l, 0)\n",
    "    history = model.fit(x_train_b, y_train_b, validation_split = 0.33, batch_size = batch_size_b, nb_epoch = 5)\n",
    "    model.fit(x_train_c, y_train_c, validation_split = 0.33, batch_size = batch_size_c, nb_epoch = 5)\n",
    "    model.fit(x_train_l, y_train_l, validation_split = 0.33, batch_size = batch_size_l, nb_epoch = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the model after training here\n",
    "model.save('./RNNtest2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 2.94921875, -0.31513414, -2.41892505, -0.0234375 , -0.05398437,\n",
       "         0.05650066,  0.02343749,  0.24183288], dtype=float32),\n",
       " array([  1.00546875e+01,  -3.76293212e-01,  -2.76434135e+00,\n",
       "          1.13964835e-02,  -1.11755379e-03,   1.14440946e-02,\n",
       "          1.13964770e-02,   7.37814903e-02], dtype=float32),\n",
       " array([ 2.63867188, -0.47462386, -2.40915179, -0.01628906,  0.01118164,\n",
       "         0.0191297 ,  0.01628907,  0.10055161], dtype=float32),\n",
       " array([  1.65332031e+00,  -4.99343842e-01,  -2.66755581e+00,\n",
       "          8.39843764e-04,   6.08886732e-03,   5.46604441e-03,\n",
       "          8.39839515e-04,   4.63350080e-02], dtype=float32)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1['track_data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 9, 8)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.76562500e+00,  -5.56291401e-01,  -2.46355295e+00,\n",
       "         -3.71582038e-03,  -2.20214855e-03,  -4.17350652e-03,\n",
       "         -3.71581782e-03,   5.02888933e-02],\n",
       "       [  2.01953125e+00,  -5.23880720e-01,  -2.15045142e+00,\n",
       "         -2.13470455e-04,  -3.21289059e-03,   2.82542314e-03,\n",
       "          2.13470572e-04,   3.04736639e-03],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_l[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
