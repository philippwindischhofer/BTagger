chunk number 0
prepare data
start training
Train on 8136 samples, validate on 2035 samples
Epoch 1/40
8136/8136 [==============================] - 2s - loss: 0.7172 - acc: 0.3589 - val_loss: 0.6571 - val_acc: 0.6629
Epoch 2/40
8136/8136 [==============================] - 1s - loss: 0.6596 - acc: 0.6557 - val_loss: 0.6466 - val_acc: 0.6634
Epoch 3/40
8136/8136 [==============================] - 1s - loss: 0.6519 - acc: 0.6556 - val_loss: 0.6458 - val_acc: 0.6634
Epoch 4/40
8136/8136 [==============================] - 1s - loss: 0.6521 - acc: 0.6555 - val_loss: 0.6422 - val_acc: 0.6634
Epoch 5/40
8136/8136 [==============================] - 1s - loss: 0.6484 - acc: 0.6555 - val_loss: 0.6383 - val_acc: 0.6634
Epoch 6/40
8136/8136 [==============================] - 1s - loss: 0.6433 - acc: 0.6556 - val_loss: 0.6369 - val_acc: 0.6634
Epoch 7/40
8136/8136 [==============================] - 1s - loss: 0.6405 - acc: 0.6556 - val_loss: 0.6367 - val_acc: 0.6634
Epoch 8/40
8136/8136 [==============================] - 1s - loss: 0.6391 - acc: 0.6556 - val_loss: 0.6348 - val_acc: 0.6634
Epoch 9/40
8136/8136 [==============================] - 1s - loss: 0.6365 - acc: 0.6557 - val_loss: 0.6304 - val_acc: 0.6634
Epoch 10/40
8136/8136 [==============================] - 1s - loss: 0.6320 - acc: 0.6559 - val_loss: 0.6258 - val_acc: 0.6634
Epoch 11/40
8136/8136 [==============================] - 1s - loss: 0.6276 - acc: 0.6556 - val_loss: 0.6226 - val_acc: 0.6649
Epoch 12/40
8136/8136 [==============================] - 1s - loss: 0.6246 - acc: 0.6550 - val_loss: 0.6198 - val_acc: 0.6639
Epoch 13/40
8136/8136 [==============================] - 1s - loss: 0.6212 - acc: 0.6559 - val_loss: 0.6173 - val_acc: 0.6619
Epoch 14/40
8136/8136 [==============================] - 1s - loss: 0.6167 - acc: 0.6620 - val_loss: 0.6181 - val_acc: 0.6688
Epoch 15/40
8136/8136 [==============================] - 1s - loss: 0.6147 - acc: 0.6649 - val_loss: 0.6182 - val_acc: 0.6668
Epoch 16/40
8136/8136 [==============================] - 1s - loss: 0.6133 - acc: 0.6684 - val_loss: 0.6130 - val_acc: 0.6737
Epoch 17/40
8136/8136 [==============================] - 1s - loss: 0.6094 - acc: 0.6699 - val_loss: 0.6099 - val_acc: 0.6737
Epoch 18/40
8136/8136 [==============================] - 1s - loss: 0.6082 - acc: 0.6696 - val_loss: 0.6070 - val_acc: 0.6771
Epoch 19/40
8136/8136 [==============================] - 1s - loss: 0.6050 - acc: 0.6754 - val_loss: 0.6048 - val_acc: 0.6806
Epoch 20/40
8136/8136 [==============================] - 1s - loss: 0.6006 - acc: 0.6797 - val_loss: 0.6019 - val_acc: 0.6811
Epoch 21/40
8136/8136 [==============================] - 1s - loss: 0.5969 - acc: 0.6823 - val_loss: 0.5921 - val_acc: 0.6978
Epoch 22/40
8136/8136 [==============================] - 1s - loss: 0.5903 - acc: 0.6899 - val_loss: 0.5837 - val_acc: 0.7057
Epoch 23/40
8136/8136 [==============================] - 1s - loss: 0.5852 - acc: 0.7016 - val_loss: 0.5749 - val_acc: 0.7145
Epoch 24/40
8136/8136 [==============================] - 1s - loss: 0.5766 - acc: 0.7085 - val_loss: 0.5684 - val_acc: 0.7292
Epoch 25/40
8136/8136 [==============================] - 1s - loss: 0.5693 - acc: 0.7142 - val_loss: 0.5516 - val_acc: 0.7469
Epoch 26/40
8136/8136 [==============================] - 1s - loss: 0.5585 - acc: 0.7295 - val_loss: 0.5381 - val_acc: 0.7577
Epoch 27/40
8136/8136 [==============================] - 1s - loss: 0.5480 - acc: 0.7392 - val_loss: 0.5348 - val_acc: 0.7631
Epoch 28/40
8136/8136 [==============================] - 1s - loss: 0.5426 - acc: 0.7455 - val_loss: 0.5296 - val_acc: 0.7538
Epoch 29/40
8136/8136 [==============================] - 1s - loss: 0.5501 - acc: 0.7367 - val_loss: 0.5101 - val_acc: 0.7676
Epoch 30/40
8136/8136 [==============================] - 1s - loss: 0.5253 - acc: 0.7521 - val_loss: 0.5302 - val_acc: 0.7459
Epoch 31/40
8136/8136 [==============================] - 1s - loss: 0.5391 - acc: 0.7413 - val_loss: 0.5186 - val_acc: 0.7646
Epoch 32/40
8136/8136 [==============================] - 1s - loss: 0.5430 - acc: 0.7499 - val_loss: 0.5132 - val_acc: 0.7705
Epoch 33/40
8136/8136 [==============================] - 1s - loss: 0.5374 - acc: 0.7543 - val_loss: 0.4972 - val_acc: 0.7730
Epoch 34/40
8136/8136 [==============================] - 1s - loss: 0.5108 - acc: 0.7671 - val_loss: 0.5297 - val_acc: 0.7410
Epoch 35/40
8136/8136 [==============================] - 1s - loss: 0.5333 - acc: 0.7464 - val_loss: 0.4882 - val_acc: 0.7877
Epoch 36/40
8136/8136 [==============================] - 1s - loss: 0.5066 - acc: 0.7693 - val_loss: 0.5009 - val_acc: 0.7730
Epoch 37/40
8136/8136 [==============================] - 1s - loss: 0.5224 - acc: 0.7607 - val_loss: 0.4920 - val_acc: 0.7799
Epoch 38/40
8136/8136 [==============================] - 1s - loss: 0.5113 - acc: 0.7654 - val_loss: 0.4871 - val_acc: 0.7828
Epoch 39/40
8136/8136 [==============================] - 1s - loss: 0.4984 - acc: 0.7772 - val_loss: 0.5111 - val_acc: 0.7671
Epoch 40/40
8136/8136 [==============================] - 1s - loss: 0.5139 - acc: 0.7656 - val_loss: 0.4892 - val_acc: 0.7838
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-1.h5
chunk number 1
prepare data
start training
Train on 8160 samples, validate on 2041 samples
Epoch 1/40
8160/8160 [==============================] - 1s - loss: 0.5111 - acc: 0.7685 - val_loss: 0.5057 - val_acc: 0.7692
Epoch 2/40
8160/8160 [==============================] - 1s - loss: 0.5100 - acc: 0.7603 - val_loss: 0.5118 - val_acc: 0.7648
Epoch 3/40
8160/8160 [==============================] - 1s - loss: 0.5155 - acc: 0.7613 - val_loss: 0.5078 - val_acc: 0.7673
Epoch 4/40
8160/8160 [==============================] - 1s - loss: 0.5114 - acc: 0.7619 - val_loss: 0.4998 - val_acc: 0.7771
Epoch 5/40
8160/8160 [==============================] - 1s - loss: 0.5031 - acc: 0.7652 - val_loss: 0.5032 - val_acc: 0.7780
Epoch 6/40
8160/8160 [==============================] - 1s - loss: 0.5054 - acc: 0.7735 - val_loss: 0.5037 - val_acc: 0.7785
Epoch 7/40
8160/8160 [==============================] - 1s - loss: 0.5060 - acc: 0.7746 - val_loss: 0.4938 - val_acc: 0.7805
Epoch 8/40
8160/8160 [==============================] - 1s - loss: 0.4978 - acc: 0.7738 - val_loss: 0.4944 - val_acc: 0.7756
Epoch 9/40
8160/8160 [==============================] - 1s - loss: 0.4993 - acc: 0.7702 - val_loss: 0.4948 - val_acc: 0.7756
Epoch 10/40
8160/8160 [==============================] - 1s - loss: 0.5001 - acc: 0.7703 - val_loss: 0.4889 - val_acc: 0.7834
Epoch 11/40
8160/8160 [==============================] - 1s - loss: 0.4943 - acc: 0.7735 - val_loss: 0.4891 - val_acc: 0.7834
Epoch 12/40
8160/8160 [==============================] - 1s - loss: 0.4936 - acc: 0.7786 - val_loss: 0.4909 - val_acc: 0.7864
Epoch 13/40
8160/8160 [==============================] - 1s - loss: 0.4947 - acc: 0.7759 - val_loss: 0.4854 - val_acc: 0.7844
Epoch 14/40
8160/8160 [==============================] - 1s - loss: 0.4896 - acc: 0.7775 - val_loss: 0.4868 - val_acc: 0.7839
Epoch 15/40
8160/8160 [==============================] - 1s - loss: 0.4914 - acc: 0.7762 - val_loss: 0.4852 - val_acc: 0.7834
Epoch 16/40
8160/8160 [==============================] - 1s - loss: 0.4900 - acc: 0.7760 - val_loss: 0.4820 - val_acc: 0.7869
Epoch 17/40
8160/8160 [==============================] - 1s - loss: 0.4866 - acc: 0.7814 - val_loss: 0.4845 - val_acc: 0.7844
Epoch 18/40
8160/8160 [==============================] - 1s - loss: 0.4888 - acc: 0.7793 - val_loss: 0.4799 - val_acc: 0.7864
Epoch 19/40
8160/8160 [==============================] - 1s - loss: 0.4851 - acc: 0.7824 - val_loss: 0.4795 - val_acc: 0.7878
Epoch 20/40
8160/8160 [==============================] - 1s - loss: 0.4855 - acc: 0.7787 - val_loss: 0.4789 - val_acc: 0.7869
Epoch 21/40
8160/8160 [==============================] - 1s - loss: 0.4848 - acc: 0.7797 - val_loss: 0.4775 - val_acc: 0.7893
Epoch 22/40
8160/8160 [==============================] - 1s - loss: 0.4824 - acc: 0.7805 - val_loss: 0.4793 - val_acc: 0.7893
Epoch 23/40
8160/8160 [==============================] - 1s - loss: 0.4835 - acc: 0.7811 - val_loss: 0.4759 - val_acc: 0.7898
Epoch 24/40
8160/8160 [==============================] - 1s - loss: 0.4807 - acc: 0.7824 - val_loss: 0.4753 - val_acc: 0.7888
Epoch 25/40
8160/8160 [==============================] - 1s - loss: 0.4810 - acc: 0.7803 - val_loss: 0.4741 - val_acc: 0.7893
Epoch 26/40
8160/8160 [==============================] - 1s - loss: 0.4799 - acc: 0.7792 - val_loss: 0.4737 - val_acc: 0.7913
Epoch 27/40
8160/8160 [==============================] - 1s - loss: 0.4786 - acc: 0.7830 - val_loss: 0.4741 - val_acc: 0.7913
Epoch 28/40
8160/8160 [==============================] - 1s - loss: 0.4786 - acc: 0.7841 - val_loss: 0.4716 - val_acc: 0.7913
Epoch 29/40
8160/8160 [==============================] - 1s - loss: 0.4767 - acc: 0.7811 - val_loss: 0.4713 - val_acc: 0.7898
Epoch 30/40
8160/8160 [==============================] - 1s - loss: 0.4769 - acc: 0.7816 - val_loss: 0.4698 - val_acc: 0.7903
Epoch 31/40
8160/8160 [==============================] - 1s - loss: 0.4753 - acc: 0.7824 - val_loss: 0.4699 - val_acc: 0.7923
Epoch 32/40
8160/8160 [==============================] - 1s - loss: 0.4750 - acc: 0.7837 - val_loss: 0.4686 - val_acc: 0.7927
Epoch 33/40
8160/8160 [==============================] - 1s - loss: 0.4738 - acc: 0.7844 - val_loss: 0.4673 - val_acc: 0.7908
Epoch 34/40
8160/8160 [==============================] - 1s - loss: 0.4731 - acc: 0.7830 - val_loss: 0.4668 - val_acc: 0.7913
Epoch 35/40
8160/8160 [==============================] - 1s - loss: 0.4724 - acc: 0.7825 - val_loss: 0.4665 - val_acc: 0.7918
Epoch 36/40
8160/8160 [==============================] - 1s - loss: 0.4714 - acc: 0.7866 - val_loss: 0.4664 - val_acc: 0.7942
Epoch 37/40
8160/8160 [==============================] - 1s - loss: 0.4710 - acc: 0.7871 - val_loss: 0.4646 - val_acc: 0.7942
Epoch 38/40
8160/8160 [==============================] - 1s - loss: 0.4699 - acc: 0.7841 - val_loss: 0.4638 - val_acc: 0.7942
Epoch 39/40
8160/8160 [==============================] - 1s - loss: 0.4696 - acc: 0.7830 - val_loss: 0.4631 - val_acc: 0.7962
Epoch 40/40
8160/8160 [==============================] - 1s - loss: 0.4685 - acc: 0.7871 - val_loss: 0.4631 - val_acc: 0.7962
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-2.h5
chunk number 2
prepare data
start training
Train on 8182 samples, validate on 2046 samples
Epoch 1/40
8182/8182 [==============================] - 1s - loss: 0.4546 - acc: 0.7976 - val_loss: 0.4505 - val_acc: 0.8006
Epoch 2/40
8182/8182 [==============================] - 1s - loss: 0.4505 - acc: 0.7964 - val_loss: 0.4525 - val_acc: 0.7991
Epoch 3/40
8182/8182 [==============================] - 1s - loss: 0.4524 - acc: 0.7974 - val_loss: 0.4500 - val_acc: 0.8001
Epoch 4/40
8182/8182 [==============================] - 1s - loss: 0.4480 - acc: 0.8005 - val_loss: 0.4532 - val_acc: 0.8001
Epoch 5/40
8182/8182 [==============================] - 1s - loss: 0.4502 - acc: 0.7999 - val_loss: 0.4475 - val_acc: 0.8025
Epoch 6/40
8182/8182 [==============================] - 1s - loss: 0.4454 - acc: 0.8009 - val_loss: 0.4488 - val_acc: 0.8011
Epoch 7/40
8182/8182 [==============================] - 1s - loss: 0.4476 - acc: 0.7986 - val_loss: 0.4462 - val_acc: 0.8021
Epoch 8/40
8182/8182 [==============================] - 1s - loss: 0.4438 - acc: 0.8009 - val_loss: 0.4484 - val_acc: 0.7996
Epoch 9/40
8182/8182 [==============================] - 1s - loss: 0.4453 - acc: 0.8014 - val_loss: 0.4445 - val_acc: 0.7991
Epoch 10/40
8182/8182 [==============================] - 1s - loss: 0.4420 - acc: 0.8003 - val_loss: 0.4459 - val_acc: 0.7972
Epoch 11/40
8182/8182 [==============================] - 1s - loss: 0.4434 - acc: 0.7981 - val_loss: 0.4443 - val_acc: 0.7991
Epoch 12/40
8182/8182 [==============================] - 1s - loss: 0.4404 - acc: 0.8018 - val_loss: 0.4463 - val_acc: 0.7996
Epoch 13/40
8182/8182 [==============================] - 1s - loss: 0.4413 - acc: 0.8021 - val_loss: 0.4439 - val_acc: 0.8016
Epoch 14/40
8182/8182 [==============================] - 1s - loss: 0.4394 - acc: 0.8018 - val_loss: 0.4439 - val_acc: 0.7986
Epoch 15/40
8182/8182 [==============================] - 1s - loss: 0.4395 - acc: 0.8016 - val_loss: 0.4433 - val_acc: 0.8006
Epoch 16/40
8182/8182 [==============================] - 1s - loss: 0.4377 - acc: 0.8030 - val_loss: 0.4434 - val_acc: 0.7977
Epoch 17/40
8182/8182 [==============================] - 1s - loss: 0.4375 - acc: 0.8032 - val_loss: 0.4430 - val_acc: 0.7972
Epoch 18/40
8182/8182 [==============================] - 1s - loss: 0.4370 - acc: 0.8016 - val_loss: 0.4426 - val_acc: 0.7967
Epoch 19/40
8182/8182 [==============================] - 1s - loss: 0.4357 - acc: 0.8027 - val_loss: 0.4439 - val_acc: 0.7981
Epoch 20/40
8182/8182 [==============================] - 1s - loss: 0.4357 - acc: 0.8024 - val_loss: 0.4427 - val_acc: 0.8001
Epoch 21/40
8182/8182 [==============================] - 1s - loss: 0.4343 - acc: 0.8020 - val_loss: 0.4426 - val_acc: 0.7986
Epoch 22/40
8182/8182 [==============================] - 1s - loss: 0.4344 - acc: 0.8025 - val_loss: 0.4417 - val_acc: 0.7991
Epoch 23/40
8182/8182 [==============================] - 1s - loss: 0.4329 - acc: 0.8026 - val_loss: 0.4416 - val_acc: 0.7986
Epoch 24/40
8182/8182 [==============================] - 1s - loss: 0.4328 - acc: 0.8043 - val_loss: 0.4410 - val_acc: 0.7986
Epoch 25/40
8182/8182 [==============================] - 1s - loss: 0.4322 - acc: 0.8038 - val_loss: 0.4404 - val_acc: 0.7991
Epoch 26/40
8182/8182 [==============================] - 1s - loss: 0.4309 - acc: 0.8036 - val_loss: 0.4413 - val_acc: 0.8006
Epoch 27/40
8182/8182 [==============================] - 1s - loss: 0.4311 - acc: 0.8058 - val_loss: 0.4404 - val_acc: 0.8006
Epoch 28/40
8182/8182 [==============================] - 1s - loss: 0.4300 - acc: 0.8029 - val_loss: 0.4398 - val_acc: 0.8001
Epoch 29/40
8182/8182 [==============================] - 1s - loss: 0.4291 - acc: 0.8047 - val_loss: 0.4402 - val_acc: 0.8016
Epoch 30/40
8182/8182 [==============================] - 1s - loss: 0.4292 - acc: 0.8069 - val_loss: 0.4391 - val_acc: 0.8016
Epoch 31/40
8182/8182 [==============================] - 1s - loss: 0.4283 - acc: 0.8054 - val_loss: 0.4384 - val_acc: 0.8011
Epoch 32/40
8182/8182 [==============================] - 1s - loss: 0.4272 - acc: 0.8057 - val_loss: 0.4391 - val_acc: 0.8006
Epoch 33/40
8182/8182 [==============================] - 1s - loss: 0.4273 - acc: 0.8071 - val_loss: 0.4387 - val_acc: 0.8040
Epoch 34/40
8182/8182 [==============================] - 1s - loss: 0.4270 - acc: 0.8054 - val_loss: 0.4383 - val_acc: 0.8016
Epoch 35/40
8182/8182 [==============================] - 1s - loss: 0.4256 - acc: 0.8066 - val_loss: 0.4381 - val_acc: 0.8030
Epoch 36/40
8182/8182 [==============================] - 1s - loss: 0.4251 - acc: 0.8071 - val_loss: 0.4378 - val_acc: 0.8025
Epoch 37/40
8182/8182 [==============================] - 1s - loss: 0.4252 - acc: 0.8071 - val_loss: 0.4381 - val_acc: 0.8025
Epoch 38/40
8182/8182 [==============================] - 1s - loss: 0.4246 - acc: 0.8077 - val_loss: 0.4372 - val_acc: 0.8030
Epoch 39/40
8182/8182 [==============================] - 1s - loss: 0.4237 - acc: 0.8074 - val_loss: 0.4373 - val_acc: 0.8025
Epoch 40/40
8182/8182 [==============================] - 1s - loss: 0.4228 - acc: 0.8088 - val_loss: 0.4374 - val_acc: 0.8035
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-3.h5
chunk number 3
prepare data
start training
Train on 8167 samples, validate on 2042 samples
Epoch 1/40
8167/8167 [==============================] - 1s - loss: 0.4561 - acc: 0.7966 - val_loss: 0.4626 - val_acc: 0.7958
Epoch 2/40
8167/8167 [==============================] - 1s - loss: 0.4573 - acc: 0.7960 - val_loss: 0.4617 - val_acc: 0.7929
Epoch 3/40
8167/8167 [==============================] - 1s - loss: 0.4525 - acc: 0.7970 - val_loss: 0.4651 - val_acc: 0.7948
Epoch 4/40
8167/8167 [==============================] - 1s - loss: 0.4547 - acc: 0.7978 - val_loss: 0.4611 - val_acc: 0.7933
Epoch 5/40
8167/8167 [==============================] - 1s - loss: 0.4534 - acc: 0.7947 - val_loss: 0.4582 - val_acc: 0.7938
Epoch 6/40
8167/8167 [==============================] - 1s - loss: 0.4498 - acc: 0.7971 - val_loss: 0.4629 - val_acc: 0.7909
Epoch 7/40
8167/8167 [==============================] - 1s - loss: 0.4535 - acc: 0.7971 - val_loss: 0.4591 - val_acc: 0.7929
Epoch 8/40
8167/8167 [==============================] - 1s - loss: 0.4501 - acc: 0.7977 - val_loss: 0.4591 - val_acc: 0.7948
Epoch 9/40
8167/8167 [==============================] - 1s - loss: 0.4492 - acc: 0.7974 - val_loss: 0.4609 - val_acc: 0.7919
Epoch 10/40
8167/8167 [==============================] - 1s - loss: 0.4497 - acc: 0.7999 - val_loss: 0.4566 - val_acc: 0.7973
Epoch 11/40
8167/8167 [==============================] - 1s - loss: 0.4461 - acc: 0.8007 - val_loss: 0.4565 - val_acc: 0.7973
Epoch 12/40
8167/8167 [==============================] - 1s - loss: 0.4476 - acc: 0.7986 - val_loss: 0.4540 - val_acc: 0.7953
Epoch 13/40
8167/8167 [==============================] - 1s - loss: 0.4448 - acc: 0.8023 - val_loss: 0.4551 - val_acc: 0.7948
Epoch 14/40
8167/8167 [==============================] - 1s - loss: 0.4457 - acc: 0.8005 - val_loss: 0.4545 - val_acc: 0.7973
Epoch 15/40
8167/8167 [==============================] - 1s - loss: 0.4446 - acc: 0.8016 - val_loss: 0.4540 - val_acc: 0.7977
Epoch 16/40
8167/8167 [==============================] - 1s - loss: 0.4434 - acc: 0.8034 - val_loss: 0.4553 - val_acc: 0.7933
Epoch 17/40
8167/8167 [==============================] - 1s - loss: 0.4441 - acc: 0.8021 - val_loss: 0.4525 - val_acc: 0.7958
Epoch 18/40
8167/8167 [==============================] - 1s - loss: 0.4417 - acc: 0.8035 - val_loss: 0.4526 - val_acc: 0.7973
Epoch 19/40
8167/8167 [==============================] - 1s - loss: 0.4417 - acc: 0.8037 - val_loss: 0.4531 - val_acc: 0.7963
Epoch 20/40
8167/8167 [==============================] - 1s - loss: 0.4411 - acc: 0.8030 - val_loss: 0.4526 - val_acc: 0.7948
Epoch 21/40
8167/8167 [==============================] - 1s - loss: 0.4393 - acc: 0.8036 - val_loss: 0.4532 - val_acc: 0.7973
Epoch 22/40
8167/8167 [==============================] - 1s - loss: 0.4394 - acc: 0.8034 - val_loss: 0.4532 - val_acc: 0.7943
Epoch 23/40
8167/8167 [==============================] - 1s - loss: 0.4389 - acc: 0.8052 - val_loss: 0.4514 - val_acc: 0.7973
Epoch 24/40
8167/8167 [==============================] - 1s - loss: 0.4377 - acc: 0.8045 - val_loss: 0.4508 - val_acc: 0.7982
Epoch 25/40
8167/8167 [==============================] - 1s - loss: 0.4372 - acc: 0.8040 - val_loss: 0.4511 - val_acc: 0.7987
Epoch 26/40
8167/8167 [==============================] - 1s - loss: 0.4369 - acc: 0.8063 - val_loss: 0.4511 - val_acc: 0.7992
Epoch 27/40
8167/8167 [==============================] - 1s - loss: 0.4362 - acc: 0.8062 - val_loss: 0.4508 - val_acc: 0.8002
Epoch 28/40
8167/8167 [==============================] - 1s - loss: 0.4353 - acc: 0.8075 - val_loss: 0.4502 - val_acc: 0.7997
Epoch 29/40
8167/8167 [==============================] - 1s - loss: 0.4349 - acc: 0.8092 - val_loss: 0.4493 - val_acc: 0.7987
Epoch 30/40
8167/8167 [==============================] - 1s - loss: 0.4347 - acc: 0.8089 - val_loss: 0.4487 - val_acc: 0.8002
Epoch 31/40
8167/8167 [==============================] - 1s - loss: 0.4338 - acc: 0.8109 - val_loss: 0.4489 - val_acc: 0.8022
Epoch 32/40
8167/8167 [==============================] - 1s - loss: 0.4328 - acc: 0.8103 - val_loss: 0.4492 - val_acc: 0.8017
Epoch 33/40
8167/8167 [==============================] - 1s - loss: 0.4323 - acc: 0.8107 - val_loss: 0.4488 - val_acc: 0.8031
Epoch 34/40
8167/8167 [==============================] - 1s - loss: 0.4318 - acc: 0.8125 - val_loss: 0.4478 - val_acc: 0.8012
Epoch 35/40
8167/8167 [==============================] - 1s - loss: 0.4314 - acc: 0.8107 - val_loss: 0.4469 - val_acc: 0.8031
Epoch 36/40
8167/8167 [==============================] - 1s - loss: 0.4309 - acc: 0.8123 - val_loss: 0.4474 - val_acc: 0.8046
Epoch 37/40
8167/8167 [==============================] - 1s - loss: 0.4305 - acc: 0.8119 - val_loss: 0.4472 - val_acc: 0.8017
Epoch 38/40
8167/8167 [==============================] - 1s - loss: 0.4302 - acc: 0.8133 - val_loss: 0.4476 - val_acc: 0.8080
Epoch 39/40
8167/8167 [==============================] - 1s - loss: 0.4306 - acc: 0.8120 - val_loss: 0.4464 - val_acc: 0.8051
Epoch 40/40
8167/8167 [==============================] - 1s - loss: 0.4309 - acc: 0.8123 - val_loss: 0.4502 - val_acc: 0.8031
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-4.h5
chunk number 4
prepare data
start training
Train on 8048 samples, validate on 2012 samples
Epoch 1/40
8048/8048 [==============================] - 2s - loss: 0.5954 - acc: 0.7289 - val_loss: 0.4724 - val_acc: 0.7744
Epoch 2/40
8048/8048 [==============================] - 2s - loss: 0.4920 - acc: 0.7710 - val_loss: 0.4452 - val_acc: 0.7947
Epoch 3/40
8048/8048 [==============================] - 2s - loss: 0.4710 - acc: 0.7834 - val_loss: 0.4316 - val_acc: 0.8082
Epoch 4/40
8048/8048 [==============================] - 2s - loss: 0.4689 - acc: 0.7921 - val_loss: 0.4350 - val_acc: 0.8216
Epoch 5/40
8048/8048 [==============================] - 2s - loss: 0.4728 - acc: 0.8011 - val_loss: 0.4356 - val_acc: 0.8196
Epoch 6/40
8048/8048 [==============================] - 2s - loss: 0.4727 - acc: 0.7991 - val_loss: 0.4335 - val_acc: 0.8141
Epoch 7/40
8048/8048 [==============================] - 2s - loss: 0.4626 - acc: 0.7997 - val_loss: 0.4390 - val_acc: 0.8131
Epoch 8/40
8048/8048 [==============================] - 2s - loss: 0.4635 - acc: 0.7992 - val_loss: 0.4433 - val_acc: 0.8141
Epoch 9/40
8048/8048 [==============================] - 2s - loss: 0.4638 - acc: 0.8000 - val_loss: 0.4482 - val_acc: 0.8126
Epoch 10/40
8048/8048 [==============================] - 2s - loss: 0.4673 - acc: 0.7978 - val_loss: 0.4399 - val_acc: 0.8141
Epoch 11/40
8048/8048 [==============================] - 2s - loss: 0.4604 - acc: 0.8011 - val_loss: 0.4316 - val_acc: 0.8156
Epoch 12/40
8048/8048 [==============================] - 2s - loss: 0.4566 - acc: 0.8018 - val_loss: 0.4268 - val_acc: 0.8151
Epoch 13/40
8048/8048 [==============================] - 2s - loss: 0.4553 - acc: 0.8028 - val_loss: 0.4226 - val_acc: 0.8196
Epoch 14/40
8048/8048 [==============================] - 2s - loss: 0.4545 - acc: 0.8027 - val_loss: 0.4235 - val_acc: 0.8201
Epoch 15/40
8048/8048 [==============================] - 2s - loss: 0.4553 - acc: 0.8001 - val_loss: 0.4202 - val_acc: 0.8186
Epoch 16/40
8048/8048 [==============================] - 2s - loss: 0.4507 - acc: 0.8044 - val_loss: 0.4213 - val_acc: 0.8156
Epoch 17/40
8048/8048 [==============================] - 2s - loss: 0.4510 - acc: 0.8024 - val_loss: 0.4266 - val_acc: 0.8101
Epoch 18/40
8048/8048 [==============================] - 2s - loss: 0.4495 - acc: 0.8017 - val_loss: 0.4323 - val_acc: 0.8091
Epoch 19/40
8048/8048 [==============================] - 2s - loss: 0.4501 - acc: 0.8028 - val_loss: 0.4233 - val_acc: 0.8101
Epoch 20/40
8048/8048 [==============================] - 2s - loss: 0.4463 - acc: 0.8043 - val_loss: 0.4202 - val_acc: 0.8082
Epoch 21/40
8048/8048 [==============================] - 2s - loss: 0.4466 - acc: 0.8042 - val_loss: 0.4177 - val_acc: 0.8151
Epoch 22/40
8048/8048 [==============================] - 2s - loss: 0.4448 - acc: 0.8073 - val_loss: 0.4211 - val_acc: 0.8141
Epoch 23/40
8048/8048 [==============================] - 2s - loss: 0.4456 - acc: 0.8044 - val_loss: 0.4188 - val_acc: 0.8161
Epoch 24/40
8048/8048 [==============================] - 2s - loss: 0.4431 - acc: 0.8072 - val_loss: 0.4219 - val_acc: 0.8116
Epoch 25/40
8048/8048 [==============================] - 2s - loss: 0.4443 - acc: 0.8064 - val_loss: 0.4238 - val_acc: 0.8166
Epoch 26/40
8048/8048 [==============================] - 2s - loss: 0.4430 - acc: 0.8065 - val_loss: 0.4225 - val_acc: 0.8166
Epoch 27/40
8048/8048 [==============================] - 2s - loss: 0.4419 - acc: 0.8082 - val_loss: 0.4187 - val_acc: 0.8126
Epoch 28/40
8048/8048 [==============================] - 2s - loss: 0.4411 - acc: 0.8065 - val_loss: 0.4177 - val_acc: 0.8106
Epoch 29/40
8048/8048 [==============================] - 2s - loss: 0.4413 - acc: 0.8044 - val_loss: 0.4178 - val_acc: 0.8151
Epoch 30/40
8048/8048 [==============================] - 2s - loss: 0.4398 - acc: 0.8079 - val_loss: 0.4176 - val_acc: 0.8201
Epoch 31/40
8048/8048 [==============================] - 2s - loss: 0.4385 - acc: 0.8098 - val_loss: 0.4166 - val_acc: 0.8196
Epoch 32/40
8048/8048 [==============================] - 2s - loss: 0.4384 - acc: 0.8086 - val_loss: 0.4165 - val_acc: 0.8186
Epoch 33/40
8048/8048 [==============================] - 2s - loss: 0.4386 - acc: 0.8094 - val_loss: 0.4159 - val_acc: 0.8211
Epoch 34/40
8048/8048 [==============================] - 2s - loss: 0.4373 - acc: 0.8099 - val_loss: 0.4150 - val_acc: 0.8186
Epoch 35/40
8048/8048 [==============================] - 2s - loss: 0.4365 - acc: 0.8096 - val_loss: 0.4142 - val_acc: 0.8166
Epoch 36/40
8048/8048 [==============================] - 2s - loss: 0.4366 - acc: 0.8095 - val_loss: 0.4147 - val_acc: 0.8156
Epoch 37/40
8048/8048 [==============================] - 2s - loss: 0.4358 - acc: 0.8096 - val_loss: 0.4164 - val_acc: 0.8186
Epoch 38/40
8048/8048 [==============================] - 2s - loss: 0.4349 - acc: 0.8095 - val_loss: 0.4170 - val_acc: 0.8171
Epoch 39/40
8048/8048 [==============================] - 2s - loss: 0.4347 - acc: 0.8100 - val_loss: 0.4161 - val_acc: 0.8181
Epoch 40/40
8048/8048 [==============================] - 2s - loss: 0.4341 - acc: 0.8108 - val_loss: 0.4151 - val_acc: 0.8216
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-5.h5
chunk number 5
prepare data
start training
Train on 8117 samples, validate on 2030 samples
Epoch 1/40
8117/8117 [==============================] - 1s - loss: 0.4767 - acc: 0.7993 - val_loss: 0.4566 - val_acc: 0.7990
Epoch 2/40
8117/8117 [==============================] - 1s - loss: 0.4564 - acc: 0.7946 - val_loss: 0.4773 - val_acc: 0.7759
Epoch 3/40
8117/8117 [==============================] - 1s - loss: 0.4732 - acc: 0.7795 - val_loss: 0.4565 - val_acc: 0.7956
Epoch 4/40
8117/8117 [==============================] - 1s - loss: 0.4535 - acc: 0.7940 - val_loss: 0.4710 - val_acc: 0.8010
Epoch 5/40
8117/8117 [==============================] - 1s - loss: 0.4677 - acc: 0.7923 - val_loss: 0.4547 - val_acc: 0.8059
Epoch 6/40
8117/8117 [==============================] - 1s - loss: 0.4528 - acc: 0.8030 - val_loss: 0.4356 - val_acc: 0.8133
Epoch 7/40
8117/8117 [==============================] - 1s - loss: 0.4370 - acc: 0.8097 - val_loss: 0.4393 - val_acc: 0.8118
Epoch 8/40
8117/8117 [==============================] - 1s - loss: 0.4433 - acc: 0.8055 - val_loss: 0.4363 - val_acc: 0.8138
Epoch 9/40
8117/8117 [==============================] - 1s - loss: 0.4400 - acc: 0.8102 - val_loss: 0.4445 - val_acc: 0.8049
Epoch 10/40
8117/8117 [==============================] - 1s - loss: 0.4455 - acc: 0.8073 - val_loss: 0.4417 - val_acc: 0.8079
Epoch 11/40
8117/8117 [==============================] - 1s - loss: 0.4425 - acc: 0.8084 - val_loss: 0.4305 - val_acc: 0.8182
Epoch 12/40
8117/8117 [==============================] - 1s - loss: 0.4337 - acc: 0.8132 - val_loss: 0.4318 - val_acc: 0.8153
Epoch 13/40
8117/8117 [==============================] - 1s - loss: 0.4359 - acc: 0.8143 - val_loss: 0.4290 - val_acc: 0.8167
Epoch 14/40
8117/8117 [==============================] - 1s - loss: 0.4334 - acc: 0.8169 - val_loss: 0.4312 - val_acc: 0.8113
Epoch 15/40
8117/8117 [==============================] - 1s - loss: 0.4351 - acc: 0.8146 - val_loss: 0.4305 - val_acc: 0.8143
Epoch 16/40
8117/8117 [==============================] - 1s - loss: 0.4348 - acc: 0.8161 - val_loss: 0.4250 - val_acc: 0.8212
Epoch 17/40
8117/8117 [==============================] - 1s - loss: 0.4300 - acc: 0.8172 - val_loss: 0.4247 - val_acc: 0.8153
Epoch 18/40
8117/8117 [==============================] - 1s - loss: 0.4298 - acc: 0.8178 - val_loss: 0.4238 - val_acc: 0.8187
Epoch 19/40
8117/8117 [==============================] - 1s - loss: 0.4269 - acc: 0.8179 - val_loss: 0.4277 - val_acc: 0.8158
Epoch 20/40
8117/8117 [==============================] - 1s - loss: 0.4274 - acc: 0.8174 - val_loss: 0.4296 - val_acc: 0.8163
Epoch 21/40
8117/8117 [==============================] - 1s - loss: 0.4276 - acc: 0.8177 - val_loss: 0.4281 - val_acc: 0.8197
Epoch 22/40
8117/8117 [==============================] - 1s - loss: 0.4262 - acc: 0.8177 - val_loss: 0.4285 - val_acc: 0.8182
Epoch 23/40
8117/8117 [==============================] - 1s - loss: 0.4269 - acc: 0.8193 - val_loss: 0.4264 - val_acc: 0.8187
Epoch 24/40
8117/8117 [==============================] - 1s - loss: 0.4244 - acc: 0.8201 - val_loss: 0.4263 - val_acc: 0.8182
Epoch 25/40
8117/8117 [==============================] - 1s - loss: 0.4240 - acc: 0.8193 - val_loss: 0.4242 - val_acc: 0.8197
Epoch 26/40
8117/8117 [==============================] - 1s - loss: 0.4226 - acc: 0.8190 - val_loss: 0.4232 - val_acc: 0.8212
Epoch 27/40
8117/8117 [==============================] - 1s - loss: 0.4226 - acc: 0.8179 - val_loss: 0.4235 - val_acc: 0.8207
Epoch 28/40
8117/8117 [==============================] - 1s - loss: 0.4230 - acc: 0.8188 - val_loss: 0.4229 - val_acc: 0.8207
Epoch 29/40
8117/8117 [==============================] - 1s - loss: 0.4214 - acc: 0.8209 - val_loss: 0.4233 - val_acc: 0.8192
Epoch 30/40
8117/8117 [==============================] - 1s - loss: 0.4205 - acc: 0.8205 - val_loss: 0.4225 - val_acc: 0.8192
Epoch 31/40
8117/8117 [==============================] - 1s - loss: 0.4189 - acc: 0.8212 - val_loss: 0.4237 - val_acc: 0.8197
Epoch 32/40
8117/8117 [==============================] - 1s - loss: 0.4198 - acc: 0.8216 - val_loss: 0.4236 - val_acc: 0.8212
Epoch 33/40
8117/8117 [==============================] - 1s - loss: 0.4194 - acc: 0.8206 - val_loss: 0.4229 - val_acc: 0.8207
Epoch 34/40
8117/8117 [==============================] - 1s - loss: 0.4186 - acc: 0.8210 - val_loss: 0.4205 - val_acc: 0.8217
Epoch 35/40
8117/8117 [==============================] - 1s - loss: 0.4171 - acc: 0.8206 - val_loss: 0.4192 - val_acc: 0.8236
Epoch 36/40
8117/8117 [==============================] - 1s - loss: 0.4171 - acc: 0.8219 - val_loss: 0.4188 - val_acc: 0.8236
Epoch 37/40
8117/8117 [==============================] - 1s - loss: 0.4170 - acc: 0.8219 - val_loss: 0.4188 - val_acc: 0.8256
Epoch 38/40
8117/8117 [==============================] - 1s - loss: 0.4166 - acc: 0.8221 - val_loss: 0.4185 - val_acc: 0.8232
Epoch 39/40
8117/8117 [==============================] - 1s - loss: 0.4156 - acc: 0.8226 - val_loss: 0.4189 - val_acc: 0.8212
Epoch 40/40
8117/8117 [==============================] - 1s - loss: 0.4151 - acc: 0.8219 - val_loss: 0.4198 - val_acc: 0.8207
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-6.h5
chunk number 6
prepare data
start training
Train on 8182 samples, validate on 2046 samples
Epoch 1/40
8182/8182 [==============================] - 1s - loss: 0.4338 - acc: 0.8146 - val_loss: 0.4288 - val_acc: 0.8167
Epoch 2/40
8182/8182 [==============================] - 1s - loss: 0.4238 - acc: 0.8141 - val_loss: 0.4369 - val_acc: 0.8128
Epoch 3/40
8182/8182 [==============================] - 1s - loss: 0.4339 - acc: 0.8082 - val_loss: 0.4278 - val_acc: 0.8182
Epoch 4/40
8182/8182 [==============================] - 1s - loss: 0.4218 - acc: 0.8151 - val_loss: 0.4356 - val_acc: 0.8236
Epoch 5/40
8182/8182 [==============================] - 1s - loss: 0.4240 - acc: 0.8181 - val_loss: 0.4356 - val_acc: 0.8231
Epoch 6/40
8182/8182 [==============================] - 1s - loss: 0.4236 - acc: 0.8183 - val_loss: 0.4271 - val_acc: 0.8216
Epoch 7/40
8182/8182 [==============================] - 1s - loss: 0.4187 - acc: 0.8174 - val_loss: 0.4314 - val_acc: 0.8192
Epoch 8/40
8182/8182 [==============================] - 1s - loss: 0.4244 - acc: 0.8156 - val_loss: 0.4242 - val_acc: 0.8206
Epoch 9/40
8182/8182 [==============================] - 1s - loss: 0.4149 - acc: 0.8208 - val_loss: 0.4333 - val_acc: 0.8221
Epoch 10/40
8182/8182 [==============================] - 1s - loss: 0.4214 - acc: 0.8175 - val_loss: 0.4239 - val_acc: 0.8260
Epoch 11/40
8182/8182 [==============================] - 1s - loss: 0.4137 - acc: 0.8214 - val_loss: 0.4241 - val_acc: 0.8211
Epoch 12/40
8182/8182 [==============================] - 1s - loss: 0.4170 - acc: 0.8195 - val_loss: 0.4230 - val_acc: 0.8206
Epoch 13/40
8182/8182 [==============================] - 1s - loss: 0.4154 - acc: 0.8201 - val_loss: 0.4238 - val_acc: 0.8260
Epoch 14/40
8182/8182 [==============================] - 1s - loss: 0.4127 - acc: 0.8211 - val_loss: 0.4270 - val_acc: 0.8240
Epoch 15/40
8182/8182 [==============================] - 1s - loss: 0.4145 - acc: 0.8195 - val_loss: 0.4217 - val_acc: 0.8240
Epoch 16/40
8182/8182 [==============================] - 1s - loss: 0.4105 - acc: 0.8228 - val_loss: 0.4241 - val_acc: 0.8245
Epoch 17/40
8182/8182 [==============================] - 1s - loss: 0.4132 - acc: 0.8219 - val_loss: 0.4226 - val_acc: 0.8240
Epoch 18/40
8182/8182 [==============================] - 1s - loss: 0.4092 - acc: 0.8230 - val_loss: 0.4273 - val_acc: 0.8255
Epoch 19/40
8182/8182 [==============================] - 1s - loss: 0.4120 - acc: 0.8201 - val_loss: 0.4222 - val_acc: 0.8260
Epoch 20/40
8182/8182 [==============================] - 1s - loss: 0.4081 - acc: 0.8223 - val_loss: 0.4223 - val_acc: 0.8236
Epoch 21/40
8182/8182 [==============================] - 1s - loss: 0.4101 - acc: 0.8228 - val_loss: 0.4208 - val_acc: 0.8250
Epoch 22/40
8182/8182 [==============================] - 1s - loss: 0.4077 - acc: 0.8240 - val_loss: 0.4239 - val_acc: 0.8280
Epoch 23/40
8182/8182 [==============================] - 1s - loss: 0.4090 - acc: 0.8203 - val_loss: 0.4212 - val_acc: 0.8280
Epoch 24/40
8182/8182 [==============================] - 1s - loss: 0.4068 - acc: 0.8238 - val_loss: 0.4211 - val_acc: 0.8275
Epoch 25/40
8182/8182 [==============================] - 1s - loss: 0.4078 - acc: 0.8252 - val_loss: 0.4209 - val_acc: 0.8270
Epoch 26/40
8182/8182 [==============================] - 1s - loss: 0.4064 - acc: 0.8252 - val_loss: 0.4232 - val_acc: 0.8236
Epoch 27/40
8182/8182 [==============================] - 1s - loss: 0.4067 - acc: 0.8213 - val_loss: 0.4215 - val_acc: 0.8250
Epoch 28/40
8182/8182 [==============================] - 1s - loss: 0.4053 - acc: 0.8236 - val_loss: 0.4208 - val_acc: 0.8236
Epoch 29/40
8182/8182 [==============================] - 1s - loss: 0.4057 - acc: 0.8264 - val_loss: 0.4205 - val_acc: 0.8270
Epoch 30/40
8182/8182 [==============================] - 1s - loss: 0.4047 - acc: 0.8258 - val_loss: 0.4222 - val_acc: 0.8280
Epoch 31/40
8182/8182 [==============================] - 1s - loss: 0.4050 - acc: 0.8223 - val_loss: 0.4206 - val_acc: 0.8265
Epoch 32/40
8182/8182 [==============================] - 1s - loss: 0.4036 - acc: 0.8249 - val_loss: 0.4204 - val_acc: 0.8260
Epoch 33/40
8182/8182 [==============================] - 1s - loss: 0.4040 - acc: 0.8271 - val_loss: 0.4203 - val_acc: 0.8260
Epoch 34/40
8182/8182 [==============================] - 1s - loss: 0.4029 - acc: 0.8267 - val_loss: 0.4214 - val_acc: 0.8260
Epoch 35/40
8182/8182 [==============================] - 1s - loss: 0.4031 - acc: 0.8250 - val_loss: 0.4196 - val_acc: 0.8250
Epoch 36/40
8182/8182 [==============================] - 1s - loss: 0.4021 - acc: 0.8267 - val_loss: 0.4193 - val_acc: 0.8240
Epoch 37/40
8182/8182 [==============================] - 1s - loss: 0.4021 - acc: 0.8269 - val_loss: 0.4199 - val_acc: 0.8289
Epoch 38/40
8182/8182 [==============================] - 1s - loss: 0.4014 - acc: 0.8273 - val_loss: 0.4201 - val_acc: 0.8280
Epoch 39/40
8182/8182 [==============================] - 1s - loss: 0.4011 - acc: 0.8272 - val_loss: 0.4193 - val_acc: 0.8236
Epoch 40/40
8182/8182 [==============================] - 1s - loss: 0.4008 - acc: 0.8274 - val_loss: 0.4194 - val_acc: 0.8255
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-7.h5
chunk number 7
prepare data
start training
Train on 8116 samples, validate on 2030 samples
Epoch 1/40
8116/8116 [==============================] - 1s - loss: 0.4506 - acc: 0.8107 - val_loss: 0.4588 - val_acc: 0.7946
Epoch 2/40
8116/8116 [==============================] - 1s - loss: 0.4665 - acc: 0.7867 - val_loss: 0.4303 - val_acc: 0.8089
Epoch 3/40
8116/8116 [==============================] - 1s - loss: 0.4438 - acc: 0.8079 - val_loss: 0.4303 - val_acc: 0.8030
Epoch 4/40
8116/8116 [==============================] - 1s - loss: 0.4411 - acc: 0.8059 - val_loss: 0.4391 - val_acc: 0.8172
Epoch 5/40
8116/8116 [==============================] - 1s - loss: 0.4406 - acc: 0.8091 - val_loss: 0.4242 - val_acc: 0.8227
Epoch 6/40
8116/8116 [==============================] - 1s - loss: 0.4271 - acc: 0.8162 - val_loss: 0.4278 - val_acc: 0.8079
Epoch 7/40
8116/8116 [==============================] - 1s - loss: 0.4333 - acc: 0.8070 - val_loss: 0.4252 - val_acc: 0.8074
Epoch 8/40
8116/8116 [==============================] - 1s - loss: 0.4287 - acc: 0.8104 - val_loss: 0.4299 - val_acc: 0.8128
Epoch 9/40
8116/8116 [==============================] - 1s - loss: 0.4285 - acc: 0.8138 - val_loss: 0.4289 - val_acc: 0.8128
Epoch 10/40
8116/8116 [==============================] - 1s - loss: 0.4260 - acc: 0.8151 - val_loss: 0.4247 - val_acc: 0.8113
Epoch 11/40
8116/8116 [==============================] - 1s - loss: 0.4238 - acc: 0.8149 - val_loss: 0.4263 - val_acc: 0.8099
Epoch 12/40
8116/8116 [==============================] - 1s - loss: 0.4256 - acc: 0.8151 - val_loss: 0.4256 - val_acc: 0.8172
Epoch 13/40
8116/8116 [==============================] - 1s - loss: 0.4220 - acc: 0.8175 - val_loss: 0.4316 - val_acc: 0.8143
Epoch 14/40
8116/8116 [==============================] - 1s - loss: 0.4258 - acc: 0.8170 - val_loss: 0.4230 - val_acc: 0.8167
Epoch 15/40
8116/8116 [==============================] - 1s - loss: 0.4201 - acc: 0.8186 - val_loss: 0.4234 - val_acc: 0.8138
Epoch 16/40
8116/8116 [==============================] - 1s - loss: 0.4229 - acc: 0.8153 - val_loss: 0.4217 - val_acc: 0.8143
Epoch 17/40
8116/8116 [==============================] - 1s - loss: 0.4197 - acc: 0.8179 - val_loss: 0.4261 - val_acc: 0.8167
Epoch 18/40
8116/8116 [==============================] - 1s - loss: 0.4214 - acc: 0.8185 - val_loss: 0.4226 - val_acc: 0.8182
Epoch 19/40
8116/8116 [==============================] - 1s - loss: 0.4185 - acc: 0.8189 - val_loss: 0.4203 - val_acc: 0.8138
Epoch 20/40
8116/8116 [==============================] - 1s - loss: 0.4181 - acc: 0.8191 - val_loss: 0.4202 - val_acc: 0.8172
Epoch 21/40
8116/8116 [==============================] - 1s - loss: 0.4174 - acc: 0.8175 - val_loss: 0.4225 - val_acc: 0.8236
Epoch 22/40
8116/8116 [==============================] - 1s - loss: 0.4167 - acc: 0.8181 - val_loss: 0.4230 - val_acc: 0.8256
Epoch 23/40
8116/8116 [==============================] - 1s - loss: 0.4166 - acc: 0.8195 - val_loss: 0.4188 - val_acc: 0.8202
Epoch 24/40
8116/8116 [==============================] - 1s - loss: 0.4153 - acc: 0.8175 - val_loss: 0.4182 - val_acc: 0.8182
Epoch 25/40
8116/8116 [==============================] - 1s - loss: 0.4151 - acc: 0.8180 - val_loss: 0.4197 - val_acc: 0.8202
Epoch 26/40
8116/8116 [==============================] - 1s - loss: 0.4141 - acc: 0.8202 - val_loss: 0.4201 - val_acc: 0.8202
Epoch 27/40
8116/8116 [==============================] - 1s - loss: 0.4140 - acc: 0.8216 - val_loss: 0.4165 - val_acc: 0.8167
Epoch 28/40
8116/8116 [==============================] - 1s - loss: 0.4132 - acc: 0.8200 - val_loss: 0.4164 - val_acc: 0.8172
Epoch 29/40
8116/8116 [==============================] - 1s - loss: 0.4127 - acc: 0.8208 - val_loss: 0.4198 - val_acc: 0.8182
Epoch 30/40
8116/8116 [==============================] - 1s - loss: 0.4129 - acc: 0.8196 - val_loss: 0.4178 - val_acc: 0.8172
Epoch 31/40
8116/8116 [==============================] - 1s - loss: 0.4116 - acc: 0.8202 - val_loss: 0.4164 - val_acc: 0.8172
Epoch 32/40
8116/8116 [==============================] - 1s - loss: 0.4118 - acc: 0.8211 - val_loss: 0.4170 - val_acc: 0.8177
Epoch 33/40
8116/8116 [==============================] - 1s - loss: 0.4105 - acc: 0.8220 - val_loss: 0.4193 - val_acc: 0.8177
Epoch 34/40
8116/8116 [==============================] - 1s - loss: 0.4111 - acc: 0.8215 - val_loss: 0.4166 - val_acc: 0.8182
Epoch 35/40
8116/8116 [==============================] - 1s - loss: 0.4101 - acc: 0.8217 - val_loss: 0.4167 - val_acc: 0.8197
Epoch 36/40
8116/8116 [==============================] - 1s - loss: 0.4095 - acc: 0.8221 - val_loss: 0.4197 - val_acc: 0.8222
Epoch 37/40
8116/8116 [==============================] - 1s - loss: 0.4097 - acc: 0.8233 - val_loss: 0.4171 - val_acc: 0.8222
Epoch 38/40
8116/8116 [==============================] - 1s - loss: 0.4086 - acc: 0.8238 - val_loss: 0.4168 - val_acc: 0.8212
Epoch 39/40
8116/8116 [==============================] - 1s - loss: 0.4084 - acc: 0.8216 - val_loss: 0.4186 - val_acc: 0.8207
Epoch 40/40
8116/8116 [==============================] - 1s - loss: 0.4083 - acc: 0.8242 - val_loss: 0.4165 - val_acc: 0.8197
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-8.h5
chunk number 8
prepare data
start training
Train on 8086 samples, validate on 2022 samples
Epoch 1/40
8086/8086 [==============================] - 2s - loss: 0.4588 - acc: 0.8056 - val_loss: 0.4566 - val_acc: 0.7868
Epoch 2/40
8086/8086 [==============================] - 2s - loss: 0.4620 - acc: 0.7976 - val_loss: 0.4313 - val_acc: 0.8101
Epoch 3/40
8086/8086 [==============================] - 2s - loss: 0.4366 - acc: 0.8060 - val_loss: 0.4297 - val_acc: 0.8061
Epoch 4/40
8086/8086 [==============================] - 2s - loss: 0.4356 - acc: 0.8029 - val_loss: 0.4352 - val_acc: 0.8076
Epoch 5/40
8086/8086 [==============================] - 2s - loss: 0.4409 - acc: 0.8159 - val_loss: 0.4197 - val_acc: 0.8170
Epoch 6/40
8086/8086 [==============================] - 2s - loss: 0.4273 - acc: 0.8136 - val_loss: 0.4261 - val_acc: 0.8131
Epoch 7/40
8086/8086 [==============================] - 2s - loss: 0.4330 - acc: 0.8070 - val_loss: 0.4189 - val_acc: 0.8170
Epoch 8/40
8086/8086 [==============================] - 2s - loss: 0.4247 - acc: 0.8160 - val_loss: 0.4271 - val_acc: 0.8160
Epoch 9/40
8086/8086 [==============================] - 2s - loss: 0.4310 - acc: 0.8167 - val_loss: 0.4199 - val_acc: 0.8190
Epoch 10/40
8086/8086 [==============================] - 2s - loss: 0.4220 - acc: 0.8156 - val_loss: 0.4246 - val_acc: 0.8180
Epoch 11/40
8086/8086 [==============================] - 2s - loss: 0.4255 - acc: 0.8150 - val_loss: 0.4238 - val_acc: 0.8150
Epoch 12/40
8086/8086 [==============================] - 2s - loss: 0.4222 - acc: 0.8178 - val_loss: 0.4273 - val_acc: 0.8091
Epoch 13/40
8086/8086 [==============================] - 2s - loss: 0.4248 - acc: 0.8139 - val_loss: 0.4202 - val_acc: 0.8190
Epoch 14/40
8086/8086 [==============================] - 2s - loss: 0.4201 - acc: 0.8178 - val_loss: 0.4190 - val_acc: 0.8215
Epoch 15/40
8086/8086 [==============================] - 2s - loss: 0.4219 - acc: 0.8172 - val_loss: 0.4159 - val_acc: 0.8220
Epoch 16/40
8086/8086 [==============================] - 2s - loss: 0.4200 - acc: 0.8189 - val_loss: 0.4172 - val_acc: 0.8180
Epoch 17/40
8086/8086 [==============================] - 2s - loss: 0.4208 - acc: 0.8181 - val_loss: 0.4151 - val_acc: 0.8190
Epoch 18/40
8086/8086 [==============================] - 2s - loss: 0.4182 - acc: 0.8198 - val_loss: 0.4167 - val_acc: 0.8200
Epoch 19/40
8086/8086 [==============================] - 2s - loss: 0.4194 - acc: 0.8215 - val_loss: 0.4178 - val_acc: 0.8160
Epoch 20/40
8086/8086 [==============================] - 2s - loss: 0.4185 - acc: 0.8186 - val_loss: 0.4166 - val_acc: 0.8175
Epoch 21/40
8086/8086 [==============================] - 2s - loss: 0.4175 - acc: 0.8202 - val_loss: 0.4152 - val_acc: 0.8195
Epoch 22/40
8086/8086 [==============================] - 2s - loss: 0.4168 - acc: 0.8204 - val_loss: 0.4148 - val_acc: 0.8175
Epoch 23/40
8086/8086 [==============================] - 2s - loss: 0.4163 - acc: 0.8198 - val_loss: 0.4159 - val_acc: 0.8145
Epoch 24/40
8086/8086 [==============================] - 2s - loss: 0.4164 - acc: 0.8209 - val_loss: 0.4159 - val_acc: 0.8155
Epoch 25/40
8086/8086 [==============================] - 2s - loss: 0.4152 - acc: 0.8194 - val_loss: 0.4163 - val_acc: 0.8160
Epoch 26/40
8086/8086 [==============================] - 2s - loss: 0.4151 - acc: 0.8213 - val_loss: 0.4167 - val_acc: 0.8150
Epoch 27/40
8086/8086 [==============================] - 2s - loss: 0.4142 - acc: 0.8206 - val_loss: 0.4171 - val_acc: 0.8121
Epoch 28/40
8086/8086 [==============================] - 2s - loss: 0.4141 - acc: 0.8207 - val_loss: 0.4153 - val_acc: 0.8145
Epoch 29/40
8086/8086 [==============================] - 2s - loss: 0.4130 - acc: 0.8214 - val_loss: 0.4137 - val_acc: 0.8160
Epoch 30/40
8086/8086 [==============================] - 2s - loss: 0.4126 - acc: 0.8215 - val_loss: 0.4131 - val_acc: 0.8165
Epoch 31/40
8086/8086 [==============================] - 2s - loss: 0.4119 - acc: 0.8218 - val_loss: 0.4135 - val_acc: 0.8170
Epoch 32/40
8086/8086 [==============================] - 2s - loss: 0.4113 - acc: 0.8219 - val_loss: 0.4139 - val_acc: 0.8175
Epoch 33/40
8086/8086 [==============================] - 2s - loss: 0.4111 - acc: 0.8218 - val_loss: 0.4141 - val_acc: 0.8170
Epoch 34/40
8086/8086 [==============================] - 2s - loss: 0.4107 - acc: 0.8230 - val_loss: 0.4136 - val_acc: 0.8210
Epoch 35/40
8086/8086 [==============================] - 2s - loss: 0.4098 - acc: 0.8243 - val_loss: 0.4124 - val_acc: 0.8195
Epoch 36/40
8086/8086 [==============================] - 2s - loss: 0.4093 - acc: 0.8240 - val_loss: 0.4122 - val_acc: 0.8225
Epoch 37/40
8086/8086 [==============================] - 2s - loss: 0.4089 - acc: 0.8241 - val_loss: 0.4120 - val_acc: 0.8220
Epoch 38/40
8086/8086 [==============================] - 2s - loss: 0.4081 - acc: 0.8254 - val_loss: 0.4120 - val_acc: 0.8185
Epoch 39/40
8086/8086 [==============================] - 2s - loss: 0.4079 - acc: 0.8249 - val_loss: 0.4129 - val_acc: 0.8229
Epoch 40/40
8086/8086 [==============================] - 2s - loss: 0.4075 - acc: 0.8254 - val_loss: 0.4124 - val_acc: 0.8170
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-9.h5
chunk number 9
prepare data
start training
Train on 8220 samples, validate on 2055 samples
Epoch 1/40
8220/8220 [==============================] - 2s - loss: 0.4183 - acc: 0.8232 - val_loss: 0.4030 - val_acc: 0.8326
Epoch 2/40
8220/8220 [==============================] - 2s - loss: 0.4294 - acc: 0.8117 - val_loss: 0.4110 - val_acc: 0.8311
Epoch 3/40
8220/8220 [==============================] - 2s - loss: 0.4293 - acc: 0.8189 - val_loss: 0.3968 - val_acc: 0.8331
Epoch 4/40
8220/8220 [==============================] - 2s - loss: 0.4156 - acc: 0.8221 - val_loss: 0.4217 - val_acc: 0.8258
Epoch 5/40
8220/8220 [==============================] - 2s - loss: 0.4444 - acc: 0.8092 - val_loss: 0.4248 - val_acc: 0.8161
Epoch 6/40
8220/8220 [==============================] - 2s - loss: 0.4387 - acc: 0.8023 - val_loss: 0.4259 - val_acc: 0.8175
Epoch 7/40
8220/8220 [==============================] - 2s - loss: 0.4396 - acc: 0.8033 - val_loss: 0.3979 - val_acc: 0.8350
Epoch 8/40
8220/8220 [==============================] - 2s - loss: 0.4175 - acc: 0.8229 - val_loss: 0.4099 - val_acc: 0.8365
Epoch 9/40
8220/8220 [==============================] - 2s - loss: 0.4300 - acc: 0.8184 - val_loss: 0.4089 - val_acc: 0.8307
Epoch 10/40
8220/8220 [==============================] - 2s - loss: 0.4243 - acc: 0.8158 - val_loss: 0.4198 - val_acc: 0.8268
Epoch 11/40
8220/8220 [==============================] - 2s - loss: 0.4338 - acc: 0.8109 - val_loss: 0.3951 - val_acc: 0.8311
Epoch 12/40
8220/8220 [==============================] - 2s - loss: 0.4131 - acc: 0.8221 - val_loss: 0.4138 - val_acc: 0.8277
Epoch 13/40
8220/8220 [==============================] - 2s - loss: 0.4332 - acc: 0.8187 - val_loss: 0.3932 - val_acc: 0.8331
Epoch 14/40
8220/8220 [==============================] - 2s - loss: 0.4115 - acc: 0.8241 - val_loss: 0.4088 - val_acc: 0.8297
Epoch 15/40
8220/8220 [==============================] - 2s - loss: 0.4244 - acc: 0.8163 - val_loss: 0.4018 - val_acc: 0.8297
Epoch 16/40
8220/8220 [==============================] - 2s - loss: 0.4183 - acc: 0.8191 - val_loss: 0.3928 - val_acc: 0.8345
Epoch 17/40
8220/8220 [==============================] - 2s - loss: 0.4130 - acc: 0.8258 - val_loss: 0.4009 - val_acc: 0.8341
Epoch 18/40
8220/8220 [==============================] - 2s - loss: 0.4209 - acc: 0.8227 - val_loss: 0.3919 - val_acc: 0.8326
Epoch 19/40
8220/8220 [==============================] - 2s - loss: 0.4098 - acc: 0.8249 - val_loss: 0.4031 - val_acc: 0.8297
Epoch 20/40
8220/8220 [==============================] - 2s - loss: 0.4200 - acc: 0.8213 - val_loss: 0.3953 - val_acc: 0.8336
Epoch 21/40
8220/8220 [==============================] - 2s - loss: 0.4118 - acc: 0.8264 - val_loss: 0.3954 - val_acc: 0.8365
Epoch 22/40
8220/8220 [==============================] - 2s - loss: 0.4133 - acc: 0.8231 - val_loss: 0.3913 - val_acc: 0.8375
Epoch 23/40
8220/8220 [==============================] - 2s - loss: 0.4092 - acc: 0.8282 - val_loss: 0.3936 - val_acc: 0.8341
Epoch 24/40
8220/8220 [==============================] - 2s - loss: 0.4113 - acc: 0.8232 - val_loss: 0.3950 - val_acc: 0.8302
Epoch 25/40
8220/8220 [==============================] - 2s - loss: 0.4121 - acc: 0.8236 - val_loss: 0.3891 - val_acc: 0.8384
Epoch 26/40
8220/8220 [==============================] - 2s - loss: 0.4069 - acc: 0.8286 - val_loss: 0.3899 - val_acc: 0.8399
Epoch 27/40
8220/8220 [==============================] - 2s - loss: 0.4092 - acc: 0.8236 - val_loss: 0.3902 - val_acc: 0.8389
Epoch 28/40
8220/8220 [==============================] - 2s - loss: 0.4065 - acc: 0.8254 - val_loss: 0.3942 - val_acc: 0.8394
Epoch 29/40
8220/8220 [==============================] - 2s - loss: 0.4080 - acc: 0.8276 - val_loss: 0.3928 - val_acc: 0.8380
Epoch 30/40
8220/8220 [==============================] - 2s - loss: 0.4052 - acc: 0.8290 - val_loss: 0.3927 - val_acc: 0.8375
Epoch 31/40
8220/8220 [==============================] - 2s - loss: 0.4057 - acc: 0.8285 - val_loss: 0.3909 - val_acc: 0.8389
Epoch 32/40
8220/8220 [==============================] - 2s - loss: 0.4040 - acc: 0.8292 - val_loss: 0.3918 - val_acc: 0.8345
Epoch 33/40
8220/8220 [==============================] - 2s - loss: 0.4045 - acc: 0.8274 - val_loss: 0.3905 - val_acc: 0.8360
Epoch 34/40
8220/8220 [==============================] - 2s - loss: 0.4039 - acc: 0.8275 - val_loss: 0.3885 - val_acc: 0.8384
Epoch 35/40
8220/8220 [==============================] - 2s - loss: 0.4033 - acc: 0.8292 - val_loss: 0.3890 - val_acc: 0.8380
Epoch 36/40
8220/8220 [==============================] - 2s - loss: 0.4027 - acc: 0.8291 - val_loss: 0.3908 - val_acc: 0.8389
Epoch 37/40
8220/8220 [==============================] - 2s - loss: 0.4027 - acc: 0.8286 - val_loss: 0.3899 - val_acc: 0.8404
Epoch 38/40
8220/8220 [==============================] - 2s - loss: 0.4008 - acc: 0.8298 - val_loss: 0.3908 - val_acc: 0.8389
Epoch 39/40
8220/8220 [==============================] - 2s - loss: 0.4015 - acc: 0.8287 - val_loss: 0.3897 - val_acc: 0.8404
Epoch 40/40
8220/8220 [==============================] - 2s - loss: 0.4007 - acc: 0.8296 - val_loss: 0.3889 - val_acc: 0.8380
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-10.h5
chunk number 10
prepare data
start training
Train on 8132 samples, validate on 2033 samples
Epoch 1/40
8132/8132 [==============================] - 1s - loss: 0.4635 - acc: 0.8109 - val_loss: 0.4610 - val_acc: 0.7954
Epoch 2/40
8132/8132 [==============================] - 1s - loss: 0.4325 - acc: 0.8067 - val_loss: 0.4874 - val_acc: 0.7757
Epoch 3/40
8132/8132 [==============================] - 1s - loss: 0.4619 - acc: 0.7864 - val_loss: 0.4773 - val_acc: 0.7860
Epoch 4/40
8132/8132 [==============================] - 1s - loss: 0.4492 - acc: 0.7999 - val_loss: 0.4579 - val_acc: 0.8072
Epoch 5/40
8132/8132 [==============================] - 1s - loss: 0.4235 - acc: 0.8218 - val_loss: 0.4782 - val_acc: 0.7988
Epoch 6/40
8132/8132 [==============================] - 1s - loss: 0.4392 - acc: 0.8153 - val_loss: 0.4592 - val_acc: 0.8082
Epoch 7/40
8132/8132 [==============================] - 1s - loss: 0.4220 - acc: 0.8202 - val_loss: 0.4458 - val_acc: 0.8131
Epoch 8/40
8132/8132 [==============================] - 1s - loss: 0.4125 - acc: 0.8260 - val_loss: 0.4553 - val_acc: 0.8101
Epoch 9/40
8132/8132 [==============================] - 1s - loss: 0.4232 - acc: 0.8222 - val_loss: 0.4548 - val_acc: 0.8116
Epoch 10/40
8132/8132 [==============================] - 1s - loss: 0.4236 - acc: 0.8195 - val_loss: 0.4485 - val_acc: 0.8111
Epoch 11/40
8132/8132 [==============================] - 1s - loss: 0.4180 - acc: 0.8234 - val_loss: 0.4480 - val_acc: 0.8042
Epoch 12/40
8132/8132 [==============================] - 1s - loss: 0.4178 - acc: 0.8214 - val_loss: 0.4409 - val_acc: 0.8096
Epoch 13/40
8132/8132 [==============================] - 1s - loss: 0.4114 - acc: 0.8246 - val_loss: 0.4367 - val_acc: 0.8210
Epoch 14/40
8132/8132 [==============================] - 1s - loss: 0.4080 - acc: 0.8278 - val_loss: 0.4410 - val_acc: 0.8146
Epoch 15/40
8132/8132 [==============================] - 1s - loss: 0.4120 - acc: 0.8262 - val_loss: 0.4441 - val_acc: 0.8141
Epoch 16/40
8132/8132 [==============================] - 1s - loss: 0.4142 - acc: 0.8258 - val_loss: 0.4426 - val_acc: 0.8141
Epoch 17/40
8132/8132 [==============================] - 1s - loss: 0.4119 - acc: 0.8261 - val_loss: 0.4393 - val_acc: 0.8131
Epoch 18/40
8132/8132 [==============================] - 1s - loss: 0.4080 - acc: 0.8267 - val_loss: 0.4372 - val_acc: 0.8195
Epoch 19/40
8132/8132 [==============================] - 1s - loss: 0.4049 - acc: 0.8264 - val_loss: 0.4365 - val_acc: 0.8190
Epoch 20/40
8132/8132 [==============================] - 1s - loss: 0.4036 - acc: 0.8283 - val_loss: 0.4387 - val_acc: 0.8141
Epoch 21/40
8132/8132 [==============================] - 1s - loss: 0.4052 - acc: 0.8286 - val_loss: 0.4415 - val_acc: 0.8121
Epoch 22/40
8132/8132 [==============================] - 1s - loss: 0.4072 - acc: 0.8285 - val_loss: 0.4404 - val_acc: 0.8141
Epoch 23/40
8132/8132 [==============================] - 1s - loss: 0.4056 - acc: 0.8285 - val_loss: 0.4373 - val_acc: 0.8180
Epoch 24/40
8132/8132 [==============================] - 1s - loss: 0.4020 - acc: 0.8308 - val_loss: 0.4355 - val_acc: 0.8200
Epoch 25/40
8132/8132 [==============================] - 1s - loss: 0.4000 - acc: 0.8289 - val_loss: 0.4340 - val_acc: 0.8200
Epoch 26/40
8132/8132 [==============================] - 1s - loss: 0.3992 - acc: 0.8291 - val_loss: 0.4333 - val_acc: 0.8180
Epoch 27/40
8132/8132 [==============================] - 1s - loss: 0.3998 - acc: 0.8318 - val_loss: 0.4331 - val_acc: 0.8155
Epoch 28/40
8132/8132 [==============================] - 1s - loss: 0.4004 - acc: 0.8304 - val_loss: 0.4320 - val_acc: 0.8180
Epoch 29/40
8132/8132 [==============================] - 1s - loss: 0.3997 - acc: 0.8303 - val_loss: 0.4311 - val_acc: 0.8175
Epoch 30/40
8132/8132 [==============================] - 1s - loss: 0.3985 - acc: 0.8308 - val_loss: 0.4309 - val_acc: 0.8175
Epoch 31/40
8132/8132 [==============================] - 1s - loss: 0.3977 - acc: 0.8313 - val_loss: 0.4308 - val_acc: 0.8180
Epoch 32/40
8132/8132 [==============================] - 1s - loss: 0.3968 - acc: 0.8317 - val_loss: 0.4317 - val_acc: 0.8195
Epoch 33/40
8132/8132 [==============================] - 1s - loss: 0.3967 - acc: 0.8318 - val_loss: 0.4329 - val_acc: 0.8185
Epoch 34/40
8132/8132 [==============================] - 1s - loss: 0.3965 - acc: 0.8330 - val_loss: 0.4335 - val_acc: 0.8224
Epoch 35/40
8132/8132 [==============================] - 1s - loss: 0.3958 - acc: 0.8334 - val_loss: 0.4333 - val_acc: 0.8200
Epoch 36/40
8132/8132 [==============================] - 1s - loss: 0.3948 - acc: 0.8348 - val_loss: 0.4326 - val_acc: 0.8180
Epoch 37/40
8132/8132 [==============================] - 1s - loss: 0.3940 - acc: 0.8342 - val_loss: 0.4326 - val_acc: 0.8195
Epoch 38/40
8132/8132 [==============================] - 1s - loss: 0.3942 - acc: 0.8331 - val_loss: 0.4326 - val_acc: 0.8180
Epoch 39/40
8132/8132 [==============================] - 1s - loss: 0.3942 - acc: 0.8340 - val_loss: 0.4324 - val_acc: 0.8214
Epoch 40/40
8132/8132 [==============================] - 1s - loss: 0.3934 - acc: 0.8347 - val_loss: 0.4320 - val_acc: 0.8205
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-11.h5
chunk number 11
prepare data
start training
Train on 8186 samples, validate on 2047 samples
Epoch 1/40
8186/8186 [==============================] - 1s - loss: 0.4266 - acc: 0.8186 - val_loss: 0.4066 - val_acc: 0.8266
Epoch 2/40
8186/8186 [==============================] - 1s - loss: 0.4218 - acc: 0.8169 - val_loss: 0.4068 - val_acc: 0.8256
Epoch 3/40
8186/8186 [==============================] - 1s - loss: 0.4223 - acc: 0.8152 - val_loss: 0.4048 - val_acc: 0.8241
Epoch 4/40
8186/8186 [==============================] - 1s - loss: 0.4174 - acc: 0.8196 - val_loss: 0.4070 - val_acc: 0.8207
Epoch 5/40
8186/8186 [==============================] - 1s - loss: 0.4183 - acc: 0.8191 - val_loss: 0.4044 - val_acc: 0.8256
Epoch 6/40
8186/8186 [==============================] - 1s - loss: 0.4151 - acc: 0.8229 - val_loss: 0.4074 - val_acc: 0.8246
Epoch 7/40
8186/8186 [==============================] - 1s - loss: 0.4176 - acc: 0.8229 - val_loss: 0.4040 - val_acc: 0.8251
Epoch 8/40
8186/8186 [==============================] - 1s - loss: 0.4136 - acc: 0.8224 - val_loss: 0.4056 - val_acc: 0.8261
Epoch 9/40
8186/8186 [==============================] - 1s - loss: 0.4151 - acc: 0.8198 - val_loss: 0.4023 - val_acc: 0.8256
Epoch 10/40
8186/8186 [==============================] - 1s - loss: 0.4118 - acc: 0.8220 - val_loss: 0.4031 - val_acc: 0.8256
Epoch 11/40
8186/8186 [==============================] - 1s - loss: 0.4128 - acc: 0.8237 - val_loss: 0.4007 - val_acc: 0.8271
Epoch 12/40
8186/8186 [==============================] - 1s - loss: 0.4104 - acc: 0.8229 - val_loss: 0.4009 - val_acc: 0.8266
Epoch 13/40
8186/8186 [==============================] - 1s - loss: 0.4106 - acc: 0.8210 - val_loss: 0.4008 - val_acc: 0.8261
Epoch 14/40
8186/8186 [==============================] - 1s - loss: 0.4100 - acc: 0.8209 - val_loss: 0.4003 - val_acc: 0.8276
Epoch 15/40
8186/8186 [==============================] - 1s - loss: 0.4089 - acc: 0.8224 - val_loss: 0.4009 - val_acc: 0.8271
Epoch 16/40
8186/8186 [==============================] - 1s - loss: 0.4090 - acc: 0.8232 - val_loss: 0.4006 - val_acc: 0.8280
Epoch 17/40
8186/8186 [==============================] - 1s - loss: 0.4076 - acc: 0.8236 - val_loss: 0.4009 - val_acc: 0.8266
Epoch 18/40
8186/8186 [==============================] - 1s - loss: 0.4075 - acc: 0.8245 - val_loss: 0.3994 - val_acc: 0.8251
Epoch 19/40
8186/8186 [==============================] - 1s - loss: 0.4060 - acc: 0.8258 - val_loss: 0.3992 - val_acc: 0.8271
Epoch 20/40
8186/8186 [==============================] - 1s - loss: 0.4065 - acc: 0.8268 - val_loss: 0.3985 - val_acc: 0.8271
Epoch 21/40
8186/8186 [==============================] - 1s - loss: 0.4056 - acc: 0.8256 - val_loss: 0.3991 - val_acc: 0.8295
Epoch 22/40
8186/8186 [==============================] - 1s - loss: 0.4060 - acc: 0.8265 - val_loss: 0.3981 - val_acc: 0.8251
Epoch 23/40
8186/8186 [==============================] - 1s - loss: 0.4049 - acc: 0.8274 - val_loss: 0.3987 - val_acc: 0.8271
Epoch 24/40
8186/8186 [==============================] - 1s - loss: 0.4049 - acc: 0.8270 - val_loss: 0.3988 - val_acc: 0.8241
Epoch 25/40
8186/8186 [==============================] - 1s - loss: 0.4038 - acc: 0.8270 - val_loss: 0.3995 - val_acc: 0.8256
Epoch 26/40
8186/8186 [==============================] - 1s - loss: 0.4040 - acc: 0.8265 - val_loss: 0.3989 - val_acc: 0.8266
Epoch 27/40
8186/8186 [==============================] - 1s - loss: 0.4031 - acc: 0.8259 - val_loss: 0.3986 - val_acc: 0.8276
Epoch 28/40
8186/8186 [==============================] - 1s - loss: 0.4032 - acc: 0.8276 - val_loss: 0.3977 - val_acc: 0.8285
Epoch 29/40
8186/8186 [==============================] - 1s - loss: 0.4026 - acc: 0.8265 - val_loss: 0.3975 - val_acc: 0.8266
Epoch 30/40
8186/8186 [==============================] - 1s - loss: 0.4026 - acc: 0.8269 - val_loss: 0.3969 - val_acc: 0.8271
Epoch 31/40
8186/8186 [==============================] - 1s - loss: 0.4020 - acc: 0.8280 - val_loss: 0.3971 - val_acc: 0.8280
Epoch 32/40
8186/8186 [==============================] - 1s - loss: 0.4018 - acc: 0.8276 - val_loss: 0.3971 - val_acc: 0.8261
Epoch 33/40
8186/8186 [==============================] - 1s - loss: 0.4011 - acc: 0.8279 - val_loss: 0.3975 - val_acc: 0.8266
Epoch 34/40
8186/8186 [==============================] - 1s - loss: 0.4009 - acc: 0.8289 - val_loss: 0.3975 - val_acc: 0.8256
Epoch 35/40
8186/8186 [==============================] - 1s - loss: 0.4005 - acc: 0.8285 - val_loss: 0.3974 - val_acc: 0.8266
Epoch 36/40
8186/8186 [==============================] - 1s - loss: 0.4003 - acc: 0.8293 - val_loss: 0.3970 - val_acc: 0.8251
Epoch 37/40
8186/8186 [==============================] - 1s - loss: 0.3998 - acc: 0.8300 - val_loss: 0.3968 - val_acc: 0.8251
Epoch 38/40
8186/8186 [==============================] - 1s - loss: 0.3996 - acc: 0.8289 - val_loss: 0.3966 - val_acc: 0.8251
Epoch 39/40
8186/8186 [==============================] - 1s - loss: 0.3993 - acc: 0.8302 - val_loss: 0.3968 - val_acc: 0.8251
Epoch 40/40
8186/8186 [==============================] - 1s - loss: 0.3989 - acc: 0.8296 - val_loss: 0.3971 - val_acc: 0.8241
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-12.h5
chunk number 12
prepare data
start training
Train on 8205 samples, validate on 2052 samples
Epoch 1/40
8205/8205 [==============================] - 1s - loss: 0.4135 - acc: 0.8238 - val_loss: 0.4087 - val_acc: 0.8275
Epoch 2/40
8205/8205 [==============================] - 1s - loss: 0.4125 - acc: 0.8247 - val_loss: 0.4090 - val_acc: 0.8294
Epoch 3/40
8205/8205 [==============================] - 1s - loss: 0.4114 - acc: 0.8238 - val_loss: 0.4083 - val_acc: 0.8299
Epoch 4/40
8205/8205 [==============================] - 1s - loss: 0.4103 - acc: 0.8243 - val_loss: 0.4082 - val_acc: 0.8343
Epoch 5/40
8205/8205 [==============================] - 1s - loss: 0.4097 - acc: 0.8247 - val_loss: 0.4087 - val_acc: 0.8328
Epoch 6/40
8205/8205 [==============================] - 1s - loss: 0.4089 - acc: 0.8238 - val_loss: 0.4091 - val_acc: 0.8338
Epoch 7/40
8205/8205 [==============================] - 1s - loss: 0.4083 - acc: 0.8255 - val_loss: 0.4087 - val_acc: 0.8338
Epoch 8/40
8205/8205 [==============================] - 1s - loss: 0.4077 - acc: 0.8260 - val_loss: 0.4087 - val_acc: 0.8348
Epoch 9/40
8205/8205 [==============================] - 1s - loss: 0.4073 - acc: 0.8268 - val_loss: 0.4092 - val_acc: 0.8367
Epoch 10/40
8205/8205 [==============================] - 1s - loss: 0.4065 - acc: 0.8284 - val_loss: 0.4093 - val_acc: 0.8348
Epoch 11/40
8205/8205 [==============================] - 1s - loss: 0.4058 - acc: 0.8288 - val_loss: 0.4083 - val_acc: 0.8333
Epoch 12/40
8205/8205 [==============================] - 1s - loss: 0.4048 - acc: 0.8291 - val_loss: 0.4077 - val_acc: 0.8324
Epoch 13/40
8205/8205 [==============================] - 1s - loss: 0.4041 - acc: 0.8284 - val_loss: 0.4079 - val_acc: 0.8319
Epoch 14/40
8205/8205 [==============================] - 1s - loss: 0.4036 - acc: 0.8297 - val_loss: 0.4080 - val_acc: 0.8319
Epoch 15/40
8205/8205 [==============================] - 1s - loss: 0.4032 - acc: 0.8299 - val_loss: 0.4076 - val_acc: 0.8304
Epoch 16/40
8205/8205 [==============================] - 1s - loss: 0.4028 - acc: 0.8302 - val_loss: 0.4082 - val_acc: 0.8285
Epoch 17/40
8205/8205 [==============================] - 1s - loss: 0.4023 - acc: 0.8310 - val_loss: 0.4091 - val_acc: 0.8265
Epoch 18/40
8205/8205 [==============================] - 1s - loss: 0.4019 - acc: 0.8311 - val_loss: 0.4088 - val_acc: 0.8275
Epoch 19/40
8205/8205 [==============================] - 1s - loss: 0.4013 - acc: 0.8321 - val_loss: 0.4086 - val_acc: 0.8304
Epoch 20/40
8205/8205 [==============================] - 1s - loss: 0.4008 - acc: 0.8322 - val_loss: 0.4090 - val_acc: 0.8289
Epoch 21/40
8205/8205 [==============================] - 1s - loss: 0.4003 - acc: 0.8328 - val_loss: 0.4089 - val_acc: 0.8314
Epoch 22/40
8205/8205 [==============================] - 1s - loss: 0.3999 - acc: 0.8329 - val_loss: 0.4086 - val_acc: 0.8324
Epoch 23/40
8205/8205 [==============================] - 1s - loss: 0.3996 - acc: 0.8328 - val_loss: 0.4089 - val_acc: 0.8289
Epoch 24/40
8205/8205 [==============================] - 1s - loss: 0.3991 - acc: 0.8332 - val_loss: 0.4090 - val_acc: 0.8299
Epoch 25/40
8205/8205 [==============================] - 1s - loss: 0.3987 - acc: 0.8333 - val_loss: 0.4086 - val_acc: 0.8289
Epoch 26/40
8205/8205 [==============================] - 1s - loss: 0.3983 - acc: 0.8327 - val_loss: 0.4086 - val_acc: 0.8275
Epoch 27/40
8205/8205 [==============================] - 1s - loss: 0.3979 - acc: 0.8335 - val_loss: 0.4089 - val_acc: 0.8280
Epoch 28/40
8205/8205 [==============================] - 1s - loss: 0.3974 - acc: 0.8340 - val_loss: 0.4085 - val_acc: 0.8299
Epoch 29/40
8205/8205 [==============================] - 1s - loss: 0.3968 - acc: 0.8344 - val_loss: 0.4087 - val_acc: 0.8294
Epoch 30/40
8205/8205 [==============================] - 1s - loss: 0.3964 - acc: 0.8344 - val_loss: 0.4092 - val_acc: 0.8275
Epoch 31/40
8205/8205 [==============================] - 1s - loss: 0.3961 - acc: 0.8340 - val_loss: 0.4089 - val_acc: 0.8289
Epoch 32/40
8205/8205 [==============================] - 1s - loss: 0.3957 - acc: 0.8346 - val_loss: 0.4092 - val_acc: 0.8265
Epoch 33/40
8205/8205 [==============================] - 1s - loss: 0.3953 - acc: 0.8350 - val_loss: 0.4095 - val_acc: 0.8275
Epoch 34/40
8205/8205 [==============================] - 1s - loss: 0.3949 - acc: 0.8351 - val_loss: 0.4091 - val_acc: 0.8265
Epoch 35/40
8205/8205 [==============================] - 1s - loss: 0.3945 - acc: 0.8352 - val_loss: 0.4093 - val_acc: 0.8270
Epoch 36/40
8205/8205 [==============================] - 1s - loss: 0.3941 - acc: 0.8353 - val_loss: 0.4092 - val_acc: 0.8246
Epoch 37/40
8205/8205 [==============================] - 1s - loss: 0.3938 - acc: 0.8346 - val_loss: 0.4089 - val_acc: 0.8246
Epoch 38/40
8205/8205 [==============================] - 1s - loss: 0.3935 - acc: 0.8356 - val_loss: 0.4094 - val_acc: 0.8246
Epoch 39/40
8205/8205 [==============================] - 1s - loss: 0.3931 - acc: 0.8350 - val_loss: 0.4091 - val_acc: 0.8260
Epoch 40/40
8205/8205 [==============================] - 1s - loss: 0.3928 - acc: 0.8364 - val_loss: 0.4095 - val_acc: 0.8265
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-13.h5
chunk number 13
prepare data
start training
Train on 8166 samples, validate on 2042 samples
Epoch 1/40
8166/8166 [==============================] - 1s - loss: 0.3975 - acc: 0.8337 - val_loss: 0.4112 - val_acc: 0.8266
Epoch 2/40
8166/8166 [==============================] - 1s - loss: 0.3986 - acc: 0.8320 - val_loss: 0.4057 - val_acc: 0.8310
Epoch 3/40
8166/8166 [==============================] - 1s - loss: 0.3933 - acc: 0.8370 - val_loss: 0.4102 - val_acc: 0.8296
Epoch 4/40
8166/8166 [==============================] - 1s - loss: 0.3977 - acc: 0.8352 - val_loss: 0.4067 - val_acc: 0.8330
Epoch 5/40
8166/8166 [==============================] - 1s - loss: 0.3930 - acc: 0.8379 - val_loss: 0.4083 - val_acc: 0.8350
Epoch 6/40
8166/8166 [==============================] - 1s - loss: 0.3933 - acc: 0.8371 - val_loss: 0.4105 - val_acc: 0.8252
Epoch 7/40
8166/8166 [==============================] - 1s - loss: 0.3936 - acc: 0.8354 - val_loss: 0.4078 - val_acc: 0.8320
Epoch 8/40
8166/8166 [==============================] - 1s - loss: 0.3903 - acc: 0.8398 - val_loss: 0.4086 - val_acc: 0.8330
Epoch 9/40
8166/8166 [==============================] - 1s - loss: 0.3913 - acc: 0.8373 - val_loss: 0.4081 - val_acc: 0.8276
Epoch 10/40
8166/8166 [==============================] - 1s - loss: 0.3894 - acc: 0.8385 - val_loss: 0.4088 - val_acc: 0.8286
Epoch 11/40
8166/8166 [==============================] - 1s - loss: 0.3894 - acc: 0.8391 - val_loss: 0.4091 - val_acc: 0.8315
Epoch 12/40
8166/8166 [==============================] - 1s - loss: 0.3896 - acc: 0.8377 - val_loss: 0.4087 - val_acc: 0.8320
Epoch 13/40
8166/8166 [==============================] - 1s - loss: 0.3880 - acc: 0.8392 - val_loss: 0.4105 - val_acc: 0.8271
Epoch 14/40
8166/8166 [==============================] - 1s - loss: 0.3888 - acc: 0.8391 - val_loss: 0.4091 - val_acc: 0.8320
Epoch 15/40
8166/8166 [==============================] - 1s - loss: 0.3872 - acc: 0.8382 - val_loss: 0.4091 - val_acc: 0.8310
Epoch 16/40
8166/8166 [==============================] - 1s - loss: 0.3867 - acc: 0.8397 - val_loss: 0.4105 - val_acc: 0.8286
Epoch 17/40
8166/8166 [==============================] - 1s - loss: 0.3872 - acc: 0.8413 - val_loss: 0.4089 - val_acc: 0.8325
Epoch 18/40
8166/8166 [==============================] - 1s - loss: 0.3858 - acc: 0.8413 - val_loss: 0.4086 - val_acc: 0.8320
Epoch 19/40
8166/8166 [==============================] - 1s - loss: 0.3851 - acc: 0.8423 - val_loss: 0.4098 - val_acc: 0.8310
Epoch 20/40
8166/8166 [==============================] - 1s - loss: 0.3854 - acc: 0.8419 - val_loss: 0.4084 - val_acc: 0.8330
Epoch 21/40
8166/8166 [==============================] - 1s - loss: 0.3846 - acc: 0.8412 - val_loss: 0.4079 - val_acc: 0.8340
Epoch 22/40
8166/8166 [==============================] - 1s - loss: 0.3835 - acc: 0.8425 - val_loss: 0.4087 - val_acc: 0.8325
Epoch 23/40
8166/8166 [==============================] - 1s - loss: 0.3837 - acc: 0.8431 - val_loss: 0.4082 - val_acc: 0.8306
Epoch 24/40
8166/8166 [==============================] - 1s - loss: 0.3838 - acc: 0.8423 - val_loss: 0.4082 - val_acc: 0.8330
Epoch 25/40
8166/8166 [==============================] - 1s - loss: 0.3824 - acc: 0.8451 - val_loss: 0.4076 - val_acc: 0.8340
Epoch 26/40
8166/8166 [==============================] - 1s - loss: 0.3816 - acc: 0.8440 - val_loss: 0.4078 - val_acc: 0.8335
Epoch 27/40
8166/8166 [==============================] - 1s - loss: 0.3816 - acc: 0.8433 - val_loss: 0.4093 - val_acc: 0.8330
Epoch 28/40
8166/8166 [==============================] - 1s - loss: 0.3819 - acc: 0.8453 - val_loss: 0.4079 - val_acc: 0.8310
Epoch 29/40
8166/8166 [==============================] - 1s - loss: 0.3820 - acc: 0.8420 - val_loss: 0.4085 - val_acc: 0.8320
Epoch 30/40
8166/8166 [==============================] - 1s - loss: 0.3810 - acc: 0.8434 - val_loss: 0.4070 - val_acc: 0.8359
Epoch 31/40
8166/8166 [==============================] - 1s - loss: 0.3804 - acc: 0.8446 - val_loss: 0.4077 - val_acc: 0.8350
Epoch 32/40
8166/8166 [==============================] - 1s - loss: 0.3794 - acc: 0.8462 - val_loss: 0.4077 - val_acc: 0.8355
Epoch 33/40
8166/8166 [==============================] - 1s - loss: 0.3787 - acc: 0.8467 - val_loss: 0.4077 - val_acc: 0.8330
Epoch 34/40
8166/8166 [==============================] - 1s - loss: 0.3785 - acc: 0.8459 - val_loss: 0.4094 - val_acc: 0.8330
Epoch 35/40
8166/8166 [==============================] - 1s - loss: 0.3791 - acc: 0.8456 - val_loss: 0.4099 - val_acc: 0.8306
Epoch 36/40
8166/8166 [==============================] - 1s - loss: 0.3813 - acc: 0.8434 - val_loss: 0.4147 - val_acc: 0.8281
Epoch 37/40
8166/8166 [==============================] - 1s - loss: 0.3835 - acc: 0.8413 - val_loss: 0.4168 - val_acc: 0.8310
Epoch 38/40
8166/8166 [==============================] - 1s - loss: 0.3914 - acc: 0.8331 - val_loss: 0.4117 - val_acc: 0.8315
Epoch 39/40
8166/8166 [==============================] - 1s - loss: 0.3806 - acc: 0.8435 - val_loss: 0.4075 - val_acc: 0.8345
Epoch 40/40
8166/8166 [==============================] - 1s - loss: 0.3774 - acc: 0.8453 - val_loss: 0.4098 - val_acc: 0.8315
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-14.h5
chunk number 14
prepare data
start training
Train on 8130 samples, validate on 2033 samples
Epoch 1/40
8130/8130 [==============================] - 1s - loss: 0.4081 - acc: 0.8285 - val_loss: 0.4100 - val_acc: 0.8239
Epoch 2/40
8130/8130 [==============================] - 1s - loss: 0.4048 - acc: 0.8300 - val_loss: 0.4120 - val_acc: 0.8259
Epoch 3/40
8130/8130 [==============================] - 1s - loss: 0.4057 - acc: 0.8279 - val_loss: 0.4115 - val_acc: 0.8259
Epoch 4/40
8130/8130 [==============================] - 1s - loss: 0.4050 - acc: 0.8312 - val_loss: 0.4100 - val_acc: 0.8288
Epoch 5/40
8130/8130 [==============================] - 1s - loss: 0.4019 - acc: 0.8316 - val_loss: 0.4131 - val_acc: 0.8288
Epoch 6/40
8130/8130 [==============================] - 1s - loss: 0.4036 - acc: 0.8310 - val_loss: 0.4096 - val_acc: 0.8283
Epoch 7/40
8130/8130 [==============================] - 1s - loss: 0.4002 - acc: 0.8314 - val_loss: 0.4095 - val_acc: 0.8293
Epoch 8/40
8130/8130 [==============================] - 1s - loss: 0.4006 - acc: 0.8312 - val_loss: 0.4087 - val_acc: 0.8293
Epoch 9/40
8130/8130 [==============================] - 1s - loss: 0.3991 - acc: 0.8342 - val_loss: 0.4077 - val_acc: 0.8278
Epoch 10/40
8130/8130 [==============================] - 1s - loss: 0.3982 - acc: 0.8343 - val_loss: 0.4078 - val_acc: 0.8278
Epoch 11/40
8130/8130 [==============================] - 1s - loss: 0.3985 - acc: 0.8332 - val_loss: 0.4065 - val_acc: 0.8269
Epoch 12/40
8130/8130 [==============================] - 1s - loss: 0.3965 - acc: 0.8348 - val_loss: 0.4081 - val_acc: 0.8288
Epoch 13/40
8130/8130 [==============================] - 1s - loss: 0.3974 - acc: 0.8354 - val_loss: 0.4070 - val_acc: 0.8264
Epoch 14/40
8130/8130 [==============================] - 1s - loss: 0.3958 - acc: 0.8358 - val_loss: 0.4068 - val_acc: 0.8278
Epoch 15/40
8130/8130 [==============================] - 1s - loss: 0.3953 - acc: 0.8360 - val_loss: 0.4068 - val_acc: 0.8283
Epoch 16/40
8130/8130 [==============================] - 1s - loss: 0.3951 - acc: 0.8355 - val_loss: 0.4055 - val_acc: 0.8269
Epoch 17/40
8130/8130 [==============================] - 1s - loss: 0.3934 - acc: 0.8383 - val_loss: 0.4059 - val_acc: 0.8278
Epoch 18/40
8130/8130 [==============================] - 1s - loss: 0.3936 - acc: 0.8365 - val_loss: 0.4059 - val_acc: 0.8298
Epoch 19/40
8130/8130 [==============================] - 1s - loss: 0.3929 - acc: 0.8376 - val_loss: 0.4058 - val_acc: 0.8293
Epoch 20/40
8130/8130 [==============================] - 1s - loss: 0.3921 - acc: 0.8379 - val_loss: 0.4064 - val_acc: 0.8273
Epoch 21/40
8130/8130 [==============================] - 1s - loss: 0.3921 - acc: 0.8386 - val_loss: 0.4061 - val_acc: 0.8303
Epoch 22/40
8130/8130 [==============================] - 1s - loss: 0.3913 - acc: 0.8376 - val_loss: 0.4059 - val_acc: 0.8308
Epoch 23/40
8130/8130 [==============================] - 1s - loss: 0.3908 - acc: 0.8380 - val_loss: 0.4059 - val_acc: 0.8288
Epoch 24/40
8130/8130 [==============================] - 1s - loss: 0.3907 - acc: 0.8391 - val_loss: 0.4055 - val_acc: 0.8333
Epoch 25/40
8130/8130 [==============================] - 1s - loss: 0.3900 - acc: 0.8396 - val_loss: 0.4055 - val_acc: 0.8328
Epoch 26/40
8130/8130 [==============================] - 1s - loss: 0.3895 - acc: 0.8397 - val_loss: 0.4057 - val_acc: 0.8273
Epoch 27/40
8130/8130 [==============================] - 1s - loss: 0.3893 - acc: 0.8411 - val_loss: 0.4057 - val_acc: 0.8323
Epoch 28/40
8130/8130 [==============================] - 1s - loss: 0.3888 - acc: 0.8392 - val_loss: 0.4055 - val_acc: 0.8313
Epoch 29/40
8130/8130 [==============================] - 1s - loss: 0.3882 - acc: 0.8400 - val_loss: 0.4054 - val_acc: 0.8278
Epoch 30/40
8130/8130 [==============================] - 1s - loss: 0.3880 - acc: 0.8408 - val_loss: 0.4055 - val_acc: 0.8328
Epoch 31/40
8130/8130 [==============================] - 1s - loss: 0.3877 - acc: 0.8413 - val_loss: 0.4051 - val_acc: 0.8283
Epoch 32/40
8130/8130 [==============================] - 1s - loss: 0.3871 - acc: 0.8412 - val_loss: 0.4053 - val_acc: 0.8293
Epoch 33/40
8130/8130 [==============================] - 1s - loss: 0.3867 - acc: 0.8415 - val_loss: 0.4056 - val_acc: 0.8328
Epoch 34/40
8130/8130 [==============================] - 1s - loss: 0.3865 - acc: 0.8413 - val_loss: 0.4054 - val_acc: 0.8293
Epoch 35/40
8130/8130 [==============================] - 1s - loss: 0.3863 - acc: 0.8415 - val_loss: 0.4055 - val_acc: 0.8333
Epoch 36/40
8130/8130 [==============================] - 1s - loss: 0.3862 - acc: 0.8412 - val_loss: 0.4050 - val_acc: 0.8303
Epoch 37/40
8130/8130 [==============================] - 1s - loss: 0.3859 - acc: 0.8424 - val_loss: 0.4050 - val_acc: 0.8323
Epoch 38/40
8130/8130 [==============================] - 1s - loss: 0.3855 - acc: 0.8415 - val_loss: 0.4047 - val_acc: 0.8308
Epoch 39/40
8130/8130 [==============================] - 1s - loss: 0.3850 - acc: 0.8432 - val_loss: 0.4044 - val_acc: 0.8328
Epoch 40/40
8130/8130 [==============================] - 1s - loss: 0.3845 - acc: 0.8429 - val_loss: 0.4043 - val_acc: 0.8318
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-15.h5
chunk number 15
prepare data
start training
Train on 8086 samples, validate on 2022 samples
Epoch 1/40
8086/8086 [==============================] - 1s - loss: 0.3983 - acc: 0.8335 - val_loss: 0.3848 - val_acc: 0.8408
Epoch 2/40
8086/8086 [==============================] - 1s - loss: 0.3980 - acc: 0.8337 - val_loss: 0.3826 - val_acc: 0.8442
Epoch 3/40
8086/8086 [==============================] - 1s - loss: 0.3958 - acc: 0.8366 - val_loss: 0.3825 - val_acc: 0.8437
Epoch 4/40
8086/8086 [==============================] - 1s - loss: 0.3945 - acc: 0.8371 - val_loss: 0.3838 - val_acc: 0.8442
Epoch 5/40
8086/8086 [==============================] - 1s - loss: 0.3942 - acc: 0.8386 - val_loss: 0.3831 - val_acc: 0.8388
Epoch 6/40
8086/8086 [==============================] - 1s - loss: 0.3933 - acc: 0.8379 - val_loss: 0.3825 - val_acc: 0.8427
Epoch 7/40
8086/8086 [==============================] - 1s - loss: 0.3920 - acc: 0.8415 - val_loss: 0.3823 - val_acc: 0.8393
Epoch 8/40
8086/8086 [==============================] - 1s - loss: 0.3909 - acc: 0.8403 - val_loss: 0.3825 - val_acc: 0.8417
Epoch 9/40
8086/8086 [==============================] - 1s - loss: 0.3907 - acc: 0.8389 - val_loss: 0.3823 - val_acc: 0.8427
Epoch 10/40
8086/8086 [==============================] - 1s - loss: 0.3902 - acc: 0.8412 - val_loss: 0.3808 - val_acc: 0.8437
Epoch 11/40
8086/8086 [==============================] - 1s - loss: 0.3883 - acc: 0.8411 - val_loss: 0.3800 - val_acc: 0.8452
Epoch 12/40
8086/8086 [==============================] - 1s - loss: 0.3871 - acc: 0.8432 - val_loss: 0.3801 - val_acc: 0.8422
Epoch 13/40
8086/8086 [==============================] - 1s - loss: 0.3872 - acc: 0.8422 - val_loss: 0.3813 - val_acc: 0.8417
Epoch 14/40
8086/8086 [==============================] - 1s - loss: 0.3875 - acc: 0.8436 - val_loss: 0.3822 - val_acc: 0.8417
Epoch 15/40
8086/8086 [==============================] - 1s - loss: 0.3889 - acc: 0.8394 - val_loss: 0.3837 - val_acc: 0.8422
Epoch 16/40
8086/8086 [==============================] - 1s - loss: 0.3889 - acc: 0.8444 - val_loss: 0.3905 - val_acc: 0.8378
Epoch 17/40
8086/8086 [==============================] - 1s - loss: 0.3960 - acc: 0.8366 - val_loss: 0.3826 - val_acc: 0.8412
Epoch 18/40
8086/8086 [==============================] - 1s - loss: 0.3863 - acc: 0.8428 - val_loss: 0.3812 - val_acc: 0.8417
Epoch 19/40
8086/8086 [==============================] - 1s - loss: 0.3846 - acc: 0.8444 - val_loss: 0.3835 - val_acc: 0.8467
Epoch 20/40
8086/8086 [==============================] - 1s - loss: 0.3874 - acc: 0.8410 - val_loss: 0.3834 - val_acc: 0.8422
Epoch 21/40
8086/8086 [==============================] - 1s - loss: 0.3861 - acc: 0.8428 - val_loss: 0.3841 - val_acc: 0.8457
Epoch 22/40
8086/8086 [==============================] - 1s - loss: 0.3869 - acc: 0.8417 - val_loss: 0.3817 - val_acc: 0.8412
Epoch 23/40
8086/8086 [==============================] - 1s - loss: 0.3839 - acc: 0.8431 - val_loss: 0.3819 - val_acc: 0.8457
Epoch 24/40
8086/8086 [==============================] - 1s - loss: 0.3829 - acc: 0.8442 - val_loss: 0.3827 - val_acc: 0.8408
Epoch 25/40
8086/8086 [==============================] - 1s - loss: 0.3819 - acc: 0.8455 - val_loss: 0.3819 - val_acc: 0.8462
Epoch 26/40
8086/8086 [==============================] - 1s - loss: 0.3818 - acc: 0.8432 - val_loss: 0.3825 - val_acc: 0.8417
Epoch 27/40
8086/8086 [==============================] - 1s - loss: 0.3819 - acc: 0.8454 - val_loss: 0.3868 - val_acc: 0.8457
Epoch 28/40
8086/8086 [==============================] - 1s - loss: 0.3873 - acc: 0.8408 - val_loss: 0.3914 - val_acc: 0.8358
Epoch 29/40
8086/8086 [==============================] - 1s - loss: 0.3915 - acc: 0.8390 - val_loss: 0.4150 - val_acc: 0.8269
Epoch 30/40
8086/8086 [==============================] - 1s - loss: 0.4182 - acc: 0.8249 - val_loss: 0.3831 - val_acc: 0.8492
Epoch 31/40
8086/8086 [==============================] - 1s - loss: 0.3853 - acc: 0.8406 - val_loss: 0.4463 - val_acc: 0.7928
Epoch 32/40
8086/8086 [==============================] - 1s - loss: 0.4488 - acc: 0.7962 - val_loss: 0.4366 - val_acc: 0.8210
Epoch 33/40
8086/8086 [==============================] - 1s - loss: 0.4473 - acc: 0.8120 - val_loss: 0.4683 - val_acc: 0.8037
Epoch 34/40
8086/8086 [==============================] - 1s - loss: 0.4806 - acc: 0.7943 - val_loss: 0.4378 - val_acc: 0.8111
Epoch 35/40
8086/8086 [==============================] - 1s - loss: 0.4513 - acc: 0.8048 - val_loss: 0.4215 - val_acc: 0.8210
Epoch 36/40
8086/8086 [==============================] - 1s - loss: 0.4363 - acc: 0.8175 - val_loss: 0.4401 - val_acc: 0.8136
Epoch 37/40
8086/8086 [==============================] - 1s - loss: 0.4572 - acc: 0.8119 - val_loss: 0.4358 - val_acc: 0.8155
Epoch 38/40
8086/8086 [==============================] - 1s - loss: 0.4511 - acc: 0.8141 - val_loss: 0.4099 - val_acc: 0.8229
Epoch 39/40
8086/8086 [==============================] - 1s - loss: 0.4240 - acc: 0.8229 - val_loss: 0.4080 - val_acc: 0.8249
Epoch 40/40
8086/8086 [==============================] - 1s - loss: 0.4194 - acc: 0.8222 - val_loss: 0.4135 - val_acc: 0.8225
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-16.h5
chunk number 16
prepare data
start training
Train on 8113 samples, validate on 2029 samples
Epoch 1/40
8113/8113 [==============================] - 1s - loss: 0.4785 - acc: 0.7960 - val_loss: 0.4129 - val_acc: 0.8295
Epoch 2/40
8113/8113 [==============================] - 1s - loss: 0.4447 - acc: 0.8125 - val_loss: 0.3934 - val_acc: 0.8314
Epoch 3/40
8113/8113 [==============================] - 1s - loss: 0.4226 - acc: 0.8150 - val_loss: 0.4425 - val_acc: 0.7945
Epoch 4/40
8113/8113 [==============================] - 1s - loss: 0.4691 - acc: 0.7796 - val_loss: 0.3873 - val_acc: 0.8457
Epoch 5/40
8113/8113 [==============================] - 1s - loss: 0.4135 - acc: 0.8216 - val_loss: 0.4180 - val_acc: 0.8295
Epoch 6/40
8113/8113 [==============================] - 1s - loss: 0.4482 - acc: 0.8086 - val_loss: 0.4211 - val_acc: 0.8280
Epoch 7/40
8113/8113 [==============================] - 1s - loss: 0.4498 - acc: 0.8061 - val_loss: 0.4054 - val_acc: 0.8379
Epoch 8/40
8113/8113 [==============================] - 1s - loss: 0.4274 - acc: 0.8165 - val_loss: 0.4149 - val_acc: 0.8324
Epoch 9/40
8113/8113 [==============================] - 1s - loss: 0.4306 - acc: 0.8101 - val_loss: 0.4183 - val_acc: 0.8310
Epoch 10/40
8113/8113 [==============================] - 1s - loss: 0.4347 - acc: 0.8103 - val_loss: 0.3958 - val_acc: 0.8482
Epoch 11/40
8113/8113 [==============================] - 1s - loss: 0.4165 - acc: 0.8245 - val_loss: 0.3954 - val_acc: 0.8433
Epoch 12/40
8113/8113 [==============================] - 1s - loss: 0.4201 - acc: 0.8212 - val_loss: 0.4033 - val_acc: 0.8369
Epoch 13/40
8113/8113 [==============================] - 1s - loss: 0.4278 - acc: 0.8191 - val_loss: 0.4038 - val_acc: 0.8354
Epoch 14/40
8113/8113 [==============================] - 1s - loss: 0.4279 - acc: 0.8173 - val_loss: 0.3981 - val_acc: 0.8423
Epoch 15/40
8113/8113 [==============================] - 1s - loss: 0.4230 - acc: 0.8197 - val_loss: 0.3935 - val_acc: 0.8462
Epoch 16/40
8113/8113 [==============================] - 1s - loss: 0.4196 - acc: 0.8193 - val_loss: 0.3916 - val_acc: 0.8482
Epoch 17/40
8113/8113 [==============================] - 1s - loss: 0.4162 - acc: 0.8186 - val_loss: 0.3890 - val_acc: 0.8438
Epoch 18/40
8113/8113 [==============================] - 1s - loss: 0.4118 - acc: 0.8216 - val_loss: 0.3905 - val_acc: 0.8462
Epoch 19/40
8113/8113 [==============================] - 1s - loss: 0.4123 - acc: 0.8218 - val_loss: 0.3933 - val_acc: 0.8443
Epoch 20/40
8113/8113 [==============================] - 1s - loss: 0.4153 - acc: 0.8226 - val_loss: 0.3936 - val_acc: 0.8423
Epoch 21/40
8113/8113 [==============================] - 1s - loss: 0.4156 - acc: 0.8219 - val_loss: 0.3905 - val_acc: 0.8457
Epoch 22/40
8113/8113 [==============================] - 1s - loss: 0.4126 - acc: 0.8223 - val_loss: 0.3864 - val_acc: 0.8492
Epoch 23/40
8113/8113 [==============================] - 1s - loss: 0.4093 - acc: 0.8226 - val_loss: 0.3829 - val_acc: 0.8512
Epoch 24/40
8113/8113 [==============================] - 1s - loss: 0.4083 - acc: 0.8234 - val_loss: 0.3808 - val_acc: 0.8526
Epoch 25/40
8113/8113 [==============================] - 1s - loss: 0.4085 - acc: 0.8265 - val_loss: 0.3793 - val_acc: 0.8531
Epoch 26/40
8113/8113 [==============================] - 1s - loss: 0.4085 - acc: 0.8260 - val_loss: 0.3784 - val_acc: 0.8517
Epoch 27/40
8113/8113 [==============================] - 1s - loss: 0.4078 - acc: 0.8262 - val_loss: 0.3771 - val_acc: 0.8521
Epoch 28/40
8113/8113 [==============================] - 1s - loss: 0.4056 - acc: 0.8261 - val_loss: 0.3764 - val_acc: 0.8517
Epoch 29/40
8113/8113 [==============================] - 1s - loss: 0.4038 - acc: 0.8269 - val_loss: 0.3795 - val_acc: 0.8507
Epoch 30/40
8113/8113 [==============================] - 1s - loss: 0.4047 - acc: 0.8278 - val_loss: 0.3813 - val_acc: 0.8512
Epoch 31/40
8113/8113 [==============================] - 1s - loss: 0.4046 - acc: 0.8272 - val_loss: 0.3808 - val_acc: 0.8482
Epoch 32/40
8113/8113 [==============================] - 1s - loss: 0.4039 - acc: 0.8279 - val_loss: 0.3794 - val_acc: 0.8502
Epoch 33/40
8113/8113 [==============================] - 1s - loss: 0.4028 - acc: 0.8279 - val_loss: 0.3776 - val_acc: 0.8507
Epoch 34/40
8113/8113 [==============================] - 1s - loss: 0.4009 - acc: 0.8273 - val_loss: 0.3788 - val_acc: 0.8536
Epoch 35/40
8113/8113 [==============================] - 1s - loss: 0.4018 - acc: 0.8271 - val_loss: 0.3786 - val_acc: 0.8531
Epoch 36/40
8113/8113 [==============================] - 1s - loss: 0.4019 - acc: 0.8283 - val_loss: 0.3771 - val_acc: 0.8521
Epoch 37/40
8113/8113 [==============================] - 1s - loss: 0.4011 - acc: 0.8283 - val_loss: 0.3765 - val_acc: 0.8502
Epoch 38/40
8113/8113 [==============================] - 1s - loss: 0.4004 - acc: 0.8287 - val_loss: 0.3761 - val_acc: 0.8526
Epoch 39/40
8113/8113 [==============================] - 1s - loss: 0.3988 - acc: 0.8294 - val_loss: 0.3778 - val_acc: 0.8546
Epoch 40/40
8113/8113 [==============================] - 1s - loss: 0.3994 - acc: 0.8278 - val_loss: 0.3774 - val_acc: 0.8521
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-17.h5
chunk number 17
prepare data
start training
Train on 8134 samples, validate on 2034 samples
Epoch 1/40
8134/8134 [==============================] - 1s - loss: 0.4736 - acc: 0.7971 - val_loss: 0.4134 - val_acc: 0.8191
Epoch 2/40
8134/8134 [==============================] - 1s - loss: 0.4053 - acc: 0.8299 - val_loss: 0.4578 - val_acc: 0.7979
Epoch 3/40
8134/8134 [==============================] - 1s - loss: 0.4429 - acc: 0.8074 - val_loss: 0.4831 - val_acc: 0.7906
Epoch 4/40
8134/8134 [==============================] - 1s - loss: 0.4674 - acc: 0.7958 - val_loss: 0.4651 - val_acc: 0.7950
Epoch 5/40
8134/8134 [==============================] - 1s - loss: 0.4502 - acc: 0.8039 - val_loss: 0.4253 - val_acc: 0.8181
Epoch 6/40
8134/8134 [==============================] - 1s - loss: 0.4131 - acc: 0.8239 - val_loss: 0.4156 - val_acc: 0.8210
Epoch 7/40
8134/8134 [==============================] - 1s - loss: 0.4115 - acc: 0.8311 - val_loss: 0.4481 - val_acc: 0.8161
Epoch 8/40
8134/8134 [==============================] - 1s - loss: 0.4484 - acc: 0.8108 - val_loss: 0.4170 - val_acc: 0.8225
Epoch 9/40
8134/8134 [==============================] - 1s - loss: 0.4125 - acc: 0.8301 - val_loss: 0.4168 - val_acc: 0.8176
Epoch 10/40
8134/8134 [==============================] - 1s - loss: 0.4056 - acc: 0.8295 - val_loss: 0.4361 - val_acc: 0.8127
Epoch 11/40
8134/8134 [==============================] - 1s - loss: 0.4227 - acc: 0.8198 - val_loss: 0.4414 - val_acc: 0.8083
Epoch 12/40
8134/8134 [==============================] - 1s - loss: 0.4275 - acc: 0.8169 - val_loss: 0.4280 - val_acc: 0.8142
Epoch 13/40
8134/8134 [==============================] - 1s - loss: 0.4145 - acc: 0.8237 - val_loss: 0.4108 - val_acc: 0.8181
Epoch 14/40
8134/8134 [==============================] - 1s - loss: 0.4002 - acc: 0.8318 - val_loss: 0.4160 - val_acc: 0.8220
Epoch 15/40
8134/8134 [==============================] - 1s - loss: 0.4111 - acc: 0.8263 - val_loss: 0.4190 - val_acc: 0.8206
Epoch 16/40
8134/8134 [==============================] - 1s - loss: 0.4147 - acc: 0.8228 - val_loss: 0.4079 - val_acc: 0.8265
Epoch 17/40
8134/8134 [==============================] - 1s - loss: 0.3977 - acc: 0.8338 - val_loss: 0.4173 - val_acc: 0.8176
Epoch 18/40
8134/8134 [==============================] - 1s - loss: 0.4027 - acc: 0.8308 - val_loss: 0.4258 - val_acc: 0.8191
Epoch 19/40
8134/8134 [==============================] - 1s - loss: 0.4096 - acc: 0.8273 - val_loss: 0.4209 - val_acc: 0.8191
Epoch 20/40
8134/8134 [==============================] - 1s - loss: 0.4047 - acc: 0.8299 - val_loss: 0.4101 - val_acc: 0.8210
Epoch 21/40
8134/8134 [==============================] - 1s - loss: 0.3961 - acc: 0.8344 - val_loss: 0.4108 - val_acc: 0.8176
Epoch 22/40
8134/8134 [==============================] - 1s - loss: 0.4013 - acc: 0.8307 - val_loss: 0.4103 - val_acc: 0.8161
Epoch 23/40
8134/8134 [==============================] - 1s - loss: 0.4007 - acc: 0.8306 - val_loss: 0.4071 - val_acc: 0.8235
Epoch 24/40
8134/8134 [==============================] - 1s - loss: 0.3930 - acc: 0.8360 - val_loss: 0.4144 - val_acc: 0.8156
Epoch 25/40
8134/8134 [==============================] - 1s - loss: 0.3976 - acc: 0.8348 - val_loss: 0.4160 - val_acc: 0.8156
Epoch 26/40
8134/8134 [==============================] - 1s - loss: 0.3990 - acc: 0.8344 - val_loss: 0.4084 - val_acc: 0.8181
Epoch 27/40
8134/8134 [==============================] - 1s - loss: 0.3932 - acc: 0.8360 - val_loss: 0.4046 - val_acc: 0.8240
Epoch 28/40
8134/8134 [==============================] - 1s - loss: 0.3927 - acc: 0.8351 - val_loss: 0.4054 - val_acc: 0.8230
Epoch 29/40
8134/8134 [==============================] - 1s - loss: 0.3946 - acc: 0.8344 - val_loss: 0.4037 - val_acc: 0.8250
Epoch 30/40
8134/8134 [==============================] - 1s - loss: 0.3897 - acc: 0.8393 - val_loss: 0.4088 - val_acc: 0.8206
Epoch 31/40
8134/8134 [==============================] - 1s - loss: 0.3917 - acc: 0.8362 - val_loss: 0.4106 - val_acc: 0.8171
Epoch 32/40
8134/8134 [==============================] - 1s - loss: 0.3925 - acc: 0.8356 - val_loss: 0.4059 - val_acc: 0.8230
Epoch 33/40
8134/8134 [==============================] - 1s - loss: 0.3887 - acc: 0.8383 - val_loss: 0.4043 - val_acc: 0.8255
Epoch 34/40
8134/8134 [==============================] - 1s - loss: 0.3894 - acc: 0.8375 - val_loss: 0.4039 - val_acc: 0.8240
Epoch 35/40
8134/8134 [==============================] - 1s - loss: 0.3892 - acc: 0.8381 - val_loss: 0.4037 - val_acc: 0.8206
Epoch 36/40
8134/8134 [==============================] - 1s - loss: 0.3869 - acc: 0.8387 - val_loss: 0.4068 - val_acc: 0.8171
Epoch 37/40
8134/8134 [==============================] - 1s - loss: 0.3885 - acc: 0.8382 - val_loss: 0.4046 - val_acc: 0.8181
Epoch 38/40
8134/8134 [==============================] - 1s - loss: 0.3870 - acc: 0.8389 - val_loss: 0.4010 - val_acc: 0.8230
Epoch 39/40
8134/8134 [==============================] - 1s - loss: 0.3856 - acc: 0.8407 - val_loss: 0.4013 - val_acc: 0.8230
Epoch 40/40
8134/8134 [==============================] - 1s - loss: 0.3865 - acc: 0.8409 - val_loss: 0.4021 - val_acc: 0.8240
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-18.h5
chunk number 18
prepare data
start training
Train on 8221 samples, validate on 2056 samples
Epoch 1/40
8221/8221 [==============================] - 1s - loss: 0.4099 - acc: 0.8285 - val_loss: 0.3811 - val_acc: 0.8390
Epoch 2/40
8221/8221 [==============================] - 1s - loss: 0.4094 - acc: 0.8298 - val_loss: 0.3801 - val_acc: 0.8395
Epoch 3/40
8221/8221 [==============================] - 1s - loss: 0.4081 - acc: 0.8274 - val_loss: 0.3804 - val_acc: 0.8385
Epoch 4/40
8221/8221 [==============================] - 1s - loss: 0.4078 - acc: 0.8290 - val_loss: 0.3795 - val_acc: 0.8395
Epoch 5/40
8221/8221 [==============================] - 1s - loss: 0.4062 - acc: 0.8306 - val_loss: 0.3803 - val_acc: 0.8366
Epoch 6/40
8221/8221 [==============================] - 1s - loss: 0.4061 - acc: 0.8296 - val_loss: 0.3796 - val_acc: 0.8371
Epoch 7/40
8221/8221 [==============================] - 1s - loss: 0.4051 - acc: 0.8285 - val_loss: 0.3793 - val_acc: 0.8375
Epoch 8/40
8221/8221 [==============================] - 1s - loss: 0.4045 - acc: 0.8293 - val_loss: 0.3791 - val_acc: 0.8405
Epoch 9/40
8221/8221 [==============================] - 1s - loss: 0.4037 - acc: 0.8312 - val_loss: 0.3791 - val_acc: 0.8405
Epoch 10/40
8221/8221 [==============================] - 1s - loss: 0.4029 - acc: 0.8299 - val_loss: 0.3795 - val_acc: 0.8385
Epoch 11/40
8221/8221 [==============================] - 1s - loss: 0.4026 - acc: 0.8318 - val_loss: 0.3792 - val_acc: 0.8395
Epoch 12/40
8221/8221 [==============================] - 1s - loss: 0.4019 - acc: 0.8323 - val_loss: 0.3791 - val_acc: 0.8414
Epoch 13/40
8221/8221 [==============================] - 1s - loss: 0.4016 - acc: 0.8320 - val_loss: 0.3788 - val_acc: 0.8400
Epoch 14/40
8221/8221 [==============================] - 1s - loss: 0.4008 - acc: 0.8330 - val_loss: 0.3789 - val_acc: 0.8385
Epoch 15/40
8221/8221 [==============================] - 1s - loss: 0.4002 - acc: 0.8330 - val_loss: 0.3788 - val_acc: 0.8405
Epoch 16/40
8221/8221 [==============================] - 1s - loss: 0.3998 - acc: 0.8325 - val_loss: 0.3786 - val_acc: 0.8400
Epoch 17/40
8221/8221 [==============================] - 1s - loss: 0.3994 - acc: 0.8314 - val_loss: 0.3785 - val_acc: 0.8410
Epoch 18/40
8221/8221 [==============================] - 1s - loss: 0.3990 - acc: 0.8329 - val_loss: 0.3782 - val_acc: 0.8419
Epoch 19/40
8221/8221 [==============================] - 1s - loss: 0.3984 - acc: 0.8326 - val_loss: 0.3780 - val_acc: 0.8414
Epoch 20/40
8221/8221 [==============================] - 1s - loss: 0.3980 - acc: 0.8341 - val_loss: 0.3777 - val_acc: 0.8410
Epoch 21/40
8221/8221 [==============================] - 1s - loss: 0.3977 - acc: 0.8327 - val_loss: 0.3778 - val_acc: 0.8419
Epoch 22/40
8221/8221 [==============================] - 1s - loss: 0.3975 - acc: 0.8325 - val_loss: 0.3780 - val_acc: 0.8405
Epoch 23/40
8221/8221 [==============================] - 1s - loss: 0.3971 - acc: 0.8336 - val_loss: 0.3783 - val_acc: 0.8390
Epoch 24/40
8221/8221 [==============================] - 1s - loss: 0.3968 - acc: 0.8335 - val_loss: 0.3785 - val_acc: 0.8390
Epoch 25/40
8221/8221 [==============================] - 1s - loss: 0.3964 - acc: 0.8346 - val_loss: 0.3786 - val_acc: 0.8400
Epoch 26/40
8221/8221 [==============================] - 1s - loss: 0.3962 - acc: 0.8351 - val_loss: 0.3787 - val_acc: 0.8410
Epoch 27/40
8221/8221 [==============================] - 1s - loss: 0.3958 - acc: 0.8354 - val_loss: 0.3786 - val_acc: 0.8390
Epoch 28/40
8221/8221 [==============================] - 1s - loss: 0.3955 - acc: 0.8354 - val_loss: 0.3782 - val_acc: 0.8390
Epoch 29/40
8221/8221 [==============================] - 1s - loss: 0.3951 - acc: 0.8354 - val_loss: 0.3780 - val_acc: 0.8400
Epoch 30/40
8221/8221 [==============================] - 1s - loss: 0.3948 - acc: 0.8357 - val_loss: 0.3780 - val_acc: 0.8400
Epoch 31/40
8221/8221 [==============================] - 1s - loss: 0.3945 - acc: 0.8353 - val_loss: 0.3782 - val_acc: 0.8395
Epoch 32/40
8221/8221 [==============================] - 1s - loss: 0.3941 - acc: 0.8351 - val_loss: 0.3785 - val_acc: 0.8385
Epoch 33/40
8221/8221 [==============================] - 1s - loss: 0.3938 - acc: 0.8360 - val_loss: 0.3786 - val_acc: 0.8400
Epoch 34/40
8221/8221 [==============================] - 1s - loss: 0.3935 - acc: 0.8371 - val_loss: 0.3788 - val_acc: 0.8410
Epoch 35/40
8221/8221 [==============================] - 1s - loss: 0.3932 - acc: 0.8366 - val_loss: 0.3789 - val_acc: 0.8414
Epoch 36/40
8221/8221 [==============================] - 1s - loss: 0.3929 - acc: 0.8366 - val_loss: 0.3787 - val_acc: 0.8414
Epoch 37/40
8221/8221 [==============================] - 1s - loss: 0.3926 - acc: 0.8374 - val_loss: 0.3785 - val_acc: 0.8419
Epoch 38/40
8221/8221 [==============================] - 1s - loss: 0.3923 - acc: 0.8376 - val_loss: 0.3784 - val_acc: 0.8414
Epoch 39/40
8221/8221 [==============================] - 1s - loss: 0.3920 - acc: 0.8386 - val_loss: 0.3784 - val_acc: 0.8410
Epoch 40/40
8221/8221 [==============================] - 1s - loss: 0.3917 - acc: 0.8388 - val_loss: 0.3782 - val_acc: 0.8414
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-19.h5
chunk number 19
prepare data
start training
Train on 8180 samples, validate on 2046 samples
Epoch 1/40
8180/8180 [==============================] - 2s - loss: 0.4385 - acc: 0.8227 - val_loss: 0.4542 - val_acc: 0.7996
Epoch 2/40
8180/8180 [==============================] - 2s - loss: 0.4612 - acc: 0.7927 - val_loss: 0.4411 - val_acc: 0.8182
Epoch 3/40
8180/8180 [==============================] - 2s - loss: 0.4820 - acc: 0.7985 - val_loss: 0.4466 - val_acc: 0.8167
Epoch 4/40
8180/8180 [==============================] - 2s - loss: 0.4847 - acc: 0.7972 - val_loss: 0.4095 - val_acc: 0.8333
Epoch 5/40
8180/8180 [==============================] - 2s - loss: 0.4432 - acc: 0.8153 - val_loss: 0.4396 - val_acc: 0.8138
Epoch 6/40
8180/8180 [==============================] - 2s - loss: 0.4579 - acc: 0.8010 - val_loss: 0.4180 - val_acc: 0.8412
Epoch 7/40
8180/8180 [==============================] - 2s - loss: 0.4326 - acc: 0.8176 - val_loss: 0.4060 - val_acc: 0.8460
Epoch 8/40
8180/8180 [==============================] - 2s - loss: 0.4226 - acc: 0.8193 - val_loss: 0.4181 - val_acc: 0.8250
Epoch 9/40
8180/8180 [==============================] - 2s - loss: 0.4343 - acc: 0.8131 - val_loss: 0.4264 - val_acc: 0.8260
Epoch 10/40
8180/8180 [==============================] - 2s - loss: 0.4396 - acc: 0.8116 - val_loss: 0.4311 - val_acc: 0.8275
Epoch 11/40
8180/8180 [==============================] - 2s - loss: 0.4402 - acc: 0.8137 - val_loss: 0.4360 - val_acc: 0.8314
Epoch 12/40
8180/8180 [==============================] - 2s - loss: 0.4424 - acc: 0.8121 - val_loss: 0.4324 - val_acc: 0.8333
Epoch 13/40
8180/8180 [==============================] - 2s - loss: 0.4390 - acc: 0.8149 - val_loss: 0.4198 - val_acc: 0.8358
Epoch 14/40
8180/8180 [==============================] - 2s - loss: 0.4297 - acc: 0.8171 - val_loss: 0.4093 - val_acc: 0.8372
Epoch 15/40
8180/8180 [==============================] - 2s - loss: 0.4240 - acc: 0.8189 - val_loss: 0.4038 - val_acc: 0.8407
Epoch 16/40
8180/8180 [==============================] - 2s - loss: 0.4214 - acc: 0.8194 - val_loss: 0.3996 - val_acc: 0.8412
Epoch 17/40
8180/8180 [==============================] - 2s - loss: 0.4183 - acc: 0.8191 - val_loss: 0.3986 - val_acc: 0.8426
Epoch 18/40
8180/8180 [==============================] - 2s - loss: 0.4174 - acc: 0.8197 - val_loss: 0.4012 - val_acc: 0.8416
Epoch 19/40
8180/8180 [==============================] - 2s - loss: 0.4205 - acc: 0.8171 - val_loss: 0.3982 - val_acc: 0.8421
Epoch 20/40
8180/8180 [==============================] - 2s - loss: 0.4187 - acc: 0.8198 - val_loss: 0.3926 - val_acc: 0.8421
Epoch 21/40
8180/8180 [==============================] - 2s - loss: 0.4156 - acc: 0.8231 - val_loss: 0.3911 - val_acc: 0.8456
Epoch 22/40
8180/8180 [==============================] - 2s - loss: 0.4149 - acc: 0.8235 - val_loss: 0.3912 - val_acc: 0.8460
Epoch 23/40
8180/8180 [==============================] - 2s - loss: 0.4122 - acc: 0.8248 - val_loss: 0.3968 - val_acc: 0.8412
Epoch 24/40
8180/8180 [==============================] - 2s - loss: 0.4117 - acc: 0.8253 - val_loss: 0.4024 - val_acc: 0.8382
Epoch 25/40
8180/8180 [==============================] - 2s - loss: 0.4141 - acc: 0.8244 - val_loss: 0.3974 - val_acc: 0.8416
Epoch 26/40
8180/8180 [==============================] - 2s - loss: 0.4117 - acc: 0.8270 - val_loss: 0.3950 - val_acc: 0.8441
Epoch 27/40
8180/8180 [==============================] - 2s - loss: 0.4121 - acc: 0.8275 - val_loss: 0.3943 - val_acc: 0.8431
Epoch 28/40
8180/8180 [==============================] - 2s - loss: 0.4108 - acc: 0.8273 - val_loss: 0.3955 - val_acc: 0.8431
Epoch 29/40
8180/8180 [==============================] - 2s - loss: 0.4084 - acc: 0.8264 - val_loss: 0.3980 - val_acc: 0.8392
Epoch 30/40
8180/8180 [==============================] - 2s - loss: 0.4095 - acc: 0.8273 - val_loss: 0.3959 - val_acc: 0.8441
Epoch 31/40
8180/8180 [==============================] - 2s - loss: 0.4087 - acc: 0.8278 - val_loss: 0.3933 - val_acc: 0.8456
Epoch 32/40
8180/8180 [==============================] - 2s - loss: 0.4085 - acc: 0.8275 - val_loss: 0.3930 - val_acc: 0.8451
Epoch 33/40
8180/8180 [==============================] - 2s - loss: 0.4088 - acc: 0.8269 - val_loss: 0.3932 - val_acc: 0.8441
Epoch 34/40
8180/8180 [==============================] - 2s - loss: 0.4073 - acc: 0.8275 - val_loss: 0.3953 - val_acc: 0.8436
Epoch 35/40
8180/8180 [==============================] - 2s - loss: 0.4061 - acc: 0.8298 - val_loss: 0.3975 - val_acc: 0.8451
Epoch 36/40
8180/8180 [==============================] - 2s - loss: 0.4060 - acc: 0.8290 - val_loss: 0.3959 - val_acc: 0.8426
Epoch 37/40
8180/8180 [==============================] - 2s - loss: 0.4055 - acc: 0.8290 - val_loss: 0.3942 - val_acc: 0.8436
Epoch 38/40
8180/8180 [==============================] - 2s - loss: 0.4056 - acc: 0.8281 - val_loss: 0.3939 - val_acc: 0.8441
Epoch 39/40
8180/8180 [==============================] - 2s - loss: 0.4054 - acc: 0.8292 - val_loss: 0.3942 - val_acc: 0.8436
Epoch 40/40
8180/8180 [==============================] - 2s - loss: 0.4045 - acc: 0.8287 - val_loss: 0.3947 - val_acc: 0.8431
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-20.h5
chunk number 20
prepare data
start training
Train on 8123 samples, validate on 2031 samples
Epoch 1/40
8123/8123 [==============================] - 1s - loss: 0.4328 - acc: 0.8157 - val_loss: 0.4076 - val_acc: 0.8173
Epoch 2/40
8123/8123 [==============================] - 1s - loss: 0.4029 - acc: 0.8343 - val_loss: 0.4232 - val_acc: 0.8134
Epoch 3/40
8123/8123 [==============================] - 1s - loss: 0.4114 - acc: 0.8288 - val_loss: 0.4385 - val_acc: 0.8075
Epoch 4/40
8123/8123 [==============================] - 1s - loss: 0.4236 - acc: 0.8226 - val_loss: 0.4346 - val_acc: 0.8085
Epoch 5/40
8123/8123 [==============================] - 1s - loss: 0.4189 - acc: 0.8242 - val_loss: 0.4170 - val_acc: 0.8178
Epoch 6/40
8123/8123 [==============================] - 1s - loss: 0.4031 - acc: 0.8313 - val_loss: 0.4111 - val_acc: 0.8154
Epoch 7/40
8123/8123 [==============================] - 1s - loss: 0.4006 - acc: 0.8311 - val_loss: 0.4222 - val_acc: 0.8055
Epoch 8/40
8123/8123 [==============================] - 1s - loss: 0.4148 - acc: 0.8252 - val_loss: 0.4087 - val_acc: 0.8178
Epoch 9/40
8123/8123 [==============================] - 1s - loss: 0.3980 - acc: 0.8333 - val_loss: 0.4107 - val_acc: 0.8203
Epoch 10/40
8123/8123 [==============================] - 1s - loss: 0.3960 - acc: 0.8376 - val_loss: 0.4201 - val_acc: 0.8173
Epoch 11/40
8123/8123 [==============================] - 1s - loss: 0.4036 - acc: 0.8341 - val_loss: 0.4183 - val_acc: 0.8218
Epoch 12/40
8123/8123 [==============================] - 1s - loss: 0.4022 - acc: 0.8359 - val_loss: 0.4076 - val_acc: 0.8232
Epoch 13/40
8123/8123 [==============================] - 1s - loss: 0.3940 - acc: 0.8376 - val_loss: 0.4062 - val_acc: 0.8213
Epoch 14/40
8123/8123 [==============================] - 1s - loss: 0.3966 - acc: 0.8347 - val_loss: 0.4087 - val_acc: 0.8183
Epoch 15/40
8123/8123 [==============================] - 1s - loss: 0.3996 - acc: 0.8337 - val_loss: 0.4038 - val_acc: 0.8252
Epoch 16/40
8123/8123 [==============================] - 1s - loss: 0.3910 - acc: 0.8366 - val_loss: 0.4097 - val_acc: 0.8242
Epoch 17/40
8123/8123 [==============================] - 1s - loss: 0.3931 - acc: 0.8375 - val_loss: 0.4138 - val_acc: 0.8223
Epoch 18/40
8123/8123 [==============================] - 1s - loss: 0.3955 - acc: 0.8377 - val_loss: 0.4095 - val_acc: 0.8252
Epoch 19/40
8123/8123 [==============================] - 1s - loss: 0.3918 - acc: 0.8381 - val_loss: 0.4073 - val_acc: 0.8242
Epoch 20/40
8123/8123 [==============================] - 1s - loss: 0.3902 - acc: 0.8375 - val_loss: 0.4099 - val_acc: 0.8178
Epoch 21/40
8123/8123 [==============================] - 1s - loss: 0.3935 - acc: 0.8374 - val_loss: 0.4068 - val_acc: 0.8237
Epoch 22/40
8123/8123 [==============================] - 1s - loss: 0.3896 - acc: 0.8393 - val_loss: 0.4068 - val_acc: 0.8282
Epoch 23/40
8123/8123 [==============================] - 1s - loss: 0.3884 - acc: 0.8402 - val_loss: 0.4092 - val_acc: 0.8257
Epoch 24/40
8123/8123 [==============================] - 1s - loss: 0.3903 - acc: 0.8381 - val_loss: 0.4063 - val_acc: 0.8267
Epoch 25/40
8123/8123 [==============================] - 1s - loss: 0.3885 - acc: 0.8405 - val_loss: 0.4029 - val_acc: 0.8267
Epoch 26/40
8123/8123 [==============================] - 1s - loss: 0.3872 - acc: 0.8395 - val_loss: 0.4036 - val_acc: 0.8232
Epoch 27/40
8123/8123 [==============================] - 1s - loss: 0.3890 - acc: 0.8381 - val_loss: 0.4024 - val_acc: 0.8242
Epoch 28/40
8123/8123 [==============================] - 1s - loss: 0.3865 - acc: 0.8391 - val_loss: 0.4037 - val_acc: 0.8277
Epoch 29/40
8123/8123 [==============================] - 1s - loss: 0.3859 - acc: 0.8414 - val_loss: 0.4055 - val_acc: 0.8252
Epoch 30/40
8123/8123 [==============================] - 1s - loss: 0.3864 - acc: 0.8403 - val_loss: 0.4042 - val_acc: 0.8277
Epoch 31/40
8123/8123 [==============================] - 1s - loss: 0.3849 - acc: 0.8408 - val_loss: 0.4040 - val_acc: 0.8277
Epoch 32/40
8123/8123 [==============================] - 1s - loss: 0.3848 - acc: 0.8398 - val_loss: 0.4046 - val_acc: 0.8237
Epoch 33/40
8123/8123 [==============================] - 1s - loss: 0.3854 - acc: 0.8393 - val_loss: 0.4034 - val_acc: 0.8287
Epoch 34/40
8123/8123 [==============================] - 1s - loss: 0.3837 - acc: 0.8409 - val_loss: 0.4038 - val_acc: 0.8277
Epoch 35/40
8123/8123 [==============================] - 1s - loss: 0.3840 - acc: 0.8422 - val_loss: 0.4031 - val_acc: 0.8287
Epoch 36/40
8123/8123 [==============================] - 1s - loss: 0.3836 - acc: 0.8429 - val_loss: 0.4014 - val_acc: 0.8272
Epoch 37/40
8123/8123 [==============================] - 1s - loss: 0.3827 - acc: 0.8424 - val_loss: 0.4013 - val_acc: 0.8267
Epoch 38/40
8123/8123 [==============================] - 1s - loss: 0.3832 - acc: 0.8406 - val_loss: 0.4011 - val_acc: 0.8277
Epoch 39/40
8123/8123 [==============================] - 1s - loss: 0.3823 - acc: 0.8414 - val_loss: 0.4021 - val_acc: 0.8287
Epoch 40/40
8123/8123 [==============================] - 1s - loss: 0.3820 - acc: 0.8438 - val_loss: 0.4027 - val_acc: 0.8277
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-21.h5
chunk number 21
prepare data
start training
Train on 8140 samples, validate on 2036 samples
Epoch 1/40
8140/8140 [==============================] - 2s - loss: 0.4779 - acc: 0.8054 - val_loss: 0.4164 - val_acc: 0.8256
Epoch 2/40
8140/8140 [==============================] - 2s - loss: 0.4331 - acc: 0.8170 - val_loss: 0.4028 - val_acc: 0.8306
Epoch 3/40
8140/8140 [==============================] - 2s - loss: 0.4179 - acc: 0.8252 - val_loss: 0.4164 - val_acc: 0.8271
Epoch 4/40
8140/8140 [==============================] - 2s - loss: 0.4231 - acc: 0.8210 - val_loss: 0.4077 - val_acc: 0.8315
Epoch 5/40
8140/8140 [==============================] - 2s - loss: 0.4106 - acc: 0.8258 - val_loss: 0.3933 - val_acc: 0.8443
Epoch 6/40
8140/8140 [==============================] - 2s - loss: 0.3998 - acc: 0.8359 - val_loss: 0.4048 - val_acc: 0.8345
Epoch 7/40
8140/8140 [==============================] - 2s - loss: 0.4136 - acc: 0.8313 - val_loss: 0.3912 - val_acc: 0.8448
Epoch 8/40
8140/8140 [==============================] - 2s - loss: 0.3953 - acc: 0.8355 - val_loss: 0.4017 - val_acc: 0.8335
Epoch 9/40
8140/8140 [==============================] - 2s - loss: 0.4045 - acc: 0.8284 - val_loss: 0.4024 - val_acc: 0.8325
Epoch 10/40
8140/8140 [==============================] - 2s - loss: 0.4055 - acc: 0.8276 - val_loss: 0.3956 - val_acc: 0.8384
Epoch 11/40
8140/8140 [==============================] - 2s - loss: 0.3991 - acc: 0.8339 - val_loss: 0.3975 - val_acc: 0.8364
Epoch 12/40
8140/8140 [==============================] - 2s - loss: 0.4019 - acc: 0.8312 - val_loss: 0.3994 - val_acc: 0.8340
Epoch 13/40
8140/8140 [==============================] - 2s - loss: 0.4029 - acc: 0.8308 - val_loss: 0.3943 - val_acc: 0.8369
Epoch 14/40
8140/8140 [==============================] - 2s - loss: 0.3958 - acc: 0.8339 - val_loss: 0.3966 - val_acc: 0.8389
Epoch 15/40
8140/8140 [==============================] - 2s - loss: 0.3969 - acc: 0.8335 - val_loss: 0.3965 - val_acc: 0.8379
Epoch 16/40
8140/8140 [==============================] - 2s - loss: 0.3966 - acc: 0.8349 - val_loss: 0.3930 - val_acc: 0.8394
Epoch 17/40
8140/8140 [==============================] - 2s - loss: 0.3939 - acc: 0.8343 - val_loss: 0.3935 - val_acc: 0.8325
Epoch 18/40
8140/8140 [==============================] - 2s - loss: 0.3945 - acc: 0.8340 - val_loss: 0.3925 - val_acc: 0.8355
Epoch 19/40
8140/8140 [==============================] - 2s - loss: 0.3936 - acc: 0.8342 - val_loss: 0.3905 - val_acc: 0.8423
Epoch 20/40
8140/8140 [==============================] - 2s - loss: 0.3918 - acc: 0.8377 - val_loss: 0.3918 - val_acc: 0.8404
Epoch 21/40
8140/8140 [==============================] - 2s - loss: 0.3928 - acc: 0.8357 - val_loss: 0.3909 - val_acc: 0.8433
Epoch 22/40
8140/8140 [==============================] - 2s - loss: 0.3918 - acc: 0.8383 - val_loss: 0.3886 - val_acc: 0.8384
Epoch 23/40
8140/8140 [==============================] - 2s - loss: 0.3917 - acc: 0.8377 - val_loss: 0.3880 - val_acc: 0.8360
Epoch 24/40
8140/8140 [==============================] - 2s - loss: 0.3909 - acc: 0.8391 - val_loss: 0.3880 - val_acc: 0.8404
Epoch 25/40
8140/8140 [==============================] - 2s - loss: 0.3891 - acc: 0.8382 - val_loss: 0.3887 - val_acc: 0.8379
Epoch 26/40
8140/8140 [==============================] - 2s - loss: 0.3890 - acc: 0.8377 - val_loss: 0.3872 - val_acc: 0.8379
Epoch 27/40
8140/8140 [==============================] - 2s - loss: 0.3884 - acc: 0.8366 - val_loss: 0.3863 - val_acc: 0.8384
Epoch 28/40
8140/8140 [==============================] - 2s - loss: 0.3877 - acc: 0.8380 - val_loss: 0.3859 - val_acc: 0.8404
Epoch 29/40
8140/8140 [==============================] - 2s - loss: 0.3870 - acc: 0.8387 - val_loss: 0.3850 - val_acc: 0.8409
Epoch 30/40
8140/8140 [==============================] - 2s - loss: 0.3857 - acc: 0.8385 - val_loss: 0.3856 - val_acc: 0.8394
Epoch 31/40
8140/8140 [==============================] - 2s - loss: 0.3855 - acc: 0.8385 - val_loss: 0.3853 - val_acc: 0.8379
Epoch 32/40
8140/8140 [==============================] - 2s - loss: 0.3853 - acc: 0.8386 - val_loss: 0.3839 - val_acc: 0.8414
Epoch 33/40
8140/8140 [==============================] - 2s - loss: 0.3845 - acc: 0.8391 - val_loss: 0.3842 - val_acc: 0.8399
Epoch 34/40
8140/8140 [==============================] - 2s - loss: 0.3840 - acc: 0.8389 - val_loss: 0.3850 - val_acc: 0.8409
Epoch 35/40
8140/8140 [==============================] - 2s - loss: 0.3832 - acc: 0.8402 - val_loss: 0.3861 - val_acc: 0.8448
Epoch 36/40
8140/8140 [==============================] - 2s - loss: 0.3833 - acc: 0.8400 - val_loss: 0.3856 - val_acc: 0.8418
Epoch 37/40
8140/8140 [==============================] - 2s - loss: 0.3827 - acc: 0.8408 - val_loss: 0.3850 - val_acc: 0.8414
Epoch 38/40
8140/8140 [==============================] - 2s - loss: 0.3823 - acc: 0.8403 - val_loss: 0.3850 - val_acc: 0.8428
Epoch 39/40
8140/8140 [==============================] - 2s - loss: 0.3818 - acc: 0.8409 - val_loss: 0.3852 - val_acc: 0.8438
Epoch 40/40
8140/8140 [==============================] - 2s - loss: 0.3816 - acc: 0.8403 - val_loss: 0.3846 - val_acc: 0.8414
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-22.h5
chunk number 22
prepare data
start training
Train on 8040 samples, validate on 2010 samples
Epoch 1/40
8040/8040 [==============================] - 1s - loss: 0.4117 - acc: 0.8255 - val_loss: 0.4039 - val_acc: 0.8328
Epoch 2/40
8040/8040 [==============================] - 1s - loss: 0.3928 - acc: 0.8338 - val_loss: 0.4178 - val_acc: 0.8249
Epoch 3/40
8040/8040 [==============================] - 1s - loss: 0.4051 - acc: 0.8272 - val_loss: 0.4210 - val_acc: 0.8209
Epoch 4/40
8040/8040 [==============================] - 1s - loss: 0.4085 - acc: 0.8256 - val_loss: 0.4076 - val_acc: 0.8299
Epoch 5/40
8040/8040 [==============================] - 1s - loss: 0.3953 - acc: 0.8328 - val_loss: 0.4039 - val_acc: 0.8274
Epoch 6/40
8040/8040 [==============================] - 1s - loss: 0.3927 - acc: 0.8346 - val_loss: 0.4174 - val_acc: 0.8244
Epoch 7/40
8040/8040 [==============================] - 1s - loss: 0.4042 - acc: 0.8277 - val_loss: 0.4043 - val_acc: 0.8284
Epoch 8/40
8040/8040 [==============================] - 1s - loss: 0.3922 - acc: 0.8351 - val_loss: 0.4047 - val_acc: 0.8303
Epoch 9/40
8040/8040 [==============================] - 1s - loss: 0.3912 - acc: 0.8351 - val_loss: 0.4113 - val_acc: 0.8274
Epoch 10/40
8040/8040 [==============================] - 1s - loss: 0.3977 - acc: 0.8311 - val_loss: 0.4086 - val_acc: 0.8279
Epoch 11/40
8040/8040 [==============================] - 1s - loss: 0.3948 - acc: 0.8337 - val_loss: 0.4016 - val_acc: 0.8318
Epoch 12/40
8040/8040 [==============================] - 1s - loss: 0.3884 - acc: 0.8364 - val_loss: 0.4067 - val_acc: 0.8259
Epoch 13/40
8040/8040 [==============================] - 1s - loss: 0.3928 - acc: 0.8332 - val_loss: 0.4053 - val_acc: 0.8294
Epoch 14/40
8040/8040 [==============================] - 1s - loss: 0.3915 - acc: 0.8330 - val_loss: 0.4005 - val_acc: 0.8313
Epoch 15/40
8040/8040 [==============================] - 1s - loss: 0.3871 - acc: 0.8377 - val_loss: 0.4046 - val_acc: 0.8303
Epoch 16/40
8040/8040 [==============================] - 1s - loss: 0.3906 - acc: 0.8338 - val_loss: 0.4041 - val_acc: 0.8313
Epoch 17/40
8040/8040 [==============================] - 1s - loss: 0.3900 - acc: 0.8348 - val_loss: 0.4001 - val_acc: 0.8333
Epoch 18/40
8040/8040 [==============================] - 1s - loss: 0.3858 - acc: 0.8372 - val_loss: 0.4028 - val_acc: 0.8348
Epoch 19/40
8040/8040 [==============================] - 1s - loss: 0.3878 - acc: 0.8361 - val_loss: 0.4029 - val_acc: 0.8333
Epoch 20/40
8040/8040 [==============================] - 1s - loss: 0.3875 - acc: 0.8369 - val_loss: 0.4000 - val_acc: 0.8338
Epoch 21/40
8040/8040 [==============================] - 1s - loss: 0.3845 - acc: 0.8377 - val_loss: 0.4021 - val_acc: 0.8328
Epoch 22/40
8040/8040 [==============================] - 1s - loss: 0.3866 - acc: 0.8353 - val_loss: 0.4015 - val_acc: 0.8333
Epoch 23/40
8040/8040 [==============================] - 1s - loss: 0.3859 - acc: 0.8366 - val_loss: 0.3997 - val_acc: 0.8398
Epoch 24/40
8040/8040 [==============================] - 1s - loss: 0.3838 - acc: 0.8381 - val_loss: 0.4016 - val_acc: 0.8333
Epoch 25/40
8040/8040 [==============================] - 1s - loss: 0.3858 - acc: 0.8371 - val_loss: 0.3994 - val_acc: 0.8393
Epoch 26/40
8040/8040 [==============================] - 1s - loss: 0.3836 - acc: 0.8386 - val_loss: 0.3992 - val_acc: 0.8363
Epoch 27/40
8040/8040 [==============================] - 1s - loss: 0.3835 - acc: 0.8376 - val_loss: 0.3997 - val_acc: 0.8363
Epoch 28/40
8040/8040 [==============================] - 1s - loss: 0.3840 - acc: 0.8371 - val_loss: 0.3981 - val_acc: 0.8358
Epoch 29/40
8040/8040 [==============================] - 1s - loss: 0.3821 - acc: 0.8399 - val_loss: 0.3991 - val_acc: 0.8388
Epoch 30/40
8040/8040 [==============================] - 1s - loss: 0.3829 - acc: 0.8403 - val_loss: 0.3983 - val_acc: 0.8378
Epoch 31/40
8040/8040 [==============================] - 1s - loss: 0.3820 - acc: 0.8400 - val_loss: 0.3977 - val_acc: 0.8358
Epoch 32/40
8040/8040 [==============================] - 1s - loss: 0.3813 - acc: 0.8396 - val_loss: 0.3979 - val_acc: 0.8373
Epoch 33/40
8040/8040 [==============================] - 1s - loss: 0.3817 - acc: 0.8402 - val_loss: 0.3970 - val_acc: 0.8373
Epoch 34/40
8040/8040 [==============================] - 1s - loss: 0.3802 - acc: 0.8393 - val_loss: 0.3978 - val_acc: 0.8388
Epoch 35/40
8040/8040 [==============================] - 1s - loss: 0.3809 - acc: 0.8397 - val_loss: 0.3968 - val_acc: 0.8383
Epoch 36/40
8040/8040 [==============================] - 1s - loss: 0.3796 - acc: 0.8394 - val_loss: 0.3969 - val_acc: 0.8338
Epoch 37/40
8040/8040 [==============================] - 1s - loss: 0.3801 - acc: 0.8400 - val_loss: 0.3964 - val_acc: 0.8393
Epoch 38/40
8040/8040 [==============================] - 1s - loss: 0.3789 - acc: 0.8403 - val_loss: 0.3973 - val_acc: 0.8388
Epoch 39/40
8040/8040 [==============================] - 1s - loss: 0.3792 - acc: 0.8414 - val_loss: 0.3967 - val_acc: 0.8368
Epoch 40/40
8040/8040 [==============================] - 1s - loss: 0.3783 - acc: 0.8422 - val_loss: 0.3968 - val_acc: 0.8363
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-23.h5
chunk number 23
prepare data
start training
Train on 8117 samples, validate on 2030 samples
Epoch 1/40
8117/8117 [==============================] - 1s - loss: 0.3828 - acc: 0.8446 - val_loss: 0.3950 - val_acc: 0.8286
Epoch 2/40
8117/8117 [==============================] - 1s - loss: 0.3790 - acc: 0.8467 - val_loss: 0.3951 - val_acc: 0.8281
Epoch 3/40
8117/8117 [==============================] - 1s - loss: 0.3807 - acc: 0.8439 - val_loss: 0.3943 - val_acc: 0.8305
Epoch 4/40
8117/8117 [==============================] - 1s - loss: 0.3774 - acc: 0.8496 - val_loss: 0.3962 - val_acc: 0.8251
Epoch 5/40
8117/8117 [==============================] - 1s - loss: 0.3788 - acc: 0.8479 - val_loss: 0.3938 - val_acc: 0.8300
Epoch 6/40
8117/8117 [==============================] - 1s - loss: 0.3769 - acc: 0.8493 - val_loss: 0.3935 - val_acc: 0.8281
Epoch 7/40
8117/8117 [==============================] - 1s - loss: 0.3772 - acc: 0.8469 - val_loss: 0.3931 - val_acc: 0.8276
Epoch 8/40
8117/8117 [==============================] - 1s - loss: 0.3761 - acc: 0.8490 - val_loss: 0.3940 - val_acc: 0.8300
Epoch 9/40
8117/8117 [==============================] - 1s - loss: 0.3752 - acc: 0.8491 - val_loss: 0.3953 - val_acc: 0.8296
Epoch 10/40
8117/8117 [==============================] - 1s - loss: 0.3748 - acc: 0.8486 - val_loss: 0.3938 - val_acc: 0.8296
Epoch 11/40
8117/8117 [==============================] - 1s - loss: 0.3733 - acc: 0.8514 - val_loss: 0.3946 - val_acc: 0.8291
Epoch 12/40
8117/8117 [==============================] - 1s - loss: 0.3736 - acc: 0.8487 - val_loss: 0.3956 - val_acc: 0.8286
Epoch 13/40
8117/8117 [==============================] - 1s - loss: 0.3722 - acc: 0.8511 - val_loss: 0.3966 - val_acc: 0.8300
Epoch 14/40
8117/8117 [==============================] - 1s - loss: 0.3722 - acc: 0.8515 - val_loss: 0.3952 - val_acc: 0.8296
Epoch 15/40
8117/8117 [==============================] - 1s - loss: 0.3711 - acc: 0.8511 - val_loss: 0.3950 - val_acc: 0.8291
Epoch 16/40
8117/8117 [==============================] - 1s - loss: 0.3711 - acc: 0.8495 - val_loss: 0.3961 - val_acc: 0.8305
Epoch 17/40
8117/8117 [==============================] - 1s - loss: 0.3704 - acc: 0.8512 - val_loss: 0.3960 - val_acc: 0.8315
Epoch 18/40
8117/8117 [==============================] - 1s - loss: 0.3701 - acc: 0.8507 - val_loss: 0.3945 - val_acc: 0.8315
Epoch 19/40
8117/8117 [==============================] - 1s - loss: 0.3698 - acc: 0.8501 - val_loss: 0.3945 - val_acc: 0.8305
Epoch 20/40
8117/8117 [==============================] - 1s - loss: 0.3693 - acc: 0.8502 - val_loss: 0.3955 - val_acc: 0.8350
Epoch 21/40
8117/8117 [==============================] - 1s - loss: 0.3691 - acc: 0.8512 - val_loss: 0.3952 - val_acc: 0.8345
Epoch 22/40
8117/8117 [==============================] - 1s - loss: 0.3684 - acc: 0.8513 - val_loss: 0.3949 - val_acc: 0.8340
Epoch 23/40
8117/8117 [==============================] - 1s - loss: 0.3683 - acc: 0.8509 - val_loss: 0.3954 - val_acc: 0.8350
Epoch 24/40
8117/8117 [==============================] - 1s - loss: 0.3677 - acc: 0.8511 - val_loss: 0.3957 - val_acc: 0.8350
Epoch 25/40
8117/8117 [==============================] - 1s - loss: 0.3674 - acc: 0.8517 - val_loss: 0.3951 - val_acc: 0.8335
Epoch 26/40
8117/8117 [==============================] - 1s - loss: 0.3670 - acc: 0.8513 - val_loss: 0.3953 - val_acc: 0.8345
Epoch 27/40
8117/8117 [==============================] - 1s - loss: 0.3663 - acc: 0.8513 - val_loss: 0.3958 - val_acc: 0.8340
Epoch 28/40
8117/8117 [==============================] - 1s - loss: 0.3661 - acc: 0.8517 - val_loss: 0.3953 - val_acc: 0.8330
Epoch 29/40
8117/8117 [==============================] - 1s - loss: 0.3656 - acc: 0.8520 - val_loss: 0.3954 - val_acc: 0.8340
Epoch 30/40
8117/8117 [==============================] - 1s - loss: 0.3652 - acc: 0.8517 - val_loss: 0.3956 - val_acc: 0.8355
Epoch 31/40
8117/8117 [==============================] - 1s - loss: 0.3649 - acc: 0.8524 - val_loss: 0.3951 - val_acc: 0.8335
Epoch 32/40
8117/8117 [==============================] - 1s - loss: 0.3646 - acc: 0.8519 - val_loss: 0.3952 - val_acc: 0.8355
Epoch 33/40
8117/8117 [==============================] - 1s - loss: 0.3642 - acc: 0.8517 - val_loss: 0.3952 - val_acc: 0.8350
Epoch 34/40
8117/8117 [==============================] - 1s - loss: 0.3639 - acc: 0.8522 - val_loss: 0.3951 - val_acc: 0.8340
Epoch 35/40
8117/8117 [==============================] - 1s - loss: 0.3635 - acc: 0.8524 - val_loss: 0.3955 - val_acc: 0.8320
Epoch 36/40
8117/8117 [==============================] - 1s - loss: 0.3632 - acc: 0.8527 - val_loss: 0.3952 - val_acc: 0.8335
Epoch 37/40
8117/8117 [==============================] - 1s - loss: 0.3628 - acc: 0.8519 - val_loss: 0.3953 - val_acc: 0.8335
Epoch 38/40
8117/8117 [==============================] - 1s - loss: 0.3624 - acc: 0.8528 - val_loss: 0.3954 - val_acc: 0.8345
Epoch 39/40
8117/8117 [==============================] - 1s - loss: 0.3620 - acc: 0.8527 - val_loss: 0.3956 - val_acc: 0.8355
Epoch 40/40
8117/8117 [==============================] - 1s - loss: 0.3618 - acc: 0.8525 - val_loss: 0.3966 - val_acc: 0.8330
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-24.h5
chunk number 24
prepare data
start training
Train on 8140 samples, validate on 2035 samples
Epoch 1/40
8140/8140 [==============================] - 1s - loss: 0.4104 - acc: 0.8310 - val_loss: 0.4137 - val_acc: 0.8236
Epoch 2/40
8140/8140 [==============================] - 1s - loss: 0.4034 - acc: 0.8295 - val_loss: 0.4168 - val_acc: 0.8147
Epoch 3/40
8140/8140 [==============================] - 1s - loss: 0.4068 - acc: 0.8268 - val_loss: 0.4083 - val_acc: 0.8256
Epoch 4/40
8140/8140 [==============================] - 1s - loss: 0.3990 - acc: 0.8335 - val_loss: 0.4050 - val_acc: 0.8305
Epoch 5/40
8140/8140 [==============================] - 1s - loss: 0.3967 - acc: 0.8338 - val_loss: 0.4089 - val_acc: 0.8275
Epoch 6/40
8140/8140 [==============================] - 1s - loss: 0.4009 - acc: 0.8283 - val_loss: 0.4048 - val_acc: 0.8300
Epoch 7/40
8140/8140 [==============================] - 1s - loss: 0.3951 - acc: 0.8366 - val_loss: 0.4082 - val_acc: 0.8300
Epoch 8/40
8140/8140 [==============================] - 1s - loss: 0.3990 - acc: 0.8356 - val_loss: 0.4033 - val_acc: 0.8305
Epoch 9/40
8140/8140 [==============================] - 1s - loss: 0.3941 - acc: 0.8365 - val_loss: 0.4028 - val_acc: 0.8285
Epoch 10/40
8140/8140 [==============================] - 1s - loss: 0.3952 - acc: 0.8330 - val_loss: 0.4016 - val_acc: 0.8314
Epoch 11/40
8140/8140 [==============================] - 1s - loss: 0.3938 - acc: 0.8351 - val_loss: 0.4015 - val_acc: 0.8305
Epoch 12/40
8140/8140 [==============================] - 1s - loss: 0.3931 - acc: 0.8362 - val_loss: 0.4030 - val_acc: 0.8324
Epoch 13/40
8140/8140 [==============================] - 1s - loss: 0.3945 - acc: 0.8349 - val_loss: 0.4003 - val_acc: 0.8305
Epoch 14/40
8140/8140 [==============================] - 1s - loss: 0.3908 - acc: 0.8380 - val_loss: 0.4022 - val_acc: 0.8265
Epoch 15/40
8140/8140 [==============================] - 1s - loss: 0.3924 - acc: 0.8371 - val_loss: 0.4012 - val_acc: 0.8270
Epoch 16/40
8140/8140 [==============================] - 1s - loss: 0.3901 - acc: 0.8380 - val_loss: 0.4018 - val_acc: 0.8285
Epoch 17/40
8140/8140 [==============================] - 1s - loss: 0.3900 - acc: 0.8377 - val_loss: 0.4029 - val_acc: 0.8280
Epoch 18/40
8140/8140 [==============================] - 1s - loss: 0.3907 - acc: 0.8364 - val_loss: 0.4016 - val_acc: 0.8280
Epoch 19/40
8140/8140 [==============================] - 1s - loss: 0.3890 - acc: 0.8388 - val_loss: 0.4028 - val_acc: 0.8280
Epoch 20/40
8140/8140 [==============================] - 1s - loss: 0.3901 - acc: 0.8385 - val_loss: 0.4014 - val_acc: 0.8246
Epoch 21/40
8140/8140 [==============================] - 1s - loss: 0.3883 - acc: 0.8385 - val_loss: 0.4016 - val_acc: 0.8314
Epoch 22/40
8140/8140 [==============================] - 1s - loss: 0.3880 - acc: 0.8378 - val_loss: 0.4016 - val_acc: 0.8324
Epoch 23/40
8140/8140 [==============================] - 1s - loss: 0.3876 - acc: 0.8378 - val_loss: 0.4008 - val_acc: 0.8280
Epoch 24/40
8140/8140 [==============================] - 1s - loss: 0.3864 - acc: 0.8392 - val_loss: 0.4015 - val_acc: 0.8256
Epoch 25/40
8140/8140 [==============================] - 1s - loss: 0.3871 - acc: 0.8380 - val_loss: 0.4007 - val_acc: 0.8285
Epoch 26/40
8140/8140 [==============================] - 1s - loss: 0.3857 - acc: 0.8408 - val_loss: 0.4013 - val_acc: 0.8300
Epoch 27/40
8140/8140 [==============================] - 1s - loss: 0.3862 - acc: 0.8403 - val_loss: 0.4003 - val_acc: 0.8260
Epoch 28/40
8140/8140 [==============================] - 1s - loss: 0.3849 - acc: 0.8405 - val_loss: 0.4009 - val_acc: 0.8246
Epoch 29/40
8140/8140 [==============================] - 1s - loss: 0.3853 - acc: 0.8388 - val_loss: 0.4007 - val_acc: 0.8226
Epoch 30/40
8140/8140 [==============================] - 1s - loss: 0.3842 - acc: 0.8409 - val_loss: 0.4015 - val_acc: 0.8265
Epoch 31/40
8140/8140 [==============================] - 1s - loss: 0.3845 - acc: 0.8413 - val_loss: 0.4013 - val_acc: 0.8231
Epoch 32/40
8140/8140 [==============================] - 1s - loss: 0.3837 - acc: 0.8409 - val_loss: 0.4015 - val_acc: 0.8241
Epoch 33/40
8140/8140 [==============================] - 1s - loss: 0.3837 - acc: 0.8397 - val_loss: 0.4010 - val_acc: 0.8216
Epoch 34/40
8140/8140 [==============================] - 1s - loss: 0.3829 - acc: 0.8424 - val_loss: 0.4013 - val_acc: 0.8231
Epoch 35/40
8140/8140 [==============================] - 1s - loss: 0.3830 - acc: 0.8419 - val_loss: 0.4012 - val_acc: 0.8226
Epoch 36/40
8140/8140 [==============================] - 1s - loss: 0.3825 - acc: 0.8430 - val_loss: 0.4017 - val_acc: 0.8231
Epoch 37/40
8140/8140 [==============================] - 1s - loss: 0.3825 - acc: 0.8412 - val_loss: 0.4015 - val_acc: 0.8231
Epoch 38/40
8140/8140 [==============================] - 1s - loss: 0.3818 - acc: 0.8425 - val_loss: 0.4016 - val_acc: 0.8241
Epoch 39/40
8140/8140 [==============================] - 1s - loss: 0.3817 - acc: 0.8435 - val_loss: 0.4013 - val_acc: 0.8241
Epoch 40/40
8140/8140 [==============================] - 1s - loss: 0.3811 - acc: 0.8435 - val_loss: 0.4015 - val_acc: 0.8226
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-25.h5
chunk number 25
prepare data
start training
Train on 8058 samples, validate on 2015 samples
Epoch 1/40
8058/8058 [==============================] - 1s - loss: 0.3864 - acc: 0.8420 - val_loss: 0.3679 - val_acc: 0.8526
Epoch 2/40
8058/8058 [==============================] - 1s - loss: 0.3848 - acc: 0.8425 - val_loss: 0.3653 - val_acc: 0.8546
Epoch 3/40
8058/8058 [==============================] - 1s - loss: 0.3826 - acc: 0.8436 - val_loss: 0.3651 - val_acc: 0.8556
Epoch 4/40
8058/8058 [==============================] - 1s - loss: 0.3815 - acc: 0.8457 - val_loss: 0.3661 - val_acc: 0.8531
Epoch 5/40
8058/8058 [==============================] - 1s - loss: 0.3808 - acc: 0.8464 - val_loss: 0.3656 - val_acc: 0.8546
Epoch 6/40
8058/8058 [==============================] - 1s - loss: 0.3798 - acc: 0.8457 - val_loss: 0.3653 - val_acc: 0.8556
Epoch 7/40
8058/8058 [==============================] - 1s - loss: 0.3793 - acc: 0.8481 - val_loss: 0.3658 - val_acc: 0.8591
Epoch 8/40
8058/8058 [==============================] - 1s - loss: 0.3789 - acc: 0.8476 - val_loss: 0.3660 - val_acc: 0.8551
Epoch 9/40
8058/8058 [==============================] - 1s - loss: 0.3784 - acc: 0.8479 - val_loss: 0.3655 - val_acc: 0.8551
Epoch 10/40
8058/8058 [==============================] - 1s - loss: 0.3777 - acc: 0.8480 - val_loss: 0.3652 - val_acc: 0.8566
Epoch 11/40
8058/8058 [==============================] - 1s - loss: 0.3772 - acc: 0.8485 - val_loss: 0.3648 - val_acc: 0.8561
Epoch 12/40
8058/8058 [==============================] - 1s - loss: 0.3766 - acc: 0.8485 - val_loss: 0.3644 - val_acc: 0.8526
Epoch 13/40
8058/8058 [==============================] - 1s - loss: 0.3761 - acc: 0.8486 - val_loss: 0.3648 - val_acc: 0.8546
Epoch 14/40
8058/8058 [==============================] - 1s - loss: 0.3755 - acc: 0.8487 - val_loss: 0.3656 - val_acc: 0.8541
Epoch 15/40
8058/8058 [==============================] - 1s - loss: 0.3746 - acc: 0.8491 - val_loss: 0.3668 - val_acc: 0.8516
Epoch 16/40
8058/8058 [==============================] - 1s - loss: 0.3742 - acc: 0.8506 - val_loss: 0.3668 - val_acc: 0.8516
Epoch 17/40
8058/8058 [==============================] - 1s - loss: 0.3738 - acc: 0.8507 - val_loss: 0.3657 - val_acc: 0.8556
Epoch 18/40
8058/8058 [==============================] - 1s - loss: 0.3732 - acc: 0.8506 - val_loss: 0.3647 - val_acc: 0.8556
Epoch 19/40
8058/8058 [==============================] - 1s - loss: 0.3727 - acc: 0.8495 - val_loss: 0.3639 - val_acc: 0.8546
Epoch 20/40
8058/8058 [==============================] - 1s - loss: 0.3722 - acc: 0.8498 - val_loss: 0.3638 - val_acc: 0.8551
Epoch 21/40
8058/8058 [==============================] - 1s - loss: 0.3720 - acc: 0.8502 - val_loss: 0.3640 - val_acc: 0.8561
Epoch 22/40
8058/8058 [==============================] - 1s - loss: 0.3716 - acc: 0.8496 - val_loss: 0.3639 - val_acc: 0.8541
Epoch 23/40
8058/8058 [==============================] - 1s - loss: 0.3710 - acc: 0.8513 - val_loss: 0.3646 - val_acc: 0.8531
Epoch 24/40
8058/8058 [==============================] - 1s - loss: 0.3705 - acc: 0.8516 - val_loss: 0.3651 - val_acc: 0.8571
Epoch 25/40
8058/8058 [==============================] - 1s - loss: 0.3701 - acc: 0.8529 - val_loss: 0.3654 - val_acc: 0.8556
Epoch 26/40
8058/8058 [==============================] - 1s - loss: 0.3698 - acc: 0.8536 - val_loss: 0.3654 - val_acc: 0.8546
Epoch 27/40
8058/8058 [==============================] - 1s - loss: 0.3693 - acc: 0.8539 - val_loss: 0.3647 - val_acc: 0.8571
Epoch 28/40
8058/8058 [==============================] - 1s - loss: 0.3689 - acc: 0.8528 - val_loss: 0.3649 - val_acc: 0.8546
Epoch 29/40
8058/8058 [==============================] - 1s - loss: 0.3685 - acc: 0.8539 - val_loss: 0.3644 - val_acc: 0.8561
Epoch 30/40
8058/8058 [==============================] - 1s - loss: 0.3681 - acc: 0.8526 - val_loss: 0.3647 - val_acc: 0.8571
Epoch 31/40
8058/8058 [==============================] - 1s - loss: 0.3677 - acc: 0.8536 - val_loss: 0.3648 - val_acc: 0.8561
Epoch 32/40
8058/8058 [==============================] - 1s - loss: 0.3673 - acc: 0.8537 - val_loss: 0.3653 - val_acc: 0.8551
Epoch 33/40
8058/8058 [==============================] - 1s - loss: 0.3670 - acc: 0.8543 - val_loss: 0.3647 - val_acc: 0.8571
Epoch 34/40
8058/8058 [==============================] - 1s - loss: 0.3667 - acc: 0.8537 - val_loss: 0.3663 - val_acc: 0.8546
Epoch 35/40
8058/8058 [==============================] - 1s - loss: 0.3668 - acc: 0.8544 - val_loss: 0.3641 - val_acc: 0.8561
Epoch 36/40
8058/8058 [==============================] - 1s - loss: 0.3684 - acc: 0.8537 - val_loss: 0.3680 - val_acc: 0.8496
Epoch 37/40
8058/8058 [==============================] - 1s - loss: 0.3686 - acc: 0.8519 - val_loss: 0.3648 - val_acc: 0.8551
Epoch 38/40
8058/8058 [==============================] - 1s - loss: 0.3709 - acc: 0.8519 - val_loss: 0.3658 - val_acc: 0.8546
Epoch 39/40
8058/8058 [==============================] - 1s - loss: 0.3665 - acc: 0.8521 - val_loss: 0.3636 - val_acc: 0.8591
Epoch 40/40
8058/8058 [==============================] - 1s - loss: 0.3650 - acc: 0.8537 - val_loss: 0.3645 - val_acc: 0.8556
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-26.h5
chunk number 26
prepare data
start training
Train on 8100 samples, validate on 2026 samples
Epoch 1/40
8100/8100 [==============================] - 1s - loss: 0.3993 - acc: 0.8375 - val_loss: 0.3947 - val_acc: 0.8411
Epoch 2/40
8100/8100 [==============================] - 1s - loss: 0.3960 - acc: 0.8372 - val_loss: 0.3936 - val_acc: 0.8450
Epoch 3/40
8100/8100 [==============================] - 1s - loss: 0.3939 - acc: 0.8379 - val_loss: 0.3976 - val_acc: 0.8376
Epoch 4/40
8100/8100 [==============================] - 1s - loss: 0.3948 - acc: 0.8379 - val_loss: 0.3955 - val_acc: 0.8406
Epoch 5/40
8100/8100 [==============================] - 1s - loss: 0.3924 - acc: 0.8393 - val_loss: 0.3967 - val_acc: 0.8366
Epoch 6/40
8100/8100 [==============================] - 1s - loss: 0.3931 - acc: 0.8405 - val_loss: 0.3971 - val_acc: 0.8386
Epoch 7/40
8100/8100 [==============================] - 1s - loss: 0.3915 - acc: 0.8406 - val_loss: 0.3964 - val_acc: 0.8401
Epoch 8/40
8100/8100 [==============================] - 1s - loss: 0.3902 - acc: 0.8402 - val_loss: 0.3948 - val_acc: 0.8391
Epoch 9/40
8100/8100 [==============================] - 1s - loss: 0.3890 - acc: 0.8406 - val_loss: 0.3930 - val_acc: 0.8421
Epoch 10/40
8100/8100 [==============================] - 1s - loss: 0.3876 - acc: 0.8405 - val_loss: 0.3932 - val_acc: 0.8416
Epoch 11/40
8100/8100 [==============================] - 1s - loss: 0.3874 - acc: 0.8405 - val_loss: 0.3939 - val_acc: 0.8421
Epoch 12/40
8100/8100 [==============================] - 1s - loss: 0.3866 - acc: 0.8410 - val_loss: 0.3949 - val_acc: 0.8386
Epoch 13/40
8100/8100 [==============================] - 1s - loss: 0.3857 - acc: 0.8412 - val_loss: 0.3959 - val_acc: 0.8371
Epoch 14/40
8100/8100 [==============================] - 1s - loss: 0.3853 - acc: 0.8416 - val_loss: 0.3963 - val_acc: 0.8371
Epoch 15/40
8100/8100 [==============================] - 1s - loss: 0.3845 - acc: 0.8417 - val_loss: 0.3948 - val_acc: 0.8361
Epoch 16/40
8100/8100 [==============================] - 1s - loss: 0.3837 - acc: 0.8426 - val_loss: 0.3938 - val_acc: 0.8346
Epoch 17/40
8100/8100 [==============================] - 1s - loss: 0.3835 - acc: 0.8412 - val_loss: 0.3939 - val_acc: 0.8337
Epoch 18/40
8100/8100 [==============================] - 1s - loss: 0.3830 - acc: 0.8417 - val_loss: 0.3948 - val_acc: 0.8346
Epoch 19/40
8100/8100 [==============================] - 1s - loss: 0.3824 - acc: 0.8442 - val_loss: 0.3963 - val_acc: 0.8337
Epoch 20/40
8100/8100 [==============================] - 1s - loss: 0.3816 - acc: 0.8438 - val_loss: 0.3976 - val_acc: 0.8337
Epoch 21/40
8100/8100 [==============================] - 1s - loss: 0.3814 - acc: 0.8448 - val_loss: 0.3984 - val_acc: 0.8356
Epoch 22/40
8100/8100 [==============================] - 1s - loss: 0.3810 - acc: 0.8446 - val_loss: 0.3983 - val_acc: 0.8351
Epoch 23/40
8100/8100 [==============================] - 1s - loss: 0.3804 - acc: 0.8440 - val_loss: 0.3972 - val_acc: 0.8371
Epoch 24/40
8100/8100 [==============================] - 1s - loss: 0.3800 - acc: 0.8431 - val_loss: 0.3979 - val_acc: 0.8322
Epoch 25/40
8100/8100 [==============================] - 1s - loss: 0.3799 - acc: 0.8440 - val_loss: 0.3973 - val_acc: 0.8361
Epoch 26/40
8100/8100 [==============================] - 1s - loss: 0.3801 - acc: 0.8430 - val_loss: 0.4022 - val_acc: 0.8312
Epoch 27/40
8100/8100 [==============================] - 1s - loss: 0.3810 - acc: 0.8433 - val_loss: 0.4020 - val_acc: 0.8263
Epoch 28/40
8100/8100 [==============================] - 1s - loss: 0.3847 - acc: 0.8390 - val_loss: 0.4020 - val_acc: 0.8332
Epoch 29/40
8100/8100 [==============================] - 1s - loss: 0.3789 - acc: 0.8458 - val_loss: 0.4007 - val_acc: 0.8322
Epoch 30/40
8100/8100 [==============================] - 1s - loss: 0.3776 - acc: 0.8462 - val_loss: 0.3986 - val_acc: 0.8346
Epoch 31/40
8100/8100 [==============================] - 1s - loss: 0.3801 - acc: 0.8430 - val_loss: 0.4013 - val_acc: 0.8317
Epoch 32/40
8100/8100 [==============================] - 1s - loss: 0.3794 - acc: 0.8433 - val_loss: 0.3972 - val_acc: 0.8351
Epoch 33/40
8100/8100 [==============================] - 1s - loss: 0.3805 - acc: 0.8446 - val_loss: 0.3998 - val_acc: 0.8317
Epoch 34/40
8100/8100 [==============================] - 1s - loss: 0.3779 - acc: 0.8435 - val_loss: 0.3968 - val_acc: 0.8361
Epoch 35/40
8100/8100 [==============================] - 1s - loss: 0.3760 - acc: 0.8464 - val_loss: 0.3973 - val_acc: 0.8351
Epoch 36/40
8100/8100 [==============================] - 1s - loss: 0.3754 - acc: 0.8464 - val_loss: 0.3997 - val_acc: 0.8302
Epoch 37/40
8100/8100 [==============================] - 1s - loss: 0.3758 - acc: 0.8463 - val_loss: 0.3977 - val_acc: 0.8337
Epoch 38/40
8100/8100 [==============================] - 1s - loss: 0.3782 - acc: 0.8440 - val_loss: 0.4042 - val_acc: 0.8297
Epoch 39/40
8100/8100 [==============================] - 1s - loss: 0.3795 - acc: 0.8454 - val_loss: 0.4031 - val_acc: 0.8292
Epoch 40/40
8100/8100 [==============================] - 1s - loss: 0.3857 - acc: 0.8389 - val_loss: 0.4012 - val_acc: 0.8342
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-27.h5
chunk number 27
prepare data
start training
Train on 8095 samples, validate on 2024 samples
Epoch 1/40
8095/8095 [==============================] - 1s - loss: 0.3843 - acc: 0.8440 - val_loss: 0.4016 - val_acc: 0.8295
Epoch 2/40
8095/8095 [==============================] - 1s - loss: 0.3829 - acc: 0.8429 - val_loss: 0.4104 - val_acc: 0.8246
Epoch 3/40
8095/8095 [==============================] - 1s - loss: 0.3834 - acc: 0.8411 - val_loss: 0.4059 - val_acc: 0.8271
Epoch 4/40
8095/8095 [==============================] - 1s - loss: 0.3802 - acc: 0.8439 - val_loss: 0.4002 - val_acc: 0.8291
Epoch 5/40
8095/8095 [==============================] - 1s - loss: 0.3827 - acc: 0.8442 - val_loss: 0.4042 - val_acc: 0.8286
Epoch 6/40
8095/8095 [==============================] - 1s - loss: 0.3781 - acc: 0.8446 - val_loss: 0.4055 - val_acc: 0.8281
Epoch 7/40
8095/8095 [==============================] - 1s - loss: 0.3780 - acc: 0.8460 - val_loss: 0.3995 - val_acc: 0.8291
Epoch 8/40
8095/8095 [==============================] - 1s - loss: 0.3779 - acc: 0.8474 - val_loss: 0.4012 - val_acc: 0.8320
Epoch 9/40
8095/8095 [==============================] - 1s - loss: 0.3753 - acc: 0.8490 - val_loss: 0.4081 - val_acc: 0.8291
Epoch 10/40
8095/8095 [==============================] - 1s - loss: 0.3765 - acc: 0.8476 - val_loss: 0.4034 - val_acc: 0.8300
Epoch 11/40
8095/8095 [==============================] - 1s - loss: 0.3741 - acc: 0.8492 - val_loss: 0.4018 - val_acc: 0.8320
Epoch 12/40
8095/8095 [==============================] - 1s - loss: 0.3755 - acc: 0.8468 - val_loss: 0.4072 - val_acc: 0.8295
Epoch 13/40
8095/8095 [==============================] - 1s - loss: 0.3735 - acc: 0.8490 - val_loss: 0.4090 - val_acc: 0.8281
Epoch 14/40
8095/8095 [==============================] - 1s - loss: 0.3735 - acc: 0.8490 - val_loss: 0.4044 - val_acc: 0.8295
Epoch 15/40
8095/8095 [==============================] - 1s - loss: 0.3728 - acc: 0.8511 - val_loss: 0.4059 - val_acc: 0.8276
Epoch 16/40
8095/8095 [==============================] - 1s - loss: 0.3714 - acc: 0.8514 - val_loss: 0.4103 - val_acc: 0.8281
Epoch 17/40
8095/8095 [==============================] - 1s - loss: 0.3716 - acc: 0.8493 - val_loss: 0.4071 - val_acc: 0.8291
Epoch 18/40
8095/8095 [==============================] - 1s - loss: 0.3701 - acc: 0.8515 - val_loss: 0.4053 - val_acc: 0.8295
Epoch 19/40
8095/8095 [==============================] - 1s - loss: 0.3703 - acc: 0.8503 - val_loss: 0.4093 - val_acc: 0.8281
Epoch 20/40
8095/8095 [==============================] - 1s - loss: 0.3699 - acc: 0.8497 - val_loss: 0.4068 - val_acc: 0.8300
Epoch 21/40
8095/8095 [==============================] - 1s - loss: 0.3683 - acc: 0.8520 - val_loss: 0.4044 - val_acc: 0.8291
Epoch 22/40
8095/8095 [==============================] - 1s - loss: 0.3688 - acc: 0.8518 - val_loss: 0.4083 - val_acc: 0.8281
Epoch 23/40
8095/8095 [==============================] - 1s - loss: 0.3678 - acc: 0.8506 - val_loss: 0.4078 - val_acc: 0.8286
Epoch 24/40
8095/8095 [==============================] - 1s - loss: 0.3673 - acc: 0.8514 - val_loss: 0.4047 - val_acc: 0.8300
Epoch 25/40
8095/8095 [==============================] - 1s - loss: 0.3674 - acc: 0.8515 - val_loss: 0.4063 - val_acc: 0.8320
Epoch 26/40
8095/8095 [==============================] - 1s - loss: 0.3664 - acc: 0.8524 - val_loss: 0.4083 - val_acc: 0.8281
Epoch 27/40
8095/8095 [==============================] - 1s - loss: 0.3664 - acc: 0.8526 - val_loss: 0.4046 - val_acc: 0.8305
Epoch 28/40
8095/8095 [==============================] - 1s - loss: 0.3658 - acc: 0.8532 - val_loss: 0.4061 - val_acc: 0.8286
Epoch 29/40
8095/8095 [==============================] - 1s - loss: 0.3650 - acc: 0.8541 - val_loss: 0.4073 - val_acc: 0.8295
Epoch 30/40
8095/8095 [==============================] - 1s - loss: 0.3646 - acc: 0.8529 - val_loss: 0.4053 - val_acc: 0.8295
Epoch 31/40
8095/8095 [==============================] - 1s - loss: 0.3642 - acc: 0.8539 - val_loss: 0.4080 - val_acc: 0.8325
Epoch 32/40
8095/8095 [==============================] - 1s - loss: 0.3638 - acc: 0.8551 - val_loss: 0.4067 - val_acc: 0.8335
Epoch 33/40
8095/8095 [==============================] - 1s - loss: 0.3632 - acc: 0.8539 - val_loss: 0.4067 - val_acc: 0.8330
Epoch 34/40
8095/8095 [==============================] - 1s - loss: 0.3628 - acc: 0.8536 - val_loss: 0.4084 - val_acc: 0.8295
Epoch 35/40
8095/8095 [==============================] - 1s - loss: 0.3626 - acc: 0.8542 - val_loss: 0.4064 - val_acc: 0.8300
Epoch 36/40
8095/8095 [==============================] - 1s - loss: 0.3622 - acc: 0.8539 - val_loss: 0.4083 - val_acc: 0.8300
Epoch 37/40
8095/8095 [==============================] - 1s - loss: 0.3617 - acc: 0.8545 - val_loss: 0.4072 - val_acc: 0.8325
Epoch 38/40
8095/8095 [==============================] - 1s - loss: 0.3613 - acc: 0.8542 - val_loss: 0.4089 - val_acc: 0.8310
Epoch 39/40
8095/8095 [==============================] - 1s - loss: 0.3610 - acc: 0.8541 - val_loss: 0.4072 - val_acc: 0.8291
Epoch 40/40
8095/8095 [==============================] - 1s - loss: 0.3610 - acc: 0.8540 - val_loss: 0.4119 - val_acc: 0.8271
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-28.h5
chunk number 28
prepare data
start training
Train on 8135 samples, validate on 2034 samples
Epoch 1/40
8135/8135 [==============================] - 2s - loss: 0.4114 - acc: 0.8310 - val_loss: 0.4286 - val_acc: 0.8206
Epoch 2/40
8135/8135 [==============================] - 2s - loss: 0.4017 - acc: 0.8366 - val_loss: 0.4195 - val_acc: 0.8230
Epoch 3/40
8135/8135 [==============================] - 2s - loss: 0.3930 - acc: 0.8396 - val_loss: 0.4190 - val_acc: 0.8186
Epoch 4/40
8135/8135 [==============================] - 2s - loss: 0.3948 - acc: 0.8363 - val_loss: 0.4217 - val_acc: 0.8230
Epoch 5/40
8135/8135 [==============================] - 2s - loss: 0.3919 - acc: 0.8396 - val_loss: 0.4235 - val_acc: 0.8210
Epoch 6/40
8135/8135 [==============================] - 2s - loss: 0.3910 - acc: 0.8398 - val_loss: 0.4230 - val_acc: 0.8206
Epoch 7/40
8135/8135 [==============================] - 2s - loss: 0.3949 - acc: 0.8374 - val_loss: 0.4220 - val_acc: 0.8215
Epoch 8/40
8135/8135 [==============================] - 2s - loss: 0.3915 - acc: 0.8360 - val_loss: 0.4235 - val_acc: 0.8206
Epoch 9/40
8135/8135 [==============================] - 2s - loss: 0.3925 - acc: 0.8336 - val_loss: 0.4206 - val_acc: 0.8255
Epoch 10/40
8135/8135 [==============================] - 2s - loss: 0.3905 - acc: 0.8382 - val_loss: 0.4183 - val_acc: 0.8250
Epoch 11/40
8135/8135 [==============================] - 2s - loss: 0.3879 - acc: 0.8396 - val_loss: 0.4244 - val_acc: 0.8240
Epoch 12/40
8135/8135 [==============================] - 2s - loss: 0.3886 - acc: 0.8407 - val_loss: 0.4229 - val_acc: 0.8186
Epoch 13/40
8135/8135 [==============================] - 2s - loss: 0.3850 - acc: 0.8403 - val_loss: 0.4246 - val_acc: 0.8186
Epoch 14/40
8135/8135 [==============================] - 2s - loss: 0.3876 - acc: 0.8374 - val_loss: 0.4243 - val_acc: 0.8176
Epoch 15/40
8135/8135 [==============================] - 2s - loss: 0.3859 - acc: 0.8418 - val_loss: 0.4185 - val_acc: 0.8250
Epoch 16/40
8135/8135 [==============================] - 2s - loss: 0.3838 - acc: 0.8406 - val_loss: 0.4188 - val_acc: 0.8210
Epoch 17/40
8135/8135 [==============================] - 2s - loss: 0.3867 - acc: 0.8395 - val_loss: 0.4198 - val_acc: 0.8196
Epoch 18/40
8135/8135 [==============================] - 2s - loss: 0.3855 - acc: 0.8396 - val_loss: 0.4180 - val_acc: 0.8220
Epoch 19/40
8135/8135 [==============================] - 2s - loss: 0.3821 - acc: 0.8413 - val_loss: 0.4217 - val_acc: 0.8171
Epoch 20/40
8135/8135 [==============================] - 2s - loss: 0.3822 - acc: 0.8415 - val_loss: 0.4224 - val_acc: 0.8196
Epoch 21/40
8135/8135 [==============================] - 2s - loss: 0.3832 - acc: 0.8402 - val_loss: 0.4181 - val_acc: 0.8201
Epoch 22/40
8135/8135 [==============================] - 2s - loss: 0.3792 - acc: 0.8433 - val_loss: 0.4148 - val_acc: 0.8235
Epoch 23/40
8135/8135 [==============================] - 2s - loss: 0.3798 - acc: 0.8407 - val_loss: 0.4138 - val_acc: 0.8245
Epoch 24/40
8135/8135 [==============================] - 2s - loss: 0.3800 - acc: 0.8423 - val_loss: 0.4158 - val_acc: 0.8230
Epoch 25/40
8135/8135 [==============================] - 2s - loss: 0.3801 - acc: 0.8404 - val_loss: 0.4155 - val_acc: 0.8181
Epoch 26/40
8135/8135 [==============================] - 2s - loss: 0.3814 - acc: 0.8413 - val_loss: 0.4170 - val_acc: 0.8215
Epoch 27/40
8135/8135 [==============================] - 2s - loss: 0.3793 - acc: 0.8440 - val_loss: 0.4208 - val_acc: 0.8250
Epoch 28/40
8135/8135 [==============================] - 2s - loss: 0.3799 - acc: 0.8423 - val_loss: 0.4155 - val_acc: 0.8215
Epoch 29/40
8135/8135 [==============================] - 2s - loss: 0.3777 - acc: 0.8433 - val_loss: 0.4152 - val_acc: 0.8260
Epoch 30/40
8135/8135 [==============================] - 2s - loss: 0.3774 - acc: 0.8430 - val_loss: 0.4163 - val_acc: 0.8265
Epoch 31/40
8135/8135 [==============================] - 2s - loss: 0.3763 - acc: 0.8440 - val_loss: 0.4189 - val_acc: 0.8201
Epoch 32/40
8135/8135 [==============================] - 2s - loss: 0.3823 - acc: 0.8380 - val_loss: 0.4226 - val_acc: 0.8240
Epoch 33/40
8135/8135 [==============================] - 2s - loss: 0.3814 - acc: 0.8417 - val_loss: 0.4154 - val_acc: 0.8245
Epoch 34/40
8135/8135 [==============================] - 2s - loss: 0.3759 - acc: 0.8434 - val_loss: 0.4158 - val_acc: 0.8250
Epoch 35/40
8135/8135 [==============================] - 2s - loss: 0.3784 - acc: 0.8417 - val_loss: 0.4152 - val_acc: 0.8260
Epoch 36/40
8135/8135 [==============================] - 2s - loss: 0.3781 - acc: 0.8409 - val_loss: 0.4160 - val_acc: 0.8225
Epoch 37/40
8135/8135 [==============================] - 2s - loss: 0.3775 - acc: 0.8422 - val_loss: 0.4199 - val_acc: 0.8225
Epoch 38/40
8135/8135 [==============================] - 2s - loss: 0.3792 - acc: 0.8413 - val_loss: 0.4202 - val_acc: 0.8201
Epoch 39/40
8135/8135 [==============================] - 2s - loss: 0.3785 - acc: 0.8412 - val_loss: 0.4187 - val_acc: 0.8215
Epoch 40/40
8135/8135 [==============================] - 2s - loss: 0.3731 - acc: 0.8451 - val_loss: 0.4210 - val_acc: 0.8255
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-29.h5
chunk number 29
prepare data
start training
Train on 8105 samples, validate on 2027 samples
Epoch 1/40
8105/8105 [==============================] - 1s - loss: 0.4212 - acc: 0.8259 - val_loss: 0.4299 - val_acc: 0.8155
Epoch 2/40
8105/8105 [==============================] - 1s - loss: 0.4097 - acc: 0.8218 - val_loss: 0.4507 - val_acc: 0.8066
Epoch 3/40
8105/8105 [==============================] - 1s - loss: 0.4305 - acc: 0.8114 - val_loss: 0.4326 - val_acc: 0.8189
Epoch 4/40
8105/8105 [==============================] - 1s - loss: 0.4116 - acc: 0.8209 - val_loss: 0.4060 - val_acc: 0.8333
Epoch 5/40
8105/8105 [==============================] - 1s - loss: 0.3886 - acc: 0.8385 - val_loss: 0.4295 - val_acc: 0.8165
Epoch 6/40
8105/8105 [==============================] - 1s - loss: 0.4215 - acc: 0.8181 - val_loss: 0.4077 - val_acc: 0.8268
Epoch 7/40
8105/8105 [==============================] - 1s - loss: 0.3894 - acc: 0.8370 - val_loss: 0.4293 - val_acc: 0.8259
Epoch 8/40
8105/8105 [==============================] - 1s - loss: 0.4077 - acc: 0.8302 - val_loss: 0.4303 - val_acc: 0.8224
Epoch 9/40
8105/8105 [==============================] - 1s - loss: 0.4098 - acc: 0.8279 - val_loss: 0.4116 - val_acc: 0.8259
Epoch 10/40
8105/8105 [==============================] - 1s - loss: 0.3925 - acc: 0.8363 - val_loss: 0.4074 - val_acc: 0.8298
Epoch 11/40
8105/8105 [==============================] - 1s - loss: 0.3963 - acc: 0.8321 - val_loss: 0.4112 - val_acc: 0.8268
Epoch 12/40
8105/8105 [==============================] - 1s - loss: 0.4005 - acc: 0.8326 - val_loss: 0.4055 - val_acc: 0.8293
Epoch 13/40
8105/8105 [==============================] - 1s - loss: 0.3868 - acc: 0.8397 - val_loss: 0.4180 - val_acc: 0.8244
Epoch 14/40
8105/8105 [==============================] - 1s - loss: 0.3971 - acc: 0.8352 - val_loss: 0.4142 - val_acc: 0.8278
Epoch 15/40
8105/8105 [==============================] - 1s - loss: 0.3933 - acc: 0.8379 - val_loss: 0.4024 - val_acc: 0.8362
Epoch 16/40
8105/8105 [==============================] - 1s - loss: 0.3848 - acc: 0.8396 - val_loss: 0.4072 - val_acc: 0.8352
Epoch 17/40
8105/8105 [==============================] - 1s - loss: 0.3946 - acc: 0.8349 - val_loss: 0.4007 - val_acc: 0.8342
Epoch 18/40
8105/8105 [==============================] - 1s - loss: 0.3834 - acc: 0.8410 - val_loss: 0.4068 - val_acc: 0.8283
Epoch 19/40
8105/8105 [==============================] - 1s - loss: 0.3877 - acc: 0.8376 - val_loss: 0.4074 - val_acc: 0.8278
Epoch 20/40
8105/8105 [==============================] - 1s - loss: 0.3887 - acc: 0.8374 - val_loss: 0.3999 - val_acc: 0.8342
Epoch 21/40
8105/8105 [==============================] - 1s - loss: 0.3829 - acc: 0.8426 - val_loss: 0.4007 - val_acc: 0.8352
Epoch 22/40
8105/8105 [==============================] - 1s - loss: 0.3859 - acc: 0.8410 - val_loss: 0.3990 - val_acc: 0.8337
Epoch 23/40
8105/8105 [==============================] - 1s - loss: 0.3824 - acc: 0.8422 - val_loss: 0.4006 - val_acc: 0.8328
Epoch 24/40
8105/8105 [==============================] - 1s - loss: 0.3814 - acc: 0.8400 - val_loss: 0.4045 - val_acc: 0.8323
Epoch 25/40
8105/8105 [==============================] - 1s - loss: 0.3837 - acc: 0.8384 - val_loss: 0.4010 - val_acc: 0.8333
Epoch 26/40
8105/8105 [==============================] - 1s - loss: 0.3803 - acc: 0.8415 - val_loss: 0.4002 - val_acc: 0.8333
Epoch 27/40
8105/8105 [==============================] - 1s - loss: 0.3810 - acc: 0.8397 - val_loss: 0.3991 - val_acc: 0.8337
Epoch 28/40
8105/8105 [==============================] - 1s - loss: 0.3787 - acc: 0.8434 - val_loss: 0.4010 - val_acc: 0.8362
Epoch 29/40
8105/8105 [==============================] - 1s - loss: 0.3782 - acc: 0.8431 - val_loss: 0.4027 - val_acc: 0.8342
Epoch 30/40
8105/8105 [==============================] - 1s - loss: 0.3793 - acc: 0.8424 - val_loss: 0.4008 - val_acc: 0.8352
Epoch 31/40
8105/8105 [==============================] - 1s - loss: 0.3777 - acc: 0.8439 - val_loss: 0.4001 - val_acc: 0.8352
Epoch 32/40
8105/8105 [==============================] - 1s - loss: 0.3777 - acc: 0.8445 - val_loss: 0.3999 - val_acc: 0.8372
Epoch 33/40
8105/8105 [==============================] - 1s - loss: 0.3763 - acc: 0.8442 - val_loss: 0.4019 - val_acc: 0.8333
Epoch 34/40
8105/8105 [==============================] - 1s - loss: 0.3761 - acc: 0.8433 - val_loss: 0.4030 - val_acc: 0.8337
Epoch 35/40
8105/8105 [==============================] - 1s - loss: 0.3765 - acc: 0.8422 - val_loss: 0.4012 - val_acc: 0.8357
Epoch 36/40
8105/8105 [==============================] - 1s - loss: 0.3752 - acc: 0.8432 - val_loss: 0.4001 - val_acc: 0.8377
Epoch 37/40
8105/8105 [==============================] - 1s - loss: 0.3750 - acc: 0.8433 - val_loss: 0.4006 - val_acc: 0.8367
Epoch 38/40
8105/8105 [==============================] - 1s - loss: 0.3738 - acc: 0.8459 - val_loss: 0.4013 - val_acc: 0.8347
Epoch 39/40
8105/8105 [==============================] - 1s - loss: 0.3745 - acc: 0.8457 - val_loss: 0.3999 - val_acc: 0.8367
Epoch 40/40
8105/8105 [==============================] - 1s - loss: 0.3735 - acc: 0.8448 - val_loss: 0.3990 - val_acc: 0.8372
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-30.h5
chunk number 30
prepare data
start training
Train on 8185 samples, validate on 2047 samples
Epoch 1/40
8185/8185 [==============================] - 1s - loss: 0.3842 - acc: 0.8437 - val_loss: 0.3697 - val_acc: 0.8451
Epoch 2/40
8185/8185 [==============================] - 1s - loss: 0.3842 - acc: 0.8441 - val_loss: 0.3684 - val_acc: 0.8490
Epoch 3/40
8185/8185 [==============================] - 1s - loss: 0.3833 - acc: 0.8441 - val_loss: 0.3672 - val_acc: 0.8525
Epoch 4/40
8185/8185 [==============================] - 1s - loss: 0.3828 - acc: 0.8430 - val_loss: 0.3668 - val_acc: 0.8515
Epoch 5/40
8185/8185 [==============================] - 1s - loss: 0.3817 - acc: 0.8441 - val_loss: 0.3674 - val_acc: 0.8534
Epoch 6/40
8185/8185 [==============================] - 1s - loss: 0.3815 - acc: 0.8439 - val_loss: 0.3660 - val_acc: 0.8544
Epoch 7/40
8185/8185 [==============================] - 1s - loss: 0.3809 - acc: 0.8434 - val_loss: 0.3655 - val_acc: 0.8554
Epoch 8/40
8185/8185 [==============================] - 1s - loss: 0.3807 - acc: 0.8436 - val_loss: 0.3665 - val_acc: 0.8539
Epoch 9/40
8185/8185 [==============================] - 1s - loss: 0.3801 - acc: 0.8431 - val_loss: 0.3676 - val_acc: 0.8559
Epoch 10/40
8185/8185 [==============================] - 1s - loss: 0.3799 - acc: 0.8434 - val_loss: 0.3667 - val_acc: 0.8534
Epoch 11/40
8185/8185 [==============================] - 1s - loss: 0.3793 - acc: 0.8437 - val_loss: 0.3665 - val_acc: 0.8554
Epoch 12/40
8185/8185 [==============================] - 1s - loss: 0.3789 - acc: 0.8436 - val_loss: 0.3671 - val_acc: 0.8534
Epoch 13/40
8185/8185 [==============================] - 1s - loss: 0.3783 - acc: 0.8439 - val_loss: 0.3666 - val_acc: 0.8539
Epoch 14/40
8185/8185 [==============================] - 1s - loss: 0.3778 - acc: 0.8435 - val_loss: 0.3657 - val_acc: 0.8549
Epoch 15/40
8185/8185 [==============================] - 1s - loss: 0.3774 - acc: 0.8448 - val_loss: 0.3658 - val_acc: 0.8564
Epoch 16/40
8185/8185 [==============================] - 1s - loss: 0.3769 - acc: 0.8446 - val_loss: 0.3664 - val_acc: 0.8564
Epoch 17/40
8185/8185 [==============================] - 1s - loss: 0.3766 - acc: 0.8453 - val_loss: 0.3659 - val_acc: 0.8578
Epoch 18/40
8185/8185 [==============================] - 1s - loss: 0.3761 - acc: 0.8453 - val_loss: 0.3656 - val_acc: 0.8559
Epoch 19/40
8185/8185 [==============================] - 1s - loss: 0.3757 - acc: 0.8451 - val_loss: 0.3663 - val_acc: 0.8554
Epoch 20/40
8185/8185 [==============================] - 1s - loss: 0.3753 - acc: 0.8448 - val_loss: 0.3658 - val_acc: 0.8549
Epoch 21/40
8185/8185 [==============================] - 1s - loss: 0.3749 - acc: 0.8457 - val_loss: 0.3653 - val_acc: 0.8554
Epoch 22/40
8185/8185 [==============================] - 1s - loss: 0.3744 - acc: 0.8459 - val_loss: 0.3659 - val_acc: 0.8544
Epoch 23/40
8185/8185 [==============================] - 1s - loss: 0.3740 - acc: 0.8452 - val_loss: 0.3654 - val_acc: 0.8559
Epoch 24/40
8185/8185 [==============================] - 1s - loss: 0.3736 - acc: 0.8457 - val_loss: 0.3650 - val_acc: 0.8549
Epoch 25/40
8185/8185 [==============================] - 1s - loss: 0.3732 - acc: 0.8465 - val_loss: 0.3653 - val_acc: 0.8549
Epoch 26/40
8185/8185 [==============================] - 1s - loss: 0.3728 - acc: 0.8459 - val_loss: 0.3647 - val_acc: 0.8554
Epoch 27/40
8185/8185 [==============================] - 1s - loss: 0.3724 - acc: 0.8470 - val_loss: 0.3646 - val_acc: 0.8549
Epoch 28/40
8185/8185 [==============================] - 1s - loss: 0.3720 - acc: 0.8470 - val_loss: 0.3648 - val_acc: 0.8539
Epoch 29/40
8185/8185 [==============================] - 1s - loss: 0.3716 - acc: 0.8468 - val_loss: 0.3642 - val_acc: 0.8544
Epoch 30/40
8185/8185 [==============================] - 1s - loss: 0.3713 - acc: 0.8468 - val_loss: 0.3644 - val_acc: 0.8544
Epoch 31/40
8185/8185 [==============================] - 1s - loss: 0.3709 - acc: 0.8476 - val_loss: 0.3641 - val_acc: 0.8549
Epoch 32/40
8185/8185 [==============================] - 1s - loss: 0.3705 - acc: 0.8484 - val_loss: 0.3641 - val_acc: 0.8554
Epoch 33/40
8185/8185 [==============================] - 1s - loss: 0.3702 - acc: 0.8480 - val_loss: 0.3643 - val_acc: 0.8549
Epoch 34/40
8185/8185 [==============================] - 1s - loss: 0.3698 - acc: 0.8481 - val_loss: 0.3642 - val_acc: 0.8539
Epoch 35/40
8185/8185 [==============================] - 1s - loss: 0.3695 - acc: 0.8484 - val_loss: 0.3646 - val_acc: 0.8564
Epoch 36/40
8185/8185 [==============================] - 1s - loss: 0.3692 - acc: 0.8483 - val_loss: 0.3639 - val_acc: 0.8549
Epoch 37/40
8185/8185 [==============================] - 1s - loss: 0.3689 - acc: 0.8486 - val_loss: 0.3649 - val_acc: 0.8544
Epoch 38/40
8185/8185 [==============================] - 1s - loss: 0.3687 - acc: 0.8487 - val_loss: 0.3636 - val_acc: 0.8564
Epoch 39/40
8185/8185 [==============================] - 1s - loss: 0.3688 - acc: 0.8487 - val_loss: 0.3665 - val_acc: 0.8525
Epoch 40/40
8185/8185 [==============================] - 1s - loss: 0.3690 - acc: 0.8484 - val_loss: 0.3640 - val_acc: 0.8549
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-31.h5
chunk number 31
prepare data
start training
Train on 8079 samples, validate on 2020 samples
Epoch 1/40
8079/8079 [==============================] - 1s - loss: 0.3819 - acc: 0.8473 - val_loss: 0.3937 - val_acc: 0.8426
Epoch 2/40
8079/8079 [==============================] - 1s - loss: 0.3793 - acc: 0.8469 - val_loss: 0.4001 - val_acc: 0.8411
Epoch 3/40
8079/8079 [==============================] - 1s - loss: 0.3797 - acc: 0.8456 - val_loss: 0.3933 - val_acc: 0.8421
Epoch 4/40
8079/8079 [==============================] - 1s - loss: 0.3760 - acc: 0.8485 - val_loss: 0.3932 - val_acc: 0.8436
Epoch 5/40
8079/8079 [==============================] - 1s - loss: 0.3764 - acc: 0.8484 - val_loss: 0.3968 - val_acc: 0.8411
Epoch 6/40
8079/8079 [==============================] - 1s - loss: 0.3745 - acc: 0.8497 - val_loss: 0.3985 - val_acc: 0.8411
Epoch 7/40
8079/8079 [==============================] - 1s - loss: 0.3743 - acc: 0.8482 - val_loss: 0.3953 - val_acc: 0.8426
Epoch 8/40
8079/8079 [==============================] - 1s - loss: 0.3738 - acc: 0.8492 - val_loss: 0.3962 - val_acc: 0.8426
Epoch 9/40
8079/8079 [==============================] - 1s - loss: 0.3728 - acc: 0.8495 - val_loss: 0.3995 - val_acc: 0.8386
Epoch 10/40
8079/8079 [==============================] - 1s - loss: 0.3728 - acc: 0.8482 - val_loss: 0.3967 - val_acc: 0.8401
Epoch 11/40
8079/8079 [==============================] - 1s - loss: 0.3708 - acc: 0.8501 - val_loss: 0.3959 - val_acc: 0.8416
Epoch 12/40
8079/8079 [==============================] - 1s - loss: 0.3707 - acc: 0.8507 - val_loss: 0.3967 - val_acc: 0.8406
Epoch 13/40
8079/8079 [==============================] - 1s - loss: 0.3695 - acc: 0.8495 - val_loss: 0.3967 - val_acc: 0.8416
Epoch 14/40
8079/8079 [==============================] - 1s - loss: 0.3690 - acc: 0.8501 - val_loss: 0.3945 - val_acc: 0.8426
Epoch 15/40
8079/8079 [==============================] - 1s - loss: 0.3684 - acc: 0.8518 - val_loss: 0.3950 - val_acc: 0.8416
Epoch 16/40
8079/8079 [==============================] - 1s - loss: 0.3675 - acc: 0.8516 - val_loss: 0.3975 - val_acc: 0.8411
Epoch 17/40
8079/8079 [==============================] - 1s - loss: 0.3676 - acc: 0.8504 - val_loss: 0.3948 - val_acc: 0.8411
Epoch 18/40
8079/8079 [==============================] - 1s - loss: 0.3670 - acc: 0.8517 - val_loss: 0.3957 - val_acc: 0.8396
Epoch 19/40
8079/8079 [==============================] - 1s - loss: 0.3658 - acc: 0.8516 - val_loss: 0.3980 - val_acc: 0.8391
Epoch 20/40
8079/8079 [==============================] - 1s - loss: 0.3661 - acc: 0.8522 - val_loss: 0.3960 - val_acc: 0.8401
Epoch 21/40
8079/8079 [==============================] - 1s - loss: 0.3657 - acc: 0.8528 - val_loss: 0.3971 - val_acc: 0.8406
Epoch 22/40
8079/8079 [==============================] - 1s - loss: 0.3646 - acc: 0.8530 - val_loss: 0.3980 - val_acc: 0.8401
Epoch 23/40
8079/8079 [==============================] - 1s - loss: 0.3644 - acc: 0.8525 - val_loss: 0.3964 - val_acc: 0.8416
Epoch 24/40
8079/8079 [==============================] - 1s - loss: 0.3644 - acc: 0.8536 - val_loss: 0.3983 - val_acc: 0.8411
Epoch 25/40
8079/8079 [==============================] - 1s - loss: 0.3637 - acc: 0.8528 - val_loss: 0.3972 - val_acc: 0.8421
Epoch 26/40
8079/8079 [==============================] - 1s - loss: 0.3628 - acc: 0.8536 - val_loss: 0.3974 - val_acc: 0.8421
Epoch 27/40
8079/8079 [==============================] - 1s - loss: 0.3624 - acc: 0.8539 - val_loss: 0.3997 - val_acc: 0.8386
Epoch 28/40
8079/8079 [==============================] - 1s - loss: 0.3624 - acc: 0.8538 - val_loss: 0.3974 - val_acc: 0.8421
Epoch 29/40
8079/8079 [==============================] - 1s - loss: 0.3623 - acc: 0.8542 - val_loss: 0.4009 - val_acc: 0.8411
Epoch 30/40
8079/8079 [==============================] - 1s - loss: 0.3621 - acc: 0.8539 - val_loss: 0.3972 - val_acc: 0.8416
Epoch 31/40
8079/8079 [==============================] - 1s - loss: 0.3618 - acc: 0.8551 - val_loss: 0.4002 - val_acc: 0.8416
Epoch 32/40
8079/8079 [==============================] - 1s - loss: 0.3610 - acc: 0.8549 - val_loss: 0.3972 - val_acc: 0.8416
Epoch 33/40
8079/8079 [==============================] - 1s - loss: 0.3599 - acc: 0.8557 - val_loss: 0.3968 - val_acc: 0.8431
Epoch 34/40
8079/8079 [==============================] - 1s - loss: 0.3594 - acc: 0.8551 - val_loss: 0.3990 - val_acc: 0.8406
Epoch 35/40
8079/8079 [==============================] - 1s - loss: 0.3598 - acc: 0.8560 - val_loss: 0.3952 - val_acc: 0.8436
Epoch 36/40
8079/8079 [==============================] - 1s - loss: 0.3604 - acc: 0.8544 - val_loss: 0.4010 - val_acc: 0.8411
Epoch 37/40
8079/8079 [==============================] - 1s - loss: 0.3604 - acc: 0.8544 - val_loss: 0.3966 - val_acc: 0.8436
Epoch 38/40
8079/8079 [==============================] - 1s - loss: 0.3621 - acc: 0.8537 - val_loss: 0.4002 - val_acc: 0.8396
Epoch 39/40
8079/8079 [==============================] - 1s - loss: 0.3589 - acc: 0.8560 - val_loss: 0.3960 - val_acc: 0.8406
Epoch 40/40
8079/8079 [==============================] - 1s - loss: 0.3579 - acc: 0.8556 - val_loss: 0.3987 - val_acc: 0.8411
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-32.h5
chunk number 32
prepare data
start training
Train on 8130 samples, validate on 2033 samples
Epoch 1/40
8130/8130 [==============================] - 1s - loss: 0.3812 - acc: 0.8444 - val_loss: 0.3693 - val_acc: 0.8549
Epoch 2/40
8130/8130 [==============================] - 1s - loss: 0.3785 - acc: 0.8448 - val_loss: 0.3680 - val_acc: 0.8534
Epoch 3/40
8130/8130 [==============================] - 1s - loss: 0.3770 - acc: 0.8455 - val_loss: 0.3694 - val_acc: 0.8588
Epoch 4/40
8130/8130 [==============================] - 1s - loss: 0.3764 - acc: 0.8462 - val_loss: 0.3651 - val_acc: 0.8549
Epoch 5/40
8130/8130 [==============================] - 1s - loss: 0.3726 - acc: 0.8481 - val_loss: 0.3682 - val_acc: 0.8485
Epoch 6/40
8130/8130 [==============================] - 1s - loss: 0.3745 - acc: 0.8469 - val_loss: 0.3654 - val_acc: 0.8534
Epoch 7/40
8130/8130 [==============================] - 1s - loss: 0.3705 - acc: 0.8506 - val_loss: 0.3693 - val_acc: 0.8554
Epoch 8/40
8130/8130 [==============================] - 1s - loss: 0.3719 - acc: 0.8501 - val_loss: 0.3659 - val_acc: 0.8544
Epoch 9/40
8130/8130 [==============================] - 1s - loss: 0.3694 - acc: 0.8506 - val_loss: 0.3652 - val_acc: 0.8539
Epoch 10/40
8130/8130 [==============================] - 1s - loss: 0.3674 - acc: 0.8528 - val_loss: 0.3687 - val_acc: 0.8549
Epoch 11/40
8130/8130 [==============================] - 1s - loss: 0.3693 - acc: 0.8508 - val_loss: 0.3666 - val_acc: 0.8524
Epoch 12/40
8130/8130 [==============================] - 1s - loss: 0.3667 - acc: 0.8538 - val_loss: 0.3665 - val_acc: 0.8510
Epoch 13/40
8130/8130 [==============================] - 1s - loss: 0.3654 - acc: 0.8529 - val_loss: 0.3689 - val_acc: 0.8554
Epoch 14/40
8130/8130 [==============================] - 1s - loss: 0.3667 - acc: 0.8506 - val_loss: 0.3666 - val_acc: 0.8515
Epoch 15/40
8130/8130 [==============================] - 1s - loss: 0.3642 - acc: 0.8539 - val_loss: 0.3661 - val_acc: 0.8519
Epoch 16/40
8130/8130 [==============================] - 1s - loss: 0.3633 - acc: 0.8550 - val_loss: 0.3679 - val_acc: 0.8569
Epoch 17/40
8130/8130 [==============================] - 1s - loss: 0.3641 - acc: 0.8526 - val_loss: 0.3670 - val_acc: 0.8510
Epoch 18/40
8130/8130 [==============================] - 1s - loss: 0.3639 - acc: 0.8523 - val_loss: 0.3670 - val_acc: 0.8529
Epoch 19/40
8130/8130 [==============================] - 1s - loss: 0.3618 - acc: 0.8544 - val_loss: 0.3693 - val_acc: 0.8559
Epoch 20/40
8130/8130 [==============================] - 1s - loss: 0.3629 - acc: 0.8535 - val_loss: 0.3684 - val_acc: 0.8485
Epoch 21/40
8130/8130 [==============================] - 1s - loss: 0.3634 - acc: 0.8517 - val_loss: 0.3674 - val_acc: 0.8544
Epoch 22/40
8130/8130 [==============================] - 1s - loss: 0.3604 - acc: 0.8557 - val_loss: 0.3694 - val_acc: 0.8544
Epoch 23/40
8130/8130 [==============================] - 1s - loss: 0.3622 - acc: 0.8538 - val_loss: 0.3688 - val_acc: 0.8519
Epoch 24/40
8130/8130 [==============================] - 1s - loss: 0.3620 - acc: 0.8517 - val_loss: 0.3678 - val_acc: 0.8554
Epoch 25/40
8130/8130 [==============================] - 1s - loss: 0.3592 - acc: 0.8565 - val_loss: 0.3704 - val_acc: 0.8554
Epoch 26/40
8130/8130 [==============================] - 1s - loss: 0.3615 - acc: 0.8547 - val_loss: 0.3690 - val_acc: 0.8519
Epoch 27/40
8130/8130 [==============================] - 1s - loss: 0.3608 - acc: 0.8518 - val_loss: 0.3680 - val_acc: 0.8544
Epoch 28/40
8130/8130 [==============================] - 1s - loss: 0.3580 - acc: 0.8558 - val_loss: 0.3710 - val_acc: 0.8515
Epoch 29/40
8130/8130 [==============================] - 1s - loss: 0.3611 - acc: 0.8533 - val_loss: 0.3691 - val_acc: 0.8544
Epoch 30/40
8130/8130 [==============================] - 1s - loss: 0.3604 - acc: 0.8524 - val_loss: 0.3677 - val_acc: 0.8534
Epoch 31/40
8130/8130 [==============================] - 1s - loss: 0.3569 - acc: 0.8565 - val_loss: 0.3714 - val_acc: 0.8510
Epoch 32/40
8130/8130 [==============================] - 1s - loss: 0.3601 - acc: 0.8538 - val_loss: 0.3697 - val_acc: 0.8515
Epoch 33/40
8130/8130 [==============================] - 1s - loss: 0.3599 - acc: 0.8519 - val_loss: 0.3686 - val_acc: 0.8524
Epoch 34/40
8130/8130 [==============================] - 1s - loss: 0.3557 - acc: 0.8566 - val_loss: 0.3724 - val_acc: 0.8519
Epoch 35/40
8130/8130 [==============================] - 1s - loss: 0.3592 - acc: 0.8540 - val_loss: 0.3704 - val_acc: 0.8505
Epoch 36/40
8130/8130 [==============================] - 1s - loss: 0.3597 - acc: 0.8538 - val_loss: 0.3687 - val_acc: 0.8534
Epoch 37/40
8130/8130 [==============================] - 1s - loss: 0.3547 - acc: 0.8574 - val_loss: 0.3733 - val_acc: 0.8524
Epoch 38/40
8130/8130 [==============================] - 1s - loss: 0.3587 - acc: 0.8536 - val_loss: 0.3712 - val_acc: 0.8465
Epoch 39/40
8130/8130 [==============================] - 1s - loss: 0.3606 - acc: 0.8520 - val_loss: 0.3691 - val_acc: 0.8544
Epoch 40/40
8130/8130 [==============================] - 1s - loss: 0.3538 - acc: 0.8576 - val_loss: 0.3763 - val_acc: 0.8519
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-33.h5
chunk number 33
prepare data
start training
Train on 8214 samples, validate on 2054 samples
Epoch 1/40
8214/8214 [==============================] - 1s - loss: 0.3871 - acc: 0.8421 - val_loss: 0.3884 - val_acc: 0.8442
Epoch 2/40
8214/8214 [==============================] - 1s - loss: 0.3802 - acc: 0.8451 - val_loss: 0.3921 - val_acc: 0.8374
Epoch 3/40
8214/8214 [==============================] - 1s - loss: 0.3845 - acc: 0.8411 - val_loss: 0.3860 - val_acc: 0.8457
Epoch 4/40
8214/8214 [==============================] - 1s - loss: 0.3773 - acc: 0.8470 - val_loss: 0.3961 - val_acc: 0.8369
Epoch 5/40
8214/8214 [==============================] - 1s - loss: 0.3827 - acc: 0.8440 - val_loss: 0.3843 - val_acc: 0.8447
Epoch 6/40
8214/8214 [==============================] - 1s - loss: 0.3754 - acc: 0.8484 - val_loss: 0.3873 - val_acc: 0.8413
Epoch 7/40
8214/8214 [==============================] - 1s - loss: 0.3789 - acc: 0.8445 - val_loss: 0.3844 - val_acc: 0.8413
Epoch 8/40
8214/8214 [==============================] - 1s - loss: 0.3749 - acc: 0.8476 - val_loss: 0.3906 - val_acc: 0.8427
Epoch 9/40
8214/8214 [==============================] - 1s - loss: 0.3767 - acc: 0.8481 - val_loss: 0.3868 - val_acc: 0.8413
Epoch 10/40
8214/8214 [==============================] - 1s - loss: 0.3731 - acc: 0.8507 - val_loss: 0.3863 - val_acc: 0.8423
Epoch 11/40
8214/8214 [==============================] - 1s - loss: 0.3736 - acc: 0.8484 - val_loss: 0.3866 - val_acc: 0.8418
Epoch 12/40
8214/8214 [==============================] - 1s - loss: 0.3724 - acc: 0.8489 - val_loss: 0.3889 - val_acc: 0.8398
Epoch 13/40
8214/8214 [==============================] - 1s - loss: 0.3711 - acc: 0.8496 - val_loss: 0.3900 - val_acc: 0.8389
Epoch 14/40
8214/8214 [==============================] - 1s - loss: 0.3708 - acc: 0.8498 - val_loss: 0.3874 - val_acc: 0.8403
Epoch 15/40
8214/8214 [==============================] - 1s - loss: 0.3699 - acc: 0.8490 - val_loss: 0.3880 - val_acc: 0.8393
Epoch 16/40
8214/8214 [==============================] - 1s - loss: 0.3697 - acc: 0.8492 - val_loss: 0.3903 - val_acc: 0.8418
Epoch 17/40
8214/8214 [==============================] - 1s - loss: 0.3683 - acc: 0.8500 - val_loss: 0.3919 - val_acc: 0.8403
Epoch 18/40
8214/8214 [==============================] - 1s - loss: 0.3683 - acc: 0.8495 - val_loss: 0.3894 - val_acc: 0.8413
Epoch 19/40
8214/8214 [==============================] - 1s - loss: 0.3676 - acc: 0.8514 - val_loss: 0.3899 - val_acc: 0.8418
Epoch 20/40
8214/8214 [==============================] - 1s - loss: 0.3671 - acc: 0.8506 - val_loss: 0.3931 - val_acc: 0.8413
Epoch 21/40
8214/8214 [==============================] - 1s - loss: 0.3671 - acc: 0.8510 - val_loss: 0.3911 - val_acc: 0.8427
Epoch 22/40
8214/8214 [==============================] - 1s - loss: 0.3655 - acc: 0.8516 - val_loss: 0.3900 - val_acc: 0.8423
Epoch 23/40
8214/8214 [==============================] - 1s - loss: 0.3664 - acc: 0.8503 - val_loss: 0.3916 - val_acc: 0.8442
Epoch 24/40
8214/8214 [==============================] - 1s - loss: 0.3647 - acc: 0.8516 - val_loss: 0.3929 - val_acc: 0.8437
Epoch 25/40
8214/8214 [==============================] - 1s - loss: 0.3646 - acc: 0.8517 - val_loss: 0.3904 - val_acc: 0.8418
Epoch 26/40
8214/8214 [==============================] - 1s - loss: 0.3646 - acc: 0.8518 - val_loss: 0.3916 - val_acc: 0.8423
Epoch 27/40
8214/8214 [==============================] - 1s - loss: 0.3631 - acc: 0.8522 - val_loss: 0.3942 - val_acc: 0.8423
Epoch 28/40
8214/8214 [==============================] - 1s - loss: 0.3636 - acc: 0.8554 - val_loss: 0.3906 - val_acc: 0.8423
Epoch 29/40
8214/8214 [==============================] - 1s - loss: 0.3632 - acc: 0.8518 - val_loss: 0.3914 - val_acc: 0.8427
Epoch 30/40
8214/8214 [==============================] - 1s - loss: 0.3618 - acc: 0.8528 - val_loss: 0.3936 - val_acc: 0.8442
Epoch 31/40
8214/8214 [==============================] - 1s - loss: 0.3625 - acc: 0.8543 - val_loss: 0.3901 - val_acc: 0.8423
Epoch 32/40
8214/8214 [==============================] - 1s - loss: 0.3624 - acc: 0.8532 - val_loss: 0.3913 - val_acc: 0.8432
Epoch 33/40
8214/8214 [==============================] - 1s - loss: 0.3607 - acc: 0.8554 - val_loss: 0.3932 - val_acc: 0.8442
Epoch 34/40
8214/8214 [==============================] - 1s - loss: 0.3611 - acc: 0.8552 - val_loss: 0.3908 - val_acc: 0.8442
Epoch 35/40
8214/8214 [==============================] - 1s - loss: 0.3617 - acc: 0.8528 - val_loss: 0.3931 - val_acc: 0.8447
Epoch 36/40
8214/8214 [==============================] - 1s - loss: 0.3599 - acc: 0.8555 - val_loss: 0.3929 - val_acc: 0.8447
Epoch 37/40
8214/8214 [==============================] - 1s - loss: 0.3594 - acc: 0.8561 - val_loss: 0.3910 - val_acc: 0.8432
Epoch 38/40
8214/8214 [==============================] - 1s - loss: 0.3603 - acc: 0.8527 - val_loss: 0.3944 - val_acc: 0.8432
Epoch 39/40
8214/8214 [==============================] - 1s - loss: 0.3597 - acc: 0.8563 - val_loss: 0.3911 - val_acc: 0.8437
Epoch 40/40
8214/8214 [==============================] - 1s - loss: 0.3589 - acc: 0.8538 - val_loss: 0.3924 - val_acc: 0.8442
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-34.h5
chunk number 34
prepare data
start training
Train on 8084 samples, validate on 2021 samples
Epoch 1/40
8084/8084 [==============================] - 2s - loss: 0.4188 - acc: 0.8311 - val_loss: 0.3702 - val_acc: 0.8471
Epoch 2/40
8084/8084 [==============================] - 2s - loss: 0.4225 - acc: 0.8189 - val_loss: 0.4181 - val_acc: 0.8352
Epoch 3/40
8084/8084 [==============================] - 2s - loss: 0.4394 - acc: 0.8215 - val_loss: 0.3965 - val_acc: 0.8407
Epoch 4/40
8084/8084 [==============================] - 2s - loss: 0.4161 - acc: 0.8274 - val_loss: 0.3893 - val_acc: 0.8372
Epoch 5/40
8084/8084 [==============================] - 2s - loss: 0.4149 - acc: 0.8286 - val_loss: 0.3935 - val_acc: 0.8417
Epoch 6/40
8084/8084 [==============================] - 2s - loss: 0.4182 - acc: 0.8217 - val_loss: 0.3770 - val_acc: 0.8441
Epoch 7/40
8084/8084 [==============================] - 2s - loss: 0.4009 - acc: 0.8325 - val_loss: 0.3834 - val_acc: 0.8417
Epoch 8/40
8084/8084 [==============================] - 2s - loss: 0.4068 - acc: 0.8282 - val_loss: 0.3840 - val_acc: 0.8407
Epoch 9/40
8084/8084 [==============================] - 2s - loss: 0.4091 - acc: 0.8293 - val_loss: 0.3762 - val_acc: 0.8407
Epoch 10/40
8084/8084 [==============================] - 2s - loss: 0.4033 - acc: 0.8293 - val_loss: 0.3787 - val_acc: 0.8392
Epoch 11/40
8084/8084 [==============================] - 2s - loss: 0.4057 - acc: 0.8263 - val_loss: 0.3733 - val_acc: 0.8431
Epoch 12/40
8084/8084 [==============================] - 2s - loss: 0.4000 - acc: 0.8271 - val_loss: 0.3630 - val_acc: 0.8476
Epoch 13/40
8084/8084 [==============================] - 2s - loss: 0.3910 - acc: 0.8349 - val_loss: 0.3661 - val_acc: 0.8451
Epoch 14/40
8084/8084 [==============================] - 2s - loss: 0.3927 - acc: 0.8358 - val_loss: 0.3686 - val_acc: 0.8427
Epoch 15/40
8084/8084 [==============================] - 2s - loss: 0.3926 - acc: 0.8337 - val_loss: 0.3719 - val_acc: 0.8486
Epoch 16/40
8084/8084 [==============================] - 2s - loss: 0.3938 - acc: 0.8381 - val_loss: 0.3715 - val_acc: 0.8471
Epoch 17/40
8084/8084 [==============================] - 2s - loss: 0.3931 - acc: 0.8383 - val_loss: 0.3635 - val_acc: 0.8521
Epoch 18/40
8084/8084 [==============================] - 2s - loss: 0.3886 - acc: 0.8381 - val_loss: 0.3652 - val_acc: 0.8496
Epoch 19/40
8084/8084 [==============================] - 2s - loss: 0.3903 - acc: 0.8383 - val_loss: 0.3643 - val_acc: 0.8451
Epoch 20/40
8084/8084 [==============================] - 2s - loss: 0.3885 - acc: 0.8392 - val_loss: 0.3673 - val_acc: 0.8476
Epoch 21/40
8084/8084 [==============================] - 2s - loss: 0.3901 - acc: 0.8384 - val_loss: 0.3645 - val_acc: 0.8476
Epoch 22/40
8084/8084 [==============================] - 2s - loss: 0.3870 - acc: 0.8407 - val_loss: 0.3631 - val_acc: 0.8481
Epoch 23/40
8084/8084 [==============================] - 2s - loss: 0.3853 - acc: 0.8392 - val_loss: 0.3632 - val_acc: 0.8456
Epoch 24/40
8084/8084 [==============================] - 2s - loss: 0.3851 - acc: 0.8393 - val_loss: 0.3609 - val_acc: 0.8535
Epoch 25/40
8084/8084 [==============================] - 2s - loss: 0.3832 - acc: 0.8396 - val_loss: 0.3623 - val_acc: 0.8511
Epoch 26/40
8084/8084 [==============================] - 2s - loss: 0.3840 - acc: 0.8412 - val_loss: 0.3597 - val_acc: 0.8516
Epoch 27/40
8084/8084 [==============================] - 2s - loss: 0.3818 - acc: 0.8412 - val_loss: 0.3596 - val_acc: 0.8506
Epoch 28/40
8084/8084 [==============================] - 2s - loss: 0.3811 - acc: 0.8413 - val_loss: 0.3591 - val_acc: 0.8506
Epoch 29/40
8084/8084 [==============================] - 2s - loss: 0.3807 - acc: 0.8433 - val_loss: 0.3586 - val_acc: 0.8486
Epoch 30/40
8084/8084 [==============================] - 2s - loss: 0.3796 - acc: 0.8438 - val_loss: 0.3587 - val_acc: 0.8491
Epoch 31/40
8084/8084 [==============================] - 2s - loss: 0.3799 - acc: 0.8438 - val_loss: 0.3574 - val_acc: 0.8535
Epoch 32/40
8084/8084 [==============================] - 2s - loss: 0.3789 - acc: 0.8430 - val_loss: 0.3564 - val_acc: 0.8540
Epoch 33/40
8084/8084 [==============================] - 2s - loss: 0.3779 - acc: 0.8436 - val_loss: 0.3563 - val_acc: 0.8516
Epoch 34/40
8084/8084 [==============================] - 2s - loss: 0.3771 - acc: 0.8433 - val_loss: 0.3552 - val_acc: 0.8530
Epoch 35/40
8084/8084 [==============================] - 2s - loss: 0.3770 - acc: 0.8430 - val_loss: 0.3547 - val_acc: 0.8525
Epoch 36/40
8084/8084 [==============================] - 2s - loss: 0.3769 - acc: 0.8439 - val_loss: 0.3552 - val_acc: 0.8530
Epoch 37/40
8084/8084 [==============================] - 2s - loss: 0.3763 - acc: 0.8444 - val_loss: 0.3540 - val_acc: 0.8506
Epoch 38/40
8084/8084 [==============================] - 2s - loss: 0.3747 - acc: 0.8472 - val_loss: 0.3545 - val_acc: 0.8496
Epoch 39/40
8084/8084 [==============================] - 2s - loss: 0.3753 - acc: 0.8466 - val_loss: 0.3544 - val_acc: 0.8530
Epoch 40/40
8084/8084 [==============================] - 2s - loss: 0.3751 - acc: 0.8457 - val_loss: 0.3536 - val_acc: 0.8521
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-35.h5
chunk number 35
prepare data
start training
Train on 8199 samples, validate on 2050 samples
Epoch 1/40
8199/8199 [==============================] - 1s - loss: 0.3961 - acc: 0.8391 - val_loss: 0.3921 - val_acc: 0.8322
Epoch 2/40
8199/8199 [==============================] - 1s - loss: 0.3884 - acc: 0.8401 - val_loss: 0.3980 - val_acc: 0.8263
Epoch 3/40
8199/8199 [==============================] - 1s - loss: 0.3909 - acc: 0.8375 - val_loss: 0.3916 - val_acc: 0.8346
Epoch 4/40
8199/8199 [==============================] - 1s - loss: 0.3836 - acc: 0.8429 - val_loss: 0.3905 - val_acc: 0.8337
Epoch 5/40
8199/8199 [==============================] - 1s - loss: 0.3903 - acc: 0.8412 - val_loss: 0.3886 - val_acc: 0.8351
Epoch 6/40
8199/8199 [==============================] - 1s - loss: 0.3834 - acc: 0.8450 - val_loss: 0.3937 - val_acc: 0.8322
Epoch 7/40
8199/8199 [==============================] - 1s - loss: 0.3850 - acc: 0.8407 - val_loss: 0.3897 - val_acc: 0.8327
Epoch 8/40
8199/8199 [==============================] - 1s - loss: 0.3827 - acc: 0.8432 - val_loss: 0.3820 - val_acc: 0.8346
Epoch 9/40
8199/8199 [==============================] - 1s - loss: 0.3805 - acc: 0.8418 - val_loss: 0.3818 - val_acc: 0.8361
Epoch 10/40
8199/8199 [==============================] - 1s - loss: 0.3823 - acc: 0.8421 - val_loss: 0.3842 - val_acc: 0.8366
Epoch 11/40
8199/8199 [==============================] - 1s - loss: 0.3788 - acc: 0.8450 - val_loss: 0.3880 - val_acc: 0.8317
Epoch 12/40
8199/8199 [==============================] - 1s - loss: 0.3807 - acc: 0.8440 - val_loss: 0.3828 - val_acc: 0.8380
Epoch 13/40
8199/8199 [==============================] - 1s - loss: 0.3763 - acc: 0.8453 - val_loss: 0.3822 - val_acc: 0.8376
Epoch 14/40
8199/8199 [==============================] - 1s - loss: 0.3789 - acc: 0.8444 - val_loss: 0.3823 - val_acc: 0.8380
Epoch 15/40
8199/8199 [==============================] - 1s - loss: 0.3746 - acc: 0.8462 - val_loss: 0.3872 - val_acc: 0.8322
Epoch 16/40
8199/8199 [==============================] - 1s - loss: 0.3764 - acc: 0.8442 - val_loss: 0.3844 - val_acc: 0.8376
Epoch 17/40
8199/8199 [==============================] - 1s - loss: 0.3742 - acc: 0.8453 - val_loss: 0.3813 - val_acc: 0.8400
Epoch 18/40
8199/8199 [==============================] - 1s - loss: 0.3765 - acc: 0.8441 - val_loss: 0.3819 - val_acc: 0.8400
Epoch 19/40
8199/8199 [==============================] - 1s - loss: 0.3713 - acc: 0.8468 - val_loss: 0.3855 - val_acc: 0.8361
Epoch 20/40
8199/8199 [==============================] - 1s - loss: 0.3737 - acc: 0.8463 - val_loss: 0.3814 - val_acc: 0.8400
Epoch 21/40
8199/8199 [==============================] - 1s - loss: 0.3708 - acc: 0.8469 - val_loss: 0.3803 - val_acc: 0.8395
Epoch 22/40
8199/8199 [==============================] - 1s - loss: 0.3723 - acc: 0.8461 - val_loss: 0.3836 - val_acc: 0.8337
Epoch 23/40
8199/8199 [==============================] - 1s - loss: 0.3710 - acc: 0.8477 - val_loss: 0.3822 - val_acc: 0.8356
Epoch 24/40
8199/8199 [==============================] - 1s - loss: 0.3699 - acc: 0.8461 - val_loss: 0.3807 - val_acc: 0.8351
Epoch 25/40
8199/8199 [==============================] - 1s - loss: 0.3716 - acc: 0.8469 - val_loss: 0.3813 - val_acc: 0.8371
Epoch 26/40
8199/8199 [==============================] - 1s - loss: 0.3680 - acc: 0.8473 - val_loss: 0.3839 - val_acc: 0.8351
Epoch 27/40
8199/8199 [==============================] - 1s - loss: 0.3689 - acc: 0.8488 - val_loss: 0.3810 - val_acc: 0.8371
Epoch 28/40
8199/8199 [==============================] - 1s - loss: 0.3674 - acc: 0.8467 - val_loss: 0.3815 - val_acc: 0.8366
Epoch 29/40
8199/8199 [==============================] - 1s - loss: 0.3663 - acc: 0.8492 - val_loss: 0.3855 - val_acc: 0.8371
Epoch 30/40
8199/8199 [==============================] - 1s - loss: 0.3669 - acc: 0.8485 - val_loss: 0.3831 - val_acc: 0.8380
Epoch 31/40
8199/8199 [==============================] - 1s - loss: 0.3652 - acc: 0.8501 - val_loss: 0.3825 - val_acc: 0.8385
Epoch 32/40
8199/8199 [==============================] - 1s - loss: 0.3658 - acc: 0.8486 - val_loss: 0.3861 - val_acc: 0.8351
Epoch 33/40
8199/8199 [==============================] - 1s - loss: 0.3655 - acc: 0.8499 - val_loss: 0.3839 - val_acc: 0.8376
Epoch 34/40
8199/8199 [==============================] - 1s - loss: 0.3640 - acc: 0.8497 - val_loss: 0.3824 - val_acc: 0.8361
Epoch 35/40
8199/8199 [==============================] - 1s - loss: 0.3655 - acc: 0.8475 - val_loss: 0.3851 - val_acc: 0.8376
Epoch 36/40
8199/8199 [==============================] - 1s - loss: 0.3643 - acc: 0.8495 - val_loss: 0.3838 - val_acc: 0.8390
Epoch 37/40
8199/8199 [==============================] - 1s - loss: 0.3631 - acc: 0.8491 - val_loss: 0.3819 - val_acc: 0.8380
Epoch 38/40
8199/8199 [==============================] - 1s - loss: 0.3646 - acc: 0.8485 - val_loss: 0.3857 - val_acc: 0.8361
Epoch 39/40
8199/8199 [==============================] - 1s - loss: 0.3636 - acc: 0.8503 - val_loss: 0.3836 - val_acc: 0.8380
Epoch 40/40
8199/8199 [==============================] - 1s - loss: 0.3616 - acc: 0.8507 - val_loss: 0.3826 - val_acc: 0.8376
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-36.h5
chunk number 36
prepare data
start training
Train on 8126 samples, validate on 2032 samples
Epoch 1/40
8126/8126 [==============================] - 1s - loss: 0.3903 - acc: 0.8378 - val_loss: 0.3795 - val_acc: 0.8455
Epoch 2/40
8126/8126 [==============================] - 1s - loss: 0.3846 - acc: 0.8421 - val_loss: 0.3842 - val_acc: 0.8440
Epoch 3/40
8126/8126 [==============================] - 1s - loss: 0.3866 - acc: 0.8421 - val_loss: 0.3765 - val_acc: 0.8415
Epoch 4/40
8126/8126 [==============================] - 1s - loss: 0.3801 - acc: 0.8419 - val_loss: 0.3818 - val_acc: 0.8415
Epoch 5/40
8126/8126 [==============================] - 1s - loss: 0.3857 - acc: 0.8369 - val_loss: 0.3809 - val_acc: 0.8435
Epoch 6/40
8126/8126 [==============================] - 1s - loss: 0.3799 - acc: 0.8405 - val_loss: 0.3855 - val_acc: 0.8410
Epoch 7/40
8126/8126 [==============================] - 1s - loss: 0.3829 - acc: 0.8420 - val_loss: 0.3789 - val_acc: 0.8410
Epoch 8/40
8126/8126 [==============================] - 1s - loss: 0.3778 - acc: 0.8424 - val_loss: 0.3789 - val_acc: 0.8425
Epoch 9/40
8126/8126 [==============================] - 1s - loss: 0.3810 - acc: 0.8410 - val_loss: 0.3765 - val_acc: 0.8420
Epoch 10/40
8126/8126 [==============================] - 1s - loss: 0.3766 - acc: 0.8447 - val_loss: 0.3792 - val_acc: 0.8410
Epoch 11/40
8126/8126 [==============================] - 1s - loss: 0.3780 - acc: 0.8431 - val_loss: 0.3766 - val_acc: 0.8391
Epoch 12/40
8126/8126 [==============================] - 1s - loss: 0.3757 - acc: 0.8449 - val_loss: 0.3743 - val_acc: 0.8460
Epoch 13/40
8126/8126 [==============================] - 1s - loss: 0.3753 - acc: 0.8452 - val_loss: 0.3738 - val_acc: 0.8455
Epoch 14/40
8126/8126 [==============================] - 1s - loss: 0.3742 - acc: 0.8453 - val_loss: 0.3768 - val_acc: 0.8425
Epoch 15/40
8126/8126 [==============================] - 1s - loss: 0.3744 - acc: 0.8459 - val_loss: 0.3764 - val_acc: 0.8420
Epoch 16/40
8126/8126 [==============================] - 1s - loss: 0.3737 - acc: 0.8457 - val_loss: 0.3743 - val_acc: 0.8425
Epoch 17/40
8126/8126 [==============================] - 1s - loss: 0.3731 - acc: 0.8448 - val_loss: 0.3743 - val_acc: 0.8415
Epoch 18/40
8126/8126 [==============================] - 1s - loss: 0.3723 - acc: 0.8458 - val_loss: 0.3756 - val_acc: 0.8401
Epoch 19/40
8126/8126 [==============================] - 1s - loss: 0.3719 - acc: 0.8467 - val_loss: 0.3752 - val_acc: 0.8406
Epoch 20/40
8126/8126 [==============================] - 1s - loss: 0.3711 - acc: 0.8469 - val_loss: 0.3751 - val_acc: 0.8420
Epoch 21/40
8126/8126 [==============================] - 1s - loss: 0.3710 - acc: 0.8467 - val_loss: 0.3752 - val_acc: 0.8391
Epoch 22/40
8126/8126 [==============================] - 1s - loss: 0.3700 - acc: 0.8469 - val_loss: 0.3768 - val_acc: 0.8406
Epoch 23/40
8126/8126 [==============================] - 1s - loss: 0.3702 - acc: 0.8468 - val_loss: 0.3761 - val_acc: 0.8386
Epoch 24/40
8126/8126 [==============================] - 1s - loss: 0.3691 - acc: 0.8478 - val_loss: 0.3760 - val_acc: 0.8425
Epoch 25/40
8126/8126 [==============================] - 1s - loss: 0.3692 - acc: 0.8478 - val_loss: 0.3762 - val_acc: 0.8391
Epoch 26/40
8126/8126 [==============================] - 1s - loss: 0.3682 - acc: 0.8480 - val_loss: 0.3764 - val_acc: 0.8401
Epoch 27/40
8126/8126 [==============================] - 1s - loss: 0.3681 - acc: 0.8478 - val_loss: 0.3757 - val_acc: 0.8415
Epoch 28/40
8126/8126 [==============================] - 1s - loss: 0.3677 - acc: 0.8479 - val_loss: 0.3753 - val_acc: 0.8420
Epoch 29/40
8126/8126 [==============================] - 1s - loss: 0.3671 - acc: 0.8475 - val_loss: 0.3755 - val_acc: 0.8406
Epoch 30/40
8126/8126 [==============================] - 1s - loss: 0.3670 - acc: 0.8468 - val_loss: 0.3748 - val_acc: 0.8406
Epoch 31/40
8126/8126 [==============================] - 1s - loss: 0.3662 - acc: 0.8473 - val_loss: 0.3748 - val_acc: 0.8406
Epoch 32/40
8126/8126 [==============================] - 1s - loss: 0.3661 - acc: 0.8479 - val_loss: 0.3752 - val_acc: 0.8410
Epoch 33/40
8126/8126 [==============================] - 1s - loss: 0.3658 - acc: 0.8479 - val_loss: 0.3750 - val_acc: 0.8401
Epoch 34/40
8126/8126 [==============================] - 1s - loss: 0.3652 - acc: 0.8479 - val_loss: 0.3752 - val_acc: 0.8406
Epoch 35/40
8126/8126 [==============================] - 1s - loss: 0.3650 - acc: 0.8488 - val_loss: 0.3757 - val_acc: 0.8401
Epoch 36/40
8126/8126 [==============================] - 1s - loss: 0.3647 - acc: 0.8476 - val_loss: 0.3757 - val_acc: 0.8396
Epoch 37/40
8126/8126 [==============================] - 1s - loss: 0.3641 - acc: 0.8475 - val_loss: 0.3758 - val_acc: 0.8406
Epoch 38/40
8126/8126 [==============================] - 1s - loss: 0.3641 - acc: 0.8483 - val_loss: 0.3761 - val_acc: 0.8406
Epoch 39/40
8126/8126 [==============================] - 1s - loss: 0.3640 - acc: 0.8490 - val_loss: 0.3755 - val_acc: 0.8415
Epoch 40/40
8126/8126 [==============================] - 1s - loss: 0.3633 - acc: 0.8483 - val_loss: 0.3752 - val_acc: 0.8415
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-37.h5
chunk number 37
prepare data
start training
Train on 8127 samples, validate on 2032 samples
Epoch 1/40
8127/8127 [==============================] - 1s - loss: 0.3971 - acc: 0.8435 - val_loss: 0.4085 - val_acc: 0.8273
Epoch 2/40
8127/8127 [==============================] - 1s - loss: 0.3957 - acc: 0.8366 - val_loss: 0.4061 - val_acc: 0.8233
Epoch 3/40
8127/8127 [==============================] - 1s - loss: 0.3853 - acc: 0.8405 - val_loss: 0.4060 - val_acc: 0.8243
Epoch 4/40
8127/8127 [==============================] - 1s - loss: 0.3849 - acc: 0.8395 - val_loss: 0.3975 - val_acc: 0.8337
Epoch 5/40
8127/8127 [==============================] - 1s - loss: 0.3827 - acc: 0.8436 - val_loss: 0.3973 - val_acc: 0.8312
Epoch 6/40
8127/8127 [==============================] - 1s - loss: 0.3810 - acc: 0.8458 - val_loss: 0.4001 - val_acc: 0.8273
Epoch 7/40
8127/8127 [==============================] - 1s - loss: 0.3811 - acc: 0.8446 - val_loss: 0.3978 - val_acc: 0.8287
Epoch 8/40
8127/8127 [==============================] - 1s - loss: 0.3784 - acc: 0.8478 - val_loss: 0.3958 - val_acc: 0.8346
Epoch 9/40
8127/8127 [==============================] - 1s - loss: 0.3799 - acc: 0.8447 - val_loss: 0.3986 - val_acc: 0.8292
Epoch 10/40
8127/8127 [==============================] - 1s - loss: 0.3778 - acc: 0.8471 - val_loss: 0.4020 - val_acc: 0.8282
Epoch 11/40
8127/8127 [==============================] - 1s - loss: 0.3786 - acc: 0.8464 - val_loss: 0.3992 - val_acc: 0.8327
Epoch 12/40
8127/8127 [==============================] - 1s - loss: 0.3766 - acc: 0.8474 - val_loss: 0.3966 - val_acc: 0.8371
Epoch 13/40
8127/8127 [==============================] - 1s - loss: 0.3759 - acc: 0.8461 - val_loss: 0.3989 - val_acc: 0.8282
Epoch 14/40
8127/8127 [==============================] - 1s - loss: 0.3736 - acc: 0.8469 - val_loss: 0.3999 - val_acc: 0.8292
Epoch 15/40
8127/8127 [==============================] - 1s - loss: 0.3735 - acc: 0.8471 - val_loss: 0.3972 - val_acc: 0.8342
Epoch 16/40
8127/8127 [==============================] - 1s - loss: 0.3730 - acc: 0.8467 - val_loss: 0.3972 - val_acc: 0.8327
Epoch 17/40
8127/8127 [==============================] - 1s - loss: 0.3715 - acc: 0.8480 - val_loss: 0.3984 - val_acc: 0.8297
Epoch 18/40
8127/8127 [==============================] - 1s - loss: 0.3711 - acc: 0.8487 - val_loss: 0.3979 - val_acc: 0.8356
Epoch 19/40
8127/8127 [==============================] - 1s - loss: 0.3704 - acc: 0.8475 - val_loss: 0.3989 - val_acc: 0.8346
Epoch 20/40
8127/8127 [==============================] - 1s - loss: 0.3694 - acc: 0.8477 - val_loss: 0.4006 - val_acc: 0.8327
Epoch 21/40
8127/8127 [==============================] - 1s - loss: 0.3690 - acc: 0.8489 - val_loss: 0.4000 - val_acc: 0.8327
Epoch 22/40
8127/8127 [==============================] - 1s - loss: 0.3694 - acc: 0.8501 - val_loss: 0.4013 - val_acc: 0.8332
Epoch 23/40
8127/8127 [==============================] - 1s - loss: 0.3687 - acc: 0.8485 - val_loss: 0.4007 - val_acc: 0.8327
Epoch 24/40
8127/8127 [==============================] - 1s - loss: 0.3675 - acc: 0.8493 - val_loss: 0.3991 - val_acc: 0.8351
Epoch 25/40
8127/8127 [==============================] - 1s - loss: 0.3676 - acc: 0.8491 - val_loss: 0.4022 - val_acc: 0.8302
Epoch 26/40
8127/8127 [==============================] - 1s - loss: 0.3677 - acc: 0.8498 - val_loss: 0.3981 - val_acc: 0.8366
Epoch 27/40
8127/8127 [==============================] - 1s - loss: 0.3663 - acc: 0.8503 - val_loss: 0.3982 - val_acc: 0.8342
Epoch 28/40
8127/8127 [==============================] - 1s - loss: 0.3652 - acc: 0.8507 - val_loss: 0.4004 - val_acc: 0.8292
Epoch 29/40
8127/8127 [==============================] - 1s - loss: 0.3656 - acc: 0.8519 - val_loss: 0.3986 - val_acc: 0.8386
Epoch 30/40
8127/8127 [==============================] - 1s - loss: 0.3677 - acc: 0.8493 - val_loss: 0.4091 - val_acc: 0.8253
Epoch 31/40
8127/8127 [==============================] - 1s - loss: 0.3726 - acc: 0.8479 - val_loss: 0.3996 - val_acc: 0.8346
Epoch 32/40
8127/8127 [==============================] - 1s - loss: 0.3645 - acc: 0.8519 - val_loss: 0.3991 - val_acc: 0.8391
Epoch 33/40
8127/8127 [==============================] - 1s - loss: 0.3669 - acc: 0.8489 - val_loss: 0.4124 - val_acc: 0.8248
Epoch 34/40
8127/8127 [==============================] - 1s - loss: 0.3751 - acc: 0.8448 - val_loss: 0.3987 - val_acc: 0.8332
Epoch 35/40
8127/8127 [==============================] - 1s - loss: 0.3635 - acc: 0.8514 - val_loss: 0.4056 - val_acc: 0.8282
Epoch 36/40
8127/8127 [==============================] - 1s - loss: 0.3796 - acc: 0.8427 - val_loss: 0.4209 - val_acc: 0.8209
Epoch 37/40
8127/8127 [==============================] - 1s - loss: 0.3814 - acc: 0.8419 - val_loss: 0.4179 - val_acc: 0.8219
Epoch 38/40
8127/8127 [==============================] - 1s - loss: 0.3781 - acc: 0.8441 - val_loss: 0.4045 - val_acc: 0.8317
Epoch 39/40
8127/8127 [==============================] - 1s - loss: 0.3751 - acc: 0.8452 - val_loss: 0.3997 - val_acc: 0.8342
Epoch 40/40
8127/8127 [==============================] - 1s - loss: 0.3660 - acc: 0.8509 - val_loss: 0.4128 - val_acc: 0.8243
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-38.h5
chunk number 38
prepare data
start training
Train on 8040 samples, validate on 2011 samples
Epoch 1/40
8040/8040 [==============================] - 1s - loss: 0.3967 - acc: 0.8352 - val_loss: 0.4038 - val_acc: 0.8339
Epoch 2/40
8040/8040 [==============================] - 1s - loss: 0.4029 - acc: 0.8311 - val_loss: 0.3991 - val_acc: 0.8399
Epoch 3/40
8040/8040 [==============================] - 1s - loss: 0.3979 - acc: 0.8331 - val_loss: 0.3921 - val_acc: 0.8454
Epoch 4/40
8040/8040 [==============================] - 1s - loss: 0.3935 - acc: 0.8371 - val_loss: 0.3940 - val_acc: 0.8424
Epoch 5/40
8040/8040 [==============================] - 1s - loss: 0.3967 - acc: 0.8359 - val_loss: 0.3907 - val_acc: 0.8439
Epoch 6/40
8040/8040 [==============================] - 1s - loss: 0.3908 - acc: 0.8373 - val_loss: 0.3959 - val_acc: 0.8419
Epoch 7/40
8040/8040 [==============================] - 1s - loss: 0.3936 - acc: 0.8358 - val_loss: 0.3931 - val_acc: 0.8429
Epoch 8/40
8040/8040 [==============================] - 1s - loss: 0.3910 - acc: 0.8359 - val_loss: 0.3905 - val_acc: 0.8424
Epoch 9/40
8040/8040 [==============================] - 1s - loss: 0.3892 - acc: 0.8386 - val_loss: 0.3920 - val_acc: 0.8424
Epoch 10/40
8040/8040 [==============================] - 1s - loss: 0.3899 - acc: 0.8398 - val_loss: 0.3914 - val_acc: 0.8454
Epoch 11/40
8040/8040 [==============================] - 1s - loss: 0.3868 - acc: 0.8400 - val_loss: 0.3948 - val_acc: 0.8424
Epoch 12/40
8040/8040 [==============================] - 1s - loss: 0.3888 - acc: 0.8392 - val_loss: 0.3908 - val_acc: 0.8458
Epoch 13/40
8040/8040 [==============================] - 1s - loss: 0.3850 - acc: 0.8412 - val_loss: 0.3920 - val_acc: 0.8444
Epoch 14/40
8040/8040 [==============================] - 1s - loss: 0.3875 - acc: 0.8391 - val_loss: 0.3887 - val_acc: 0.8468
Epoch 15/40
8040/8040 [==============================] - 1s - loss: 0.3831 - acc: 0.8412 - val_loss: 0.3903 - val_acc: 0.8434
Epoch 16/40
8040/8040 [==============================] - 1s - loss: 0.3842 - acc: 0.8415 - val_loss: 0.3872 - val_acc: 0.8454
Epoch 17/40
8040/8040 [==============================] - 1s - loss: 0.3820 - acc: 0.8433 - val_loss: 0.3885 - val_acc: 0.8449
Epoch 18/40
8040/8040 [==============================] - 1s - loss: 0.3836 - acc: 0.8407 - val_loss: 0.3884 - val_acc: 0.8468
Epoch 19/40
8040/8040 [==============================] - 1s - loss: 0.3810 - acc: 0.8448 - val_loss: 0.3908 - val_acc: 0.8434
Epoch 20/40
8040/8040 [==============================] - 1s - loss: 0.3817 - acc: 0.8437 - val_loss: 0.3900 - val_acc: 0.8488
Epoch 21/40
8040/8040 [==============================] - 1s - loss: 0.3798 - acc: 0.8454 - val_loss: 0.3924 - val_acc: 0.8434
Epoch 22/40
8040/8040 [==============================] - 1s - loss: 0.3810 - acc: 0.8438 - val_loss: 0.3914 - val_acc: 0.8463
Epoch 23/40
8040/8040 [==============================] - 1s - loss: 0.3800 - acc: 0.8444 - val_loss: 0.3906 - val_acc: 0.8454
Epoch 24/40
8040/8040 [==============================] - 1s - loss: 0.3788 - acc: 0.8461 - val_loss: 0.3912 - val_acc: 0.8444
Epoch 25/40
8040/8040 [==============================] - 1s - loss: 0.3799 - acc: 0.8418 - val_loss: 0.3902 - val_acc: 0.8483
Epoch 26/40
8040/8040 [==============================] - 1s - loss: 0.3774 - acc: 0.8460 - val_loss: 0.3923 - val_acc: 0.8463
Epoch 27/40
8040/8040 [==============================] - 1s - loss: 0.3780 - acc: 0.8459 - val_loss: 0.3933 - val_acc: 0.8458
Epoch 28/40
8040/8040 [==============================] - 1s - loss: 0.3764 - acc: 0.8458 - val_loss: 0.3936 - val_acc: 0.8454
Epoch 29/40
8040/8040 [==============================] - 1s - loss: 0.3754 - acc: 0.8463 - val_loss: 0.3937 - val_acc: 0.8463
Epoch 30/40
8040/8040 [==============================] - 1s - loss: 0.3760 - acc: 0.8470 - val_loss: 0.3937 - val_acc: 0.8439
Epoch 31/40
8040/8040 [==============================] - 1s - loss: 0.3750 - acc: 0.8451 - val_loss: 0.3932 - val_acc: 0.8463
Epoch 32/40
8040/8040 [==============================] - 1s - loss: 0.3741 - acc: 0.8475 - val_loss: 0.3939 - val_acc: 0.8458
Epoch 33/40
8040/8040 [==============================] - 1s - loss: 0.3737 - acc: 0.8469 - val_loss: 0.3956 - val_acc: 0.8444
Epoch 34/40
8040/8040 [==============================] - 1s - loss: 0.3737 - acc: 0.8469 - val_loss: 0.3955 - val_acc: 0.8439
Epoch 35/40
8040/8040 [==============================] - 1s - loss: 0.3737 - acc: 0.8478 - val_loss: 0.3949 - val_acc: 0.8429
Epoch 36/40
8040/8040 [==============================] - 1s - loss: 0.3726 - acc: 0.8464 - val_loss: 0.3943 - val_acc: 0.8439
Epoch 37/40
8040/8040 [==============================] - 1s - loss: 0.3721 - acc: 0.8460 - val_loss: 0.3943 - val_acc: 0.8454
Epoch 38/40
8040/8040 [==============================] - 1s - loss: 0.3721 - acc: 0.8490 - val_loss: 0.3949 - val_acc: 0.8458
Epoch 39/40
8040/8040 [==============================] - 1s - loss: 0.3715 - acc: 0.8470 - val_loss: 0.3947 - val_acc: 0.8473
Epoch 40/40
8040/8040 [==============================] - 1s - loss: 0.3707 - acc: 0.8485 - val_loss: 0.3950 - val_acc: 0.8473
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-39.h5
chunk number 39
prepare data
start training
Train on 8109 samples, validate on 2028 samples
Epoch 1/40
8109/8109 [==============================] - 1s - loss: 0.3849 - acc: 0.8389 - val_loss: 0.3675 - val_acc: 0.8639
Epoch 2/40
8109/8109 [==============================] - 1s - loss: 0.3829 - acc: 0.8385 - val_loss: 0.3657 - val_acc: 0.8614
Epoch 3/40
8109/8109 [==============================] - 1s - loss: 0.3816 - acc: 0.8388 - val_loss: 0.3660 - val_acc: 0.8600
Epoch 4/40
8109/8109 [==============================] - 1s - loss: 0.3815 - acc: 0.8404 - val_loss: 0.3655 - val_acc: 0.8629
Epoch 5/40
8109/8109 [==============================] - 1s - loss: 0.3806 - acc: 0.8403 - val_loss: 0.3654 - val_acc: 0.8609
Epoch 6/40
8109/8109 [==============================] - 1s - loss: 0.3790 - acc: 0.8424 - val_loss: 0.3671 - val_acc: 0.8614
Epoch 7/40
8109/8109 [==============================] - 1s - loss: 0.3774 - acc: 0.8413 - val_loss: 0.3696 - val_acc: 0.8624
Epoch 8/40
8109/8109 [==============================] - 1s - loss: 0.3770 - acc: 0.8414 - val_loss: 0.3690 - val_acc: 0.8654
Epoch 9/40
8109/8109 [==============================] - 1s - loss: 0.3760 - acc: 0.8417 - val_loss: 0.3671 - val_acc: 0.8654
Epoch 10/40
8109/8109 [==============================] - 1s - loss: 0.3744 - acc: 0.8430 - val_loss: 0.3658 - val_acc: 0.8649
Epoch 11/40
8109/8109 [==============================] - 1s - loss: 0.3736 - acc: 0.8435 - val_loss: 0.3666 - val_acc: 0.8629
Epoch 12/40
8109/8109 [==============================] - 1s - loss: 0.3729 - acc: 0.8441 - val_loss: 0.3680 - val_acc: 0.8654
Epoch 13/40
8109/8109 [==============================] - 1s - loss: 0.3720 - acc: 0.8450 - val_loss: 0.3695 - val_acc: 0.8639
Epoch 14/40
8109/8109 [==============================] - 1s - loss: 0.3711 - acc: 0.8440 - val_loss: 0.3699 - val_acc: 0.8639
Epoch 15/40
8109/8109 [==============================] - 1s - loss: 0.3701 - acc: 0.8450 - val_loss: 0.3694 - val_acc: 0.8614
Epoch 16/40
8109/8109 [==============================] - 1s - loss: 0.3692 - acc: 0.8454 - val_loss: 0.3692 - val_acc: 0.8634
Epoch 17/40
8109/8109 [==============================] - 1s - loss: 0.3688 - acc: 0.8456 - val_loss: 0.3699 - val_acc: 0.8624
Epoch 18/40
8109/8109 [==============================] - 1s - loss: 0.3681 - acc: 0.8452 - val_loss: 0.3695 - val_acc: 0.8634
Epoch 19/40
8109/8109 [==============================] - 1s - loss: 0.3672 - acc: 0.8449 - val_loss: 0.3715 - val_acc: 0.8619
Epoch 20/40
8109/8109 [==============================] - 1s - loss: 0.3666 - acc: 0.8455 - val_loss: 0.3705 - val_acc: 0.8639
Epoch 21/40
8109/8109 [==============================] - 1s - loss: 0.3662 - acc: 0.8463 - val_loss: 0.3737 - val_acc: 0.8570
Epoch 22/40
8109/8109 [==============================] - 1s - loss: 0.3661 - acc: 0.8457 - val_loss: 0.3714 - val_acc: 0.8619
Epoch 23/40
8109/8109 [==============================] - 1s - loss: 0.3684 - acc: 0.8430 - val_loss: 0.3785 - val_acc: 0.8526
Epoch 24/40
8109/8109 [==============================] - 1s - loss: 0.3694 - acc: 0.8439 - val_loss: 0.3758 - val_acc: 0.8570
Epoch 25/40
8109/8109 [==============================] - 1s - loss: 0.3739 - acc: 0.8389 - val_loss: 0.3747 - val_acc: 0.8560
Epoch 26/40
8109/8109 [==============================] - 1s - loss: 0.3653 - acc: 0.8471 - val_loss: 0.3721 - val_acc: 0.8590
Epoch 27/40
8109/8109 [==============================] - 1s - loss: 0.3630 - acc: 0.8467 - val_loss: 0.3729 - val_acc: 0.8600
Epoch 28/40
8109/8109 [==============================] - 1s - loss: 0.3654 - acc: 0.8438 - val_loss: 0.3807 - val_acc: 0.8540
Epoch 29/40
8109/8109 [==============================] - 1s - loss: 0.3683 - acc: 0.8430 - val_loss: 0.3767 - val_acc: 0.8540
Epoch 30/40
8109/8109 [==============================] - 1s - loss: 0.3721 - acc: 0.8398 - val_loss: 0.3726 - val_acc: 0.8575
Epoch 31/40
8109/8109 [==============================] - 1s - loss: 0.3622 - acc: 0.8481 - val_loss: 0.3764 - val_acc: 0.8540
Epoch 32/40
8109/8109 [==============================] - 1s - loss: 0.3653 - acc: 0.8472 - val_loss: 0.3782 - val_acc: 0.8540
Epoch 33/40
8109/8109 [==============================] - 1s - loss: 0.3734 - acc: 0.8393 - val_loss: 0.3727 - val_acc: 0.8609
Epoch 34/40
8109/8109 [==============================] - 1s - loss: 0.3605 - acc: 0.8472 - val_loss: 0.3834 - val_acc: 0.8491
Epoch 35/40
8109/8109 [==============================] - 1s - loss: 0.3702 - acc: 0.8419 - val_loss: 0.3862 - val_acc: 0.8422
Epoch 36/40
8109/8109 [==============================] - 1s - loss: 0.3843 - acc: 0.8350 - val_loss: 0.3740 - val_acc: 0.8600
Epoch 37/40
8109/8109 [==============================] - 1s - loss: 0.3677 - acc: 0.8415 - val_loss: 0.4088 - val_acc: 0.8279
Epoch 38/40
8109/8109 [==============================] - 1s - loss: 0.3980 - acc: 0.8270 - val_loss: 0.3843 - val_acc: 0.8491
Epoch 39/40
8109/8109 [==============================] - 1s - loss: 0.3812 - acc: 0.8362 - val_loss: 0.3934 - val_acc: 0.8432
Epoch 40/40
8109/8109 [==============================] - 1s - loss: 0.3921 - acc: 0.8315 - val_loss: 0.3786 - val_acc: 0.8550
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-40.h5
chunk number 40
prepare data
start training
Train on 8199 samples, validate on 2050 samples
Epoch 1/40
8199/8199 [==============================] - 2s - loss: 0.4038 - acc: 0.8391 - val_loss: 0.4321 - val_acc: 0.8132
Epoch 2/40
8199/8199 [==============================] - 2s - loss: 0.4048 - acc: 0.8294 - val_loss: 0.4105 - val_acc: 0.8263
Epoch 3/40
8199/8199 [==============================] - 2s - loss: 0.3840 - acc: 0.8451 - val_loss: 0.4082 - val_acc: 0.8259
Epoch 4/40
8199/8199 [==============================] - 2s - loss: 0.3871 - acc: 0.8444 - val_loss: 0.4135 - val_acc: 0.8229
Epoch 5/40
8199/8199 [==============================] - 2s - loss: 0.3926 - acc: 0.8399 - val_loss: 0.4115 - val_acc: 0.8205
Epoch 6/40
8199/8199 [==============================] - 2s - loss: 0.3875 - acc: 0.8425 - val_loss: 0.4133 - val_acc: 0.8205
Epoch 7/40
8199/8199 [==============================] - 2s - loss: 0.3871 - acc: 0.8440 - val_loss: 0.4126 - val_acc: 0.8254
Epoch 8/40
8199/8199 [==============================] - 2s - loss: 0.3847 - acc: 0.8456 - val_loss: 0.4073 - val_acc: 0.8283
Epoch 9/40
8199/8199 [==============================] - 2s - loss: 0.3793 - acc: 0.8453 - val_loss: 0.4097 - val_acc: 0.8332
Epoch 10/40
8199/8199 [==============================] - 2s - loss: 0.3803 - acc: 0.8477 - val_loss: 0.4100 - val_acc: 0.8307
Epoch 11/40
8199/8199 [==============================] - 2s - loss: 0.3807 - acc: 0.8467 - val_loss: 0.4105 - val_acc: 0.8317
Epoch 12/40
8199/8199 [==============================] - 2s - loss: 0.3794 - acc: 0.8469 - val_loss: 0.4099 - val_acc: 0.8307
Epoch 13/40
8199/8199 [==============================] - 2s - loss: 0.3773 - acc: 0.8477 - val_loss: 0.4071 - val_acc: 0.8302
Epoch 14/40
8199/8199 [==============================] - 2s - loss: 0.3745 - acc: 0.8473 - val_loss: 0.4076 - val_acc: 0.8254
Epoch 15/40
8199/8199 [==============================] - 2s - loss: 0.3755 - acc: 0.8484 - val_loss: 0.4087 - val_acc: 0.8229
Epoch 16/40
8199/8199 [==============================] - 2s - loss: 0.3758 - acc: 0.8473 - val_loss: 0.4083 - val_acc: 0.8263
Epoch 17/40
8199/8199 [==============================] - 2s - loss: 0.3745 - acc: 0.8471 - val_loss: 0.4066 - val_acc: 0.8298
Epoch 18/40
8199/8199 [==============================] - 2s - loss: 0.3723 - acc: 0.8478 - val_loss: 0.4053 - val_acc: 0.8317
Epoch 19/40
8199/8199 [==============================] - 2s - loss: 0.3706 - acc: 0.8489 - val_loss: 0.4062 - val_acc: 0.8307
Epoch 20/40
8199/8199 [==============================] - 2s - loss: 0.3716 - acc: 0.8491 - val_loss: 0.4067 - val_acc: 0.8302
Epoch 21/40
8199/8199 [==============================] - 2s - loss: 0.3714 - acc: 0.8506 - val_loss: 0.4055 - val_acc: 0.8337
Epoch 22/40
8199/8199 [==============================] - 2s - loss: 0.3693 - acc: 0.8507 - val_loss: 0.4049 - val_acc: 0.8327
Epoch 23/40
8199/8199 [==============================] - 2s - loss: 0.3684 - acc: 0.8508 - val_loss: 0.4051 - val_acc: 0.8317
Epoch 24/40
8199/8199 [==============================] - 2s - loss: 0.3684 - acc: 0.8513 - val_loss: 0.4054 - val_acc: 0.8283
Epoch 25/40
8199/8199 [==============================] - 2s - loss: 0.3684 - acc: 0.8508 - val_loss: 0.4055 - val_acc: 0.8322
Epoch 26/40
8199/8199 [==============================] - 2s - loss: 0.3675 - acc: 0.8524 - val_loss: 0.4052 - val_acc: 0.8346
Epoch 27/40
8199/8199 [==============================] - 2s - loss: 0.3662 - acc: 0.8527 - val_loss: 0.4051 - val_acc: 0.8332
Epoch 28/40
8199/8199 [==============================] - 2s - loss: 0.3654 - acc: 0.8513 - val_loss: 0.4055 - val_acc: 0.8312
Epoch 29/40
8199/8199 [==============================] - 2s - loss: 0.3649 - acc: 0.8512 - val_loss: 0.4057 - val_acc: 0.8317
Epoch 30/40
8199/8199 [==============================] - 2s - loss: 0.3644 - acc: 0.8502 - val_loss: 0.4060 - val_acc: 0.8337
Epoch 31/40
8199/8199 [==============================] - 2s - loss: 0.3640 - acc: 0.8514 - val_loss: 0.4063 - val_acc: 0.8327
Epoch 32/40
8199/8199 [==============================] - 2s - loss: 0.3637 - acc: 0.8517 - val_loss: 0.4052 - val_acc: 0.8351
Epoch 33/40
8199/8199 [==============================] - 2s - loss: 0.3632 - acc: 0.8530 - val_loss: 0.4046 - val_acc: 0.8356
Epoch 34/40
8199/8199 [==============================] - 2s - loss: 0.3624 - acc: 0.8525 - val_loss: 0.4046 - val_acc: 0.8332
Epoch 35/40
8199/8199 [==============================] - 2s - loss: 0.3620 - acc: 0.8528 - val_loss: 0.4057 - val_acc: 0.8341
Epoch 36/40
8199/8199 [==============================] - 2s - loss: 0.3618 - acc: 0.8529 - val_loss: 0.4054 - val_acc: 0.8337
Epoch 37/40
8199/8199 [==============================] - 2s - loss: 0.3611 - acc: 0.8533 - val_loss: 0.4061 - val_acc: 0.8327
Epoch 38/40
8199/8199 [==============================] - 2s - loss: 0.3605 - acc: 0.8538 - val_loss: 0.4080 - val_acc: 0.8346
Epoch 39/40
8199/8199 [==============================] - 2s - loss: 0.3603 - acc: 0.8539 - val_loss: 0.4067 - val_acc: 0.8356
Epoch 40/40
8199/8199 [==============================] - 2s - loss: 0.3595 - acc: 0.8540 - val_loss: 0.4069 - val_acc: 0.8337
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-41.h5
chunk number 41
prepare data
start training
Train on 8120 samples, validate on 2031 samples
Epoch 1/40
8120/8120 [==============================] - 1s - loss: 0.4381 - acc: 0.8200 - val_loss: 0.3788 - val_acc: 0.8400
Epoch 2/40
8120/8120 [==============================] - 1s - loss: 0.3936 - acc: 0.8365 - val_loss: 0.4090 - val_acc: 0.8227
Epoch 3/40
8120/8120 [==============================] - 1s - loss: 0.4200 - acc: 0.8188 - val_loss: 0.4268 - val_acc: 0.8149
Epoch 4/40
8120/8120 [==============================] - 1s - loss: 0.4373 - acc: 0.8107 - val_loss: 0.4155 - val_acc: 0.8227
Epoch 5/40
8120/8120 [==============================] - 1s - loss: 0.4253 - acc: 0.8172 - val_loss: 0.3856 - val_acc: 0.8360
Epoch 6/40
8120/8120 [==============================] - 1s - loss: 0.3975 - acc: 0.8302 - val_loss: 0.3764 - val_acc: 0.8469
Epoch 7/40
8120/8120 [==============================] - 1s - loss: 0.3917 - acc: 0.8387 - val_loss: 0.4088 - val_acc: 0.8227
Epoch 8/40
8120/8120 [==============================] - 1s - loss: 0.4239 - acc: 0.8202 - val_loss: 0.3725 - val_acc: 0.8498
Epoch 9/40
8120/8120 [==============================] - 1s - loss: 0.3881 - acc: 0.8404 - val_loss: 0.3790 - val_acc: 0.8419
Epoch 10/40
8120/8120 [==============================] - 1s - loss: 0.3912 - acc: 0.8349 - val_loss: 0.3954 - val_acc: 0.8311
Epoch 11/40
8120/8120 [==============================] - 1s - loss: 0.4056 - acc: 0.8282 - val_loss: 0.3928 - val_acc: 0.8321
Epoch 12/40
8120/8120 [==============================] - 1s - loss: 0.4028 - acc: 0.8293 - val_loss: 0.3752 - val_acc: 0.8464
Epoch 13/40
8120/8120 [==============================] - 1s - loss: 0.3861 - acc: 0.8385 - val_loss: 0.3725 - val_acc: 0.8508
Epoch 14/40
8120/8120 [==============================] - 1s - loss: 0.3839 - acc: 0.8410 - val_loss: 0.3895 - val_acc: 0.8370
Epoch 15/40
8120/8120 [==============================] - 1s - loss: 0.3986 - acc: 0.8324 - val_loss: 0.3703 - val_acc: 0.8508
Epoch 16/40
8120/8120 [==============================] - 1s - loss: 0.3790 - acc: 0.8437 - val_loss: 0.3775 - val_acc: 0.8400
Epoch 17/40
8120/8120 [==============================] - 1s - loss: 0.3846 - acc: 0.8397 - val_loss: 0.3843 - val_acc: 0.8380
Epoch 18/40
8120/8120 [==============================] - 1s - loss: 0.3907 - acc: 0.8360 - val_loss: 0.3781 - val_acc: 0.8390
Epoch 19/40
8120/8120 [==============================] - 1s - loss: 0.3838 - acc: 0.8397 - val_loss: 0.3725 - val_acc: 0.8424
Epoch 20/40
8120/8120 [==============================] - 1s - loss: 0.3770 - acc: 0.8448 - val_loss: 0.3840 - val_acc: 0.8351
Epoch 21/40
8120/8120 [==============================] - 1s - loss: 0.3863 - acc: 0.8400 - val_loss: 0.3710 - val_acc: 0.8454
Epoch 22/40
8120/8120 [==============================] - 1s - loss: 0.3759 - acc: 0.8463 - val_loss: 0.3696 - val_acc: 0.8464
Epoch 23/40
8120/8120 [==============================] - 1s - loss: 0.3773 - acc: 0.8429 - val_loss: 0.3723 - val_acc: 0.8454
Epoch 24/40
8120/8120 [==============================] - 1s - loss: 0.3813 - acc: 0.8411 - val_loss: 0.3676 - val_acc: 0.8488
Epoch 25/40
8120/8120 [==============================] - 1s - loss: 0.3763 - acc: 0.8431 - val_loss: 0.3681 - val_acc: 0.8474
Epoch 26/40
8120/8120 [==============================] - 1s - loss: 0.3747 - acc: 0.8445 - val_loss: 0.3733 - val_acc: 0.8390
Epoch 27/40
8120/8120 [==============================] - 1s - loss: 0.3773 - acc: 0.8446 - val_loss: 0.3660 - val_acc: 0.8464
Epoch 28/40
8120/8120 [==============================] - 1s - loss: 0.3717 - acc: 0.8448 - val_loss: 0.3684 - val_acc: 0.8449
Epoch 29/40
8120/8120 [==============================] - 1s - loss: 0.3748 - acc: 0.8437 - val_loss: 0.3683 - val_acc: 0.8444
Epoch 30/40
8120/8120 [==============================] - 1s - loss: 0.3737 - acc: 0.8454 - val_loss: 0.3687 - val_acc: 0.8429
Epoch 31/40
8120/8120 [==============================] - 1s - loss: 0.3714 - acc: 0.8462 - val_loss: 0.3723 - val_acc: 0.8380
Epoch 32/40
8120/8120 [==============================] - 1s - loss: 0.3734 - acc: 0.8446 - val_loss: 0.3655 - val_acc: 0.8459
Epoch 33/40
8120/8120 [==============================] - 1s - loss: 0.3694 - acc: 0.8479 - val_loss: 0.3644 - val_acc: 0.8513
Epoch 34/40
8120/8120 [==============================] - 1s - loss: 0.3708 - acc: 0.8458 - val_loss: 0.3634 - val_acc: 0.8518
Epoch 35/40
8120/8120 [==============================] - 1s - loss: 0.3702 - acc: 0.8459 - val_loss: 0.3643 - val_acc: 0.8484
Epoch 36/40
8120/8120 [==============================] - 1s - loss: 0.3695 - acc: 0.8472 - val_loss: 0.3653 - val_acc: 0.8429
Epoch 37/40
8120/8120 [==============================] - 1s - loss: 0.3691 - acc: 0.8477 - val_loss: 0.3639 - val_acc: 0.8493
Epoch 38/40
8120/8120 [==============================] - 1s - loss: 0.3675 - acc: 0.8475 - val_loss: 0.3655 - val_acc: 0.8474
Epoch 39/40
8120/8120 [==============================] - 1s - loss: 0.3679 - acc: 0.8483 - val_loss: 0.3670 - val_acc: 0.8454
Epoch 40/40
8120/8120 [==============================] - 1s - loss: 0.3673 - acc: 0.8484 - val_loss: 0.3682 - val_acc: 0.8444
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-42.h5
chunk number 42
prepare data
start training
Train on 8105 samples, validate on 2027 samples
Epoch 1/40
8105/8105 [==============================] - 1s - loss: 0.3795 - acc: 0.8422 - val_loss: 0.3798 - val_acc: 0.8377
Epoch 2/40
8105/8105 [==============================] - 1s - loss: 0.3776 - acc: 0.8418 - val_loss: 0.3703 - val_acc: 0.8495
Epoch 3/40
8105/8105 [==============================] - 1s - loss: 0.3726 - acc: 0.8466 - val_loss: 0.3669 - val_acc: 0.8550
Epoch 4/40
8105/8105 [==============================] - 1s - loss: 0.3717 - acc: 0.8489 - val_loss: 0.3697 - val_acc: 0.8525
Epoch 5/40
8105/8105 [==============================] - 1s - loss: 0.3705 - acc: 0.8479 - val_loss: 0.3698 - val_acc: 0.8495
Epoch 6/40
8105/8105 [==============================] - 1s - loss: 0.3699 - acc: 0.8481 - val_loss: 0.3680 - val_acc: 0.8545
Epoch 7/40
8105/8105 [==============================] - 1s - loss: 0.3686 - acc: 0.8486 - val_loss: 0.3689 - val_acc: 0.8515
Epoch 8/40
8105/8105 [==============================] - 1s - loss: 0.3665 - acc: 0.8492 - val_loss: 0.3725 - val_acc: 0.8495
Epoch 9/40
8105/8105 [==============================] - 1s - loss: 0.3653 - acc: 0.8481 - val_loss: 0.3732 - val_acc: 0.8490
Epoch 10/40
8105/8105 [==============================] - 1s - loss: 0.3646 - acc: 0.8482 - val_loss: 0.3728 - val_acc: 0.8525
Epoch 11/40
8105/8105 [==============================] - 1s - loss: 0.3645 - acc: 0.8474 - val_loss: 0.3726 - val_acc: 0.8520
Epoch 12/40
8105/8105 [==============================] - 1s - loss: 0.3639 - acc: 0.8489 - val_loss: 0.3730 - val_acc: 0.8500
Epoch 13/40
8105/8105 [==============================] - 1s - loss: 0.3630 - acc: 0.8484 - val_loss: 0.3725 - val_acc: 0.8515
Epoch 14/40
8105/8105 [==============================] - 1s - loss: 0.3626 - acc: 0.8489 - val_loss: 0.3698 - val_acc: 0.8550
Epoch 15/40
8105/8105 [==============================] - 1s - loss: 0.3614 - acc: 0.8501 - val_loss: 0.3687 - val_acc: 0.8545
Epoch 16/40
8105/8105 [==============================] - 1s - loss: 0.3611 - acc: 0.8508 - val_loss: 0.3694 - val_acc: 0.8520
Epoch 17/40
8105/8105 [==============================] - 1s - loss: 0.3600 - acc: 0.8516 - val_loss: 0.3712 - val_acc: 0.8510
Epoch 18/40
8105/8105 [==============================] - 1s - loss: 0.3599 - acc: 0.8514 - val_loss: 0.3702 - val_acc: 0.8535
Epoch 19/40
8105/8105 [==============================] - 1s - loss: 0.3588 - acc: 0.8527 - val_loss: 0.3701 - val_acc: 0.8535
Epoch 20/40
8105/8105 [==============================] - 1s - loss: 0.3584 - acc: 0.8514 - val_loss: 0.3713 - val_acc: 0.8520
Epoch 21/40
8105/8105 [==============================] - 1s - loss: 0.3575 - acc: 0.8519 - val_loss: 0.3705 - val_acc: 0.8550
Epoch 22/40
8105/8105 [==============================] - 1s - loss: 0.3566 - acc: 0.8519 - val_loss: 0.3691 - val_acc: 0.8559
Epoch 23/40
8105/8105 [==============================] - 1s - loss: 0.3562 - acc: 0.8524 - val_loss: 0.3699 - val_acc: 0.8550
Epoch 24/40
8105/8105 [==============================] - 1s - loss: 0.3556 - acc: 0.8517 - val_loss: 0.3701 - val_acc: 0.8530
Epoch 25/40
8105/8105 [==============================] - 1s - loss: 0.3549 - acc: 0.8517 - val_loss: 0.3702 - val_acc: 0.8545
Epoch 26/40
8105/8105 [==============================] - 1s - loss: 0.3544 - acc: 0.8531 - val_loss: 0.3713 - val_acc: 0.8505
Epoch 27/40
8105/8105 [==============================] - 1s - loss: 0.3540 - acc: 0.8524 - val_loss: 0.3716 - val_acc: 0.8505
Epoch 28/40
8105/8105 [==============================] - 1s - loss: 0.3536 - acc: 0.8532 - val_loss: 0.3711 - val_acc: 0.8540
Epoch 29/40
8105/8105 [==============================] - 1s - loss: 0.3532 - acc: 0.8543 - val_loss: 0.3716 - val_acc: 0.8535
Epoch 30/40
8105/8105 [==============================] - 1s - loss: 0.3527 - acc: 0.8538 - val_loss: 0.3710 - val_acc: 0.8550
Epoch 31/40
8105/8105 [==============================] - 1s - loss: 0.3522 - acc: 0.8549 - val_loss: 0.3708 - val_acc: 0.8555
Epoch 32/40
8105/8105 [==============================] - 1s - loss: 0.3518 - acc: 0.8543 - val_loss: 0.3710 - val_acc: 0.8559
Epoch 33/40
8105/8105 [==============================] - 1s - loss: 0.3514 - acc: 0.8550 - val_loss: 0.3708 - val_acc: 0.8569
Epoch 34/40
8105/8105 [==============================] - 1s - loss: 0.3509 - acc: 0.8563 - val_loss: 0.3711 - val_acc: 0.8559
Epoch 35/40
8105/8105 [==============================] - 1s - loss: 0.3505 - acc: 0.8560 - val_loss: 0.3713 - val_acc: 0.8550
Epoch 36/40
8105/8105 [==============================] - 1s - loss: 0.3501 - acc: 0.8565 - val_loss: 0.3710 - val_acc: 0.8574
Epoch 37/40
8105/8105 [==============================] - 1s - loss: 0.3496 - acc: 0.8564 - val_loss: 0.3713 - val_acc: 0.8569
Epoch 38/40
8105/8105 [==============================] - 1s - loss: 0.3493 - acc: 0.8556 - val_loss: 0.3703 - val_acc: 0.8574
Epoch 39/40
8105/8105 [==============================] - 1s - loss: 0.3495 - acc: 0.8565 - val_loss: 0.3736 - val_acc: 0.8535
Epoch 40/40
8105/8105 [==============================] - 1s - loss: 0.3504 - acc: 0.8548 - val_loss: 0.3706 - val_acc: 0.8604
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-43.h5
chunk number 43
prepare data
start training
Train on 8189 samples, validate on 2048 samples
Epoch 1/40
8189/8189 [==============================] - 1s - loss: 0.3985 - acc: 0.8381 - val_loss: 0.3760 - val_acc: 0.8545
Epoch 2/40
8189/8189 [==============================] - 1s - loss: 0.4007 - acc: 0.8378 - val_loss: 0.3743 - val_acc: 0.8555
Epoch 3/40
8189/8189 [==============================] - 1s - loss: 0.3985 - acc: 0.8391 - val_loss: 0.3718 - val_acc: 0.8555
Epoch 4/40
8189/8189 [==============================] - 1s - loss: 0.3936 - acc: 0.8388 - val_loss: 0.3761 - val_acc: 0.8540
Epoch 5/40
8189/8189 [==============================] - 1s - loss: 0.3966 - acc: 0.8356 - val_loss: 0.3679 - val_acc: 0.8555
Epoch 6/40
8189/8189 [==============================] - 1s - loss: 0.3916 - acc: 0.8400 - val_loss: 0.3707 - val_acc: 0.8604
Epoch 7/40
8189/8189 [==============================] - 1s - loss: 0.3943 - acc: 0.8369 - val_loss: 0.3668 - val_acc: 0.8564
Epoch 8/40
8189/8189 [==============================] - 1s - loss: 0.3892 - acc: 0.8404 - val_loss: 0.3715 - val_acc: 0.8555
Epoch 9/40
8189/8189 [==============================] - 1s - loss: 0.3925 - acc: 0.8391 - val_loss: 0.3678 - val_acc: 0.8594
Epoch 10/40
8189/8189 [==============================] - 1s - loss: 0.3872 - acc: 0.8416 - val_loss: 0.3711 - val_acc: 0.8584
Epoch 11/40
8189/8189 [==============================] - 1s - loss: 0.3895 - acc: 0.8398 - val_loss: 0.3693 - val_acc: 0.8623
Epoch 12/40
8189/8189 [==============================] - 1s - loss: 0.3870 - acc: 0.8402 - val_loss: 0.3687 - val_acc: 0.8579
Epoch 13/40
8189/8189 [==============================] - 1s - loss: 0.3875 - acc: 0.8419 - val_loss: 0.3666 - val_acc: 0.8608
Epoch 14/40
8189/8189 [==============================] - 1s - loss: 0.3855 - acc: 0.8428 - val_loss: 0.3661 - val_acc: 0.8608
Epoch 15/40
8189/8189 [==============================] - 1s - loss: 0.3852 - acc: 0.8403 - val_loss: 0.3654 - val_acc: 0.8604
Epoch 16/40
8189/8189 [==============================] - 1s - loss: 0.3847 - acc: 0.8392 - val_loss: 0.3646 - val_acc: 0.8608
Epoch 17/40
8189/8189 [==============================] - 1s - loss: 0.3831 - acc: 0.8405 - val_loss: 0.3663 - val_acc: 0.8618
Epoch 18/40
8189/8189 [==============================] - 1s - loss: 0.3834 - acc: 0.8428 - val_loss: 0.3660 - val_acc: 0.8589
Epoch 19/40
8189/8189 [==============================] - 1s - loss: 0.3818 - acc: 0.8431 - val_loss: 0.3673 - val_acc: 0.8579
Epoch 20/40
8189/8189 [==============================] - 1s - loss: 0.3821 - acc: 0.8430 - val_loss: 0.3671 - val_acc: 0.8579
Epoch 21/40
8189/8189 [==============================] - 1s - loss: 0.3805 - acc: 0.8441 - val_loss: 0.3680 - val_acc: 0.8608
Epoch 22/40
8189/8189 [==============================] - 1s - loss: 0.3808 - acc: 0.8443 - val_loss: 0.3665 - val_acc: 0.8599
Epoch 23/40
8189/8189 [==============================] - 1s - loss: 0.3794 - acc: 0.8449 - val_loss: 0.3667 - val_acc: 0.8584
Epoch 24/40
8189/8189 [==============================] - 1s - loss: 0.3796 - acc: 0.8447 - val_loss: 0.3668 - val_acc: 0.8608
Epoch 25/40
8189/8189 [==============================] - 1s - loss: 0.3783 - acc: 0.8450 - val_loss: 0.3681 - val_acc: 0.8608
Epoch 26/40
8189/8189 [==============================] - 1s - loss: 0.3782 - acc: 0.8445 - val_loss: 0.3679 - val_acc: 0.8560
Epoch 27/40
8189/8189 [==============================] - 1s - loss: 0.3774 - acc: 0.8443 - val_loss: 0.3678 - val_acc: 0.8589
Epoch 28/40
8189/8189 [==============================] - 1s - loss: 0.3768 - acc: 0.8437 - val_loss: 0.3679 - val_acc: 0.8579
Epoch 29/40
8189/8189 [==============================] - 1s - loss: 0.3766 - acc: 0.8450 - val_loss: 0.3661 - val_acc: 0.8599
Epoch 30/40
8189/8189 [==============================] - 1s - loss: 0.3755 - acc: 0.8439 - val_loss: 0.3657 - val_acc: 0.8584
Epoch 31/40
8189/8189 [==============================] - 1s - loss: 0.3754 - acc: 0.8442 - val_loss: 0.3665 - val_acc: 0.8623
Epoch 32/40
8189/8189 [==============================] - 1s - loss: 0.3748 - acc: 0.8474 - val_loss: 0.3670 - val_acc: 0.8608
Epoch 33/40
8189/8189 [==============================] - 1s - loss: 0.3739 - acc: 0.8466 - val_loss: 0.3676 - val_acc: 0.8599
Epoch 34/40
8189/8189 [==============================] - 1s - loss: 0.3736 - acc: 0.8464 - val_loss: 0.3685 - val_acc: 0.8579
Epoch 35/40
8189/8189 [==============================] - 1s - loss: 0.3732 - acc: 0.8470 - val_loss: 0.3674 - val_acc: 0.8579
Epoch 36/40
8189/8189 [==============================] - 1s - loss: 0.3724 - acc: 0.8471 - val_loss: 0.3672 - val_acc: 0.8594
Epoch 37/40
8189/8189 [==============================] - 1s - loss: 0.3720 - acc: 0.8458 - val_loss: 0.3680 - val_acc: 0.8579
Epoch 38/40
8189/8189 [==============================] - 1s - loss: 0.3716 - acc: 0.8472 - val_loss: 0.3682 - val_acc: 0.8618
Epoch 39/40
8189/8189 [==============================] - 1s - loss: 0.3710 - acc: 0.8466 - val_loss: 0.3690 - val_acc: 0.8584
Epoch 40/40
8189/8189 [==============================] - 1s - loss: 0.3705 - acc: 0.8475 - val_loss: 0.3686 - val_acc: 0.8604
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-44.h5
chunk number 44
prepare data
start training
Train on 8052 samples, validate on 2013 samples
Epoch 1/40
8052/8052 [==============================] - 1s - loss: 0.3853 - acc: 0.8399 - val_loss: 0.3619 - val_acc: 0.8530
Epoch 2/40
8052/8052 [==============================] - 1s - loss: 0.3851 - acc: 0.8382 - val_loss: 0.3600 - val_acc: 0.8535
Epoch 3/40
8052/8052 [==============================] - 1s - loss: 0.3831 - acc: 0.8405 - val_loss: 0.3600 - val_acc: 0.8539
Epoch 4/40
8052/8052 [==============================] - 1s - loss: 0.3822 - acc: 0.8414 - val_loss: 0.3605 - val_acc: 0.8549
Epoch 5/40
8052/8052 [==============================] - 1s - loss: 0.3812 - acc: 0.8394 - val_loss: 0.3602 - val_acc: 0.8544
Epoch 6/40
8052/8052 [==============================] - 1s - loss: 0.3804 - acc: 0.8409 - val_loss: 0.3597 - val_acc: 0.8535
Epoch 7/40
8052/8052 [==============================] - 1s - loss: 0.3797 - acc: 0.8420 - val_loss: 0.3588 - val_acc: 0.8544
Epoch 8/40
8052/8052 [==============================] - 1s - loss: 0.3784 - acc: 0.8426 - val_loss: 0.3581 - val_acc: 0.8554
Epoch 9/40
8052/8052 [==============================] - 1s - loss: 0.3777 - acc: 0.8417 - val_loss: 0.3567 - val_acc: 0.8554
Epoch 10/40
8052/8052 [==============================] - 1s - loss: 0.3766 - acc: 0.8425 - val_loss: 0.3560 - val_acc: 0.8574
Epoch 11/40
8052/8052 [==============================] - 1s - loss: 0.3757 - acc: 0.8434 - val_loss: 0.3558 - val_acc: 0.8584
Epoch 12/40
8052/8052 [==============================] - 1s - loss: 0.3750 - acc: 0.8440 - val_loss: 0.3549 - val_acc: 0.8584
Epoch 13/40
8052/8052 [==============================] - 1s - loss: 0.3740 - acc: 0.8435 - val_loss: 0.3541 - val_acc: 0.8604
Epoch 14/40
8052/8052 [==============================] - 1s - loss: 0.3733 - acc: 0.8444 - val_loss: 0.3542 - val_acc: 0.8629
Epoch 15/40
8052/8052 [==============================] - 1s - loss: 0.3725 - acc: 0.8434 - val_loss: 0.3534 - val_acc: 0.8609
Epoch 16/40
8052/8052 [==============================] - 1s - loss: 0.3717 - acc: 0.8434 - val_loss: 0.3533 - val_acc: 0.8629
Epoch 17/40
8052/8052 [==============================] - 1s - loss: 0.3710 - acc: 0.8446 - val_loss: 0.3539 - val_acc: 0.8614
Epoch 18/40
8052/8052 [==============================] - 1s - loss: 0.3703 - acc: 0.8443 - val_loss: 0.3536 - val_acc: 0.8614
Epoch 19/40
8052/8052 [==============================] - 1s - loss: 0.3695 - acc: 0.8466 - val_loss: 0.3538 - val_acc: 0.8619
Epoch 20/40
8052/8052 [==============================] - 1s - loss: 0.3688 - acc: 0.8462 - val_loss: 0.3539 - val_acc: 0.8634
Epoch 21/40
8052/8052 [==============================] - 1s - loss: 0.3682 - acc: 0.8477 - val_loss: 0.3535 - val_acc: 0.8644
Epoch 22/40
8052/8052 [==============================] - 1s - loss: 0.3675 - acc: 0.8484 - val_loss: 0.3537 - val_acc: 0.8634
Epoch 23/40
8052/8052 [==============================] - 1s - loss: 0.3669 - acc: 0.8472 - val_loss: 0.3531 - val_acc: 0.8654
Epoch 24/40
8052/8052 [==============================] - 1s - loss: 0.3664 - acc: 0.8465 - val_loss: 0.3533 - val_acc: 0.8639
Epoch 25/40
8052/8052 [==============================] - 1s - loss: 0.3659 - acc: 0.8479 - val_loss: 0.3526 - val_acc: 0.8624
Epoch 26/40
8052/8052 [==============================] - 1s - loss: 0.3654 - acc: 0.8474 - val_loss: 0.3525 - val_acc: 0.8624
Epoch 27/40
8052/8052 [==============================] - 1s - loss: 0.3649 - acc: 0.8487 - val_loss: 0.3517 - val_acc: 0.8634
Epoch 28/40
8052/8052 [==============================] - 1s - loss: 0.3643 - acc: 0.8482 - val_loss: 0.3515 - val_acc: 0.8629
Epoch 29/40
8052/8052 [==============================] - 1s - loss: 0.3638 - acc: 0.8485 - val_loss: 0.3513 - val_acc: 0.8624
Epoch 30/40
8052/8052 [==============================] - 1s - loss: 0.3632 - acc: 0.8494 - val_loss: 0.3514 - val_acc: 0.8644
Epoch 31/40
8052/8052 [==============================] - 1s - loss: 0.3627 - acc: 0.8500 - val_loss: 0.3515 - val_acc: 0.8644
Epoch 32/40
8052/8052 [==============================] - 1s - loss: 0.3622 - acc: 0.8506 - val_loss: 0.3514 - val_acc: 0.8639
Epoch 33/40
8052/8052 [==============================] - 1s - loss: 0.3617 - acc: 0.8507 - val_loss: 0.3516 - val_acc: 0.8619
Epoch 34/40
8052/8052 [==============================] - 1s - loss: 0.3612 - acc: 0.8503 - val_loss: 0.3510 - val_acc: 0.8644
Epoch 35/40
8052/8052 [==============================] - 1s - loss: 0.3609 - acc: 0.8511 - val_loss: 0.3527 - val_acc: 0.8609
Epoch 36/40
8052/8052 [==============================] - 1s - loss: 0.3608 - acc: 0.8508 - val_loss: 0.3517 - val_acc: 0.8629
Epoch 37/40
8052/8052 [==============================] - 1s - loss: 0.3623 - acc: 0.8508 - val_loss: 0.3584 - val_acc: 0.8584
Epoch 38/40
8052/8052 [==============================] - 1s - loss: 0.3645 - acc: 0.8475 - val_loss: 0.3580 - val_acc: 0.8584
Epoch 39/40
8052/8052 [==============================] - 1s - loss: 0.3701 - acc: 0.8471 - val_loss: 0.3512 - val_acc: 0.8644
Epoch 40/40
8052/8052 [==============================] - 1s - loss: 0.3589 - acc: 0.8517 - val_loss: 0.3690 - val_acc: 0.8525
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-45.h5
chunk number 45
prepare data
start training
Train on 8124 samples, validate on 2032 samples
Epoch 1/40
8124/8124 [==============================] - 1s - loss: 0.3874 - acc: 0.8344 - val_loss: 0.3850 - val_acc: 0.8415
Epoch 2/40
8124/8124 [==============================] - 1s - loss: 0.3839 - acc: 0.8423 - val_loss: 0.3840 - val_acc: 0.8396
Epoch 3/40
8124/8124 [==============================] - 1s - loss: 0.3794 - acc: 0.8426 - val_loss: 0.3850 - val_acc: 0.8406
Epoch 4/40
8124/8124 [==============================] - 1s - loss: 0.3769 - acc: 0.8413 - val_loss: 0.3836 - val_acc: 0.8420
Epoch 5/40
8124/8124 [==============================] - 1s - loss: 0.3750 - acc: 0.8432 - val_loss: 0.3888 - val_acc: 0.8401
Epoch 6/40
8124/8124 [==============================] - 1s - loss: 0.3793 - acc: 0.8431 - val_loss: 0.3817 - val_acc: 0.8396
Epoch 7/40
8124/8124 [==============================] - 1s - loss: 0.3721 - acc: 0.8468 - val_loss: 0.3828 - val_acc: 0.8445
Epoch 8/40
8124/8124 [==============================] - 1s - loss: 0.3739 - acc: 0.8455 - val_loss: 0.3809 - val_acc: 0.8420
Epoch 9/40
8124/8124 [==============================] - 1s - loss: 0.3736 - acc: 0.8484 - val_loss: 0.3805 - val_acc: 0.8391
Epoch 10/40
8124/8124 [==============================] - 1s - loss: 0.3744 - acc: 0.8491 - val_loss: 0.3810 - val_acc: 0.8435
Epoch 11/40
8124/8124 [==============================] - 1s - loss: 0.3724 - acc: 0.8465 - val_loss: 0.3838 - val_acc: 0.8430
Epoch 12/40
8124/8124 [==============================] - 1s - loss: 0.3732 - acc: 0.8470 - val_loss: 0.3802 - val_acc: 0.8445
Epoch 13/40
8124/8124 [==============================] - 1s - loss: 0.3706 - acc: 0.8495 - val_loss: 0.3805 - val_acc: 0.8410
Epoch 14/40
8124/8124 [==============================] - 1s - loss: 0.3709 - acc: 0.8503 - val_loss: 0.3821 - val_acc: 0.8450
Epoch 15/40
8124/8124 [==============================] - 1s - loss: 0.3686 - acc: 0.8486 - val_loss: 0.3842 - val_acc: 0.8420
Epoch 16/40
8124/8124 [==============================] - 1s - loss: 0.3685 - acc: 0.8496 - val_loss: 0.3827 - val_acc: 0.8406
Epoch 17/40
8124/8124 [==============================] - 1s - loss: 0.3671 - acc: 0.8496 - val_loss: 0.3825 - val_acc: 0.8376
Epoch 18/40
8124/8124 [==============================] - 1s - loss: 0.3666 - acc: 0.8506 - val_loss: 0.3822 - val_acc: 0.8410
Epoch 19/40
8124/8124 [==============================] - 1s - loss: 0.3653 - acc: 0.8506 - val_loss: 0.3817 - val_acc: 0.8415
Epoch 20/40
8124/8124 [==============================] - 1s - loss: 0.3649 - acc: 0.8500 - val_loss: 0.3786 - val_acc: 0.8396
Epoch 21/40
8124/8124 [==============================] - 1s - loss: 0.3638 - acc: 0.8534 - val_loss: 0.3785 - val_acc: 0.8410
Epoch 22/40
8124/8124 [==============================] - 1s - loss: 0.3632 - acc: 0.8541 - val_loss: 0.3813 - val_acc: 0.8410
Epoch 23/40
8124/8124 [==============================] - 1s - loss: 0.3625 - acc: 0.8530 - val_loss: 0.3814 - val_acc: 0.8420
Epoch 24/40
8124/8124 [==============================] - 1s - loss: 0.3612 - acc: 0.8546 - val_loss: 0.3816 - val_acc: 0.8410
Epoch 25/40
8124/8124 [==============================] - 1s - loss: 0.3606 - acc: 0.8529 - val_loss: 0.3799 - val_acc: 0.8381
Epoch 26/40
8124/8124 [==============================] - 1s - loss: 0.3600 - acc: 0.8535 - val_loss: 0.3792 - val_acc: 0.8391
Epoch 27/40
8124/8124 [==============================] - 1s - loss: 0.3594 - acc: 0.8545 - val_loss: 0.3803 - val_acc: 0.8401
Epoch 28/40
8124/8124 [==============================] - 1s - loss: 0.3583 - acc: 0.8566 - val_loss: 0.3830 - val_acc: 0.8401
Epoch 29/40
8124/8124 [==============================] - 1s - loss: 0.3584 - acc: 0.8541 - val_loss: 0.3810 - val_acc: 0.8371
Epoch 30/40
8124/8124 [==============================] - 1s - loss: 0.3576 - acc: 0.8546 - val_loss: 0.3813 - val_acc: 0.8396
Epoch 31/40
8124/8124 [==============================] - 1s - loss: 0.3566 - acc: 0.8560 - val_loss: 0.3803 - val_acc: 0.8396
Epoch 32/40
8124/8124 [==============================] - 1s - loss: 0.3565 - acc: 0.8541 - val_loss: 0.3823 - val_acc: 0.8381
Epoch 33/40
8124/8124 [==============================] - 1s - loss: 0.3554 - acc: 0.8557 - val_loss: 0.3830 - val_acc: 0.8391
Epoch 34/40
8124/8124 [==============================] - 1s - loss: 0.3551 - acc: 0.8556 - val_loss: 0.3861 - val_acc: 0.8396
Epoch 35/40
8124/8124 [==============================] - 1s - loss: 0.3545 - acc: 0.8550 - val_loss: 0.3823 - val_acc: 0.8371
Epoch 36/40
8124/8124 [==============================] - 1s - loss: 0.3532 - acc: 0.8552 - val_loss: 0.3814 - val_acc: 0.8381
Epoch 37/40
8124/8124 [==============================] - 1s - loss: 0.3528 - acc: 0.8551 - val_loss: 0.3824 - val_acc: 0.8376
Epoch 38/40
8124/8124 [==============================] - 1s - loss: 0.3527 - acc: 0.8560 - val_loss: 0.3887 - val_acc: 0.8356
Epoch 39/40
8124/8124 [==============================] - 1s - loss: 0.3558 - acc: 0.8554 - val_loss: 0.3864 - val_acc: 0.8386
Epoch 40/40
8124/8124 [==============================] - 1s - loss: 0.3653 - acc: 0.8474 - val_loss: 0.3836 - val_acc: 0.8406
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-46.h5
chunk number 46
prepare data
start training
Train on 8172 samples, validate on 2043 samples
Epoch 1/40
8172/8172 [==============================] - 1s - loss: 0.3685 - acc: 0.8501 - val_loss: 0.3875 - val_acc: 0.8439
Epoch 2/40
8172/8172 [==============================] - 1s - loss: 0.3745 - acc: 0.8448 - val_loss: 0.3833 - val_acc: 0.8492
Epoch 3/40
8172/8172 [==============================] - 1s - loss: 0.3667 - acc: 0.8495 - val_loss: 0.3891 - val_acc: 0.8429
Epoch 4/40
8172/8172 [==============================] - 1s - loss: 0.3741 - acc: 0.8468 - val_loss: 0.3799 - val_acc: 0.8488
Epoch 5/40
8172/8172 [==============================] - 1s - loss: 0.3662 - acc: 0.8473 - val_loss: 0.3883 - val_acc: 0.8439
Epoch 6/40
8172/8172 [==============================] - 1s - loss: 0.3743 - acc: 0.8418 - val_loss: 0.3804 - val_acc: 0.8488
Epoch 7/40
8172/8172 [==============================] - 1s - loss: 0.3638 - acc: 0.8497 - val_loss: 0.3862 - val_acc: 0.8443
Epoch 8/40
8172/8172 [==============================] - 1s - loss: 0.3681 - acc: 0.8500 - val_loss: 0.3808 - val_acc: 0.8463
Epoch 9/40
8172/8172 [==============================] - 1s - loss: 0.3611 - acc: 0.8538 - val_loss: 0.3846 - val_acc: 0.8468
Epoch 10/40
8172/8172 [==============================] - 1s - loss: 0.3667 - acc: 0.8499 - val_loss: 0.3799 - val_acc: 0.8488
Epoch 11/40
8172/8172 [==============================] - 1s - loss: 0.3593 - acc: 0.8523 - val_loss: 0.3812 - val_acc: 0.8483
Epoch 12/40
8172/8172 [==============================] - 1s - loss: 0.3611 - acc: 0.8508 - val_loss: 0.3769 - val_acc: 0.8468
Epoch 13/40
8172/8172 [==============================] - 1s - loss: 0.3585 - acc: 0.8517 - val_loss: 0.3765 - val_acc: 0.8483
Epoch 14/40
8172/8172 [==============================] - 1s - loss: 0.3587 - acc: 0.8516 - val_loss: 0.3744 - val_acc: 0.8473
Epoch 15/40
8172/8172 [==============================] - 1s - loss: 0.3557 - acc: 0.8535 - val_loss: 0.3767 - val_acc: 0.8463
Epoch 16/40
8172/8172 [==============================] - 1s - loss: 0.3563 - acc: 0.8558 - val_loss: 0.3783 - val_acc: 0.8507
Epoch 17/40
8172/8172 [==============================] - 1s - loss: 0.3565 - acc: 0.8545 - val_loss: 0.3787 - val_acc: 0.8502
Epoch 18/40
8172/8172 [==============================] - 1s - loss: 0.3539 - acc: 0.8541 - val_loss: 0.3797 - val_acc: 0.8492
Epoch 19/40
8172/8172 [==============================] - 1s - loss: 0.3534 - acc: 0.8539 - val_loss: 0.3786 - val_acc: 0.8507
Epoch 20/40
8172/8172 [==============================] - 1s - loss: 0.3522 - acc: 0.8534 - val_loss: 0.3783 - val_acc: 0.8492
Epoch 21/40
8172/8172 [==============================] - 1s - loss: 0.3525 - acc: 0.8545 - val_loss: 0.3772 - val_acc: 0.8473
Epoch 22/40
8172/8172 [==============================] - 1s - loss: 0.3510 - acc: 0.8551 - val_loss: 0.3786 - val_acc: 0.8483
Epoch 23/40
8172/8172 [==============================] - 1s - loss: 0.3495 - acc: 0.8562 - val_loss: 0.3812 - val_acc: 0.8473
Epoch 24/40
8172/8172 [==============================] - 1s - loss: 0.3500 - acc: 0.8555 - val_loss: 0.3814 - val_acc: 0.8478
Epoch 25/40
8172/8172 [==============================] - 1s - loss: 0.3491 - acc: 0.8560 - val_loss: 0.3790 - val_acc: 0.8507
Epoch 26/40
8172/8172 [==============================] - 1s - loss: 0.3474 - acc: 0.8568 - val_loss: 0.3773 - val_acc: 0.8527
Epoch 27/40
8172/8172 [==============================] - 1s - loss: 0.3472 - acc: 0.8562 - val_loss: 0.3771 - val_acc: 0.8483
Epoch 28/40
8172/8172 [==============================] - 1s - loss: 0.3471 - acc: 0.8583 - val_loss: 0.3800 - val_acc: 0.8488
Epoch 29/40
8172/8172 [==============================] - 1s - loss: 0.3476 - acc: 0.8578 - val_loss: 0.3833 - val_acc: 0.8453
Epoch 30/40
8172/8172 [==============================] - 1s - loss: 0.3484 - acc: 0.8574 - val_loss: 0.3828 - val_acc: 0.8497
Epoch 31/40
8172/8172 [==============================] - 1s - loss: 0.3449 - acc: 0.8611 - val_loss: 0.3819 - val_acc: 0.8502
Epoch 32/40
8172/8172 [==============================] - 1s - loss: 0.3436 - acc: 0.8611 - val_loss: 0.3813 - val_acc: 0.8517
Epoch 33/40
8172/8172 [==============================] - 1s - loss: 0.3433 - acc: 0.8607 - val_loss: 0.3807 - val_acc: 0.8478
Epoch 34/40
8172/8172 [==============================] - 1s - loss: 0.3427 - acc: 0.8620 - val_loss: 0.3830 - val_acc: 0.8497
Epoch 35/40
8172/8172 [==============================] - 1s - loss: 0.3461 - acc: 0.8590 - val_loss: 0.3834 - val_acc: 0.8419
Epoch 36/40
8172/8172 [==============================] - 1s - loss: 0.3479 - acc: 0.8577 - val_loss: 0.3820 - val_acc: 0.8517
Epoch 37/40
8172/8172 [==============================] - 1s - loss: 0.3427 - acc: 0.8616 - val_loss: 0.3808 - val_acc: 0.8507
Epoch 38/40
8172/8172 [==============================] - 1s - loss: 0.3402 - acc: 0.8638 - val_loss: 0.3818 - val_acc: 0.8507
Epoch 39/40
8172/8172 [==============================] - 1s - loss: 0.3419 - acc: 0.8627 - val_loss: 0.3914 - val_acc: 0.8448
Epoch 40/40
8172/8172 [==============================] - 1s - loss: 0.3565 - acc: 0.8522 - val_loss: 0.4011 - val_acc: 0.8350
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-47.h5
chunk number 47
prepare data
start training
Train on 8160 samples, validate on 2040 samples
Epoch 1/40
8160/8160 [==============================] - 1s - loss: 0.3987 - acc: 0.8304 - val_loss: 0.4047 - val_acc: 0.8353
Epoch 2/40
8160/8160 [==============================] - 1s - loss: 0.3867 - acc: 0.8438 - val_loss: 0.3921 - val_acc: 0.8324
Epoch 3/40
8160/8160 [==============================] - 1s - loss: 0.3799 - acc: 0.8450 - val_loss: 0.4038 - val_acc: 0.8265
Epoch 4/40
8160/8160 [==============================] - 1s - loss: 0.3924 - acc: 0.8348 - val_loss: 0.3951 - val_acc: 0.8377
Epoch 5/40
8160/8160 [==============================] - 1s - loss: 0.3824 - acc: 0.8445 - val_loss: 0.4017 - val_acc: 0.8319
Epoch 6/40
8160/8160 [==============================] - 1s - loss: 0.3903 - acc: 0.8392 - val_loss: 0.3928 - val_acc: 0.8397
Epoch 7/40
8160/8160 [==============================] - 1s - loss: 0.3803 - acc: 0.8435 - val_loss: 0.3966 - val_acc: 0.8363
Epoch 8/40
8160/8160 [==============================] - 1s - loss: 0.3815 - acc: 0.8474 - val_loss: 0.4017 - val_acc: 0.8304
Epoch 9/40
8160/8160 [==============================] - 1s - loss: 0.3854 - acc: 0.8439 - val_loss: 0.3920 - val_acc: 0.8382
Epoch 10/40
8160/8160 [==============================] - 1s - loss: 0.3758 - acc: 0.8457 - val_loss: 0.3964 - val_acc: 0.8363
Epoch 11/40
8160/8160 [==============================] - 1s - loss: 0.3820 - acc: 0.8429 - val_loss: 0.3955 - val_acc: 0.8368
Epoch 12/40
8160/8160 [==============================] - 1s - loss: 0.3795 - acc: 0.8439 - val_loss: 0.3917 - val_acc: 0.8358
Epoch 13/40
8160/8160 [==============================] - 1s - loss: 0.3726 - acc: 0.8484 - val_loss: 0.4009 - val_acc: 0.8250
Epoch 14/40
8160/8160 [==============================] - 1s - loss: 0.3796 - acc: 0.8457 - val_loss: 0.3890 - val_acc: 0.8358
Epoch 15/40
8160/8160 [==============================] - 1s - loss: 0.3693 - acc: 0.8491 - val_loss: 0.3922 - val_acc: 0.8392
Epoch 16/40
8160/8160 [==============================] - 1s - loss: 0.3732 - acc: 0.8471 - val_loss: 0.3908 - val_acc: 0.8348
Epoch 17/40
8160/8160 [==============================] - 1s - loss: 0.3710 - acc: 0.8504 - val_loss: 0.3908 - val_acc: 0.8319
Epoch 18/40
8160/8160 [==============================] - 1s - loss: 0.3678 - acc: 0.8484 - val_loss: 0.3946 - val_acc: 0.8328
Epoch 19/40
8160/8160 [==============================] - 1s - loss: 0.3698 - acc: 0.8469 - val_loss: 0.3888 - val_acc: 0.8343
Epoch 20/40
8160/8160 [==============================] - 1s - loss: 0.3652 - acc: 0.8520 - val_loss: 0.3908 - val_acc: 0.8343
Epoch 21/40
8160/8160 [==============================] - 1s - loss: 0.3669 - acc: 0.8485 - val_loss: 0.3897 - val_acc: 0.8348
Epoch 22/40
8160/8160 [==============================] - 1s - loss: 0.3644 - acc: 0.8520 - val_loss: 0.3903 - val_acc: 0.8328
Epoch 23/40
8160/8160 [==============================] - 1s - loss: 0.3658 - acc: 0.8520 - val_loss: 0.3880 - val_acc: 0.8353
Epoch 24/40
8160/8160 [==============================] - 1s - loss: 0.3632 - acc: 0.8510 - val_loss: 0.3882 - val_acc: 0.8309
Epoch 25/40
8160/8160 [==============================] - 1s - loss: 0.3639 - acc: 0.8522 - val_loss: 0.3891 - val_acc: 0.8353
Epoch 26/40
8160/8160 [==============================] - 1s - loss: 0.3630 - acc: 0.8473 - val_loss: 0.3874 - val_acc: 0.8348
Epoch 27/40
8160/8160 [==============================] - 1s - loss: 0.3601 - acc: 0.8509 - val_loss: 0.3873 - val_acc: 0.8358
Epoch 28/40
8160/8160 [==============================] - 1s - loss: 0.3604 - acc: 0.8516 - val_loss: 0.3873 - val_acc: 0.8373
Epoch 29/40
8160/8160 [==============================] - 1s - loss: 0.3587 - acc: 0.8522 - val_loss: 0.3888 - val_acc: 0.8363
Epoch 30/40
8160/8160 [==============================] - 1s - loss: 0.3597 - acc: 0.8543 - val_loss: 0.3847 - val_acc: 0.8392
Epoch 31/40
8160/8160 [==============================] - 1s - loss: 0.3564 - acc: 0.8554 - val_loss: 0.3843 - val_acc: 0.8373
Epoch 32/40
8160/8160 [==============================] - 1s - loss: 0.3576 - acc: 0.8547 - val_loss: 0.3874 - val_acc: 0.8353
Epoch 33/40
8160/8160 [==============================] - 1s - loss: 0.3565 - acc: 0.8532 - val_loss: 0.3852 - val_acc: 0.8358
Epoch 34/40
8160/8160 [==============================] - 1s - loss: 0.3543 - acc: 0.8545 - val_loss: 0.3855 - val_acc: 0.8368
Epoch 35/40
8160/8160 [==============================] - 1s - loss: 0.3550 - acc: 0.8561 - val_loss: 0.3851 - val_acc: 0.8392
Epoch 36/40
8160/8160 [==============================] - 1s - loss: 0.3531 - acc: 0.8564 - val_loss: 0.3855 - val_acc: 0.8382
Epoch 37/40
8160/8160 [==============================] - 1s - loss: 0.3535 - acc: 0.8566 - val_loss: 0.3831 - val_acc: 0.8417
Epoch 38/40
8160/8160 [==============================] - 1s - loss: 0.3544 - acc: 0.8569 - val_loss: 0.3834 - val_acc: 0.8412
Epoch 39/40
8160/8160 [==============================] - 1s - loss: 0.3519 - acc: 0.8586 - val_loss: 0.3855 - val_acc: 0.8373
Epoch 40/40
8160/8160 [==============================] - 1s - loss: 0.3521 - acc: 0.8564 - val_loss: 0.3851 - val_acc: 0.8387
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-48.h5
chunk number 48
prepare data
start training
Train on 8160 samples, validate on 2040 samples
Epoch 1/40
8160/8160 [==============================] - 1s - loss: 0.3755 - acc: 0.8420 - val_loss: 0.4065 - val_acc: 0.8294
Epoch 2/40
8160/8160 [==============================] - 1s - loss: 0.3770 - acc: 0.8430 - val_loss: 0.4007 - val_acc: 0.8387
Epoch 3/40
8160/8160 [==============================] - 1s - loss: 0.3731 - acc: 0.8434 - val_loss: 0.3966 - val_acc: 0.8417
Epoch 4/40
8160/8160 [==============================] - 1s - loss: 0.3726 - acc: 0.8435 - val_loss: 0.3962 - val_acc: 0.8373
Epoch 5/40
8160/8160 [==============================] - 1s - loss: 0.3708 - acc: 0.8458 - val_loss: 0.3955 - val_acc: 0.8387
Epoch 6/40
8160/8160 [==============================] - 1s - loss: 0.3703 - acc: 0.8457 - val_loss: 0.3925 - val_acc: 0.8397
Epoch 7/40
8160/8160 [==============================] - 1s - loss: 0.3679 - acc: 0.8458 - val_loss: 0.3935 - val_acc: 0.8412
Epoch 8/40
8160/8160 [==============================] - 1s - loss: 0.3670 - acc: 0.8462 - val_loss: 0.3965 - val_acc: 0.8343
Epoch 9/40
8160/8160 [==============================] - 1s - loss: 0.3667 - acc: 0.8490 - val_loss: 0.3964 - val_acc: 0.8368
Epoch 10/40
8160/8160 [==============================] - 1s - loss: 0.3655 - acc: 0.8480 - val_loss: 0.3950 - val_acc: 0.8377
Epoch 11/40
8160/8160 [==============================] - 1s - loss: 0.3649 - acc: 0.8467 - val_loss: 0.3949 - val_acc: 0.8353
Epoch 12/40
8160/8160 [==============================] - 1s - loss: 0.3635 - acc: 0.8495 - val_loss: 0.3953 - val_acc: 0.8319
Epoch 13/40
8160/8160 [==============================] - 1s - loss: 0.3631 - acc: 0.8507 - val_loss: 0.3945 - val_acc: 0.8348
Epoch 14/40
8160/8160 [==============================] - 1s - loss: 0.3624 - acc: 0.8491 - val_loss: 0.3960 - val_acc: 0.8299
Epoch 15/40
8160/8160 [==============================] - 1s - loss: 0.3607 - acc: 0.8512 - val_loss: 0.3972 - val_acc: 0.8279
Epoch 16/40
8160/8160 [==============================] - 1s - loss: 0.3598 - acc: 0.8520 - val_loss: 0.3967 - val_acc: 0.8309
Epoch 17/40
8160/8160 [==============================] - 1s - loss: 0.3596 - acc: 0.8517 - val_loss: 0.3991 - val_acc: 0.8240
Epoch 18/40
8160/8160 [==============================] - 1s - loss: 0.3587 - acc: 0.8523 - val_loss: 0.3964 - val_acc: 0.8284
Epoch 19/40
8160/8160 [==============================] - 1s - loss: 0.3576 - acc: 0.8532 - val_loss: 0.3977 - val_acc: 0.8250
Epoch 20/40
8160/8160 [==============================] - 1s - loss: 0.3567 - acc: 0.8539 - val_loss: 0.3968 - val_acc: 0.8260
Epoch 21/40
8160/8160 [==============================] - 1s - loss: 0.3557 - acc: 0.8544 - val_loss: 0.3976 - val_acc: 0.8245
Epoch 22/40
8160/8160 [==============================] - 1s - loss: 0.3551 - acc: 0.8543 - val_loss: 0.3979 - val_acc: 0.8245
Epoch 23/40
8160/8160 [==============================] - 1s - loss: 0.3545 - acc: 0.8545 - val_loss: 0.3975 - val_acc: 0.8275
Epoch 24/40
8160/8160 [==============================] - 1s - loss: 0.3538 - acc: 0.8539 - val_loss: 0.3981 - val_acc: 0.8255
Epoch 25/40
8160/8160 [==============================] - 1s - loss: 0.3532 - acc: 0.8550 - val_loss: 0.3978 - val_acc: 0.8270
Epoch 26/40
8160/8160 [==============================] - 1s - loss: 0.3526 - acc: 0.8553 - val_loss: 0.4006 - val_acc: 0.8221
Epoch 27/40
8160/8160 [==============================] - 1s - loss: 0.3525 - acc: 0.8558 - val_loss: 0.3988 - val_acc: 0.8299
Epoch 28/40
8160/8160 [==============================] - 1s - loss: 0.3552 - acc: 0.8554 - val_loss: 0.4062 - val_acc: 0.8216
Epoch 29/40
8160/8160 [==============================] - 1s - loss: 0.3588 - acc: 0.8509 - val_loss: 0.4011 - val_acc: 0.8314
Epoch 30/40
8160/8160 [==============================] - 1s - loss: 0.3602 - acc: 0.8511 - val_loss: 0.4022 - val_acc: 0.8235
Epoch 31/40
8160/8160 [==============================] - 1s - loss: 0.3535 - acc: 0.8537 - val_loss: 0.4016 - val_acc: 0.8216
Epoch 32/40
8160/8160 [==============================] - 1s - loss: 0.3516 - acc: 0.8560 - val_loss: 0.4016 - val_acc: 0.8314
Epoch 33/40
8160/8160 [==============================] - 1s - loss: 0.3579 - acc: 0.8528 - val_loss: 0.4071 - val_acc: 0.8167
Epoch 34/40
8160/8160 [==============================] - 1s - loss: 0.3594 - acc: 0.8512 - val_loss: 0.4026 - val_acc: 0.8304
Epoch 35/40
8160/8160 [==============================] - 1s - loss: 0.3600 - acc: 0.8504 - val_loss: 0.4020 - val_acc: 0.8270
Epoch 36/40
8160/8160 [==============================] - 1s - loss: 0.3521 - acc: 0.8555 - val_loss: 0.4020 - val_acc: 0.8245
Epoch 37/40
8160/8160 [==============================] - 1s - loss: 0.3509 - acc: 0.8550 - val_loss: 0.4006 - val_acc: 0.8299
Epoch 38/40
8160/8160 [==============================] - 1s - loss: 0.3564 - acc: 0.8534 - val_loss: 0.4018 - val_acc: 0.8176
Epoch 39/40
8160/8160 [==============================] - 1s - loss: 0.3512 - acc: 0.8549 - val_loss: 0.3978 - val_acc: 0.8250
Epoch 40/40
8160/8160 [==============================] - 1s - loss: 0.3472 - acc: 0.8586 - val_loss: 0.3991 - val_acc: 0.8284
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-49.h5
chunk number 49
prepare data
start training
Train on 8063 samples, validate on 2016 samples
Epoch 1/40
8063/8063 [==============================] - 1s - loss: 0.3716 - acc: 0.8486 - val_loss: 0.3957 - val_acc: 0.8428
Epoch 2/40
8063/8063 [==============================] - 1s - loss: 0.3708 - acc: 0.8488 - val_loss: 0.3956 - val_acc: 0.8418
Epoch 3/40
8063/8063 [==============================] - 1s - loss: 0.3695 - acc: 0.8501 - val_loss: 0.3940 - val_acc: 0.8383
Epoch 4/40
8063/8063 [==============================] - 1s - loss: 0.3684 - acc: 0.8492 - val_loss: 0.3919 - val_acc: 0.8408
Epoch 5/40
8063/8063 [==============================] - 1s - loss: 0.3669 - acc: 0.8497 - val_loss: 0.3905 - val_acc: 0.8363
Epoch 6/40
8063/8063 [==============================] - 1s - loss: 0.3652 - acc: 0.8503 - val_loss: 0.3905 - val_acc: 0.8383
Epoch 7/40
8063/8063 [==============================] - 1s - loss: 0.3643 - acc: 0.8520 - val_loss: 0.3901 - val_acc: 0.8373
Epoch 8/40
8063/8063 [==============================] - 1s - loss: 0.3630 - acc: 0.8512 - val_loss: 0.3897 - val_acc: 0.8378
Epoch 9/40
8063/8063 [==============================] - 1s - loss: 0.3616 - acc: 0.8528 - val_loss: 0.3893 - val_acc: 0.8393
Epoch 10/40
8063/8063 [==============================] - 1s - loss: 0.3606 - acc: 0.8534 - val_loss: 0.3885 - val_acc: 0.8388
Epoch 11/40
8063/8063 [==============================] - 1s - loss: 0.3599 - acc: 0.8534 - val_loss: 0.3888 - val_acc: 0.8393
Epoch 12/40
8063/8063 [==============================] - 1s - loss: 0.3591 - acc: 0.8546 - val_loss: 0.3899 - val_acc: 0.8383
Epoch 13/40
8063/8063 [==============================] - 1s - loss: 0.3584 - acc: 0.8549 - val_loss: 0.3902 - val_acc: 0.8393
Epoch 14/40
8063/8063 [==============================] - 1s - loss: 0.3575 - acc: 0.8559 - val_loss: 0.3913 - val_acc: 0.8388
Epoch 15/40
8063/8063 [==============================] - 1s - loss: 0.3566 - acc: 0.8561 - val_loss: 0.3929 - val_acc: 0.8383
Epoch 16/40
8063/8063 [==============================] - 1s - loss: 0.3560 - acc: 0.8558 - val_loss: 0.3929 - val_acc: 0.8373
Epoch 17/40
8063/8063 [==============================] - 1s - loss: 0.3552 - acc: 0.8560 - val_loss: 0.3930 - val_acc: 0.8373
Epoch 18/40
8063/8063 [==============================] - 1s - loss: 0.3544 - acc: 0.8566 - val_loss: 0.3932 - val_acc: 0.8403
Epoch 19/40
8063/8063 [==============================] - 1s - loss: 0.3536 - acc: 0.8577 - val_loss: 0.3937 - val_acc: 0.8388
Epoch 20/40
8063/8063 [==============================] - 1s - loss: 0.3528 - acc: 0.8584 - val_loss: 0.3939 - val_acc: 0.8398
Epoch 21/40
8063/8063 [==============================] - 1s - loss: 0.3520 - acc: 0.8590 - val_loss: 0.3942 - val_acc: 0.8398
Epoch 22/40
8063/8063 [==============================] - 1s - loss: 0.3513 - acc: 0.8582 - val_loss: 0.3943 - val_acc: 0.8388
Epoch 23/40
8063/8063 [==============================] - 1s - loss: 0.3506 - acc: 0.8596 - val_loss: 0.3944 - val_acc: 0.8368
Epoch 24/40
8063/8063 [==============================] - 1s - loss: 0.3499 - acc: 0.8600 - val_loss: 0.3946 - val_acc: 0.8393
Epoch 25/40
8063/8063 [==============================] - 1s - loss: 0.3494 - acc: 0.8607 - val_loss: 0.3957 - val_acc: 0.8378
Epoch 26/40
8063/8063 [==============================] - 1s - loss: 0.3496 - acc: 0.8601 - val_loss: 0.3959 - val_acc: 0.8418
Epoch 27/40
8063/8063 [==============================] - 1s - loss: 0.3517 - acc: 0.8594 - val_loss: 0.4018 - val_acc: 0.8318
Epoch 28/40
8063/8063 [==============================] - 1s - loss: 0.3558 - acc: 0.8574 - val_loss: 0.3967 - val_acc: 0.8413
Epoch 29/40
8063/8063 [==============================] - 1s - loss: 0.3525 - acc: 0.8584 - val_loss: 0.4005 - val_acc: 0.8323
Epoch 30/40
8063/8063 [==============================] - 1s - loss: 0.3521 - acc: 0.8595 - val_loss: 0.3951 - val_acc: 0.8408
Epoch 31/40
8063/8063 [==============================] - 1s - loss: 0.3467 - acc: 0.8606 - val_loss: 0.3951 - val_acc: 0.8403
Epoch 32/40
8063/8063 [==============================] - 1s - loss: 0.3459 - acc: 0.8606 - val_loss: 0.3995 - val_acc: 0.8318
Epoch 33/40
8063/8063 [==============================] - 1s - loss: 0.3507 - acc: 0.8595 - val_loss: 0.4000 - val_acc: 0.8378
Epoch 34/40
8063/8063 [==============================] - 1s - loss: 0.3515 - acc: 0.8597 - val_loss: 0.4032 - val_acc: 0.8279
Epoch 35/40
8063/8063 [==============================] - 1s - loss: 0.3530 - acc: 0.8577 - val_loss: 0.3968 - val_acc: 0.8393
Epoch 36/40
8063/8063 [==============================] - 1s - loss: 0.3459 - acc: 0.8620 - val_loss: 0.3969 - val_acc: 0.8378
Epoch 37/40
8063/8063 [==============================] - 1s - loss: 0.3436 - acc: 0.8616 - val_loss: 0.3979 - val_acc: 0.8383
Epoch 38/40
8063/8063 [==============================] - 1s - loss: 0.3430 - acc: 0.8621 - val_loss: 0.3986 - val_acc: 0.8403
Epoch 39/40
8063/8063 [==============================] - 1s - loss: 0.3440 - acc: 0.8630 - val_loss: 0.4055 - val_acc: 0.8289
Epoch 40/40
8063/8063 [==============================] - 1s - loss: 0.3485 - acc: 0.8602 - val_loss: 0.4011 - val_acc: 0.8388
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-50.h5
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_allparams/model-final.h5
