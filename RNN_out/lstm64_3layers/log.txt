chunk number 0
prepare data
start training
Train on 8136 samples, validate on 2035 samples
Epoch 1/40
8136/8136 [==============================] - 2s - loss: 0.7246 - acc: 0.3401 - val_loss: 0.6806 - val_acc: 0.6501
Epoch 2/40
8136/8136 [==============================] - 1s - loss: 0.6822 - acc: 0.6448 - val_loss: 0.6604 - val_acc: 0.6663
Epoch 3/40
8136/8136 [==============================] - 1s - loss: 0.6638 - acc: 0.6550 - val_loss: 0.6491 - val_acc: 0.6663
Epoch 4/40
8136/8136 [==============================] - 1s - loss: 0.6542 - acc: 0.6549 - val_loss: 0.6420 - val_acc: 0.6663
Epoch 5/40
8136/8136 [==============================] - 1s - loss: 0.6488 - acc: 0.6549 - val_loss: 0.6367 - val_acc: 0.6663
Epoch 6/40
8136/8136 [==============================] - 1s - loss: 0.6448 - acc: 0.6549 - val_loss: 0.6312 - val_acc: 0.6663
Epoch 7/40
8136/8136 [==============================] - 1s - loss: 0.6400 - acc: 0.6547 - val_loss: 0.6261 - val_acc: 0.6663
Epoch 8/40
8136/8136 [==============================] - 1s - loss: 0.6348 - acc: 0.6547 - val_loss: 0.6235 - val_acc: 0.6688
Epoch 9/40
8136/8136 [==============================] - 1s - loss: 0.6316 - acc: 0.6561 - val_loss: 0.6218 - val_acc: 0.6658
Epoch 10/40
8136/8136 [==============================] - 1s - loss: 0.6296 - acc: 0.6550 - val_loss: 0.6171 - val_acc: 0.6673
Epoch 11/40
8136/8136 [==============================] - 1s - loss: 0.6257 - acc: 0.6557 - val_loss: 0.6108 - val_acc: 0.6717
Epoch 12/40
8136/8136 [==============================] - 1s - loss: 0.6214 - acc: 0.6562 - val_loss: 0.6066 - val_acc: 0.6742
Epoch 13/40
8136/8136 [==============================] - 1s - loss: 0.6193 - acc: 0.6577 - val_loss: 0.6029 - val_acc: 0.6742
Epoch 14/40
8136/8136 [==============================] - 1s - loss: 0.6165 - acc: 0.6609 - val_loss: 0.6006 - val_acc: 0.6889
Epoch 15/40
8136/8136 [==============================] - 1s - loss: 0.6129 - acc: 0.6665 - val_loss: 0.5999 - val_acc: 0.6865
Epoch 16/40
8136/8136 [==============================] - 1s - loss: 0.6109 - acc: 0.6696 - val_loss: 0.5925 - val_acc: 0.6948
Epoch 17/40
8136/8136 [==============================] - 1s - loss: 0.6051 - acc: 0.6767 - val_loss: 0.5875 - val_acc: 0.7007
Epoch 18/40
8136/8136 [==============================] - 1s - loss: 0.6017 - acc: 0.6814 - val_loss: 0.5813 - val_acc: 0.7120
Epoch 19/40
8136/8136 [==============================] - 1s - loss: 0.5930 - acc: 0.6887 - val_loss: 0.5791 - val_acc: 0.7224
Epoch 20/40
8136/8136 [==============================] - 1s - loss: 0.5868 - acc: 0.7002 - val_loss: 0.5685 - val_acc: 0.7115
Epoch 21/40
8136/8136 [==============================] - 1s - loss: 0.5773 - acc: 0.7023 - val_loss: 0.5614 - val_acc: 0.7371
Epoch 22/40
8136/8136 [==============================] - 1s - loss: 0.5661 - acc: 0.7180 - val_loss: 0.5602 - val_acc: 0.7287
Epoch 23/40
8136/8136 [==============================] - 1s - loss: 0.5606 - acc: 0.7221 - val_loss: 0.5649 - val_acc: 0.7170
Epoch 24/40
8136/8136 [==============================] - 1s - loss: 0.5675 - acc: 0.7204 - val_loss: 0.5601 - val_acc: 0.7174
Epoch 25/40
8136/8136 [==============================] - 1s - loss: 0.5560 - acc: 0.7265 - val_loss: 0.5426 - val_acc: 0.7351
Epoch 26/40
8136/8136 [==============================] - 1s - loss: 0.5428 - acc: 0.7338 - val_loss: 0.5372 - val_acc: 0.7445
Epoch 27/40
8136/8136 [==============================] - 1s - loss: 0.5394 - acc: 0.7396 - val_loss: 0.5412 - val_acc: 0.7459
Epoch 28/40
8136/8136 [==============================] - 1s - loss: 0.5404 - acc: 0.7458 - val_loss: 0.5302 - val_acc: 0.7533
Epoch 29/40
8136/8136 [==============================] - 1s - loss: 0.5367 - acc: 0.7450 - val_loss: 0.5171 - val_acc: 0.7597
Epoch 30/40
8136/8136 [==============================] - 1s - loss: 0.5229 - acc: 0.7581 - val_loss: 0.5402 - val_acc: 0.7366
Epoch 31/40
8136/8136 [==============================] - 1s - loss: 0.5393 - acc: 0.7344 - val_loss: 0.5198 - val_acc: 0.7607
Epoch 32/40
8136/8136 [==============================] - 1s - loss: 0.5279 - acc: 0.7544 - val_loss: 0.5214 - val_acc: 0.7587
Epoch 33/40
8136/8136 [==============================] - 1s - loss: 0.5298 - acc: 0.7534 - val_loss: 0.5107 - val_acc: 0.7730
Epoch 34/40
8136/8136 [==============================] - 1s - loss: 0.5157 - acc: 0.7624 - val_loss: 0.5210 - val_acc: 0.7622
Epoch 35/40
8136/8136 [==============================] - 1s - loss: 0.5236 - acc: 0.7560 - val_loss: 0.5064 - val_acc: 0.7656
Epoch 36/40
8136/8136 [==============================] - 1s - loss: 0.5144 - acc: 0.7607 - val_loss: 0.5112 - val_acc: 0.7607
Epoch 37/40
8136/8136 [==============================] - 1s - loss: 0.5191 - acc: 0.7577 - val_loss: 0.4990 - val_acc: 0.7749
Epoch 38/40
8136/8136 [==============================] - 1s - loss: 0.5047 - acc: 0.7693 - val_loss: 0.5165 - val_acc: 0.7715
Epoch 39/40
8136/8136 [==============================] - 1s - loss: 0.5179 - acc: 0.7666 - val_loss: 0.4965 - val_acc: 0.7720
Epoch 40/40
8136/8136 [==============================] - 1s - loss: 0.5033 - acc: 0.7668 - val_loss: 0.5033 - val_acc: 0.7690
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-1.h5
chunk number 1
prepare data
start training
Train on 8160 samples, validate on 2041 samples
Epoch 1/40
8160/8160 [==============================] - 1s - loss: 0.5264 - acc: 0.7526 - val_loss: 0.5034 - val_acc: 0.7658
Epoch 2/40
8160/8160 [==============================] - 1s - loss: 0.5132 - acc: 0.7625 - val_loss: 0.5091 - val_acc: 0.7668
Epoch 3/40
8160/8160 [==============================] - 1s - loss: 0.5216 - acc: 0.7614 - val_loss: 0.4972 - val_acc: 0.7722
Epoch 4/40
8160/8160 [==============================] - 1s - loss: 0.5067 - acc: 0.7683 - val_loss: 0.5043 - val_acc: 0.7683
Epoch 5/40
8160/8160 [==============================] - 1s - loss: 0.5126 - acc: 0.7647 - val_loss: 0.4956 - val_acc: 0.7712
Epoch 6/40
8160/8160 [==============================] - 1s - loss: 0.5044 - acc: 0.7694 - val_loss: 0.5006 - val_acc: 0.7643
Epoch 7/40
8160/8160 [==============================] - 1s - loss: 0.5106 - acc: 0.7653 - val_loss: 0.4930 - val_acc: 0.7727
Epoch 8/40
8160/8160 [==============================] - 1s - loss: 0.5009 - acc: 0.7727 - val_loss: 0.4984 - val_acc: 0.7712
Epoch 9/40
8160/8160 [==============================] - 1s - loss: 0.5050 - acc: 0.7670 - val_loss: 0.4908 - val_acc: 0.7790
Epoch 10/40
8160/8160 [==============================] - 1s - loss: 0.4984 - acc: 0.7735 - val_loss: 0.4929 - val_acc: 0.7785
Epoch 11/40
8160/8160 [==============================] - 1s - loss: 0.5020 - acc: 0.7714 - val_loss: 0.4879 - val_acc: 0.7785
Epoch 12/40
8160/8160 [==============================] - 1s - loss: 0.4956 - acc: 0.7743 - val_loss: 0.4889 - val_acc: 0.7785
Epoch 13/40
8160/8160 [==============================] - 1s - loss: 0.4969 - acc: 0.7734 - val_loss: 0.4874 - val_acc: 0.7810
Epoch 14/40
8160/8160 [==============================] - 1s - loss: 0.4955 - acc: 0.7740 - val_loss: 0.4847 - val_acc: 0.7800
Epoch 15/40
8160/8160 [==============================] - 1s - loss: 0.4920 - acc: 0.7776 - val_loss: 0.4873 - val_acc: 0.7780
Epoch 16/40
8160/8160 [==============================] - 1s - loss: 0.4930 - acc: 0.7762 - val_loss: 0.4845 - val_acc: 0.7800
Epoch 17/40
8160/8160 [==============================] - 1s - loss: 0.4898 - acc: 0.7789 - val_loss: 0.4848 - val_acc: 0.7800
Epoch 18/40
8160/8160 [==============================] - 1s - loss: 0.4900 - acc: 0.7784 - val_loss: 0.4867 - val_acc: 0.7795
Epoch 19/40
8160/8160 [==============================] - 1s - loss: 0.4905 - acc: 0.7794 - val_loss: 0.4827 - val_acc: 0.7766
Epoch 20/40
8160/8160 [==============================] - 1s - loss: 0.4869 - acc: 0.7776 - val_loss: 0.4841 - val_acc: 0.7727
Epoch 21/40
8160/8160 [==============================] - 1s - loss: 0.4887 - acc: 0.7752 - val_loss: 0.4863 - val_acc: 0.7800
Epoch 22/40
8160/8160 [==============================] - 1s - loss: 0.4900 - acc: 0.7812 - val_loss: 0.4806 - val_acc: 0.7810
Epoch 23/40
8160/8160 [==============================] - 1s - loss: 0.4841 - acc: 0.7801 - val_loss: 0.4840 - val_acc: 0.7780
Epoch 24/40
8160/8160 [==============================] - 1s - loss: 0.4888 - acc: 0.7766 - val_loss: 0.4906 - val_acc: 0.7761
Epoch 25/40
8160/8160 [==============================] - 1s - loss: 0.4937 - acc: 0.7767 - val_loss: 0.4796 - val_acc: 0.7825
Epoch 26/40
8160/8160 [==============================] - 1s - loss: 0.4830 - acc: 0.7819 - val_loss: 0.4950 - val_acc: 0.7648
Epoch 27/40
8160/8160 [==============================] - 1s - loss: 0.5011 - acc: 0.7689 - val_loss: 0.4947 - val_acc: 0.7727
Epoch 28/40
8160/8160 [==============================] - 1s - loss: 0.4981 - acc: 0.7728 - val_loss: 0.4971 - val_acc: 0.7722
Epoch 29/40
8160/8160 [==============================] - 1s - loss: 0.4999 - acc: 0.7711 - val_loss: 0.4793 - val_acc: 0.7761
Epoch 30/40
8160/8160 [==============================] - 1s - loss: 0.4823 - acc: 0.7815 - val_loss: 0.4950 - val_acc: 0.7702
Epoch 31/40
8160/8160 [==============================] - 1s - loss: 0.5015 - acc: 0.7732 - val_loss: 0.4854 - val_acc: 0.7761
Epoch 32/40
8160/8160 [==============================] - 1s - loss: 0.4875 - acc: 0.7768 - val_loss: 0.4985 - val_acc: 0.7668
Epoch 33/40
8160/8160 [==============================] - 1s - loss: 0.5004 - acc: 0.7692 - val_loss: 0.4854 - val_acc: 0.7761
Epoch 34/40
8160/8160 [==============================] - 1s - loss: 0.4880 - acc: 0.7757 - val_loss: 0.4842 - val_acc: 0.7785
Epoch 35/40
8160/8160 [==============================] - 1s - loss: 0.4883 - acc: 0.7804 - val_loss: 0.4861 - val_acc: 0.7795
Epoch 36/40
8160/8160 [==============================] - 1s - loss: 0.4905 - acc: 0.7788 - val_loss: 0.4802 - val_acc: 0.7800
Epoch 37/40
8160/8160 [==============================] - 1s - loss: 0.4826 - acc: 0.7804 - val_loss: 0.4881 - val_acc: 0.7732
Epoch 38/40
8160/8160 [==============================] - 1s - loss: 0.4902 - acc: 0.7750 - val_loss: 0.4779 - val_acc: 0.7820
Epoch 39/40
8160/8160 [==============================] - 1s - loss: 0.4795 - acc: 0.7819 - val_loss: 0.4826 - val_acc: 0.7795
Epoch 40/40
8160/8160 [==============================] - 1s - loss: 0.4871 - acc: 0.7805 - val_loss: 0.4738 - val_acc: 0.7839
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-2.h5
chunk number 2
prepare data
start training
Train on 8182 samples, validate on 2046 samples
Epoch 1/40
8182/8182 [==============================] - 1s - loss: 0.4623 - acc: 0.7938 - val_loss: 0.4794 - val_acc: 0.7859
Epoch 2/40
8182/8182 [==============================] - 1s - loss: 0.4699 - acc: 0.7882 - val_loss: 0.4817 - val_acc: 0.7864
Epoch 3/40
8182/8182 [==============================] - 1s - loss: 0.4715 - acc: 0.7878 - val_loss: 0.4654 - val_acc: 0.7903
Epoch 4/40
8182/8182 [==============================] - 1s - loss: 0.4566 - acc: 0.7959 - val_loss: 0.4810 - val_acc: 0.7820
Epoch 5/40
8182/8182 [==============================] - 1s - loss: 0.4746 - acc: 0.7868 - val_loss: 0.4699 - val_acc: 0.7884
Epoch 6/40
8182/8182 [==============================] - 1s - loss: 0.4600 - acc: 0.7932 - val_loss: 0.4811 - val_acc: 0.7874
Epoch 7/40
8182/8182 [==============================] - 1s - loss: 0.4708 - acc: 0.7906 - val_loss: 0.4618 - val_acc: 0.7879
Epoch 8/40
8182/8182 [==============================] - 1s - loss: 0.4536 - acc: 0.7950 - val_loss: 0.4791 - val_acc: 0.7791
Epoch 9/40
8182/8182 [==============================] - 1s - loss: 0.4728 - acc: 0.7886 - val_loss: 0.4633 - val_acc: 0.7874
Epoch 10/40
8182/8182 [==============================] - 1s - loss: 0.4528 - acc: 0.7948 - val_loss: 0.4765 - val_acc: 0.7869
Epoch 11/40
8182/8182 [==============================] - 1s - loss: 0.4638 - acc: 0.7931 - val_loss: 0.4670 - val_acc: 0.7893
Epoch 12/40
8182/8182 [==============================] - 1s - loss: 0.4543 - acc: 0.7947 - val_loss: 0.4678 - val_acc: 0.7864
Epoch 13/40
8182/8182 [==============================] - 1s - loss: 0.4576 - acc: 0.7958 - val_loss: 0.4622 - val_acc: 0.7884
Epoch 14/40
8182/8182 [==============================] - 1s - loss: 0.4526 - acc: 0.7972 - val_loss: 0.4641 - val_acc: 0.7854
Epoch 15/40
8182/8182 [==============================] - 1s - loss: 0.4533 - acc: 0.7954 - val_loss: 0.4652 - val_acc: 0.7864
Epoch 16/40
8182/8182 [==============================] - 1s - loss: 0.4546 - acc: 0.7955 - val_loss: 0.4575 - val_acc: 0.7908
Epoch 17/40
8182/8182 [==============================] - 1s - loss: 0.4486 - acc: 0.7964 - val_loss: 0.4622 - val_acc: 0.7879
Epoch 18/40
8182/8182 [==============================] - 1s - loss: 0.4534 - acc: 0.7969 - val_loss: 0.4605 - val_acc: 0.7928
Epoch 19/40
8182/8182 [==============================] - 1s - loss: 0.4475 - acc: 0.7974 - val_loss: 0.4662 - val_acc: 0.7893
Epoch 20/40
8182/8182 [==============================] - 1s - loss: 0.4513 - acc: 0.7959 - val_loss: 0.4597 - val_acc: 0.7903
Epoch 21/40
8182/8182 [==============================] - 1s - loss: 0.4458 - acc: 0.7980 - val_loss: 0.4608 - val_acc: 0.7889
Epoch 22/40
8182/8182 [==============================] - 1s - loss: 0.4500 - acc: 0.7971 - val_loss: 0.4566 - val_acc: 0.7937
Epoch 23/40
8182/8182 [==============================] - 1s - loss: 0.4445 - acc: 0.7983 - val_loss: 0.4606 - val_acc: 0.7913
Epoch 24/40
8182/8182 [==============================] - 1s - loss: 0.4477 - acc: 0.7969 - val_loss: 0.4553 - val_acc: 0.7928
Epoch 25/40
8182/8182 [==============================] - 1s - loss: 0.4434 - acc: 0.7982 - val_loss: 0.4577 - val_acc: 0.7913
Epoch 26/40
8182/8182 [==============================] - 1s - loss: 0.4462 - acc: 0.7982 - val_loss: 0.4583 - val_acc: 0.7947
Epoch 27/40
8182/8182 [==============================] - 1s - loss: 0.4428 - acc: 0.8008 - val_loss: 0.4609 - val_acc: 0.7928
Epoch 28/40
8182/8182 [==============================] - 1s - loss: 0.4443 - acc: 0.7988 - val_loss: 0.4561 - val_acc: 0.7923
Epoch 29/40
8182/8182 [==============================] - 1s - loss: 0.4415 - acc: 0.7992 - val_loss: 0.4550 - val_acc: 0.7937
Epoch 30/40
8182/8182 [==============================] - 1s - loss: 0.4425 - acc: 0.7989 - val_loss: 0.4552 - val_acc: 0.7933
Epoch 31/40
8182/8182 [==============================] - 1s - loss: 0.4413 - acc: 0.8009 - val_loss: 0.4549 - val_acc: 0.7923
Epoch 32/40
8182/8182 [==============================] - 1s - loss: 0.4409 - acc: 0.8004 - val_loss: 0.4528 - val_acc: 0.7923
Epoch 33/40
8182/8182 [==============================] - 1s - loss: 0.4401 - acc: 0.8004 - val_loss: 0.4531 - val_acc: 0.7952
Epoch 34/40
8182/8182 [==============================] - 1s - loss: 0.4387 - acc: 0.8016 - val_loss: 0.4555 - val_acc: 0.7937
Epoch 35/40
8182/8182 [==============================] - 1s - loss: 0.4393 - acc: 0.8019 - val_loss: 0.4529 - val_acc: 0.7937
Epoch 36/40
8182/8182 [==============================] - 1s - loss: 0.4375 - acc: 0.8011 - val_loss: 0.4510 - val_acc: 0.7972
Epoch 37/40
8182/8182 [==============================] - 1s - loss: 0.4381 - acc: 0.8029 - val_loss: 0.4501 - val_acc: 0.7957
Epoch 38/40
8182/8182 [==============================] - 1s - loss: 0.4363 - acc: 0.8016 - val_loss: 0.4507 - val_acc: 0.7937
Epoch 39/40
8182/8182 [==============================] - 1s - loss: 0.4364 - acc: 0.8029 - val_loss: 0.4487 - val_acc: 0.7947
Epoch 40/40
8182/8182 [==============================] - 1s - loss: 0.4354 - acc: 0.8022 - val_loss: 0.4492 - val_acc: 0.7942
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-3.h5
chunk number 3
prepare data
start training
Train on 8167 samples, validate on 2042 samples
Epoch 1/40
8167/8167 [==============================] - 1s - loss: 0.4660 - acc: 0.7920 - val_loss: 0.4794 - val_acc: 0.7791
Epoch 2/40
8167/8167 [==============================] - 1s - loss: 0.4658 - acc: 0.7920 - val_loss: 0.4772 - val_acc: 0.7762
Epoch 3/40
8167/8167 [==============================] - 1s - loss: 0.4636 - acc: 0.7921 - val_loss: 0.4742 - val_acc: 0.7757
Epoch 4/40
8167/8167 [==============================] - 1s - loss: 0.4637 - acc: 0.7942 - val_loss: 0.4733 - val_acc: 0.7777
Epoch 5/40
8167/8167 [==============================] - 1s - loss: 0.4608 - acc: 0.7943 - val_loss: 0.4765 - val_acc: 0.7821
Epoch 6/40
8167/8167 [==============================] - 1s - loss: 0.4613 - acc: 0.7925 - val_loss: 0.4736 - val_acc: 0.7835
Epoch 7/40
8167/8167 [==============================] - 1s - loss: 0.4592 - acc: 0.7942 - val_loss: 0.4726 - val_acc: 0.7782
Epoch 8/40
8167/8167 [==============================] - 1s - loss: 0.4594 - acc: 0.7969 - val_loss: 0.4733 - val_acc: 0.7796
Epoch 9/40
8167/8167 [==============================] - 1s - loss: 0.4573 - acc: 0.7969 - val_loss: 0.4760 - val_acc: 0.7811
Epoch 10/40
8167/8167 [==============================] - 1s - loss: 0.4578 - acc: 0.7947 - val_loss: 0.4733 - val_acc: 0.7801
Epoch 11/40
8167/8167 [==============================] - 1s - loss: 0.4561 - acc: 0.7977 - val_loss: 0.4709 - val_acc: 0.7806
Epoch 12/40
8167/8167 [==============================] - 1s - loss: 0.4557 - acc: 0.7985 - val_loss: 0.4695 - val_acc: 0.7826
Epoch 13/40
8167/8167 [==============================] - 1s - loss: 0.4539 - acc: 0.7977 - val_loss: 0.4695 - val_acc: 0.7816
Epoch 14/40
8167/8167 [==============================] - 1s - loss: 0.4540 - acc: 0.7967 - val_loss: 0.4661 - val_acc: 0.7855
Epoch 15/40
8167/8167 [==============================] - 1s - loss: 0.4526 - acc: 0.7993 - val_loss: 0.4650 - val_acc: 0.7835
Epoch 16/40
8167/8167 [==============================] - 1s - loss: 0.4519 - acc: 0.8000 - val_loss: 0.4666 - val_acc: 0.7850
Epoch 17/40
8167/8167 [==============================] - 1s - loss: 0.4513 - acc: 0.8007 - val_loss: 0.4658 - val_acc: 0.7865
Epoch 18/40
8167/8167 [==============================] - 1s - loss: 0.4507 - acc: 0.8002 - val_loss: 0.4633 - val_acc: 0.7855
Epoch 19/40
8167/8167 [==============================] - 1s - loss: 0.4501 - acc: 0.8004 - val_loss: 0.4630 - val_acc: 0.7870
Epoch 20/40
8167/8167 [==============================] - 1s - loss: 0.4487 - acc: 0.8015 - val_loss: 0.4631 - val_acc: 0.7860
Epoch 21/40
8167/8167 [==============================] - 1s - loss: 0.4483 - acc: 0.8007 - val_loss: 0.4617 - val_acc: 0.7880
Epoch 22/40
8167/8167 [==============================] - 1s - loss: 0.4474 - acc: 0.8016 - val_loss: 0.4631 - val_acc: 0.7845
Epoch 23/40
8167/8167 [==============================] - 1s - loss: 0.4462 - acc: 0.8009 - val_loss: 0.4641 - val_acc: 0.7860
Epoch 24/40
8167/8167 [==============================] - 1s - loss: 0.4459 - acc: 0.8005 - val_loss: 0.4622 - val_acc: 0.7855
Epoch 25/40
8167/8167 [==============================] - 1s - loss: 0.4450 - acc: 0.8013 - val_loss: 0.4614 - val_acc: 0.7870
Epoch 26/40
8167/8167 [==============================] - 1s - loss: 0.4441 - acc: 0.8020 - val_loss: 0.4610 - val_acc: 0.7880
Epoch 27/40
8167/8167 [==============================] - 1s - loss: 0.4436 - acc: 0.8005 - val_loss: 0.4598 - val_acc: 0.7889
Epoch 28/40
8167/8167 [==============================] - 1s - loss: 0.4428 - acc: 0.8021 - val_loss: 0.4610 - val_acc: 0.7889
Epoch 29/40
8167/8167 [==============================] - 1s - loss: 0.4422 - acc: 0.8014 - val_loss: 0.4594 - val_acc: 0.7889
Epoch 30/40
8167/8167 [==============================] - 1s - loss: 0.4413 - acc: 0.8013 - val_loss: 0.4581 - val_acc: 0.7909
Epoch 31/40
8167/8167 [==============================] - 1s - loss: 0.4405 - acc: 0.8021 - val_loss: 0.4579 - val_acc: 0.7919
Epoch 32/40
8167/8167 [==============================] - 1s - loss: 0.4400 - acc: 0.8027 - val_loss: 0.4570 - val_acc: 0.7919
Epoch 33/40
8167/8167 [==============================] - 1s - loss: 0.4395 - acc: 0.8046 - val_loss: 0.4594 - val_acc: 0.7914
Epoch 34/40
8167/8167 [==============================] - 1s - loss: 0.4394 - acc: 0.8040 - val_loss: 0.4565 - val_acc: 0.7933
Epoch 35/40
8167/8167 [==============================] - 1s - loss: 0.4396 - acc: 0.8058 - val_loss: 0.4634 - val_acc: 0.7904
Epoch 36/40
8167/8167 [==============================] - 1s - loss: 0.4426 - acc: 0.8030 - val_loss: 0.4604 - val_acc: 0.7919
Epoch 37/40
8167/8167 [==============================] - 1s - loss: 0.4455 - acc: 0.8034 - val_loss: 0.4789 - val_acc: 0.7865
Epoch 38/40
8167/8167 [==============================] - 1s - loss: 0.4541 - acc: 0.7985 - val_loss: 0.4585 - val_acc: 0.7924
Epoch 39/40
8167/8167 [==============================] - 1s - loss: 0.4369 - acc: 0.8060 - val_loss: 0.4687 - val_acc: 0.7845
Epoch 40/40
8167/8167 [==============================] - 1s - loss: 0.4545 - acc: 0.7993 - val_loss: 0.4985 - val_acc: 0.7757
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-4.h5
chunk number 4
prepare data
start training
Train on 8048 samples, validate on 2012 samples
Epoch 1/40
8048/8048 [==============================] - 2s - loss: 0.5995 - acc: 0.7563 - val_loss: 0.5263 - val_acc: 0.7749
Epoch 2/40
8048/8048 [==============================] - 2s - loss: 0.5273 - acc: 0.7717 - val_loss: 0.5513 - val_acc: 0.7117
Epoch 3/40
8048/8048 [==============================] - 2s - loss: 0.5633 - acc: 0.7042 - val_loss: 0.4847 - val_acc: 0.7773
Epoch 4/40
8048/8048 [==============================] - 2s - loss: 0.4818 - acc: 0.7802 - val_loss: 0.4996 - val_acc: 0.7734
Epoch 5/40
8048/8048 [==============================] - 2s - loss: 0.4967 - acc: 0.7766 - val_loss: 0.4853 - val_acc: 0.7853
Epoch 6/40
8048/8048 [==============================] - 2s - loss: 0.4803 - acc: 0.7888 - val_loss: 0.4926 - val_acc: 0.7783
Epoch 7/40
8048/8048 [==============================] - 2s - loss: 0.4863 - acc: 0.7855 - val_loss: 0.4958 - val_acc: 0.7783
Epoch 8/40
8048/8048 [==============================] - 2s - loss: 0.4874 - acc: 0.7865 - val_loss: 0.4832 - val_acc: 0.7873
Epoch 9/40
8048/8048 [==============================] - 2s - loss: 0.4783 - acc: 0.7905 - val_loss: 0.4850 - val_acc: 0.7898
Epoch 10/40
8048/8048 [==============================] - 2s - loss: 0.4843 - acc: 0.7848 - val_loss: 0.4811 - val_acc: 0.7888
Epoch 11/40
8048/8048 [==============================] - 2s - loss: 0.4783 - acc: 0.7894 - val_loss: 0.4772 - val_acc: 0.7957
Epoch 12/40
8048/8048 [==============================] - 2s - loss: 0.4720 - acc: 0.7962 - val_loss: 0.4844 - val_acc: 0.7893
Epoch 13/40
8048/8048 [==============================] - 2s - loss: 0.4756 - acc: 0.7956 - val_loss: 0.4788 - val_acc: 0.7917
Epoch 14/40
8048/8048 [==============================] - 2s - loss: 0.4694 - acc: 0.7970 - val_loss: 0.4686 - val_acc: 0.8002
Epoch 15/40
8048/8048 [==============================] - 2s - loss: 0.4626 - acc: 0.7985 - val_loss: 0.4683 - val_acc: 0.8002
Epoch 16/40
8048/8048 [==============================] - 2s - loss: 0.4646 - acc: 0.7954 - val_loss: 0.4670 - val_acc: 0.7992
Epoch 17/40
8048/8048 [==============================] - 2s - loss: 0.4588 - acc: 0.7988 - val_loss: 0.4729 - val_acc: 0.7903
Epoch 18/40
8048/8048 [==============================] - 2s - loss: 0.4572 - acc: 0.7956 - val_loss: 0.4770 - val_acc: 0.7773
Epoch 19/40
8048/8048 [==============================] - 2s - loss: 0.4593 - acc: 0.7922 - val_loss: 0.4660 - val_acc: 0.7962
Epoch 20/40
8048/8048 [==============================] - 2s - loss: 0.4540 - acc: 0.7997 - val_loss: 0.4660 - val_acc: 0.8042
Epoch 21/40
8048/8048 [==============================] - 2s - loss: 0.4575 - acc: 0.7988 - val_loss: 0.4650 - val_acc: 0.7982
Epoch 22/40
8048/8048 [==============================] - 2s - loss: 0.4535 - acc: 0.7992 - val_loss: 0.4689 - val_acc: 0.7848
Epoch 23/40
8048/8048 [==============================] - 2s - loss: 0.4538 - acc: 0.7945 - val_loss: 0.4670 - val_acc: 0.7878
Epoch 24/40
8048/8048 [==============================] - 2s - loss: 0.4512 - acc: 0.7958 - val_loss: 0.4636 - val_acc: 0.7957
Epoch 25/40
8048/8048 [==============================] - 2s - loss: 0.4508 - acc: 0.8004 - val_loss: 0.4635 - val_acc: 0.7952
Epoch 26/40
8048/8048 [==============================] - 2s - loss: 0.4501 - acc: 0.7998 - val_loss: 0.4652 - val_acc: 0.7962
Epoch 27/40
8048/8048 [==============================] - 2s - loss: 0.4486 - acc: 0.7985 - val_loss: 0.4638 - val_acc: 0.7967
Epoch 28/40
8048/8048 [==============================] - 2s - loss: 0.4483 - acc: 0.7986 - val_loss: 0.4599 - val_acc: 0.7957
Epoch 29/40
8048/8048 [==============================] - 2s - loss: 0.4472 - acc: 0.8006 - val_loss: 0.4591 - val_acc: 0.7967
Epoch 30/40
8048/8048 [==============================] - 2s - loss: 0.4471 - acc: 0.8004 - val_loss: 0.4582 - val_acc: 0.8017
Epoch 31/40
8048/8048 [==============================] - 2s - loss: 0.4456 - acc: 0.8008 - val_loss: 0.4585 - val_acc: 0.7987
Epoch 32/40
8048/8048 [==============================] - 2s - loss: 0.4455 - acc: 0.8012 - val_loss: 0.4566 - val_acc: 0.8012
Epoch 33/40
8048/8048 [==============================] - 2s - loss: 0.4438 - acc: 0.8004 - val_loss: 0.4570 - val_acc: 0.8027
Epoch 34/40
8048/8048 [==============================] - 2s - loss: 0.4439 - acc: 0.8013 - val_loss: 0.4565 - val_acc: 0.8017
Epoch 35/40
8048/8048 [==============================] - 2s - loss: 0.4421 - acc: 0.8031 - val_loss: 0.4573 - val_acc: 0.7997
Epoch 36/40
8048/8048 [==============================] - 2s - loss: 0.4424 - acc: 0.8042 - val_loss: 0.4549 - val_acc: 0.8027
Epoch 37/40
8048/8048 [==============================] - 2s - loss: 0.4405 - acc: 0.8028 - val_loss: 0.4546 - val_acc: 0.8017
Epoch 38/40
8048/8048 [==============================] - 2s - loss: 0.4406 - acc: 0.8032 - val_loss: 0.4543 - val_acc: 0.8032
Epoch 39/40
8048/8048 [==============================] - 2s - loss: 0.4393 - acc: 0.8038 - val_loss: 0.4556 - val_acc: 0.8032
Epoch 40/40
8048/8048 [==============================] - 2s - loss: 0.4394 - acc: 0.8044 - val_loss: 0.4546 - val_acc: 0.8022
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-5.h5
chunk number 5
prepare data
start training
Train on 8117 samples, validate on 2030 samples
Epoch 1/40
8117/8117 [==============================] - 1s - loss: 0.4870 - acc: 0.7981 - val_loss: 0.4724 - val_acc: 0.7906
Epoch 2/40
8117/8117 [==============================] - 1s - loss: 0.4668 - acc: 0.7956 - val_loss: 0.4883 - val_acc: 0.7778
Epoch 3/40
8117/8117 [==============================] - 1s - loss: 0.4884 - acc: 0.7860 - val_loss: 0.4786 - val_acc: 0.7808
Epoch 4/40
8117/8117 [==============================] - 1s - loss: 0.4777 - acc: 0.7872 - val_loss: 0.4550 - val_acc: 0.7966
Epoch 5/40
8117/8117 [==============================] - 1s - loss: 0.4527 - acc: 0.8020 - val_loss: 0.4692 - val_acc: 0.8005
Epoch 6/40
8117/8117 [==============================] - 1s - loss: 0.4686 - acc: 0.8009 - val_loss: 0.4745 - val_acc: 0.7946
Epoch 7/40
8117/8117 [==============================] - 1s - loss: 0.4730 - acc: 0.7966 - val_loss: 0.4515 - val_acc: 0.7985
Epoch 8/40
8117/8117 [==============================] - 1s - loss: 0.4485 - acc: 0.8042 - val_loss: 0.4606 - val_acc: 0.7931
Epoch 9/40
8117/8117 [==============================] - 1s - loss: 0.4564 - acc: 0.7973 - val_loss: 0.4662 - val_acc: 0.7921
Epoch 10/40
8117/8117 [==============================] - 1s - loss: 0.4611 - acc: 0.7964 - val_loss: 0.4557 - val_acc: 0.7951
Epoch 11/40
8117/8117 [==============================] - 1s - loss: 0.4475 - acc: 0.8023 - val_loss: 0.4547 - val_acc: 0.8025
Epoch 12/40
8117/8117 [==============================] - 1s - loss: 0.4426 - acc: 0.8081 - val_loss: 0.4677 - val_acc: 0.7946
Epoch 13/40
8117/8117 [==============================] - 1s - loss: 0.4538 - acc: 0.8005 - val_loss: 0.4520 - val_acc: 0.7970
Epoch 14/40
8117/8117 [==============================] - 1s - loss: 0.4369 - acc: 0.8098 - val_loss: 0.4610 - val_acc: 0.7916
Epoch 15/40
8117/8117 [==============================] - 1s - loss: 0.4453 - acc: 0.8039 - val_loss: 0.4603 - val_acc: 0.7892
Epoch 16/40
8117/8117 [==============================] - 1s - loss: 0.4444 - acc: 0.8035 - val_loss: 0.4498 - val_acc: 0.7990
Epoch 17/40
8117/8117 [==============================] - 1s - loss: 0.4346 - acc: 0.8115 - val_loss: 0.4539 - val_acc: 0.7980
Epoch 18/40
8117/8117 [==============================] - 1s - loss: 0.4413 - acc: 0.8083 - val_loss: 0.4467 - val_acc: 0.7990
Epoch 19/40
8117/8117 [==============================] - 1s - loss: 0.4336 - acc: 0.8118 - val_loss: 0.4493 - val_acc: 0.7995
Epoch 20/40
8117/8117 [==============================] - 1s - loss: 0.4353 - acc: 0.8103 - val_loss: 0.4523 - val_acc: 0.7970
Epoch 21/40
8117/8117 [==============================] - 1s - loss: 0.4384 - acc: 0.8104 - val_loss: 0.4450 - val_acc: 0.8025
Epoch 22/40
8117/8117 [==============================] - 1s - loss: 0.4318 - acc: 0.8134 - val_loss: 0.4442 - val_acc: 0.7995
Epoch 23/40
8117/8117 [==============================] - 1s - loss: 0.4328 - acc: 0.8127 - val_loss: 0.4423 - val_acc: 0.8049
Epoch 24/40
8117/8117 [==============================] - 1s - loss: 0.4305 - acc: 0.8142 - val_loss: 0.4445 - val_acc: 0.7990
Epoch 25/40
8117/8117 [==============================] - 1s - loss: 0.4300 - acc: 0.8146 - val_loss: 0.4471 - val_acc: 0.8000
Epoch 26/40
8117/8117 [==============================] - 1s - loss: 0.4317 - acc: 0.8157 - val_loss: 0.4415 - val_acc: 0.8034
Epoch 27/40
8117/8117 [==============================] - 1s - loss: 0.4268 - acc: 0.8166 - val_loss: 0.4437 - val_acc: 0.8015
Epoch 28/40
8117/8117 [==============================] - 1s - loss: 0.4304 - acc: 0.8134 - val_loss: 0.4415 - val_acc: 0.7995
Epoch 29/40
8117/8117 [==============================] - 1s - loss: 0.4265 - acc: 0.8145 - val_loss: 0.4447 - val_acc: 0.8054
Epoch 30/40
8117/8117 [==============================] - 1s - loss: 0.4281 - acc: 0.8180 - val_loss: 0.4427 - val_acc: 0.8039
Epoch 31/40
8117/8117 [==============================] - 1s - loss: 0.4259 - acc: 0.8182 - val_loss: 0.4404 - val_acc: 0.8020
Epoch 32/40
8117/8117 [==============================] - 1s - loss: 0.4249 - acc: 0.8174 - val_loss: 0.4408 - val_acc: 0.8005
Epoch 33/40
8117/8117 [==============================] - 1s - loss: 0.4255 - acc: 0.8179 - val_loss: 0.4411 - val_acc: 0.8069
Epoch 34/40
8117/8117 [==============================] - 1s - loss: 0.4238 - acc: 0.8190 - val_loss: 0.4428 - val_acc: 0.8039
Epoch 35/40
8117/8117 [==============================] - 1s - loss: 0.4248 - acc: 0.8179 - val_loss: 0.4400 - val_acc: 0.8039
Epoch 36/40
8117/8117 [==============================] - 1s - loss: 0.4226 - acc: 0.8191 - val_loss: 0.4408 - val_acc: 0.8025
Epoch 37/40
8117/8117 [==============================] - 1s - loss: 0.4239 - acc: 0.8200 - val_loss: 0.4400 - val_acc: 0.8020
Epoch 38/40
8117/8117 [==============================] - 1s - loss: 0.4217 - acc: 0.8204 - val_loss: 0.4420 - val_acc: 0.8049
Epoch 39/40
8117/8117 [==============================] - 1s - loss: 0.4222 - acc: 0.8200 - val_loss: 0.4414 - val_acc: 0.8039
Epoch 40/40
8117/8117 [==============================] - 1s - loss: 0.4211 - acc: 0.8217 - val_loss: 0.4407 - val_acc: 0.8030
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-6.h5
chunk number 6
prepare data
start training
Train on 8182 samples, validate on 2046 samples
Epoch 1/40
8182/8182 [==============================] - 1s - loss: 0.4579 - acc: 0.8071 - val_loss: 0.4614 - val_acc: 0.7908
Epoch 2/40
8182/8182 [==============================] - 1s - loss: 0.4309 - acc: 0.8108 - val_loss: 0.4834 - val_acc: 0.7854
Epoch 3/40
8182/8182 [==============================] - 1s - loss: 0.4486 - acc: 0.8025 - val_loss: 0.4833 - val_acc: 0.7849
Epoch 4/40
8182/8182 [==============================] - 1s - loss: 0.4487 - acc: 0.8019 - val_loss: 0.4596 - val_acc: 0.7937
Epoch 5/40
8182/8182 [==============================] - 1s - loss: 0.4294 - acc: 0.8118 - val_loss: 0.4602 - val_acc: 0.7996
Epoch 6/40
8182/8182 [==============================] - 1s - loss: 0.4367 - acc: 0.8123 - val_loss: 0.4619 - val_acc: 0.7991
Epoch 7/40
8182/8182 [==============================] - 1s - loss: 0.4390 - acc: 0.8117 - val_loss: 0.4548 - val_acc: 0.7977
Epoch 8/40
8182/8182 [==============================] - 1s - loss: 0.4251 - acc: 0.8159 - val_loss: 0.4698 - val_acc: 0.7918
Epoch 9/40
8182/8182 [==============================] - 1s - loss: 0.4359 - acc: 0.8092 - val_loss: 0.4664 - val_acc: 0.7937
Epoch 10/40
8182/8182 [==============================] - 1s - loss: 0.4321 - acc: 0.8114 - val_loss: 0.4514 - val_acc: 0.8006
Epoch 11/40
8182/8182 [==============================] - 1s - loss: 0.4214 - acc: 0.8181 - val_loss: 0.4582 - val_acc: 0.7972
Epoch 12/40
8182/8182 [==============================] - 1s - loss: 0.4333 - acc: 0.8126 - val_loss: 0.4504 - val_acc: 0.7991
Epoch 13/40
8182/8182 [==============================] - 1s - loss: 0.4211 - acc: 0.8172 - val_loss: 0.4608 - val_acc: 0.7991
Epoch 14/40
8182/8182 [==============================] - 1s - loss: 0.4249 - acc: 0.8164 - val_loss: 0.4642 - val_acc: 0.7977
Epoch 15/40
8182/8182 [==============================] - 1s - loss: 0.4269 - acc: 0.8165 - val_loss: 0.4521 - val_acc: 0.7986
Epoch 16/40
8182/8182 [==============================] - 1s - loss: 0.4188 - acc: 0.8194 - val_loss: 0.4545 - val_acc: 0.7972
Epoch 17/40
8182/8182 [==============================] - 1s - loss: 0.4258 - acc: 0.8143 - val_loss: 0.4510 - val_acc: 0.8021
Epoch 18/40
8182/8182 [==============================] - 1s - loss: 0.4186 - acc: 0.8185 - val_loss: 0.4583 - val_acc: 0.8030
Epoch 19/40
8182/8182 [==============================] - 1s - loss: 0.4206 - acc: 0.8192 - val_loss: 0.4592 - val_acc: 0.8030
Epoch 20/40
8182/8182 [==============================] - 1s - loss: 0.4210 - acc: 0.8191 - val_loss: 0.4504 - val_acc: 0.8016
Epoch 21/40
8182/8182 [==============================] - 1s - loss: 0.4161 - acc: 0.8208 - val_loss: 0.4516 - val_acc: 0.7967
Epoch 22/40
8182/8182 [==============================] - 1s - loss: 0.4206 - acc: 0.8174 - val_loss: 0.4498 - val_acc: 0.8030
Epoch 23/40
8182/8182 [==============================] - 1s - loss: 0.4155 - acc: 0.8205 - val_loss: 0.4561 - val_acc: 0.8030
Epoch 24/40
8182/8182 [==============================] - 1s - loss: 0.4179 - acc: 0.8192 - val_loss: 0.4547 - val_acc: 0.8050
Epoch 25/40
8182/8182 [==============================] - 1s - loss: 0.4165 - acc: 0.8202 - val_loss: 0.4501 - val_acc: 0.8001
Epoch 26/40
8182/8182 [==============================] - 1s - loss: 0.4148 - acc: 0.8208 - val_loss: 0.4511 - val_acc: 0.7986
Epoch 27/40
8182/8182 [==============================] - 1s - loss: 0.4165 - acc: 0.8202 - val_loss: 0.4515 - val_acc: 0.7996
Epoch 28/40
8182/8182 [==============================] - 1s - loss: 0.4136 - acc: 0.8216 - val_loss: 0.4556 - val_acc: 0.8035
Epoch 29/40
8182/8182 [==============================] - 1s - loss: 0.4155 - acc: 0.8214 - val_loss: 0.4523 - val_acc: 0.7991
Epoch 30/40
8182/8182 [==============================] - 1s - loss: 0.4133 - acc: 0.8224 - val_loss: 0.4506 - val_acc: 0.8001
Epoch 31/40
8182/8182 [==============================] - 1s - loss: 0.4140 - acc: 0.8207 - val_loss: 0.4503 - val_acc: 0.8006
Epoch 32/40
8182/8182 [==============================] - 1s - loss: 0.4131 - acc: 0.8227 - val_loss: 0.4523 - val_acc: 0.8016
Epoch 33/40
8182/8182 [==============================] - 1s - loss: 0.4125 - acc: 0.8220 - val_loss: 0.4533 - val_acc: 0.8025
Epoch 34/40
8182/8182 [==============================] - 1s - loss: 0.4128 - acc: 0.8222 - val_loss: 0.4501 - val_acc: 0.8016
Epoch 35/40
8182/8182 [==============================] - 1s - loss: 0.4113 - acc: 0.8228 - val_loss: 0.4497 - val_acc: 0.8011
Epoch 36/40
8182/8182 [==============================] - 1s - loss: 0.4122 - acc: 0.8220 - val_loss: 0.4498 - val_acc: 0.8040
Epoch 37/40
8182/8182 [==============================] - 1s - loss: 0.4106 - acc: 0.8229 - val_loss: 0.4519 - val_acc: 0.8045
Epoch 38/40
8182/8182 [==============================] - 1s - loss: 0.4112 - acc: 0.8228 - val_loss: 0.4501 - val_acc: 0.8025
Epoch 39/40
8182/8182 [==============================] - 1s - loss: 0.4101 - acc: 0.8240 - val_loss: 0.4487 - val_acc: 0.8001
Epoch 40/40
8182/8182 [==============================] - 1s - loss: 0.4102 - acc: 0.8230 - val_loss: 0.4488 - val_acc: 0.8021
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-7.h5
chunk number 7
prepare data
start training
Train on 8116 samples, validate on 2030 samples
Epoch 1/40
8116/8116 [==============================] - 1s - loss: 0.4530 - acc: 0.8008 - val_loss: 0.4460 - val_acc: 0.7975
Epoch 2/40
8116/8116 [==============================] - 1s - loss: 0.4438 - acc: 0.8040 - val_loss: 0.4452 - val_acc: 0.7966
Epoch 3/40
8116/8116 [==============================] - 1s - loss: 0.4418 - acc: 0.8046 - val_loss: 0.4583 - val_acc: 0.7985
Epoch 4/40
8116/8116 [==============================] - 1s - loss: 0.4435 - acc: 0.8057 - val_loss: 0.4541 - val_acc: 0.8010
Epoch 5/40
8116/8116 [==============================] - 1s - loss: 0.4391 - acc: 0.8083 - val_loss: 0.4452 - val_acc: 0.8025
Epoch 6/40
8116/8116 [==============================] - 1s - loss: 0.4379 - acc: 0.8062 - val_loss: 0.4424 - val_acc: 0.8069
Epoch 7/40
8116/8116 [==============================] - 1s - loss: 0.4327 - acc: 0.8106 - val_loss: 0.4511 - val_acc: 0.8020
Epoch 8/40
8116/8116 [==============================] - 1s - loss: 0.4349 - acc: 0.8096 - val_loss: 0.4486 - val_acc: 0.8034
Epoch 9/40
8116/8116 [==============================] - 1s - loss: 0.4330 - acc: 0.8105 - val_loss: 0.4399 - val_acc: 0.8079
Epoch 10/40
8116/8116 [==============================] - 1s - loss: 0.4295 - acc: 0.8121 - val_loss: 0.4409 - val_acc: 0.8074
Epoch 11/40
8116/8116 [==============================] - 1s - loss: 0.4320 - acc: 0.8104 - val_loss: 0.4418 - val_acc: 0.8034
Epoch 12/40
8116/8116 [==============================] - 1s - loss: 0.4278 - acc: 0.8130 - val_loss: 0.4463 - val_acc: 0.8049
Epoch 13/40
8116/8116 [==============================] - 1s - loss: 0.4311 - acc: 0.8095 - val_loss: 0.4392 - val_acc: 0.8089
Epoch 14/40
8116/8116 [==============================] - 1s - loss: 0.4254 - acc: 0.8142 - val_loss: 0.4410 - val_acc: 0.8064
Epoch 15/40
8116/8116 [==============================] - 1s - loss: 0.4302 - acc: 0.8100 - val_loss: 0.4404 - val_acc: 0.8069
Epoch 16/40
8116/8116 [==============================] - 1s - loss: 0.4246 - acc: 0.8136 - val_loss: 0.4459 - val_acc: 0.8074
Epoch 17/40
8116/8116 [==============================] - 1s - loss: 0.4278 - acc: 0.8138 - val_loss: 0.4421 - val_acc: 0.8099
Epoch 18/40
8116/8116 [==============================] - 1s - loss: 0.4243 - acc: 0.8158 - val_loss: 0.4405 - val_acc: 0.8025
Epoch 19/40
8116/8116 [==============================] - 1s - loss: 0.4263 - acc: 0.8109 - val_loss: 0.4398 - val_acc: 0.8064
Epoch 20/40
8116/8116 [==============================] - 1s - loss: 0.4228 - acc: 0.8153 - val_loss: 0.4434 - val_acc: 0.8099
Epoch 21/40
8116/8116 [==============================] - 1s - loss: 0.4241 - acc: 0.8170 - val_loss: 0.4410 - val_acc: 0.8074
Epoch 22/40
8116/8116 [==============================] - 1s - loss: 0.4220 - acc: 0.8167 - val_loss: 0.4384 - val_acc: 0.8020
Epoch 23/40
8116/8116 [==============================] - 1s - loss: 0.4226 - acc: 0.8154 - val_loss: 0.4375 - val_acc: 0.8059
Epoch 24/40
8116/8116 [==============================] - 1s - loss: 0.4206 - acc: 0.8167 - val_loss: 0.4400 - val_acc: 0.8089
Epoch 25/40
8116/8116 [==============================] - 1s - loss: 0.4212 - acc: 0.8162 - val_loss: 0.4380 - val_acc: 0.8084
Epoch 26/40
8116/8116 [==============================] - 1s - loss: 0.4196 - acc: 0.8163 - val_loss: 0.4364 - val_acc: 0.8069
Epoch 27/40
8116/8116 [==============================] - 1s - loss: 0.4202 - acc: 0.8152 - val_loss: 0.4370 - val_acc: 0.8044
Epoch 28/40
8116/8116 [==============================] - 1s - loss: 0.4185 - acc: 0.8178 - val_loss: 0.4393 - val_acc: 0.8099
Epoch 29/40
8116/8116 [==============================] - 1s - loss: 0.4192 - acc: 0.8179 - val_loss: 0.4370 - val_acc: 0.8074
Epoch 30/40
8116/8116 [==============================] - 1s - loss: 0.4177 - acc: 0.8180 - val_loss: 0.4359 - val_acc: 0.8069
Epoch 31/40
8116/8116 [==============================] - 1s - loss: 0.4184 - acc: 0.8178 - val_loss: 0.4369 - val_acc: 0.8074
Epoch 32/40
8116/8116 [==============================] - 1s - loss: 0.4171 - acc: 0.8189 - val_loss: 0.4380 - val_acc: 0.8074
Epoch 33/40
8116/8116 [==============================] - 1s - loss: 0.4172 - acc: 0.8183 - val_loss: 0.4361 - val_acc: 0.8084
Epoch 34/40
8116/8116 [==============================] - 1s - loss: 0.4164 - acc: 0.8175 - val_loss: 0.4362 - val_acc: 0.8079
Epoch 35/40
8116/8116 [==============================] - 1s - loss: 0.4161 - acc: 0.8175 - val_loss: 0.4381 - val_acc: 0.8074
Epoch 36/40
8116/8116 [==============================] - 1s - loss: 0.4160 - acc: 0.8189 - val_loss: 0.4371 - val_acc: 0.8069
Epoch 37/40
8116/8116 [==============================] - 1s - loss: 0.4152 - acc: 0.8186 - val_loss: 0.4364 - val_acc: 0.8079
Epoch 38/40
8116/8116 [==============================] - 1s - loss: 0.4154 - acc: 0.8175 - val_loss: 0.4377 - val_acc: 0.8074
Epoch 39/40
8116/8116 [==============================] - 1s - loss: 0.4146 - acc: 0.8185 - val_loss: 0.4378 - val_acc: 0.8064
Epoch 40/40
8116/8116 [==============================] - 1s - loss: 0.4144 - acc: 0.8189 - val_loss: 0.4362 - val_acc: 0.8074
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-8.h5
chunk number 8
prepare data
start training
Train on 8086 samples, validate on 2022 samples
Epoch 1/40
8086/8086 [==============================] - 2s - loss: 0.4618 - acc: 0.8025 - val_loss: 0.4288 - val_acc: 0.8022
Epoch 2/40
8086/8086 [==============================] - 2s - loss: 0.4645 - acc: 0.7841 - val_loss: 0.4039 - val_acc: 0.8259
Epoch 3/40
8086/8086 [==============================] - 2s - loss: 0.4459 - acc: 0.8050 - val_loss: 0.4081 - val_acc: 0.8309
Epoch 4/40
8086/8086 [==============================] - 2s - loss: 0.4519 - acc: 0.8018 - val_loss: 0.4040 - val_acc: 0.8304
Epoch 5/40
8086/8086 [==============================] - 2s - loss: 0.4447 - acc: 0.8046 - val_loss: 0.4118 - val_acc: 0.8264
Epoch 6/40
8086/8086 [==============================] - 2s - loss: 0.4500 - acc: 0.8023 - val_loss: 0.4027 - val_acc: 0.8314
Epoch 7/40
8086/8086 [==============================] - 2s - loss: 0.4423 - acc: 0.8056 - val_loss: 0.4054 - val_acc: 0.8309
Epoch 8/40
8086/8086 [==============================] - 2s - loss: 0.4442 - acc: 0.8046 - val_loss: 0.4046 - val_acc: 0.8294
Epoch 9/40
8086/8086 [==============================] - 2s - loss: 0.4395 - acc: 0.8081 - val_loss: 0.4128 - val_acc: 0.8239
Epoch 10/40
8086/8086 [==============================] - 2s - loss: 0.4441 - acc: 0.8072 - val_loss: 0.4074 - val_acc: 0.8309
Epoch 11/40
8086/8086 [==============================] - 2s - loss: 0.4396 - acc: 0.8087 - val_loss: 0.4084 - val_acc: 0.8274
Epoch 12/40
8086/8086 [==============================] - 2s - loss: 0.4420 - acc: 0.8060 - val_loss: 0.4059 - val_acc: 0.8299
Epoch 13/40
8086/8086 [==============================] - 2s - loss: 0.4381 - acc: 0.8086 - val_loss: 0.4094 - val_acc: 0.8259
Epoch 14/40
8086/8086 [==============================] - 2s - loss: 0.4399 - acc: 0.8082 - val_loss: 0.4041 - val_acc: 0.8284
Epoch 15/40
8086/8086 [==============================] - 2s - loss: 0.4371 - acc: 0.8092 - val_loss: 0.4030 - val_acc: 0.8318
Epoch 16/40
8086/8086 [==============================] - 2s - loss: 0.4384 - acc: 0.8067 - val_loss: 0.4011 - val_acc: 0.8309
Epoch 17/40
8086/8086 [==============================] - 2s - loss: 0.4356 - acc: 0.8084 - val_loss: 0.4036 - val_acc: 0.8269
Epoch 18/40
8086/8086 [==============================] - 2s - loss: 0.4360 - acc: 0.8092 - val_loss: 0.4004 - val_acc: 0.8299
Epoch 19/40
8086/8086 [==============================] - 2s - loss: 0.4332 - acc: 0.8097 - val_loss: 0.4004 - val_acc: 0.8323
Epoch 20/40
8086/8086 [==============================] - 2s - loss: 0.4343 - acc: 0.8103 - val_loss: 0.4003 - val_acc: 0.8304
Epoch 21/40
8086/8086 [==============================] - 2s - loss: 0.4322 - acc: 0.8108 - val_loss: 0.4026 - val_acc: 0.8284
Epoch 22/40
8086/8086 [==============================] - 2s - loss: 0.4331 - acc: 0.8110 - val_loss: 0.3985 - val_acc: 0.8333
Epoch 23/40
8086/8086 [==============================] - 2s - loss: 0.4312 - acc: 0.8102 - val_loss: 0.3973 - val_acc: 0.8338
Epoch 24/40
8086/8086 [==============================] - 2s - loss: 0.4308 - acc: 0.8100 - val_loss: 0.3990 - val_acc: 0.8304
Epoch 25/40
8086/8086 [==============================] - 2s - loss: 0.4310 - acc: 0.8113 - val_loss: 0.3966 - val_acc: 0.8299
Epoch 26/40
8086/8086 [==============================] - 2s - loss: 0.4296 - acc: 0.8123 - val_loss: 0.3962 - val_acc: 0.8353
Epoch 27/40
8086/8086 [==============================] - 2s - loss: 0.4299 - acc: 0.8103 - val_loss: 0.3980 - val_acc: 0.8294
Epoch 28/40
8086/8086 [==============================] - 2s - loss: 0.4289 - acc: 0.8119 - val_loss: 0.3976 - val_acc: 0.8309
Epoch 29/40
8086/8086 [==============================] - 2s - loss: 0.4279 - acc: 0.8125 - val_loss: 0.3972 - val_acc: 0.8348
Epoch 30/40
8086/8086 [==============================] - 2s - loss: 0.4280 - acc: 0.8118 - val_loss: 0.3989 - val_acc: 0.8328
Epoch 31/40
8086/8086 [==============================] - 2s - loss: 0.4271 - acc: 0.8125 - val_loss: 0.3981 - val_acc: 0.8348
Epoch 32/40
8086/8086 [==============================] - 2s - loss: 0.4264 - acc: 0.8135 - val_loss: 0.3966 - val_acc: 0.8333
Epoch 33/40
8086/8086 [==============================] - 2s - loss: 0.4262 - acc: 0.8125 - val_loss: 0.3976 - val_acc: 0.8333
Epoch 34/40
8086/8086 [==============================] - 2s - loss: 0.4256 - acc: 0.8114 - val_loss: 0.3970 - val_acc: 0.8304
Epoch 35/40
8086/8086 [==============================] - 2s - loss: 0.4251 - acc: 0.8133 - val_loss: 0.3966 - val_acc: 0.8328
Epoch 36/40
8086/8086 [==============================] - 2s - loss: 0.4248 - acc: 0.8139 - val_loss: 0.3986 - val_acc: 0.8333
Epoch 37/40
8086/8086 [==============================] - 2s - loss: 0.4243 - acc: 0.8156 - val_loss: 0.3981 - val_acc: 0.8318
Epoch 38/40
8086/8086 [==============================] - 2s - loss: 0.4236 - acc: 0.8155 - val_loss: 0.3980 - val_acc: 0.8318
Epoch 39/40
8086/8086 [==============================] - 2s - loss: 0.4233 - acc: 0.8167 - val_loss: 0.3988 - val_acc: 0.8323
Epoch 40/40
8086/8086 [==============================] - 2s - loss: 0.4229 - acc: 0.8171 - val_loss: 0.3966 - val_acc: 0.8318
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-9.h5
chunk number 9
prepare data
start training
Train on 8220 samples, validate on 2055 samples
Epoch 1/40
8220/8220 [==============================] - 2s - loss: 0.4282 - acc: 0.8174 - val_loss: 0.4005 - val_acc: 0.8365
Epoch 2/40
8220/8220 [==============================] - 2s - loss: 0.4286 - acc: 0.8178 - val_loss: 0.3931 - val_acc: 0.8414
Epoch 3/40
8220/8220 [==============================] - 2s - loss: 0.4256 - acc: 0.8187 - val_loss: 0.3927 - val_acc: 0.8477
Epoch 4/40
8220/8220 [==============================] - 2s - loss: 0.4227 - acc: 0.8185 - val_loss: 0.3981 - val_acc: 0.8404
Epoch 5/40
8220/8220 [==============================] - 2s - loss: 0.4252 - acc: 0.8141 - val_loss: 0.3931 - val_acc: 0.8404
Epoch 6/40
8220/8220 [==============================] - 2s - loss: 0.4244 - acc: 0.8185 - val_loss: 0.3919 - val_acc: 0.8409
Epoch 7/40
8220/8220 [==============================] - 2s - loss: 0.4194 - acc: 0.8192 - val_loss: 0.4001 - val_acc: 0.8380
Epoch 8/40
8220/8220 [==============================] - 2s - loss: 0.4236 - acc: 0.8178 - val_loss: 0.3960 - val_acc: 0.8370
Epoch 9/40
8220/8220 [==============================] - 2s - loss: 0.4246 - acc: 0.8168 - val_loss: 0.3933 - val_acc: 0.8375
Epoch 10/40
8220/8220 [==============================] - 2s - loss: 0.4182 - acc: 0.8193 - val_loss: 0.4056 - val_acc: 0.8336
Epoch 11/40
8220/8220 [==============================] - 2s - loss: 0.4255 - acc: 0.8167 - val_loss: 0.3986 - val_acc: 0.8365
Epoch 12/40
8220/8220 [==============================] - 2s - loss: 0.4255 - acc: 0.8161 - val_loss: 0.3950 - val_acc: 0.8360
Epoch 13/40
8220/8220 [==============================] - 2s - loss: 0.4191 - acc: 0.8181 - val_loss: 0.4132 - val_acc: 0.8307
Epoch 14/40
8220/8220 [==============================] - 2s - loss: 0.4302 - acc: 0.8136 - val_loss: 0.3983 - val_acc: 0.8355
Epoch 15/40
8220/8220 [==============================] - 2s - loss: 0.4227 - acc: 0.8169 - val_loss: 0.3977 - val_acc: 0.8350
Epoch 16/40
8220/8220 [==============================] - 2s - loss: 0.4214 - acc: 0.8178 - val_loss: 0.4089 - val_acc: 0.8287
Epoch 17/40
8220/8220 [==============================] - 2s - loss: 0.4254 - acc: 0.8168 - val_loss: 0.3944 - val_acc: 0.8375
Epoch 18/40
8220/8220 [==============================] - 2s - loss: 0.4154 - acc: 0.8206 - val_loss: 0.3957 - val_acc: 0.8350
Epoch 19/40
8220/8220 [==============================] - 2s - loss: 0.4199 - acc: 0.8209 - val_loss: 0.3958 - val_acc: 0.8370
Epoch 20/40
8220/8220 [==============================] - 2s - loss: 0.4146 - acc: 0.8193 - val_loss: 0.3982 - val_acc: 0.8350
Epoch 21/40
8220/8220 [==============================] - 2s - loss: 0.4157 - acc: 0.8182 - val_loss: 0.3937 - val_acc: 0.8345
Epoch 22/40
8220/8220 [==============================] - 2s - loss: 0.4180 - acc: 0.8217 - val_loss: 0.3917 - val_acc: 0.8389
Epoch 23/40
8220/8220 [==============================] - 2s - loss: 0.4125 - acc: 0.8231 - val_loss: 0.4036 - val_acc: 0.8316
Epoch 24/40
8220/8220 [==============================] - 2s - loss: 0.4195 - acc: 0.8175 - val_loss: 0.3941 - val_acc: 0.8370
Epoch 25/40
8220/8220 [==============================] - 2s - loss: 0.4197 - acc: 0.8210 - val_loss: 0.3906 - val_acc: 0.8404
Epoch 26/40
8220/8220 [==============================] - 2s - loss: 0.4127 - acc: 0.8237 - val_loss: 0.4120 - val_acc: 0.8345
Epoch 27/40
8220/8220 [==============================] - 2s - loss: 0.4278 - acc: 0.8173 - val_loss: 0.3981 - val_acc: 0.8350
Epoch 28/40
8220/8220 [==============================] - 2s - loss: 0.4247 - acc: 0.8164 - val_loss: 0.3935 - val_acc: 0.8384
Epoch 29/40
8220/8220 [==============================] - 2s - loss: 0.4166 - acc: 0.8192 - val_loss: 0.4189 - val_acc: 0.8341
Epoch 30/40
8220/8220 [==============================] - 2s - loss: 0.4340 - acc: 0.8152 - val_loss: 0.3955 - val_acc: 0.8360
Epoch 31/40
8220/8220 [==============================] - 2s - loss: 0.4178 - acc: 0.8184 - val_loss: 0.3977 - val_acc: 0.8365
Epoch 32/40
8220/8220 [==============================] - 2s - loss: 0.4208 - acc: 0.8180 - val_loss: 0.4040 - val_acc: 0.8394
Epoch 33/40
8220/8220 [==============================] - 2s - loss: 0.4177 - acc: 0.8212 - val_loss: 0.3952 - val_acc: 0.8370
Epoch 34/40
8220/8220 [==============================] - 2s - loss: 0.4102 - acc: 0.8232 - val_loss: 0.3945 - val_acc: 0.8326
Epoch 35/40
8220/8220 [==============================] - 2s - loss: 0.4179 - acc: 0.8214 - val_loss: 0.3918 - val_acc: 0.8399
Epoch 36/40
8220/8220 [==============================] - 2s - loss: 0.4083 - acc: 0.8260 - val_loss: 0.4054 - val_acc: 0.8360
Epoch 37/40
8220/8220 [==============================] - 2s - loss: 0.4175 - acc: 0.8208 - val_loss: 0.3925 - val_acc: 0.8365
Epoch 38/40
8220/8220 [==============================] - 2s - loss: 0.4124 - acc: 0.8243 - val_loss: 0.3925 - val_acc: 0.8384
Epoch 39/40
8220/8220 [==============================] - 2s - loss: 0.4104 - acc: 0.8247 - val_loss: 0.4045 - val_acc: 0.8336
Epoch 40/40
8220/8220 [==============================] - 2s - loss: 0.4151 - acc: 0.8226 - val_loss: 0.3930 - val_acc: 0.8399
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-10.h5
chunk number 10
prepare data
start training
Train on 8132 samples, validate on 2033 samples
Epoch 1/40
8132/8132 [==============================] - 1s - loss: 0.4821 - acc: 0.7896 - val_loss: 0.4669 - val_acc: 0.7988
Epoch 2/40
8132/8132 [==============================] - 1s - loss: 0.4544 - acc: 0.8004 - val_loss: 0.5289 - val_acc: 0.7531
Epoch 3/40
8132/8132 [==============================] - 1s - loss: 0.5119 - acc: 0.7670 - val_loss: 0.5402 - val_acc: 0.7462
Epoch 4/40
8132/8132 [==============================] - 1s - loss: 0.5213 - acc: 0.7611 - val_loss: 0.5089 - val_acc: 0.7644
Epoch 5/40
8132/8132 [==============================] - 1s - loss: 0.4909 - acc: 0.7750 - val_loss: 0.4729 - val_acc: 0.7983
Epoch 6/40
8132/8132 [==============================] - 1s - loss: 0.4570 - acc: 0.8039 - val_loss: 0.4789 - val_acc: 0.8052
Epoch 7/40
8132/8132 [==============================] - 1s - loss: 0.4643 - acc: 0.8044 - val_loss: 0.4992 - val_acc: 0.7782
Epoch 8/40
8132/8132 [==============================] - 1s - loss: 0.4843 - acc: 0.7827 - val_loss: 0.4595 - val_acc: 0.8077
Epoch 9/40
8132/8132 [==============================] - 1s - loss: 0.4457 - acc: 0.8162 - val_loss: 0.4446 - val_acc: 0.8042
Epoch 10/40
8132/8132 [==============================] - 1s - loss: 0.4301 - acc: 0.8137 - val_loss: 0.4607 - val_acc: 0.7993
Epoch 11/40
8132/8132 [==============================] - 1s - loss: 0.4450 - acc: 0.8077 - val_loss: 0.4685 - val_acc: 0.8003
Epoch 12/40
8132/8132 [==============================] - 1s - loss: 0.4526 - acc: 0.8036 - val_loss: 0.4613 - val_acc: 0.8003
Epoch 13/40
8132/8132 [==============================] - 1s - loss: 0.4476 - acc: 0.8056 - val_loss: 0.4557 - val_acc: 0.8018
Epoch 14/40
8132/8132 [==============================] - 1s - loss: 0.4459 - acc: 0.8055 - val_loss: 0.4490 - val_acc: 0.8023
Epoch 15/40
8132/8132 [==============================] - 1s - loss: 0.4379 - acc: 0.8105 - val_loss: 0.4393 - val_acc: 0.8047
Epoch 16/40
8132/8132 [==============================] - 1s - loss: 0.4250 - acc: 0.8162 - val_loss: 0.4391 - val_acc: 0.8131
Epoch 17/40
8132/8132 [==============================] - 1s - loss: 0.4241 - acc: 0.8167 - val_loss: 0.4436 - val_acc: 0.8077
Epoch 18/40
8132/8132 [==============================] - 1s - loss: 0.4286 - acc: 0.8167 - val_loss: 0.4476 - val_acc: 0.8052
Epoch 19/40
8132/8132 [==============================] - 1s - loss: 0.4323 - acc: 0.8164 - val_loss: 0.4472 - val_acc: 0.8047
Epoch 20/40
8132/8132 [==============================] - 1s - loss: 0.4318 - acc: 0.8176 - val_loss: 0.4423 - val_acc: 0.8106
Epoch 21/40
8132/8132 [==============================] - 1s - loss: 0.4270 - acc: 0.8205 - val_loss: 0.4353 - val_acc: 0.8160
Epoch 22/40
8132/8132 [==============================] - 1s - loss: 0.4210 - acc: 0.8206 - val_loss: 0.4305 - val_acc: 0.8151
Epoch 23/40
8132/8132 [==============================] - 1s - loss: 0.4178 - acc: 0.8214 - val_loss: 0.4305 - val_acc: 0.8131
Epoch 24/40
8132/8132 [==============================] - 1s - loss: 0.4193 - acc: 0.8198 - val_loss: 0.4317 - val_acc: 0.8087
Epoch 25/40
8132/8132 [==============================] - 1s - loss: 0.4199 - acc: 0.8206 - val_loss: 0.4315 - val_acc: 0.8096
Epoch 26/40
8132/8132 [==============================] - 1s - loss: 0.4175 - acc: 0.8217 - val_loss: 0.4309 - val_acc: 0.8170
Epoch 27/40
8132/8132 [==============================] - 1s - loss: 0.4150 - acc: 0.8203 - val_loss: 0.4292 - val_acc: 0.8160
Epoch 28/40
8132/8132 [==============================] - 1s - loss: 0.4125 - acc: 0.8219 - val_loss: 0.4309 - val_acc: 0.8121
Epoch 29/40
8132/8132 [==============================] - 1s - loss: 0.4145 - acc: 0.8207 - val_loss: 0.4321 - val_acc: 0.8126
Epoch 30/40
8132/8132 [==============================] - 1s - loss: 0.4160 - acc: 0.8211 - val_loss: 0.4291 - val_acc: 0.8111
Epoch 31/40
8132/8132 [==============================] - 1s - loss: 0.4126 - acc: 0.8227 - val_loss: 0.4273 - val_acc: 0.8195
Epoch 32/40
8132/8132 [==============================] - 1s - loss: 0.4108 - acc: 0.8250 - val_loss: 0.4241 - val_acc: 0.8180
Epoch 33/40
8132/8132 [==============================] - 1s - loss: 0.4088 - acc: 0.8244 - val_loss: 0.4238 - val_acc: 0.8170
Epoch 34/40
8132/8132 [==============================] - 1s - loss: 0.4108 - acc: 0.8227 - val_loss: 0.4242 - val_acc: 0.8170
Epoch 35/40
8132/8132 [==============================] - 1s - loss: 0.4117 - acc: 0.8230 - val_loss: 0.4238 - val_acc: 0.8210
Epoch 36/40
8132/8132 [==============================] - 1s - loss: 0.4093 - acc: 0.8255 - val_loss: 0.4254 - val_acc: 0.8205
Epoch 37/40
8132/8132 [==============================] - 1s - loss: 0.4091 - acc: 0.8261 - val_loss: 0.4247 - val_acc: 0.8210
Epoch 38/40
8132/8132 [==============================] - 1s - loss: 0.4080 - acc: 0.8255 - val_loss: 0.4249 - val_acc: 0.8170
Epoch 39/40
8132/8132 [==============================] - 1s - loss: 0.4084 - acc: 0.8246 - val_loss: 0.4244 - val_acc: 0.8165
Epoch 40/40
8132/8132 [==============================] - 1s - loss: 0.4072 - acc: 0.8246 - val_loss: 0.4246 - val_acc: 0.8180
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-11.h5
chunk number 11
prepare data
start training
Train on 8186 samples, validate on 2047 samples
Epoch 1/40
8186/8186 [==============================] - 1s - loss: 0.4432 - acc: 0.8158 - val_loss: 0.4141 - val_acc: 0.8232
Epoch 2/40
8186/8186 [==============================] - 1s - loss: 0.4264 - acc: 0.8129 - val_loss: 0.4326 - val_acc: 0.8065
Epoch 3/40
8186/8186 [==============================] - 1s - loss: 0.4437 - acc: 0.7972 - val_loss: 0.4123 - val_acc: 0.8232
Epoch 4/40
8186/8186 [==============================] - 1s - loss: 0.4237 - acc: 0.8166 - val_loss: 0.4218 - val_acc: 0.8188
Epoch 5/40
8186/8186 [==============================] - 1s - loss: 0.4329 - acc: 0.8132 - val_loss: 0.4184 - val_acc: 0.8217
Epoch 6/40
8186/8186 [==============================] - 1s - loss: 0.4255 - acc: 0.8158 - val_loss: 0.4257 - val_acc: 0.8188
Epoch 7/40
8186/8186 [==============================] - 1s - loss: 0.4272 - acc: 0.8158 - val_loss: 0.4270 - val_acc: 0.8192
Epoch 8/40
8186/8186 [==============================] - 1s - loss: 0.4280 - acc: 0.8164 - val_loss: 0.4171 - val_acc: 0.8207
Epoch 9/40
8186/8186 [==============================] - 1s - loss: 0.4221 - acc: 0.8164 - val_loss: 0.4195 - val_acc: 0.8139
Epoch 10/40
8186/8186 [==============================] - 1s - loss: 0.4266 - acc: 0.8149 - val_loss: 0.4160 - val_acc: 0.8183
Epoch 11/40
8186/8186 [==============================] - 1s - loss: 0.4226 - acc: 0.8166 - val_loss: 0.4169 - val_acc: 0.8271
Epoch 12/40
8186/8186 [==============================] - 1s - loss: 0.4218 - acc: 0.8158 - val_loss: 0.4192 - val_acc: 0.8207
Epoch 13/40
8186/8186 [==============================] - 1s - loss: 0.4230 - acc: 0.8166 - val_loss: 0.4154 - val_acc: 0.8256
Epoch 14/40
8186/8186 [==============================] - 1s - loss: 0.4187 - acc: 0.8183 - val_loss: 0.4181 - val_acc: 0.8197
Epoch 15/40
8186/8186 [==============================] - 1s - loss: 0.4212 - acc: 0.8186 - val_loss: 0.4171 - val_acc: 0.8212
Epoch 16/40
8186/8186 [==============================] - 1s - loss: 0.4198 - acc: 0.8180 - val_loss: 0.4158 - val_acc: 0.8227
Epoch 17/40
8186/8186 [==============================] - 1s - loss: 0.4185 - acc: 0.8174 - val_loss: 0.4154 - val_acc: 0.8192
Epoch 18/40
8186/8186 [==============================] - 1s - loss: 0.4193 - acc: 0.8191 - val_loss: 0.4105 - val_acc: 0.8236
Epoch 19/40
8186/8186 [==============================] - 1s - loss: 0.4168 - acc: 0.8179 - val_loss: 0.4106 - val_acc: 0.8246
Epoch 20/40
8186/8186 [==============================] - 1s - loss: 0.4185 - acc: 0.8171 - val_loss: 0.4092 - val_acc: 0.8241
Epoch 21/40
8186/8186 [==============================] - 1s - loss: 0.4172 - acc: 0.8168 - val_loss: 0.4096 - val_acc: 0.8241
Epoch 22/40
8186/8186 [==============================] - 1s - loss: 0.4165 - acc: 0.8180 - val_loss: 0.4109 - val_acc: 0.8236
Epoch 23/40
8186/8186 [==============================] - 1s - loss: 0.4162 - acc: 0.8186 - val_loss: 0.4110 - val_acc: 0.8236
Epoch 24/40
8186/8186 [==============================] - 1s - loss: 0.4155 - acc: 0.8186 - val_loss: 0.4120 - val_acc: 0.8241
Epoch 25/40
8186/8186 [==============================] - 1s - loss: 0.4163 - acc: 0.8193 - val_loss: 0.4105 - val_acc: 0.8232
Epoch 26/40
8186/8186 [==============================] - 1s - loss: 0.4147 - acc: 0.8187 - val_loss: 0.4105 - val_acc: 0.8202
Epoch 27/40
8186/8186 [==============================] - 1s - loss: 0.4149 - acc: 0.8193 - val_loss: 0.4085 - val_acc: 0.8236
Epoch 28/40
8186/8186 [==============================] - 1s - loss: 0.4140 - acc: 0.8198 - val_loss: 0.4079 - val_acc: 0.8227
Epoch 29/40
8186/8186 [==============================] - 1s - loss: 0.4143 - acc: 0.8201 - val_loss: 0.4081 - val_acc: 0.8227
Epoch 30/40
8186/8186 [==============================] - 1s - loss: 0.4137 - acc: 0.8213 - val_loss: 0.4097 - val_acc: 0.8227
Epoch 31/40
8186/8186 [==============================] - 1s - loss: 0.4131 - acc: 0.8203 - val_loss: 0.4110 - val_acc: 0.8212
Epoch 32/40
8186/8186 [==============================] - 1s - loss: 0.4131 - acc: 0.8208 - val_loss: 0.4102 - val_acc: 0.8212
Epoch 33/40
8186/8186 [==============================] - 1s - loss: 0.4126 - acc: 0.8216 - val_loss: 0.4097 - val_acc: 0.8217
Epoch 34/40
8186/8186 [==============================] - 1s - loss: 0.4127 - acc: 0.8212 - val_loss: 0.4091 - val_acc: 0.8232
Epoch 35/40
8186/8186 [==============================] - 1s - loss: 0.4119 - acc: 0.8213 - val_loss: 0.4092 - val_acc: 0.8236
Epoch 36/40
8186/8186 [==============================] - 1s - loss: 0.4122 - acc: 0.8202 - val_loss: 0.4077 - val_acc: 0.8232
Epoch 37/40
8186/8186 [==============================] - 1s - loss: 0.4115 - acc: 0.8208 - val_loss: 0.4076 - val_acc: 0.8207
Epoch 38/40
8186/8186 [==============================] - 1s - loss: 0.4115 - acc: 0.8208 - val_loss: 0.4082 - val_acc: 0.8227
Epoch 39/40
8186/8186 [==============================] - 1s - loss: 0.4108 - acc: 0.8213 - val_loss: 0.4091 - val_acc: 0.8217
Epoch 40/40
8186/8186 [==============================] - 1s - loss: 0.4110 - acc: 0.8213 - val_loss: 0.4077 - val_acc: 0.8207
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-12.h5
chunk number 12
prepare data
start training
Train on 8205 samples, validate on 2052 samples
Epoch 1/40
8205/8205 [==============================] - 1s - loss: 0.4195 - acc: 0.8224 - val_loss: 0.4256 - val_acc: 0.8124
Epoch 2/40
8205/8205 [==============================] - 1s - loss: 0.4192 - acc: 0.8234 - val_loss: 0.4252 - val_acc: 0.8119
Epoch 3/40
8205/8205 [==============================] - 1s - loss: 0.4180 - acc: 0.8238 - val_loss: 0.4255 - val_acc: 0.8168
Epoch 4/40
8205/8205 [==============================] - 1s - loss: 0.4177 - acc: 0.8245 - val_loss: 0.4252 - val_acc: 0.8163
Epoch 5/40
8205/8205 [==============================] - 1s - loss: 0.4167 - acc: 0.8257 - val_loss: 0.4252 - val_acc: 0.8114
Epoch 6/40
8205/8205 [==============================] - 1s - loss: 0.4163 - acc: 0.8254 - val_loss: 0.4251 - val_acc: 0.8119
Epoch 7/40
8205/8205 [==============================] - 1s - loss: 0.4156 - acc: 0.8252 - val_loss: 0.4248 - val_acc: 0.8148
Epoch 8/40
8205/8205 [==============================] - 1s - loss: 0.4149 - acc: 0.8260 - val_loss: 0.4244 - val_acc: 0.8148
Epoch 9/40
8205/8205 [==============================] - 1s - loss: 0.4141 - acc: 0.8267 - val_loss: 0.4236 - val_acc: 0.8114
Epoch 10/40
8205/8205 [==============================] - 1s - loss: 0.4134 - acc: 0.8274 - val_loss: 0.4233 - val_acc: 0.8099
Epoch 11/40
8205/8205 [==============================] - 1s - loss: 0.4129 - acc: 0.8268 - val_loss: 0.4231 - val_acc: 0.8138
Epoch 12/40
8205/8205 [==============================] - 1s - loss: 0.4123 - acc: 0.8285 - val_loss: 0.4231 - val_acc: 0.8134
Epoch 13/40
8205/8205 [==============================] - 1s - loss: 0.4119 - acc: 0.8280 - val_loss: 0.4228 - val_acc: 0.8148
Epoch 14/40
8205/8205 [==============================] - 1s - loss: 0.4113 - acc: 0.8283 - val_loss: 0.4229 - val_acc: 0.8129
Epoch 15/40
8205/8205 [==============================] - 1s - loss: 0.4111 - acc: 0.8283 - val_loss: 0.4232 - val_acc: 0.8158
Epoch 16/40
8205/8205 [==============================] - 1s - loss: 0.4107 - acc: 0.8285 - val_loss: 0.4236 - val_acc: 0.8158
Epoch 17/40
8205/8205 [==============================] - 1s - loss: 0.4103 - acc: 0.8284 - val_loss: 0.4233 - val_acc: 0.8143
Epoch 18/40
8205/8205 [==============================] - 1s - loss: 0.4099 - acc: 0.8289 - val_loss: 0.4232 - val_acc: 0.8143
Epoch 19/40
8205/8205 [==============================] - 1s - loss: 0.4096 - acc: 0.8285 - val_loss: 0.4231 - val_acc: 0.8148
Epoch 20/40
8205/8205 [==============================] - 1s - loss: 0.4091 - acc: 0.8291 - val_loss: 0.4232 - val_acc: 0.8158
Epoch 21/40
8205/8205 [==============================] - 1s - loss: 0.4089 - acc: 0.8279 - val_loss: 0.4229 - val_acc: 0.8138
Epoch 22/40
8205/8205 [==============================] - 1s - loss: 0.4085 - acc: 0.8285 - val_loss: 0.4228 - val_acc: 0.8143
Epoch 23/40
8205/8205 [==============================] - 1s - loss: 0.4083 - acc: 0.8285 - val_loss: 0.4228 - val_acc: 0.8153
Epoch 24/40
8205/8205 [==============================] - 1s - loss: 0.4080 - acc: 0.8283 - val_loss: 0.4228 - val_acc: 0.8158
Epoch 25/40
8205/8205 [==============================] - 1s - loss: 0.4077 - acc: 0.8279 - val_loss: 0.4225 - val_acc: 0.8163
Epoch 26/40
8205/8205 [==============================] - 1s - loss: 0.4073 - acc: 0.8294 - val_loss: 0.4223 - val_acc: 0.8158
Epoch 27/40
8205/8205 [==============================] - 1s - loss: 0.4070 - acc: 0.8300 - val_loss: 0.4221 - val_acc: 0.8173
Epoch 28/40
8205/8205 [==============================] - 1s - loss: 0.4067 - acc: 0.8293 - val_loss: 0.4219 - val_acc: 0.8168
Epoch 29/40
8205/8205 [==============================] - 1s - loss: 0.4065 - acc: 0.8296 - val_loss: 0.4216 - val_acc: 0.8163
Epoch 30/40
8205/8205 [==============================] - 1s - loss: 0.4062 - acc: 0.8299 - val_loss: 0.4214 - val_acc: 0.8163
Epoch 31/40
8205/8205 [==============================] - 1s - loss: 0.4059 - acc: 0.8293 - val_loss: 0.4214 - val_acc: 0.8173
Epoch 32/40
8205/8205 [==============================] - 1s - loss: 0.4056 - acc: 0.8297 - val_loss: 0.4213 - val_acc: 0.8168
Epoch 33/40
8205/8205 [==============================] - 1s - loss: 0.4053 - acc: 0.8296 - val_loss: 0.4212 - val_acc: 0.8173
Epoch 34/40
8205/8205 [==============================] - 1s - loss: 0.4051 - acc: 0.8294 - val_loss: 0.4212 - val_acc: 0.8177
Epoch 35/40
8205/8205 [==============================] - 1s - loss: 0.4048 - acc: 0.8293 - val_loss: 0.4213 - val_acc: 0.8177
Epoch 36/40
8205/8205 [==============================] - 1s - loss: 0.4046 - acc: 0.8283 - val_loss: 0.4212 - val_acc: 0.8173
Epoch 37/40
8205/8205 [==============================] - 1s - loss: 0.4043 - acc: 0.8291 - val_loss: 0.4210 - val_acc: 0.8182
Epoch 38/40
8205/8205 [==============================] - 1s - loss: 0.4041 - acc: 0.8290 - val_loss: 0.4210 - val_acc: 0.8182
Epoch 39/40
8205/8205 [==============================] - 1s - loss: 0.4038 - acc: 0.8295 - val_loss: 0.4209 - val_acc: 0.8197
Epoch 40/40
8205/8205 [==============================] - 1s - loss: 0.4036 - acc: 0.8295 - val_loss: 0.4207 - val_acc: 0.8182
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-13.h5
chunk number 13
prepare data
start training
Train on 8166 samples, validate on 2042 samples
Epoch 1/40
8166/8166 [==============================] - 1s - loss: 0.4092 - acc: 0.8275 - val_loss: 0.4112 - val_acc: 0.8296
Epoch 2/40
8166/8166 [==============================] - 1s - loss: 0.4082 - acc: 0.8287 - val_loss: 0.4122 - val_acc: 0.8301
Epoch 3/40
8166/8166 [==============================] - 1s - loss: 0.4081 - acc: 0.8270 - val_loss: 0.4102 - val_acc: 0.8286
Epoch 4/40
8166/8166 [==============================] - 1s - loss: 0.4058 - acc: 0.8282 - val_loss: 0.4124 - val_acc: 0.8262
Epoch 5/40
8166/8166 [==============================] - 1s - loss: 0.4071 - acc: 0.8276 - val_loss: 0.4117 - val_acc: 0.8281
Epoch 6/40
8166/8166 [==============================] - 1s - loss: 0.4052 - acc: 0.8289 - val_loss: 0.4131 - val_acc: 0.8276
Epoch 7/40
8166/8166 [==============================] - 1s - loss: 0.4060 - acc: 0.8276 - val_loss: 0.4107 - val_acc: 0.8257
Epoch 8/40
8166/8166 [==============================] - 1s - loss: 0.4040 - acc: 0.8288 - val_loss: 0.4115 - val_acc: 0.8208
Epoch 9/40
8166/8166 [==============================] - 1s - loss: 0.4048 - acc: 0.8298 - val_loss: 0.4102 - val_acc: 0.8296
Epoch 10/40
8166/8166 [==============================] - 1s - loss: 0.4034 - acc: 0.8298 - val_loss: 0.4105 - val_acc: 0.8301
Epoch 11/40
8166/8166 [==============================] - 1s - loss: 0.4035 - acc: 0.8292 - val_loss: 0.4098 - val_acc: 0.8227
Epoch 12/40
8166/8166 [==============================] - 1s - loss: 0.4028 - acc: 0.8299 - val_loss: 0.4095 - val_acc: 0.8227
Epoch 13/40
8166/8166 [==============================] - 1s - loss: 0.4021 - acc: 0.8301 - val_loss: 0.4105 - val_acc: 0.8286
Epoch 14/40
8166/8166 [==============================] - 1s - loss: 0.4022 - acc: 0.8303 - val_loss: 0.4100 - val_acc: 0.8281
Epoch 15/40
8166/8166 [==============================] - 1s - loss: 0.4012 - acc: 0.8294 - val_loss: 0.4111 - val_acc: 0.8217
Epoch 16/40
8166/8166 [==============================] - 1s - loss: 0.4016 - acc: 0.8303 - val_loss: 0.4099 - val_acc: 0.8301
Epoch 17/40
8166/8166 [==============================] - 1s - loss: 0.4000 - acc: 0.8300 - val_loss: 0.4106 - val_acc: 0.8291
Epoch 18/40
8166/8166 [==============================] - 1s - loss: 0.4005 - acc: 0.8297 - val_loss: 0.4098 - val_acc: 0.8296
Epoch 19/40
8166/8166 [==============================] - 1s - loss: 0.3995 - acc: 0.8316 - val_loss: 0.4100 - val_acc: 0.8276
Epoch 20/40
8166/8166 [==============================] - 1s - loss: 0.3995 - acc: 0.8317 - val_loss: 0.4096 - val_acc: 0.8296
Epoch 21/40
8166/8166 [==============================] - 1s - loss: 0.3991 - acc: 0.8309 - val_loss: 0.4092 - val_acc: 0.8286
Epoch 22/40
8166/8166 [==============================] - 1s - loss: 0.3986 - acc: 0.8317 - val_loss: 0.4095 - val_acc: 0.8262
Epoch 23/40
8166/8166 [==============================] - 1s - loss: 0.3986 - acc: 0.8320 - val_loss: 0.4088 - val_acc: 0.8262
Epoch 24/40
8166/8166 [==============================] - 1s - loss: 0.3976 - acc: 0.8317 - val_loss: 0.4091 - val_acc: 0.8276
Epoch 25/40
8166/8166 [==============================] - 1s - loss: 0.3976 - acc: 0.8311 - val_loss: 0.4087 - val_acc: 0.8257
Epoch 26/40
8166/8166 [==============================] - 1s - loss: 0.3970 - acc: 0.8330 - val_loss: 0.4084 - val_acc: 0.8262
Epoch 27/40
8166/8166 [==============================] - 1s - loss: 0.3966 - acc: 0.8324 - val_loss: 0.4082 - val_acc: 0.8266
Epoch 28/40
8166/8166 [==============================] - 1s - loss: 0.3965 - acc: 0.8314 - val_loss: 0.4076 - val_acc: 0.8247
Epoch 29/40
8166/8166 [==============================] - 1s - loss: 0.3959 - acc: 0.8320 - val_loss: 0.4076 - val_acc: 0.8242
Epoch 30/40
8166/8166 [==============================] - 1s - loss: 0.3958 - acc: 0.8331 - val_loss: 0.4073 - val_acc: 0.8276
Epoch 31/40
8166/8166 [==============================] - 1s - loss: 0.3954 - acc: 0.8320 - val_loss: 0.4071 - val_acc: 0.8262
Epoch 32/40
8166/8166 [==============================] - 1s - loss: 0.3950 - acc: 0.8328 - val_loss: 0.4073 - val_acc: 0.8266
Epoch 33/40
8166/8166 [==============================] - 1s - loss: 0.3949 - acc: 0.8331 - val_loss: 0.4071 - val_acc: 0.8247
Epoch 34/40
8166/8166 [==============================] - 1s - loss: 0.3944 - acc: 0.8335 - val_loss: 0.4071 - val_acc: 0.8252
Epoch 35/40
8166/8166 [==============================] - 1s - loss: 0.3941 - acc: 0.8338 - val_loss: 0.4072 - val_acc: 0.8271
Epoch 36/40
8166/8166 [==============================] - 1s - loss: 0.3939 - acc: 0.8336 - val_loss: 0.4069 - val_acc: 0.8252
Epoch 37/40
8166/8166 [==============================] - 1s - loss: 0.3934 - acc: 0.8347 - val_loss: 0.4069 - val_acc: 0.8257
Epoch 38/40
8166/8166 [==============================] - 1s - loss: 0.3932 - acc: 0.8346 - val_loss: 0.4069 - val_acc: 0.8262
Epoch 39/40
8166/8166 [==============================] - 1s - loss: 0.3929 - acc: 0.8338 - val_loss: 0.4067 - val_acc: 0.8266
Epoch 40/40
8166/8166 [==============================] - 1s - loss: 0.3925 - acc: 0.8348 - val_loss: 0.4068 - val_acc: 0.8271
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-14.h5
chunk number 14
prepare data
start training
Train on 8130 samples, validate on 2033 samples
Epoch 1/40
8130/8130 [==============================] - 1s - loss: 0.4127 - acc: 0.8235 - val_loss: 0.4079 - val_acc: 0.8313
Epoch 2/40
8130/8130 [==============================] - 1s - loss: 0.4125 - acc: 0.8260 - val_loss: 0.4065 - val_acc: 0.8352
Epoch 3/40
8130/8130 [==============================] - 1s - loss: 0.4116 - acc: 0.8247 - val_loss: 0.4060 - val_acc: 0.8362
Epoch 4/40
8130/8130 [==============================] - 1s - loss: 0.4113 - acc: 0.8247 - val_loss: 0.4071 - val_acc: 0.8318
Epoch 5/40
8130/8130 [==============================] - 1s - loss: 0.4106 - acc: 0.8262 - val_loss: 0.4066 - val_acc: 0.8313
Epoch 6/40
8130/8130 [==============================] - 1s - loss: 0.4098 - acc: 0.8262 - val_loss: 0.4060 - val_acc: 0.8323
Epoch 7/40
8130/8130 [==============================] - 1s - loss: 0.4097 - acc: 0.8247 - val_loss: 0.4067 - val_acc: 0.8278
Epoch 8/40
8130/8130 [==============================] - 1s - loss: 0.4089 - acc: 0.8268 - val_loss: 0.4061 - val_acc: 0.8293
Epoch 9/40
8130/8130 [==============================] - 1s - loss: 0.4082 - acc: 0.8266 - val_loss: 0.4044 - val_acc: 0.8293
Epoch 10/40
8130/8130 [==============================] - 1s - loss: 0.4079 - acc: 0.8273 - val_loss: 0.4046 - val_acc: 0.8298
Epoch 11/40
8130/8130 [==============================] - 1s - loss: 0.4069 - acc: 0.8283 - val_loss: 0.4043 - val_acc: 0.8298
Epoch 12/40
8130/8130 [==============================] - 1s - loss: 0.4064 - acc: 0.8283 - val_loss: 0.4027 - val_acc: 0.8342
Epoch 13/40
8130/8130 [==============================] - 1s - loss: 0.4059 - acc: 0.8284 - val_loss: 0.4033 - val_acc: 0.8308
Epoch 14/40
8130/8130 [==============================] - 1s - loss: 0.4051 - acc: 0.8289 - val_loss: 0.4033 - val_acc: 0.8313
Epoch 15/40
8130/8130 [==============================] - 1s - loss: 0.4046 - acc: 0.8289 - val_loss: 0.4019 - val_acc: 0.8337
Epoch 16/40
8130/8130 [==============================] - 1s - loss: 0.4041 - acc: 0.8287 - val_loss: 0.4021 - val_acc: 0.8347
Epoch 17/40
8130/8130 [==============================] - 1s - loss: 0.4034 - acc: 0.8292 - val_loss: 0.4016 - val_acc: 0.8333
Epoch 18/40
8130/8130 [==============================] - 1s - loss: 0.4028 - acc: 0.8288 - val_loss: 0.4004 - val_acc: 0.8357
Epoch 19/40
8130/8130 [==============================] - 1s - loss: 0.4024 - acc: 0.8289 - val_loss: 0.4010 - val_acc: 0.8347
Epoch 20/40
8130/8130 [==============================] - 1s - loss: 0.4019 - acc: 0.8293 - val_loss: 0.4009 - val_acc: 0.8342
Epoch 21/40
8130/8130 [==============================] - 1s - loss: 0.4014 - acc: 0.8304 - val_loss: 0.4001 - val_acc: 0.8342
Epoch 22/40
8130/8130 [==============================] - 1s - loss: 0.4010 - acc: 0.8298 - val_loss: 0.4008 - val_acc: 0.8347
Epoch 23/40
8130/8130 [==============================] - 1s - loss: 0.4005 - acc: 0.8310 - val_loss: 0.4006 - val_acc: 0.8337
Epoch 24/40
8130/8130 [==============================] - 1s - loss: 0.4001 - acc: 0.8309 - val_loss: 0.4000 - val_acc: 0.8337
Epoch 25/40
8130/8130 [==============================] - 1s - loss: 0.3997 - acc: 0.8321 - val_loss: 0.4008 - val_acc: 0.8342
Epoch 26/40
8130/8130 [==============================] - 1s - loss: 0.3993 - acc: 0.8314 - val_loss: 0.4002 - val_acc: 0.8357
Epoch 27/40
8130/8130 [==============================] - 1s - loss: 0.3988 - acc: 0.8319 - val_loss: 0.3998 - val_acc: 0.8337
Epoch 28/40
8130/8130 [==============================] - 1s - loss: 0.3984 - acc: 0.8320 - val_loss: 0.4005 - val_acc: 0.8342
Epoch 29/40
8130/8130 [==============================] - 1s - loss: 0.3981 - acc: 0.8306 - val_loss: 0.3998 - val_acc: 0.8342
Epoch 30/40
8130/8130 [==============================] - 1s - loss: 0.3976 - acc: 0.8316 - val_loss: 0.4000 - val_acc: 0.8367
Epoch 31/40
8130/8130 [==============================] - 1s - loss: 0.3972 - acc: 0.8310 - val_loss: 0.4006 - val_acc: 0.8362
Epoch 32/40
8130/8130 [==============================] - 1s - loss: 0.3969 - acc: 0.8301 - val_loss: 0.3998 - val_acc: 0.8377
Epoch 33/40
8130/8130 [==============================] - 1s - loss: 0.3965 - acc: 0.8305 - val_loss: 0.4001 - val_acc: 0.8352
Epoch 34/40
8130/8130 [==============================] - 1s - loss: 0.3961 - acc: 0.8310 - val_loss: 0.3997 - val_acc: 0.8367
Epoch 35/40
8130/8130 [==============================] - 1s - loss: 0.3958 - acc: 0.8309 - val_loss: 0.3992 - val_acc: 0.8372
Epoch 36/40
8130/8130 [==============================] - 1s - loss: 0.3954 - acc: 0.8312 - val_loss: 0.3998 - val_acc: 0.8372
Epoch 37/40
8130/8130 [==============================] - 1s - loss: 0.3951 - acc: 0.8314 - val_loss: 0.3989 - val_acc: 0.8357
Epoch 38/40
8130/8130 [==============================] - 1s - loss: 0.3947 - acc: 0.8322 - val_loss: 0.3996 - val_acc: 0.8357
Epoch 39/40
8130/8130 [==============================] - 1s - loss: 0.3943 - acc: 0.8311 - val_loss: 0.3989 - val_acc: 0.8362
Epoch 40/40
8130/8130 [==============================] - 1s - loss: 0.3939 - acc: 0.8323 - val_loss: 0.3991 - val_acc: 0.8352
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-15.h5
chunk number 15
prepare data
start training
Train on 8086 samples, validate on 2022 samples
Epoch 1/40
8086/8086 [==============================] - 1s - loss: 0.4065 - acc: 0.8280 - val_loss: 0.4054 - val_acc: 0.8338
Epoch 2/40
8086/8086 [==============================] - 1s - loss: 0.4053 - acc: 0.8262 - val_loss: 0.4049 - val_acc: 0.8274
Epoch 3/40
8086/8086 [==============================] - 1s - loss: 0.4039 - acc: 0.8282 - val_loss: 0.4054 - val_acc: 0.8299
Epoch 4/40
8086/8086 [==============================] - 1s - loss: 0.4028 - acc: 0.8292 - val_loss: 0.4047 - val_acc: 0.8314
Epoch 5/40
8086/8086 [==============================] - 1s - loss: 0.4015 - acc: 0.8293 - val_loss: 0.4042 - val_acc: 0.8338
Epoch 6/40
8086/8086 [==============================] - 1s - loss: 0.4003 - acc: 0.8293 - val_loss: 0.4053 - val_acc: 0.8328
Epoch 7/40
8086/8086 [==============================] - 1s - loss: 0.3994 - acc: 0.8293 - val_loss: 0.4051 - val_acc: 0.8343
Epoch 8/40
8086/8086 [==============================] - 1s - loss: 0.3986 - acc: 0.8288 - val_loss: 0.4063 - val_acc: 0.8333
Epoch 9/40
8086/8086 [==============================] - 1s - loss: 0.3976 - acc: 0.8295 - val_loss: 0.4048 - val_acc: 0.8343
Epoch 10/40
8086/8086 [==============================] - 1s - loss: 0.3966 - acc: 0.8301 - val_loss: 0.4048 - val_acc: 0.8333
Epoch 11/40
8086/8086 [==============================] - 1s - loss: 0.3959 - acc: 0.8322 - val_loss: 0.4054 - val_acc: 0.8353
Epoch 12/40
8086/8086 [==============================] - 1s - loss: 0.3956 - acc: 0.8323 - val_loss: 0.4047 - val_acc: 0.8323
Epoch 13/40
8086/8086 [==============================] - 1s - loss: 0.3950 - acc: 0.8326 - val_loss: 0.4062 - val_acc: 0.8338
Epoch 14/40
8086/8086 [==============================] - 1s - loss: 0.3943 - acc: 0.8322 - val_loss: 0.4060 - val_acc: 0.8328
Epoch 15/40
8086/8086 [==============================] - 1s - loss: 0.3937 - acc: 0.8321 - val_loss: 0.4062 - val_acc: 0.8323
Epoch 16/40
8086/8086 [==============================] - 1s - loss: 0.3931 - acc: 0.8332 - val_loss: 0.4062 - val_acc: 0.8323
Epoch 17/40
8086/8086 [==============================] - 1s - loss: 0.3926 - acc: 0.8332 - val_loss: 0.4056 - val_acc: 0.8323
Epoch 18/40
8086/8086 [==============================] - 1s - loss: 0.3921 - acc: 0.8333 - val_loss: 0.4061 - val_acc: 0.8328
Epoch 19/40
8086/8086 [==============================] - 1s - loss: 0.3915 - acc: 0.8345 - val_loss: 0.4054 - val_acc: 0.8358
Epoch 20/40
8086/8086 [==============================] - 1s - loss: 0.3907 - acc: 0.8356 - val_loss: 0.4058 - val_acc: 0.8358
Epoch 21/40
8086/8086 [==============================] - 1s - loss: 0.3901 - acc: 0.8354 - val_loss: 0.4050 - val_acc: 0.8353
Epoch 22/40
8086/8086 [==============================] - 1s - loss: 0.3896 - acc: 0.8363 - val_loss: 0.4054 - val_acc: 0.8343
Epoch 23/40
8086/8086 [==============================] - 1s - loss: 0.3891 - acc: 0.8349 - val_loss: 0.4045 - val_acc: 0.8358
Epoch 24/40
8086/8086 [==============================] - 1s - loss: 0.3888 - acc: 0.8354 - val_loss: 0.4068 - val_acc: 0.8338
Epoch 25/40
8086/8086 [==============================] - 1s - loss: 0.3889 - acc: 0.8356 - val_loss: 0.4059 - val_acc: 0.8333
Epoch 26/40
8086/8086 [==============================] - 1s - loss: 0.3906 - acc: 0.8358 - val_loss: 0.4175 - val_acc: 0.8299
Epoch 27/40
8086/8086 [==============================] - 1s - loss: 0.3995 - acc: 0.8334 - val_loss: 0.4112 - val_acc: 0.8259
Epoch 28/40
8086/8086 [==============================] - 1s - loss: 0.3982 - acc: 0.8337 - val_loss: 0.4273 - val_acc: 0.8229
Epoch 29/40
8086/8086 [==============================] - 1s - loss: 0.4109 - acc: 0.8272 - val_loss: 0.4032 - val_acc: 0.8363
Epoch 30/40
8086/8086 [==============================] - 1s - loss: 0.3869 - acc: 0.8365 - val_loss: 0.4120 - val_acc: 0.8279
Epoch 31/40
8086/8086 [==============================] - 1s - loss: 0.3992 - acc: 0.8347 - val_loss: 0.4503 - val_acc: 0.8116
Epoch 32/40
8086/8086 [==============================] - 1s - loss: 0.4385 - acc: 0.8131 - val_loss: 0.4204 - val_acc: 0.8259
Epoch 33/40
8086/8086 [==============================] - 1s - loss: 0.4022 - acc: 0.8303 - val_loss: 0.5045 - val_acc: 0.7676
Epoch 34/40
8086/8086 [==============================] - 1s - loss: 0.5049 - acc: 0.7656 - val_loss: 0.4818 - val_acc: 0.7918
Epoch 35/40
8086/8086 [==============================] - 1s - loss: 0.4711 - acc: 0.7930 - val_loss: 0.5375 - val_acc: 0.7631
Epoch 36/40
8086/8086 [==============================] - 1s - loss: 0.5240 - acc: 0.7651 - val_loss: 0.4986 - val_acc: 0.7750
Epoch 37/40
8086/8086 [==============================] - 1s - loss: 0.4885 - acc: 0.7788 - val_loss: 0.4320 - val_acc: 0.8165
Epoch 38/40
8086/8086 [==============================] - 1s - loss: 0.4297 - acc: 0.8120 - val_loss: 0.4789 - val_acc: 0.8136
Epoch 39/40
8086/8086 [==============================] - 1s - loss: 0.4834 - acc: 0.8131 - val_loss: 0.4681 - val_acc: 0.8145
Epoch 40/40
8086/8086 [==============================] - 1s - loss: 0.4702 - acc: 0.8177 - val_loss: 0.4325 - val_acc: 0.8101
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-16.h5
chunk number 16
prepare data
start training
Train on 8113 samples, validate on 2029 samples
Epoch 1/40
8113/8113 [==============================] - 1s - loss: 0.4435 - acc: 0.8019 - val_loss: 0.4644 - val_acc: 0.7910
Epoch 2/40
8113/8113 [==============================] - 1s - loss: 0.4575 - acc: 0.7951 - val_loss: 0.4542 - val_acc: 0.8019
Epoch 3/40
8113/8113 [==============================] - 1s - loss: 0.4467 - acc: 0.8066 - val_loss: 0.4271 - val_acc: 0.8167
Epoch 4/40
8113/8113 [==============================] - 1s - loss: 0.4242 - acc: 0.8175 - val_loss: 0.4403 - val_acc: 0.8142
Epoch 5/40
8113/8113 [==============================] - 1s - loss: 0.4439 - acc: 0.8004 - val_loss: 0.4273 - val_acc: 0.8117
Epoch 6/40
8113/8113 [==============================] - 1s - loss: 0.4246 - acc: 0.8122 - val_loss: 0.4372 - val_acc: 0.8152
Epoch 7/40
8113/8113 [==============================] - 1s - loss: 0.4245 - acc: 0.8208 - val_loss: 0.4514 - val_acc: 0.8107
Epoch 8/40
8113/8113 [==============================] - 1s - loss: 0.4345 - acc: 0.8178 - val_loss: 0.4393 - val_acc: 0.8137
Epoch 9/40
8113/8113 [==============================] - 1s - loss: 0.4252 - acc: 0.8216 - val_loss: 0.4311 - val_acc: 0.8186
Epoch 10/40
8113/8113 [==============================] - 1s - loss: 0.4241 - acc: 0.8172 - val_loss: 0.4260 - val_acc: 0.8142
Epoch 11/40
8113/8113 [==============================] - 1s - loss: 0.4214 - acc: 0.8179 - val_loss: 0.4189 - val_acc: 0.8201
Epoch 12/40
8113/8113 [==============================] - 1s - loss: 0.4096 - acc: 0.8247 - val_loss: 0.4281 - val_acc: 0.8196
Epoch 13/40
8113/8113 [==============================] - 1s - loss: 0.4161 - acc: 0.8199 - val_loss: 0.4280 - val_acc: 0.8167
Epoch 14/40
8113/8113 [==============================] - 1s - loss: 0.4170 - acc: 0.8194 - val_loss: 0.4234 - val_acc: 0.8181
Epoch 15/40
8113/8113 [==============================] - 1s - loss: 0.4152 - acc: 0.8214 - val_loss: 0.4170 - val_acc: 0.8250
Epoch 16/40
8113/8113 [==============================] - 1s - loss: 0.4086 - acc: 0.8250 - val_loss: 0.4170 - val_acc: 0.8221
Epoch 17/40
8113/8113 [==============================] - 1s - loss: 0.4071 - acc: 0.8276 - val_loss: 0.4215 - val_acc: 0.8216
Epoch 18/40
8113/8113 [==============================] - 1s - loss: 0.4107 - acc: 0.8288 - val_loss: 0.4225 - val_acc: 0.8221
Epoch 19/40
8113/8113 [==============================] - 1s - loss: 0.4105 - acc: 0.8284 - val_loss: 0.4195 - val_acc: 0.8216
Epoch 20/40
8113/8113 [==============================] - 1s - loss: 0.4060 - acc: 0.8299 - val_loss: 0.4185 - val_acc: 0.8221
Epoch 21/40
8113/8113 [==============================] - 1s - loss: 0.4033 - acc: 0.8293 - val_loss: 0.4205 - val_acc: 0.8231
Epoch 22/40
8113/8113 [==============================] - 1s - loss: 0.4051 - acc: 0.8285 - val_loss: 0.4203 - val_acc: 0.8221
Epoch 23/40
8113/8113 [==============================] - 1s - loss: 0.4058 - acc: 0.8276 - val_loss: 0.4150 - val_acc: 0.8241
Epoch 24/40
8113/8113 [==============================] - 1s - loss: 0.4028 - acc: 0.8287 - val_loss: 0.4101 - val_acc: 0.8255
Epoch 25/40
8113/8113 [==============================] - 1s - loss: 0.4014 - acc: 0.8297 - val_loss: 0.4103 - val_acc: 0.8275
Epoch 26/40
8113/8113 [==============================] - 1s - loss: 0.4028 - acc: 0.8290 - val_loss: 0.4130 - val_acc: 0.8255
Epoch 27/40
8113/8113 [==============================] - 1s - loss: 0.4026 - acc: 0.8309 - val_loss: 0.4135 - val_acc: 0.8221
Epoch 28/40
8113/8113 [==============================] - 1s - loss: 0.4006 - acc: 0.8310 - val_loss: 0.4134 - val_acc: 0.8221
Epoch 29/40
8113/8113 [==============================] - 1s - loss: 0.3993 - acc: 0.8314 - val_loss: 0.4163 - val_acc: 0.8211
Epoch 30/40
8113/8113 [==============================] - 1s - loss: 0.4004 - acc: 0.8314 - val_loss: 0.4183 - val_acc: 0.8196
Epoch 31/40
8113/8113 [==============================] - 1s - loss: 0.4001 - acc: 0.8310 - val_loss: 0.4167 - val_acc: 0.8206
Epoch 32/40
8113/8113 [==============================] - 1s - loss: 0.3987 - acc: 0.8326 - val_loss: 0.4131 - val_acc: 0.8226
Epoch 33/40
8113/8113 [==============================] - 1s - loss: 0.3976 - acc: 0.8306 - val_loss: 0.4119 - val_acc: 0.8221
Epoch 34/40
8113/8113 [==============================] - 1s - loss: 0.3979 - acc: 0.8308 - val_loss: 0.4126 - val_acc: 0.8250
Epoch 35/40
8113/8113 [==============================] - 1s - loss: 0.3976 - acc: 0.8327 - val_loss: 0.4119 - val_acc: 0.8250
Epoch 36/40
8113/8113 [==============================] - 1s - loss: 0.3967 - acc: 0.8319 - val_loss: 0.4105 - val_acc: 0.8270
Epoch 37/40
8113/8113 [==============================] - 1s - loss: 0.3962 - acc: 0.8320 - val_loss: 0.4120 - val_acc: 0.8231
Epoch 38/40
8113/8113 [==============================] - 1s - loss: 0.3958 - acc: 0.8332 - val_loss: 0.4148 - val_acc: 0.8226
Epoch 39/40
8113/8113 [==============================] - 1s - loss: 0.3954 - acc: 0.8330 - val_loss: 0.4146 - val_acc: 0.8206
Epoch 40/40
8113/8113 [==============================] - 1s - loss: 0.3946 - acc: 0.8341 - val_loss: 0.4138 - val_acc: 0.8206
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-17.h5
chunk number 17
prepare data
start training
Train on 8134 samples, validate on 2034 samples
Epoch 1/40
8134/8134 [==============================] - 1s - loss: 0.4280 - acc: 0.8226 - val_loss: 0.4311 - val_acc: 0.8201
Epoch 2/40
8134/8134 [==============================] - 1s - loss: 0.4070 - acc: 0.8274 - val_loss: 0.4481 - val_acc: 0.8048
Epoch 3/40
8134/8134 [==============================] - 1s - loss: 0.4192 - acc: 0.8198 - val_loss: 0.4482 - val_acc: 0.8053
Epoch 4/40
8134/8134 [==============================] - 1s - loss: 0.4189 - acc: 0.8210 - val_loss: 0.4289 - val_acc: 0.8181
Epoch 5/40
8134/8134 [==============================] - 1s - loss: 0.4033 - acc: 0.8275 - val_loss: 0.4301 - val_acc: 0.8230
Epoch 6/40
8134/8134 [==============================] - 1s - loss: 0.4134 - acc: 0.8235 - val_loss: 0.4273 - val_acc: 0.8240
Epoch 7/40
8134/8134 [==============================] - 1s - loss: 0.4091 - acc: 0.8262 - val_loss: 0.4312 - val_acc: 0.8191
Epoch 8/40
8134/8134 [==============================] - 1s - loss: 0.4035 - acc: 0.8302 - val_loss: 0.4427 - val_acc: 0.8156
Epoch 9/40
8134/8134 [==============================] - 1s - loss: 0.4120 - acc: 0.8275 - val_loss: 0.4332 - val_acc: 0.8181
Epoch 10/40
8134/8134 [==============================] - 1s - loss: 0.4039 - acc: 0.8283 - val_loss: 0.4235 - val_acc: 0.8206
Epoch 11/40
8134/8134 [==============================] - 1s - loss: 0.4011 - acc: 0.8276 - val_loss: 0.4262 - val_acc: 0.8206
Epoch 12/40
8134/8134 [==============================] - 1s - loss: 0.4069 - acc: 0.8255 - val_loss: 0.4240 - val_acc: 0.8220
Epoch 13/40
8134/8134 [==============================] - 1s - loss: 0.3975 - acc: 0.8296 - val_loss: 0.4338 - val_acc: 0.8196
Epoch 14/40
8134/8134 [==============================] - 1s - loss: 0.4039 - acc: 0.8274 - val_loss: 0.4275 - val_acc: 0.8156
Epoch 15/40
8134/8134 [==============================] - 1s - loss: 0.3992 - acc: 0.8300 - val_loss: 0.4193 - val_acc: 0.8245
Epoch 16/40
8134/8134 [==============================] - 1s - loss: 0.3977 - acc: 0.8307 - val_loss: 0.4204 - val_acc: 0.8235
Epoch 17/40
8134/8134 [==============================] - 1s - loss: 0.4007 - acc: 0.8283 - val_loss: 0.4227 - val_acc: 0.8196
Epoch 18/40
8134/8134 [==============================] - 1s - loss: 0.3954 - acc: 0.8292 - val_loss: 0.4302 - val_acc: 0.8196
Epoch 19/40
8134/8134 [==============================] - 1s - loss: 0.3995 - acc: 0.8286 - val_loss: 0.4237 - val_acc: 0.8191
Epoch 20/40
8134/8134 [==============================] - 1s - loss: 0.3946 - acc: 0.8310 - val_loss: 0.4204 - val_acc: 0.8235
Epoch 21/40
8134/8134 [==============================] - 1s - loss: 0.3967 - acc: 0.8301 - val_loss: 0.4200 - val_acc: 0.8245
Epoch 22/40
8134/8134 [==============================] - 1s - loss: 0.3941 - acc: 0.8322 - val_loss: 0.4261 - val_acc: 0.8201
Epoch 23/40
8134/8134 [==============================] - 1s - loss: 0.3945 - acc: 0.8300 - val_loss: 0.4272 - val_acc: 0.8191
Epoch 24/40
8134/8134 [==============================] - 1s - loss: 0.3949 - acc: 0.8307 - val_loss: 0.4204 - val_acc: 0.8245
Epoch 25/40
8134/8134 [==============================] - 1s - loss: 0.3924 - acc: 0.8339 - val_loss: 0.4197 - val_acc: 0.8225
Epoch 26/40
8134/8134 [==============================] - 1s - loss: 0.3943 - acc: 0.8311 - val_loss: 0.4221 - val_acc: 0.8250
Epoch 27/40
8134/8134 [==============================] - 1s - loss: 0.3914 - acc: 0.8321 - val_loss: 0.4260 - val_acc: 0.8225
Epoch 28/40
8134/8134 [==============================] - 1s - loss: 0.3931 - acc: 0.8297 - val_loss: 0.4208 - val_acc: 0.8265
Epoch 29/40
8134/8134 [==============================] - 1s - loss: 0.3906 - acc: 0.8328 - val_loss: 0.4189 - val_acc: 0.8240
Epoch 30/40
8134/8134 [==============================] - 1s - loss: 0.3923 - acc: 0.8327 - val_loss: 0.4198 - val_acc: 0.8269
Epoch 31/40
8134/8134 [==============================] - 1s - loss: 0.3899 - acc: 0.8330 - val_loss: 0.4239 - val_acc: 0.8225
Epoch 32/40
8134/8134 [==============================] - 1s - loss: 0.3910 - acc: 0.8313 - val_loss: 0.4211 - val_acc: 0.8245
Epoch 33/40
8134/8134 [==============================] - 1s - loss: 0.3894 - acc: 0.8326 - val_loss: 0.4189 - val_acc: 0.8294
Epoch 34/40
8134/8134 [==============================] - 1s - loss: 0.3901 - acc: 0.8353 - val_loss: 0.4200 - val_acc: 0.8269
Epoch 35/40
8134/8134 [==============================] - 1s - loss: 0.3886 - acc: 0.8342 - val_loss: 0.4238 - val_acc: 0.8235
Epoch 36/40
8134/8134 [==============================] - 1s - loss: 0.3892 - acc: 0.8324 - val_loss: 0.4219 - val_acc: 0.8260
Epoch 37/40
8134/8134 [==============================] - 1s - loss: 0.3882 - acc: 0.8338 - val_loss: 0.4194 - val_acc: 0.8255
Epoch 38/40
8134/8134 [==============================] - 1s - loss: 0.3885 - acc: 0.8339 - val_loss: 0.4199 - val_acc: 0.8284
Epoch 39/40
8134/8134 [==============================] - 1s - loss: 0.3874 - acc: 0.8340 - val_loss: 0.4225 - val_acc: 0.8265
Epoch 40/40
8134/8134 [==============================] - 1s - loss: 0.3878 - acc: 0.8328 - val_loss: 0.4206 - val_acc: 0.8269
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-18.h5
chunk number 18
prepare data
start training
Train on 8221 samples, validate on 2056 samples
Epoch 1/40
8221/8221 [==============================] - 1s - loss: 0.4131 - acc: 0.8216 - val_loss: 0.3908 - val_acc: 0.8380
Epoch 2/40
8221/8221 [==============================] - 1s - loss: 0.4127 - acc: 0.8223 - val_loss: 0.3905 - val_acc: 0.8410
Epoch 3/40
8221/8221 [==============================] - 1s - loss: 0.4095 - acc: 0.8234 - val_loss: 0.3938 - val_acc: 0.8385
Epoch 4/40
8221/8221 [==============================] - 1s - loss: 0.4102 - acc: 0.8222 - val_loss: 0.3923 - val_acc: 0.8414
Epoch 5/40
8221/8221 [==============================] - 1s - loss: 0.4085 - acc: 0.8228 - val_loss: 0.3917 - val_acc: 0.8371
Epoch 6/40
8221/8221 [==============================] - 1s - loss: 0.4085 - acc: 0.8247 - val_loss: 0.3897 - val_acc: 0.8410
Epoch 7/40
8221/8221 [==============================] - 1s - loss: 0.4063 - acc: 0.8244 - val_loss: 0.3900 - val_acc: 0.8385
Epoch 8/40
8221/8221 [==============================] - 1s - loss: 0.4063 - acc: 0.8219 - val_loss: 0.3879 - val_acc: 0.8410
Epoch 9/40
8221/8221 [==============================] - 1s - loss: 0.4050 - acc: 0.8234 - val_loss: 0.3864 - val_acc: 0.8405
Epoch 10/40
8221/8221 [==============================] - 1s - loss: 0.4050 - acc: 0.8228 - val_loss: 0.3860 - val_acc: 0.8429
Epoch 11/40
8221/8221 [==============================] - 1s - loss: 0.4037 - acc: 0.8248 - val_loss: 0.3874 - val_acc: 0.8400
Epoch 12/40
8221/8221 [==============================] - 1s - loss: 0.4034 - acc: 0.8224 - val_loss: 0.3869 - val_acc: 0.8390
Epoch 13/40
8221/8221 [==============================] - 1s - loss: 0.4027 - acc: 0.8242 - val_loss: 0.3864 - val_acc: 0.8419
Epoch 14/40
8221/8221 [==============================] - 1s - loss: 0.4027 - acc: 0.8257 - val_loss: 0.3864 - val_acc: 0.8410
Epoch 15/40
8221/8221 [==============================] - 1s - loss: 0.4022 - acc: 0.8259 - val_loss: 0.3868 - val_acc: 0.8395
Epoch 16/40
8221/8221 [==============================] - 1s - loss: 0.4018 - acc: 0.8254 - val_loss: 0.3861 - val_acc: 0.8410
Epoch 17/40
8221/8221 [==============================] - 1s - loss: 0.4012 - acc: 0.8248 - val_loss: 0.3852 - val_acc: 0.8405
Epoch 18/40
8221/8221 [==============================] - 1s - loss: 0.4009 - acc: 0.8242 - val_loss: 0.3851 - val_acc: 0.8400
Epoch 19/40
8221/8221 [==============================] - 1s - loss: 0.4005 - acc: 0.8256 - val_loss: 0.3857 - val_acc: 0.8390
Epoch 20/40
8221/8221 [==============================] - 1s - loss: 0.4003 - acc: 0.8248 - val_loss: 0.3855 - val_acc: 0.8395
Epoch 21/40
8221/8221 [==============================] - 1s - loss: 0.3997 - acc: 0.8246 - val_loss: 0.3857 - val_acc: 0.8390
Epoch 22/40
8221/8221 [==============================] - 1s - loss: 0.3996 - acc: 0.8251 - val_loss: 0.3857 - val_acc: 0.8395
Epoch 23/40
8221/8221 [==============================] - 1s - loss: 0.3990 - acc: 0.8258 - val_loss: 0.3861 - val_acc: 0.8400
Epoch 24/40
8221/8221 [==============================] - 1s - loss: 0.3989 - acc: 0.8257 - val_loss: 0.3855 - val_acc: 0.8405
Epoch 25/40
8221/8221 [==============================] - 1s - loss: 0.3982 - acc: 0.8263 - val_loss: 0.3853 - val_acc: 0.8395
Epoch 26/40
8221/8221 [==============================] - 1s - loss: 0.3981 - acc: 0.8264 - val_loss: 0.3850 - val_acc: 0.8405
Epoch 27/40
8221/8221 [==============================] - 1s - loss: 0.3976 - acc: 0.8257 - val_loss: 0.3852 - val_acc: 0.8414
Epoch 28/40
8221/8221 [==============================] - 1s - loss: 0.3975 - acc: 0.8251 - val_loss: 0.3848 - val_acc: 0.8424
Epoch 29/40
8221/8221 [==============================] - 1s - loss: 0.3971 - acc: 0.8261 - val_loss: 0.3848 - val_acc: 0.8424
Epoch 30/40
8221/8221 [==============================] - 1s - loss: 0.3969 - acc: 0.8261 - val_loss: 0.3851 - val_acc: 0.8395
Epoch 31/40
8221/8221 [==============================] - 1s - loss: 0.3966 - acc: 0.8267 - val_loss: 0.3850 - val_acc: 0.8419
Epoch 32/40
8221/8221 [==============================] - 1s - loss: 0.3963 - acc: 0.8268 - val_loss: 0.3849 - val_acc: 0.8414
Epoch 33/40
8221/8221 [==============================] - 1s - loss: 0.3961 - acc: 0.8271 - val_loss: 0.3847 - val_acc: 0.8414
Epoch 34/40
8221/8221 [==============================] - 1s - loss: 0.3956 - acc: 0.8279 - val_loss: 0.3846 - val_acc: 0.8405
Epoch 35/40
8221/8221 [==============================] - 1s - loss: 0.3954 - acc: 0.8274 - val_loss: 0.3843 - val_acc: 0.8405
Epoch 36/40
8221/8221 [==============================] - 1s - loss: 0.3951 - acc: 0.8271 - val_loss: 0.3842 - val_acc: 0.8410
Epoch 37/40
8221/8221 [==============================] - 1s - loss: 0.3948 - acc: 0.8275 - val_loss: 0.3843 - val_acc: 0.8395
Epoch 38/40
8221/8221 [==============================] - 1s - loss: 0.3946 - acc: 0.8286 - val_loss: 0.3841 - val_acc: 0.8400
Epoch 39/40
8221/8221 [==============================] - 1s - loss: 0.3942 - acc: 0.8281 - val_loss: 0.3840 - val_acc: 0.8419
Epoch 40/40
8221/8221 [==============================] - 1s - loss: 0.3940 - acc: 0.8287 - val_loss: 0.3840 - val_acc: 0.8405
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-19.h5
chunk number 19
prepare data
start training
Train on 8180 samples, validate on 2046 samples
Epoch 1/40
8180/8180 [==============================] - 2s - loss: 0.4316 - acc: 0.8270 - val_loss: 0.4502 - val_acc: 0.8006
Epoch 2/40
8180/8180 [==============================] - 2s - loss: 0.4347 - acc: 0.8137 - val_loss: 0.4415 - val_acc: 0.8133
Epoch 3/40
8180/8180 [==============================] - 2s - loss: 0.4384 - acc: 0.8209 - val_loss: 0.4278 - val_acc: 0.8177
Epoch 4/40
8180/8180 [==============================] - 2s - loss: 0.4204 - acc: 0.8268 - val_loss: 0.4619 - val_acc: 0.8001
Epoch 5/40
8180/8180 [==============================] - 2s - loss: 0.4536 - acc: 0.8004 - val_loss: 0.4453 - val_acc: 0.8089
Epoch 6/40
8180/8180 [==============================] - 2s - loss: 0.4493 - acc: 0.8034 - val_loss: 0.4583 - val_acc: 0.7972
Epoch 7/40
8180/8180 [==============================] - 2s - loss: 0.4656 - acc: 0.7932 - val_loss: 0.4233 - val_acc: 0.8148
Epoch 8/40
8180/8180 [==============================] - 2s - loss: 0.4280 - acc: 0.8159 - val_loss: 0.4424 - val_acc: 0.8113
Epoch 9/40
8180/8180 [==============================] - 2s - loss: 0.4430 - acc: 0.8209 - val_loss: 0.4294 - val_acc: 0.8201
Epoch 10/40
8180/8180 [==============================] - 2s - loss: 0.4314 - acc: 0.8256 - val_loss: 0.4186 - val_acc: 0.8211
Epoch 11/40
8180/8180 [==============================] - 2s - loss: 0.4238 - acc: 0.8211 - val_loss: 0.4314 - val_acc: 0.8138
Epoch 12/40
8180/8180 [==============================] - 2s - loss: 0.4389 - acc: 0.8143 - val_loss: 0.4270 - val_acc: 0.8133
Epoch 13/40
8180/8180 [==============================] - 2s - loss: 0.4335 - acc: 0.8169 - val_loss: 0.4179 - val_acc: 0.8236
Epoch 14/40
8180/8180 [==============================] - 2s - loss: 0.4212 - acc: 0.8256 - val_loss: 0.4293 - val_acc: 0.8270
Epoch 15/40
8180/8180 [==============================] - 2s - loss: 0.4309 - acc: 0.8170 - val_loss: 0.4211 - val_acc: 0.8255
Epoch 16/40
8180/8180 [==============================] - 2s - loss: 0.4217 - acc: 0.8221 - val_loss: 0.4139 - val_acc: 0.8245
Epoch 17/40
8180/8180 [==============================] - 2s - loss: 0.4159 - acc: 0.8249 - val_loss: 0.4198 - val_acc: 0.8211
Epoch 18/40
8180/8180 [==============================] - 2s - loss: 0.4223 - acc: 0.8224 - val_loss: 0.4180 - val_acc: 0.8231
Epoch 19/40
8180/8180 [==============================] - 2s - loss: 0.4194 - acc: 0.8246 - val_loss: 0.4157 - val_acc: 0.8255
Epoch 20/40
8180/8180 [==============================] - 2s - loss: 0.4148 - acc: 0.8289 - val_loss: 0.4226 - val_acc: 0.8250
Epoch 21/40
8180/8180 [==============================] - 2s - loss: 0.4198 - acc: 0.8265 - val_loss: 0.4139 - val_acc: 0.8284
Epoch 22/40
8180/8180 [==============================] - 2s - loss: 0.4117 - acc: 0.8302 - val_loss: 0.4156 - val_acc: 0.8226
Epoch 23/40
8180/8180 [==============================] - 2s - loss: 0.4150 - acc: 0.8273 - val_loss: 0.4140 - val_acc: 0.8245
Epoch 24/40
8180/8180 [==============================] - 2s - loss: 0.4141 - acc: 0.8278 - val_loss: 0.4109 - val_acc: 0.8270
Epoch 25/40
8180/8180 [==============================] - 2s - loss: 0.4092 - acc: 0.8311 - val_loss: 0.4137 - val_acc: 0.8275
Epoch 26/40
8180/8180 [==============================] - 2s - loss: 0.4123 - acc: 0.8281 - val_loss: 0.4087 - val_acc: 0.8314
Epoch 27/40
8180/8180 [==============================] - 2s - loss: 0.4080 - acc: 0.8314 - val_loss: 0.4111 - val_acc: 0.8270
Epoch 28/40
8180/8180 [==============================] - 2s - loss: 0.4103 - acc: 0.8281 - val_loss: 0.4101 - val_acc: 0.8270
Epoch 29/40
8180/8180 [==============================] - 2s - loss: 0.4089 - acc: 0.8286 - val_loss: 0.4084 - val_acc: 0.8275
Epoch 30/40
8180/8180 [==============================] - 2s - loss: 0.4058 - acc: 0.8331 - val_loss: 0.4124 - val_acc: 0.8275
Epoch 31/40
8180/8180 [==============================] - 2s - loss: 0.4082 - acc: 0.8335 - val_loss: 0.4094 - val_acc: 0.8255
Epoch 32/40
8180/8180 [==============================] - 2s - loss: 0.4049 - acc: 0.8331 - val_loss: 0.4100 - val_acc: 0.8240
Epoch 33/40
8180/8180 [==============================] - 2s - loss: 0.4069 - acc: 0.8301 - val_loss: 0.4082 - val_acc: 0.8245
Epoch 34/40
8180/8180 [==============================] - 2s - loss: 0.4050 - acc: 0.8304 - val_loss: 0.4084 - val_acc: 0.8299
Epoch 35/40
8180/8180 [==============================] - 2s - loss: 0.4035 - acc: 0.8346 - val_loss: 0.4101 - val_acc: 0.8280
Epoch 36/40
8180/8180 [==============================] - 2s - loss: 0.4044 - acc: 0.8340 - val_loss: 0.4071 - val_acc: 0.8289
Epoch 37/40
8180/8180 [==============================] - 2s - loss: 0.4024 - acc: 0.8324 - val_loss: 0.4082 - val_acc: 0.8294
Epoch 38/40
8180/8180 [==============================] - 2s - loss: 0.4034 - acc: 0.8324 - val_loss: 0.4077 - val_acc: 0.8299
Epoch 39/40
8180/8180 [==============================] - 2s - loss: 0.4014 - acc: 0.8346 - val_loss: 0.4105 - val_acc: 0.8289
Epoch 40/40
8180/8180 [==============================] - 2s - loss: 0.4027 - acc: 0.8322 - val_loss: 0.4083 - val_acc: 0.8314
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-20.h5
chunk number 20
prepare data
start training
Train on 8123 samples, validate on 2031 samples
Epoch 1/40
8123/8123 [==============================] - 1s - loss: 0.4121 - acc: 0.8267 - val_loss: 0.4010 - val_acc: 0.8331
Epoch 2/40
8123/8123 [==============================] - 1s - loss: 0.4037 - acc: 0.8288 - val_loss: 0.4132 - val_acc: 0.8227
Epoch 3/40
8123/8123 [==============================] - 1s - loss: 0.4144 - acc: 0.8233 - val_loss: 0.4074 - val_acc: 0.8282
Epoch 4/40
8123/8123 [==============================] - 1s - loss: 0.4086 - acc: 0.8268 - val_loss: 0.3969 - val_acc: 0.8370
Epoch 5/40
8123/8123 [==============================] - 1s - loss: 0.4007 - acc: 0.8336 - val_loss: 0.4055 - val_acc: 0.8375
Epoch 6/40
8123/8123 [==============================] - 1s - loss: 0.4114 - acc: 0.8296 - val_loss: 0.3958 - val_acc: 0.8395
Epoch 7/40
8123/8123 [==============================] - 1s - loss: 0.4006 - acc: 0.8337 - val_loss: 0.3989 - val_acc: 0.8355
Epoch 8/40
8123/8123 [==============================] - 1s - loss: 0.4017 - acc: 0.8300 - val_loss: 0.4031 - val_acc: 0.8306
Epoch 9/40
8123/8123 [==============================] - 1s - loss: 0.4056 - acc: 0.8284 - val_loss: 0.3973 - val_acc: 0.8365
Epoch 10/40
8123/8123 [==============================] - 1s - loss: 0.3993 - acc: 0.8305 - val_loss: 0.3984 - val_acc: 0.8370
Epoch 11/40
8123/8123 [==============================] - 1s - loss: 0.4001 - acc: 0.8316 - val_loss: 0.3994 - val_acc: 0.8385
Epoch 12/40
8123/8123 [==============================] - 1s - loss: 0.4000 - acc: 0.8326 - val_loss: 0.3972 - val_acc: 0.8380
Epoch 13/40
8123/8123 [==============================] - 1s - loss: 0.3960 - acc: 0.8347 - val_loss: 0.4022 - val_acc: 0.8321
Epoch 14/40
8123/8123 [==============================] - 1s - loss: 0.3995 - acc: 0.8341 - val_loss: 0.4002 - val_acc: 0.8336
Epoch 15/40
8123/8123 [==============================] - 1s - loss: 0.3975 - acc: 0.8359 - val_loss: 0.3959 - val_acc: 0.8400
Epoch 16/40
8123/8123 [==============================] - 1s - loss: 0.3947 - acc: 0.8368 - val_loss: 0.3974 - val_acc: 0.8390
Epoch 17/40
8123/8123 [==============================] - 1s - loss: 0.3966 - acc: 0.8359 - val_loss: 0.3939 - val_acc: 0.8424
Epoch 18/40
8123/8123 [==============================] - 1s - loss: 0.3934 - acc: 0.8363 - val_loss: 0.3958 - val_acc: 0.8395
Epoch 19/40
8123/8123 [==============================] - 1s - loss: 0.3953 - acc: 0.8348 - val_loss: 0.3955 - val_acc: 0.8395
Epoch 20/40
8123/8123 [==============================] - 1s - loss: 0.3942 - acc: 0.8360 - val_loss: 0.3943 - val_acc: 0.8419
Epoch 21/40
8123/8123 [==============================] - 1s - loss: 0.3918 - acc: 0.8384 - val_loss: 0.3971 - val_acc: 0.8400
Epoch 22/40
8123/8123 [==============================] - 1s - loss: 0.3934 - acc: 0.8365 - val_loss: 0.3956 - val_acc: 0.8400
Epoch 23/40
8123/8123 [==============================] - 1s - loss: 0.3911 - acc: 0.8377 - val_loss: 0.3975 - val_acc: 0.8351
Epoch 24/40
8123/8123 [==============================] - 1s - loss: 0.3922 - acc: 0.8375 - val_loss: 0.3959 - val_acc: 0.8380
Epoch 25/40
8123/8123 [==============================] - 1s - loss: 0.3908 - acc: 0.8374 - val_loss: 0.3949 - val_acc: 0.8380
Epoch 26/40
8123/8123 [==============================] - 1s - loss: 0.3899 - acc: 0.8381 - val_loss: 0.3954 - val_acc: 0.8385
Epoch 27/40
8123/8123 [==============================] - 1s - loss: 0.3905 - acc: 0.8377 - val_loss: 0.3934 - val_acc: 0.8429
Epoch 28/40
8123/8123 [==============================] - 1s - loss: 0.3890 - acc: 0.8387 - val_loss: 0.3941 - val_acc: 0.8415
Epoch 29/40
8123/8123 [==============================] - 1s - loss: 0.3895 - acc: 0.8384 - val_loss: 0.3930 - val_acc: 0.8434
Epoch 30/40
8123/8123 [==============================] - 1s - loss: 0.3879 - acc: 0.8390 - val_loss: 0.3940 - val_acc: 0.8415
Epoch 31/40
8123/8123 [==============================] - 1s - loss: 0.3888 - acc: 0.8373 - val_loss: 0.3931 - val_acc: 0.8434
Epoch 32/40
8123/8123 [==============================] - 1s - loss: 0.3873 - acc: 0.8391 - val_loss: 0.3936 - val_acc: 0.8415
Epoch 33/40
8123/8123 [==============================] - 1s - loss: 0.3876 - acc: 0.8395 - val_loss: 0.3928 - val_acc: 0.8449
Epoch 34/40
8123/8123 [==============================] - 1s - loss: 0.3866 - acc: 0.8405 - val_loss: 0.3937 - val_acc: 0.8419
Epoch 35/40
8123/8123 [==============================] - 1s - loss: 0.3868 - acc: 0.8391 - val_loss: 0.3932 - val_acc: 0.8429
Epoch 36/40
8123/8123 [==============================] - 1s - loss: 0.3860 - acc: 0.8403 - val_loss: 0.3934 - val_acc: 0.8439
Epoch 37/40
8123/8123 [==============================] - 1s - loss: 0.3859 - acc: 0.8401 - val_loss: 0.3933 - val_acc: 0.8434
Epoch 38/40
8123/8123 [==============================] - 1s - loss: 0.3853 - acc: 0.8405 - val_loss: 0.3936 - val_acc: 0.8415
Epoch 39/40
8123/8123 [==============================] - 1s - loss: 0.3853 - acc: 0.8402 - val_loss: 0.3932 - val_acc: 0.8434
Epoch 40/40
8123/8123 [==============================] - 1s - loss: 0.3847 - acc: 0.8409 - val_loss: 0.3930 - val_acc: 0.8439
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-21.h5
chunk number 21
prepare data
start training
Train on 8140 samples, validate on 2036 samples
Epoch 1/40
8140/8140 [==============================] - 2s - loss: 0.4557 - acc: 0.8106 - val_loss: 0.4217 - val_acc: 0.8242
Epoch 2/40
8140/8140 [==============================] - 2s - loss: 0.4308 - acc: 0.8156 - val_loss: 0.4046 - val_acc: 0.8310
Epoch 3/40
8140/8140 [==============================] - 2s - loss: 0.4113 - acc: 0.8237 - val_loss: 0.4132 - val_acc: 0.8281
Epoch 4/40
8140/8140 [==============================] - 2s - loss: 0.4156 - acc: 0.8221 - val_loss: 0.4138 - val_acc: 0.8266
Epoch 5/40
8140/8140 [==============================] - 2s - loss: 0.4114 - acc: 0.8264 - val_loss: 0.4152 - val_acc: 0.8315
Epoch 6/40
8140/8140 [==============================] - 2s - loss: 0.4100 - acc: 0.8275 - val_loss: 0.4091 - val_acc: 0.8360
Epoch 7/40
8140/8140 [==============================] - 2s - loss: 0.4036 - acc: 0.8290 - val_loss: 0.4042 - val_acc: 0.8335
Epoch 8/40
8140/8140 [==============================] - 2s - loss: 0.3998 - acc: 0.8297 - val_loss: 0.4014 - val_acc: 0.8350
Epoch 9/40
8140/8140 [==============================] - 2s - loss: 0.3975 - acc: 0.8313 - val_loss: 0.4011 - val_acc: 0.8394
Epoch 10/40
8140/8140 [==============================] - 2s - loss: 0.3978 - acc: 0.8314 - val_loss: 0.4017 - val_acc: 0.8345
Epoch 11/40
8140/8140 [==============================] - 2s - loss: 0.4000 - acc: 0.8296 - val_loss: 0.3975 - val_acc: 0.8394
Epoch 12/40
8140/8140 [==============================] - 2s - loss: 0.3989 - acc: 0.8310 - val_loss: 0.3966 - val_acc: 0.8389
Epoch 13/40
8140/8140 [==============================] - 2s - loss: 0.3991 - acc: 0.8326 - val_loss: 0.3939 - val_acc: 0.8389
Epoch 14/40
8140/8140 [==============================] - 2s - loss: 0.3952 - acc: 0.8339 - val_loss: 0.3960 - val_acc: 0.8394
Epoch 15/40
8140/8140 [==============================] - 2s - loss: 0.3967 - acc: 0.8286 - val_loss: 0.3935 - val_acc: 0.8369
Epoch 16/40
8140/8140 [==============================] - 2s - loss: 0.3946 - acc: 0.8322 - val_loss: 0.3945 - val_acc: 0.8364
Epoch 17/40
8140/8140 [==============================] - 2s - loss: 0.3956 - acc: 0.8330 - val_loss: 0.3944 - val_acc: 0.8345
Epoch 18/40
8140/8140 [==============================] - 2s - loss: 0.3949 - acc: 0.8321 - val_loss: 0.3947 - val_acc: 0.8340
Epoch 19/40
8140/8140 [==============================] - 2s - loss: 0.3939 - acc: 0.8317 - val_loss: 0.3946 - val_acc: 0.8355
Epoch 20/40
8140/8140 [==============================] - 2s - loss: 0.3934 - acc: 0.8323 - val_loss: 0.3935 - val_acc: 0.8394
Epoch 21/40
8140/8140 [==============================] - 2s - loss: 0.3915 - acc: 0.8353 - val_loss: 0.3934 - val_acc: 0.8394
Epoch 22/40
8140/8140 [==============================] - 2s - loss: 0.3910 - acc: 0.8361 - val_loss: 0.3921 - val_acc: 0.8414
Epoch 23/40
8140/8140 [==============================] - 2s - loss: 0.3900 - acc: 0.8340 - val_loss: 0.3917 - val_acc: 0.8399
Epoch 24/40
8140/8140 [==============================] - 2s - loss: 0.3895 - acc: 0.8349 - val_loss: 0.3908 - val_acc: 0.8418
Epoch 25/40
8140/8140 [==============================] - 2s - loss: 0.3887 - acc: 0.8361 - val_loss: 0.3906 - val_acc: 0.8418
Epoch 26/40
8140/8140 [==============================] - 2s - loss: 0.3884 - acc: 0.8351 - val_loss: 0.3908 - val_acc: 0.8404
Epoch 27/40
8140/8140 [==============================] - 2s - loss: 0.3883 - acc: 0.8367 - val_loss: 0.3905 - val_acc: 0.8379
Epoch 28/40
8140/8140 [==============================] - 2s - loss: 0.3871 - acc: 0.8365 - val_loss: 0.3912 - val_acc: 0.8399
Epoch 29/40
8140/8140 [==============================] - 2s - loss: 0.3869 - acc: 0.8340 - val_loss: 0.3907 - val_acc: 0.8394
Epoch 30/40
8140/8140 [==============================] - 2s - loss: 0.3856 - acc: 0.8350 - val_loss: 0.3915 - val_acc: 0.8418
Epoch 31/40
8140/8140 [==============================] - 2s - loss: 0.3859 - acc: 0.8372 - val_loss: 0.3918 - val_acc: 0.8399
Epoch 32/40
8140/8140 [==============================] - 2s - loss: 0.3854 - acc: 0.8355 - val_loss: 0.3923 - val_acc: 0.8399
Epoch 33/40
8140/8140 [==============================] - 2s - loss: 0.3853 - acc: 0.8349 - val_loss: 0.3921 - val_acc: 0.8418
Epoch 34/40
8140/8140 [==============================] - 2s - loss: 0.3845 - acc: 0.8361 - val_loss: 0.3918 - val_acc: 0.8423
Epoch 35/40
8140/8140 [==============================] - 2s - loss: 0.3836 - acc: 0.8362 - val_loss: 0.3918 - val_acc: 0.8404
Epoch 36/40
8140/8140 [==============================] - 2s - loss: 0.3833 - acc: 0.8360 - val_loss: 0.3914 - val_acc: 0.8414
Epoch 37/40
8140/8140 [==============================] - 2s - loss: 0.3827 - acc: 0.8361 - val_loss: 0.3914 - val_acc: 0.8428
Epoch 38/40
8140/8140 [==============================] - 2s - loss: 0.3827 - acc: 0.8367 - val_loss: 0.3916 - val_acc: 0.8399
Epoch 39/40
8140/8140 [==============================] - 2s - loss: 0.3825 - acc: 0.8366 - val_loss: 0.3910 - val_acc: 0.8428
Epoch 40/40
8140/8140 [==============================] - 2s - loss: 0.3818 - acc: 0.8361 - val_loss: 0.3908 - val_acc: 0.8418
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-22.h5
chunk number 22
prepare data
start training
Train on 8040 samples, validate on 2010 samples
Epoch 1/40
8040/8040 [==============================] - 1s - loss: 0.3960 - acc: 0.8393 - val_loss: 0.4137 - val_acc: 0.8214
Epoch 2/40
8040/8040 [==============================] - 1s - loss: 0.3937 - acc: 0.8354 - val_loss: 0.4160 - val_acc: 0.8204
Epoch 3/40
8040/8040 [==============================] - 1s - loss: 0.3945 - acc: 0.8350 - val_loss: 0.4118 - val_acc: 0.8234
Epoch 4/40
8040/8040 [==============================] - 1s - loss: 0.3913 - acc: 0.8361 - val_loss: 0.4137 - val_acc: 0.8169
Epoch 5/40
8040/8040 [==============================] - 1s - loss: 0.3934 - acc: 0.8358 - val_loss: 0.4111 - val_acc: 0.8224
Epoch 6/40
8040/8040 [==============================] - 1s - loss: 0.3901 - acc: 0.8368 - val_loss: 0.4137 - val_acc: 0.8209
Epoch 7/40
8040/8040 [==============================] - 1s - loss: 0.3908 - acc: 0.8368 - val_loss: 0.4117 - val_acc: 0.8229
Epoch 8/40
8040/8040 [==============================] - 1s - loss: 0.3890 - acc: 0.8374 - val_loss: 0.4109 - val_acc: 0.8219
Epoch 9/40
8040/8040 [==============================] - 1s - loss: 0.3887 - acc: 0.8374 - val_loss: 0.4113 - val_acc: 0.8204
Epoch 10/40
8040/8040 [==============================] - 1s - loss: 0.3882 - acc: 0.8378 - val_loss: 0.4118 - val_acc: 0.8244
Epoch 11/40
8040/8040 [==============================] - 1s - loss: 0.3874 - acc: 0.8388 - val_loss: 0.4130 - val_acc: 0.8254
Epoch 12/40
8040/8040 [==============================] - 1s - loss: 0.3873 - acc: 0.8394 - val_loss: 0.4114 - val_acc: 0.8234
Epoch 13/40
8040/8040 [==============================] - 1s - loss: 0.3857 - acc: 0.8403 - val_loss: 0.4119 - val_acc: 0.8239
Epoch 14/40
8040/8040 [==============================] - 1s - loss: 0.3857 - acc: 0.8376 - val_loss: 0.4120 - val_acc: 0.8239
Epoch 15/40
8040/8040 [==============================] - 1s - loss: 0.3843 - acc: 0.8389 - val_loss: 0.4135 - val_acc: 0.8239
Epoch 16/40
8040/8040 [==============================] - 1s - loss: 0.3847 - acc: 0.8392 - val_loss: 0.4124 - val_acc: 0.8224
Epoch 17/40
8040/8040 [==============================] - 1s - loss: 0.3835 - acc: 0.8403 - val_loss: 0.4133 - val_acc: 0.8254
Epoch 18/40
8040/8040 [==============================] - 1s - loss: 0.3837 - acc: 0.8400 - val_loss: 0.4129 - val_acc: 0.8229
Epoch 19/40
8040/8040 [==============================] - 1s - loss: 0.3824 - acc: 0.8419 - val_loss: 0.4142 - val_acc: 0.8234
Epoch 20/40
8040/8040 [==============================] - 1s - loss: 0.3825 - acc: 0.8405 - val_loss: 0.4135 - val_acc: 0.8239
Epoch 21/40
8040/8040 [==============================] - 1s - loss: 0.3816 - acc: 0.8425 - val_loss: 0.4142 - val_acc: 0.8254
Epoch 22/40
8040/8040 [==============================] - 1s - loss: 0.3817 - acc: 0.8422 - val_loss: 0.4143 - val_acc: 0.8259
Epoch 23/40
8040/8040 [==============================] - 1s - loss: 0.3807 - acc: 0.8437 - val_loss: 0.4150 - val_acc: 0.8254
Epoch 24/40
8040/8040 [==============================] - 1s - loss: 0.3804 - acc: 0.8435 - val_loss: 0.4143 - val_acc: 0.8239
Epoch 25/40
8040/8040 [==============================] - 1s - loss: 0.3798 - acc: 0.8432 - val_loss: 0.4144 - val_acc: 0.8224
Epoch 26/40
8040/8040 [==============================] - 1s - loss: 0.3795 - acc: 0.8435 - val_loss: 0.4150 - val_acc: 0.8249
Epoch 27/40
8040/8040 [==============================] - 1s - loss: 0.3792 - acc: 0.8437 - val_loss: 0.4146 - val_acc: 0.8259
Epoch 28/40
8040/8040 [==============================] - 1s - loss: 0.3786 - acc: 0.8439 - val_loss: 0.4141 - val_acc: 0.8249
Epoch 29/40
8040/8040 [==============================] - 1s - loss: 0.3783 - acc: 0.8450 - val_loss: 0.4140 - val_acc: 0.8254
Epoch 30/40
8040/8040 [==============================] - 1s - loss: 0.3776 - acc: 0.8447 - val_loss: 0.4147 - val_acc: 0.8249
Epoch 31/40
8040/8040 [==============================] - 1s - loss: 0.3775 - acc: 0.8450 - val_loss: 0.4140 - val_acc: 0.8249
Epoch 32/40
8040/8040 [==============================] - 1s - loss: 0.3769 - acc: 0.8450 - val_loss: 0.4141 - val_acc: 0.8234
Epoch 33/40
8040/8040 [==============================] - 1s - loss: 0.3766 - acc: 0.8450 - val_loss: 0.4147 - val_acc: 0.8239
Epoch 34/40
8040/8040 [==============================] - 1s - loss: 0.3762 - acc: 0.8456 - val_loss: 0.4145 - val_acc: 0.8239
Epoch 35/40
8040/8040 [==============================] - 1s - loss: 0.3757 - acc: 0.8451 - val_loss: 0.4145 - val_acc: 0.8234
Epoch 36/40
8040/8040 [==============================] - 1s - loss: 0.3755 - acc: 0.8459 - val_loss: 0.4149 - val_acc: 0.8244
Epoch 37/40
8040/8040 [==============================] - 1s - loss: 0.3751 - acc: 0.8449 - val_loss: 0.4152 - val_acc: 0.8239
Epoch 38/40
8040/8040 [==============================] - 1s - loss: 0.3748 - acc: 0.8453 - val_loss: 0.4151 - val_acc: 0.8224
Epoch 39/40
8040/8040 [==============================] - 1s - loss: 0.3745 - acc: 0.8451 - val_loss: 0.4154 - val_acc: 0.8224
Epoch 40/40
8040/8040 [==============================] - 1s - loss: 0.3740 - acc: 0.8454 - val_loss: 0.4157 - val_acc: 0.8234
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-23.h5
chunk number 23
prepare data
start training
Train on 8117 samples, validate on 2030 samples
Epoch 1/40
8117/8117 [==============================] - 1s - loss: 0.3927 - acc: 0.8379 - val_loss: 0.3872 - val_acc: 0.8399
Epoch 2/40
8117/8117 [==============================] - 1s - loss: 0.3921 - acc: 0.8390 - val_loss: 0.3841 - val_acc: 0.8453
Epoch 3/40
8117/8117 [==============================] - 1s - loss: 0.3893 - acc: 0.8424 - val_loss: 0.3836 - val_acc: 0.8399
Epoch 4/40
8117/8117 [==============================] - 1s - loss: 0.3895 - acc: 0.8398 - val_loss: 0.3825 - val_acc: 0.8384
Epoch 5/40
8117/8117 [==============================] - 1s - loss: 0.3870 - acc: 0.8438 - val_loss: 0.3854 - val_acc: 0.8409
Epoch 6/40
8117/8117 [==============================] - 1s - loss: 0.3877 - acc: 0.8433 - val_loss: 0.3828 - val_acc: 0.8419
Epoch 7/40
8117/8117 [==============================] - 1s - loss: 0.3851 - acc: 0.8456 - val_loss: 0.3827 - val_acc: 0.8414
Epoch 8/40
8117/8117 [==============================] - 1s - loss: 0.3844 - acc: 0.8464 - val_loss: 0.3856 - val_acc: 0.8399
Epoch 9/40
8117/8117 [==============================] - 1s - loss: 0.3845 - acc: 0.8450 - val_loss: 0.3833 - val_acc: 0.8438
Epoch 10/40
8117/8117 [==============================] - 1s - loss: 0.3832 - acc: 0.8466 - val_loss: 0.3832 - val_acc: 0.8424
Epoch 11/40
8117/8117 [==============================] - 1s - loss: 0.3835 - acc: 0.8466 - val_loss: 0.3848 - val_acc: 0.8414
Epoch 12/40
8117/8117 [==============================] - 1s - loss: 0.3827 - acc: 0.8465 - val_loss: 0.3833 - val_acc: 0.8433
Epoch 13/40
8117/8117 [==============================] - 1s - loss: 0.3814 - acc: 0.8476 - val_loss: 0.3829 - val_acc: 0.8443
Epoch 14/40
8117/8117 [==============================] - 1s - loss: 0.3814 - acc: 0.8476 - val_loss: 0.3840 - val_acc: 0.8399
Epoch 15/40
8117/8117 [==============================] - 1s - loss: 0.3809 - acc: 0.8472 - val_loss: 0.3826 - val_acc: 0.8443
Epoch 16/40
8117/8117 [==============================] - 1s - loss: 0.3801 - acc: 0.8479 - val_loss: 0.3822 - val_acc: 0.8438
Epoch 17/40
8117/8117 [==============================] - 1s - loss: 0.3798 - acc: 0.8472 - val_loss: 0.3826 - val_acc: 0.8433
Epoch 18/40
8117/8117 [==============================] - 1s - loss: 0.3794 - acc: 0.8485 - val_loss: 0.3814 - val_acc: 0.8424
Epoch 19/40
8117/8117 [==============================] - 1s - loss: 0.3785 - acc: 0.8488 - val_loss: 0.3812 - val_acc: 0.8433
Epoch 20/40
8117/8117 [==============================] - 1s - loss: 0.3780 - acc: 0.8483 - val_loss: 0.3820 - val_acc: 0.8424
Epoch 21/40
8117/8117 [==============================] - 1s - loss: 0.3777 - acc: 0.8486 - val_loss: 0.3814 - val_acc: 0.8438
Epoch 22/40
8117/8117 [==============================] - 1s - loss: 0.3771 - acc: 0.8487 - val_loss: 0.3817 - val_acc: 0.8433
Epoch 23/40
8117/8117 [==============================] - 1s - loss: 0.3764 - acc: 0.8490 - val_loss: 0.3822 - val_acc: 0.8424
Epoch 24/40
8117/8117 [==============================] - 1s - loss: 0.3761 - acc: 0.8497 - val_loss: 0.3814 - val_acc: 0.8433
Epoch 25/40
8117/8117 [==============================] - 1s - loss: 0.3758 - acc: 0.8488 - val_loss: 0.3816 - val_acc: 0.8424
Epoch 26/40
8117/8117 [==============================] - 1s - loss: 0.3752 - acc: 0.8496 - val_loss: 0.3815 - val_acc: 0.8424
Epoch 27/40
8117/8117 [==============================] - 1s - loss: 0.3748 - acc: 0.8488 - val_loss: 0.3810 - val_acc: 0.8424
Epoch 28/40
8117/8117 [==============================] - 1s - loss: 0.3747 - acc: 0.8486 - val_loss: 0.3816 - val_acc: 0.8443
Epoch 29/40
8117/8117 [==============================] - 1s - loss: 0.3743 - acc: 0.8483 - val_loss: 0.3805 - val_acc: 0.8429
Epoch 30/40
8117/8117 [==============================] - 1s - loss: 0.3739 - acc: 0.8492 - val_loss: 0.3808 - val_acc: 0.8433
Epoch 31/40
8117/8117 [==============================] - 1s - loss: 0.3733 - acc: 0.8498 - val_loss: 0.3805 - val_acc: 0.8424
Epoch 32/40
8117/8117 [==============================] - 1s - loss: 0.3728 - acc: 0.8507 - val_loss: 0.3808 - val_acc: 0.8419
Epoch 33/40
8117/8117 [==============================] - 1s - loss: 0.3724 - acc: 0.8508 - val_loss: 0.3809 - val_acc: 0.8419
Epoch 34/40
8117/8117 [==============================] - 1s - loss: 0.3720 - acc: 0.8513 - val_loss: 0.3807 - val_acc: 0.8429
Epoch 35/40
8117/8117 [==============================] - 1s - loss: 0.3717 - acc: 0.8520 - val_loss: 0.3813 - val_acc: 0.8443
Epoch 36/40
8117/8117 [==============================] - 1s - loss: 0.3716 - acc: 0.8513 - val_loss: 0.3815 - val_acc: 0.8433
Epoch 37/40
8117/8117 [==============================] - 1s - loss: 0.3725 - acc: 0.8498 - val_loss: 0.3861 - val_acc: 0.8414
Epoch 38/40
8117/8117 [==============================] - 1s - loss: 0.3763 - acc: 0.8465 - val_loss: 0.3879 - val_acc: 0.8438
Epoch 39/40
8117/8117 [==============================] - 1s - loss: 0.3831 - acc: 0.8448 - val_loss: 0.3821 - val_acc: 0.8453
Epoch 40/40
8117/8117 [==============================] - 1s - loss: 0.3718 - acc: 0.8501 - val_loss: 0.3966 - val_acc: 0.8379
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-24.h5
chunk number 24
prepare data
start training
Train on 8140 samples, validate on 2035 samples
Epoch 1/40
8140/8140 [==============================] - 1s - loss: 0.4712 - acc: 0.8007 - val_loss: 0.4517 - val_acc: 0.8108
Epoch 2/40
8140/8140 [==============================] - 1s - loss: 0.4642 - acc: 0.8026 - val_loss: 0.4956 - val_acc: 0.7813
Epoch 3/40
8140/8140 [==============================] - 1s - loss: 0.5074 - acc: 0.7767 - val_loss: 0.4769 - val_acc: 0.7803
Epoch 4/40
8140/8140 [==============================] - 1s - loss: 0.4871 - acc: 0.7740 - val_loss: 0.4642 - val_acc: 0.7926
Epoch 5/40
8140/8140 [==============================] - 1s - loss: 0.4748 - acc: 0.7873 - val_loss: 0.4538 - val_acc: 0.8049
Epoch 6/40
8140/8140 [==============================] - 1s - loss: 0.4646 - acc: 0.8037 - val_loss: 0.4498 - val_acc: 0.8167
Epoch 7/40
8140/8140 [==============================] - 1s - loss: 0.4607 - acc: 0.8188 - val_loss: 0.4532 - val_acc: 0.8216
Epoch 8/40
8140/8140 [==============================] - 1s - loss: 0.4659 - acc: 0.8173 - val_loss: 0.4349 - val_acc: 0.8246
Epoch 9/40
8140/8140 [==============================] - 1s - loss: 0.4483 - acc: 0.8240 - val_loss: 0.4100 - val_acc: 0.8256
Epoch 10/40
8140/8140 [==============================] - 1s - loss: 0.4236 - acc: 0.8237 - val_loss: 0.4102 - val_acc: 0.8201
Epoch 11/40
8140/8140 [==============================] - 1s - loss: 0.4252 - acc: 0.8179 - val_loss: 0.4126 - val_acc: 0.8246
Epoch 12/40
8140/8140 [==============================] - 1s - loss: 0.4293 - acc: 0.8173 - val_loss: 0.4022 - val_acc: 0.8310
Epoch 13/40
8140/8140 [==============================] - 1s - loss: 0.4210 - acc: 0.8227 - val_loss: 0.3979 - val_acc: 0.8344
Epoch 14/40
8140/8140 [==============================] - 1s - loss: 0.4174 - acc: 0.8225 - val_loss: 0.3997 - val_acc: 0.8265
Epoch 15/40
8140/8140 [==============================] - 1s - loss: 0.4189 - acc: 0.8222 - val_loss: 0.3922 - val_acc: 0.8329
Epoch 16/40
8140/8140 [==============================] - 1s - loss: 0.4068 - acc: 0.8276 - val_loss: 0.4035 - val_acc: 0.8256
Epoch 17/40
8140/8140 [==============================] - 1s - loss: 0.4144 - acc: 0.8270 - val_loss: 0.4075 - val_acc: 0.8231
Epoch 18/40
8140/8140 [==============================] - 1s - loss: 0.4169 - acc: 0.8258 - val_loss: 0.4020 - val_acc: 0.8241
Epoch 19/40
8140/8140 [==============================] - 1s - loss: 0.4127 - acc: 0.8275 - val_loss: 0.3944 - val_acc: 0.8310
Epoch 20/40
8140/8140 [==============================] - 1s - loss: 0.4069 - acc: 0.8274 - val_loss: 0.3884 - val_acc: 0.8334
Epoch 21/40
8140/8140 [==============================] - 1s - loss: 0.4021 - acc: 0.8274 - val_loss: 0.3918 - val_acc: 0.8354
Epoch 22/40
8140/8140 [==============================] - 1s - loss: 0.4059 - acc: 0.8303 - val_loss: 0.3938 - val_acc: 0.8383
Epoch 23/40
8140/8140 [==============================] - 1s - loss: 0.4080 - acc: 0.8280 - val_loss: 0.3911 - val_acc: 0.8369
Epoch 24/40
8140/8140 [==============================] - 1s - loss: 0.4040 - acc: 0.8301 - val_loss: 0.3881 - val_acc: 0.8344
Epoch 25/40
8140/8140 [==============================] - 1s - loss: 0.4000 - acc: 0.8326 - val_loss: 0.3904 - val_acc: 0.8310
Epoch 26/40
8140/8140 [==============================] - 1s - loss: 0.4009 - acc: 0.8338 - val_loss: 0.3938 - val_acc: 0.8290
Epoch 27/40
8140/8140 [==============================] - 1s - loss: 0.4031 - acc: 0.8281 - val_loss: 0.3937 - val_acc: 0.8280
Epoch 28/40
8140/8140 [==============================] - 1s - loss: 0.4023 - acc: 0.8283 - val_loss: 0.3889 - val_acc: 0.8339
Epoch 29/40
8140/8140 [==============================] - 1s - loss: 0.3985 - acc: 0.8319 - val_loss: 0.3852 - val_acc: 0.8364
Epoch 30/40
8140/8140 [==============================] - 1s - loss: 0.3977 - acc: 0.8333 - val_loss: 0.3854 - val_acc: 0.8403
Epoch 31/40
8140/8140 [==============================] - 1s - loss: 0.3993 - acc: 0.8312 - val_loss: 0.3852 - val_acc: 0.8393
Epoch 32/40
8140/8140 [==============================] - 1s - loss: 0.3984 - acc: 0.8339 - val_loss: 0.3860 - val_acc: 0.8373
Epoch 33/40
8140/8140 [==============================] - 1s - loss: 0.3979 - acc: 0.8337 - val_loss: 0.3841 - val_acc: 0.8364
Epoch 34/40
8140/8140 [==============================] - 1s - loss: 0.3956 - acc: 0.8343 - val_loss: 0.3850 - val_acc: 0.8344
Epoch 35/40
8140/8140 [==============================] - 1s - loss: 0.3965 - acc: 0.8330 - val_loss: 0.3856 - val_acc: 0.8339
Epoch 36/40
8140/8140 [==============================] - 1s - loss: 0.3958 - acc: 0.8333 - val_loss: 0.3863 - val_acc: 0.8369
Epoch 37/40
8140/8140 [==============================] - 1s - loss: 0.3954 - acc: 0.8337 - val_loss: 0.3848 - val_acc: 0.8383
Epoch 38/40
8140/8140 [==============================] - 1s - loss: 0.3942 - acc: 0.8340 - val_loss: 0.3831 - val_acc: 0.8359
Epoch 39/40
8140/8140 [==============================] - 1s - loss: 0.3937 - acc: 0.8338 - val_loss: 0.3835 - val_acc: 0.8344
Epoch 40/40
8140/8140 [==============================] - 1s - loss: 0.3942 - acc: 0.8332 - val_loss: 0.3840 - val_acc: 0.8388
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-25.h5
chunk number 25
prepare data
start training
Train on 8058 samples, validate on 2015 samples
Epoch 1/40
8058/8058 [==============================] - 1s - loss: 0.3935 - acc: 0.8353 - val_loss: 0.3826 - val_acc: 0.8392
Epoch 2/40
8058/8058 [==============================] - 1s - loss: 0.3889 - acc: 0.8374 - val_loss: 0.3854 - val_acc: 0.8387
Epoch 3/40
8058/8058 [==============================] - 1s - loss: 0.3911 - acc: 0.8348 - val_loss: 0.3810 - val_acc: 0.8422
Epoch 4/40
8058/8058 [==============================] - 1s - loss: 0.3887 - acc: 0.8374 - val_loss: 0.3803 - val_acc: 0.8417
Epoch 5/40
8058/8058 [==============================] - 1s - loss: 0.3882 - acc: 0.8363 - val_loss: 0.3807 - val_acc: 0.8452
Epoch 6/40
8058/8058 [==============================] - 1s - loss: 0.3857 - acc: 0.8392 - val_loss: 0.3815 - val_acc: 0.8447
Epoch 7/40
8058/8058 [==============================] - 1s - loss: 0.3854 - acc: 0.8378 - val_loss: 0.3790 - val_acc: 0.8462
Epoch 8/40
8058/8058 [==============================] - 1s - loss: 0.3845 - acc: 0.8399 - val_loss: 0.3796 - val_acc: 0.8442
Epoch 9/40
8058/8058 [==============================] - 1s - loss: 0.3838 - acc: 0.8398 - val_loss: 0.3824 - val_acc: 0.8407
Epoch 10/40
8058/8058 [==============================] - 1s - loss: 0.3829 - acc: 0.8421 - val_loss: 0.3836 - val_acc: 0.8372
Epoch 11/40
8058/8058 [==============================] - 1s - loss: 0.3829 - acc: 0.8426 - val_loss: 0.3817 - val_acc: 0.8397
Epoch 12/40
8058/8058 [==============================] - 1s - loss: 0.3822 - acc: 0.8410 - val_loss: 0.3820 - val_acc: 0.8417
Epoch 13/40
8058/8058 [==============================] - 1s - loss: 0.3818 - acc: 0.8408 - val_loss: 0.3839 - val_acc: 0.8387
Epoch 14/40
8058/8058 [==============================] - 1s - loss: 0.3810 - acc: 0.8446 - val_loss: 0.3843 - val_acc: 0.8382
Epoch 15/40
8058/8058 [==============================] - 1s - loss: 0.3807 - acc: 0.8454 - val_loss: 0.3825 - val_acc: 0.8427
Epoch 16/40
8058/8058 [==============================] - 1s - loss: 0.3799 - acc: 0.8430 - val_loss: 0.3822 - val_acc: 0.8437
Epoch 17/40
8058/8058 [==============================] - 1s - loss: 0.3793 - acc: 0.8439 - val_loss: 0.3840 - val_acc: 0.8357
Epoch 18/40
8058/8058 [==============================] - 1s - loss: 0.3789 - acc: 0.8457 - val_loss: 0.3834 - val_acc: 0.8362
Epoch 19/40
8058/8058 [==============================] - 1s - loss: 0.3783 - acc: 0.8456 - val_loss: 0.3816 - val_acc: 0.8437
Epoch 20/40
8058/8058 [==============================] - 1s - loss: 0.3782 - acc: 0.8448 - val_loss: 0.3819 - val_acc: 0.8397
Epoch 21/40
8058/8058 [==============================] - 1s - loss: 0.3774 - acc: 0.8457 - val_loss: 0.3842 - val_acc: 0.8367
Epoch 22/40
8058/8058 [==============================] - 1s - loss: 0.3774 - acc: 0.8444 - val_loss: 0.3823 - val_acc: 0.8377
Epoch 23/40
8058/8058 [==============================] - 1s - loss: 0.3763 - acc: 0.8461 - val_loss: 0.3817 - val_acc: 0.8397
Epoch 24/40
8058/8058 [==============================] - 1s - loss: 0.3763 - acc: 0.8457 - val_loss: 0.3831 - val_acc: 0.8367
Epoch 25/40
8058/8058 [==============================] - 1s - loss: 0.3755 - acc: 0.8464 - val_loss: 0.3844 - val_acc: 0.8357
Epoch 26/40
8058/8058 [==============================] - 1s - loss: 0.3754 - acc: 0.8457 - val_loss: 0.3828 - val_acc: 0.8382
Epoch 27/40
8058/8058 [==============================] - 1s - loss: 0.3749 - acc: 0.8466 - val_loss: 0.3831 - val_acc: 0.8357
Epoch 28/40
8058/8058 [==============================] - 1s - loss: 0.3745 - acc: 0.8462 - val_loss: 0.3851 - val_acc: 0.8347
Epoch 29/40
8058/8058 [==============================] - 1s - loss: 0.3743 - acc: 0.8465 - val_loss: 0.3838 - val_acc: 0.8347
Epoch 30/40
8058/8058 [==============================] - 1s - loss: 0.3735 - acc: 0.8469 - val_loss: 0.3830 - val_acc: 0.8392
Epoch 31/40
8058/8058 [==============================] - 1s - loss: 0.3734 - acc: 0.8457 - val_loss: 0.3841 - val_acc: 0.8342
Epoch 32/40
8058/8058 [==============================] - 1s - loss: 0.3728 - acc: 0.8470 - val_loss: 0.3842 - val_acc: 0.8367
Epoch 33/40
8058/8058 [==============================] - 1s - loss: 0.3725 - acc: 0.8472 - val_loss: 0.3829 - val_acc: 0.8372
Epoch 34/40
8058/8058 [==============================] - 1s - loss: 0.3722 - acc: 0.8469 - val_loss: 0.3836 - val_acc: 0.8372
Epoch 35/40
8058/8058 [==============================] - 1s - loss: 0.3717 - acc: 0.8481 - val_loss: 0.3848 - val_acc: 0.8367
Epoch 36/40
8058/8058 [==============================] - 1s - loss: 0.3715 - acc: 0.8481 - val_loss: 0.3837 - val_acc: 0.8382
Epoch 37/40
8058/8058 [==============================] - 1s - loss: 0.3711 - acc: 0.8481 - val_loss: 0.3842 - val_acc: 0.8372
Epoch 38/40
8058/8058 [==============================] - 1s - loss: 0.3707 - acc: 0.8481 - val_loss: 0.3854 - val_acc: 0.8357
Epoch 39/40
8058/8058 [==============================] - 1s - loss: 0.3705 - acc: 0.8482 - val_loss: 0.3841 - val_acc: 0.8382
Epoch 40/40
8058/8058 [==============================] - 1s - loss: 0.3700 - acc: 0.8483 - val_loss: 0.3840 - val_acc: 0.8372
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-26.h5
chunk number 26
prepare data
start training
Train on 8100 samples, validate on 2026 samples
Epoch 1/40
8100/8100 [==============================] - 1s - loss: 0.3999 - acc: 0.8381 - val_loss: 0.4162 - val_acc: 0.8164
Epoch 2/40
8100/8100 [==============================] - 1s - loss: 0.3996 - acc: 0.8325 - val_loss: 0.4113 - val_acc: 0.8243
Epoch 3/40
8100/8100 [==============================] - 1s - loss: 0.3947 - acc: 0.8380 - val_loss: 0.4130 - val_acc: 0.8258
Epoch 4/40
8100/8100 [==============================] - 1s - loss: 0.3952 - acc: 0.8383 - val_loss: 0.4107 - val_acc: 0.8228
Epoch 5/40
8100/8100 [==============================] - 1s - loss: 0.3937 - acc: 0.8384 - val_loss: 0.4116 - val_acc: 0.8253
Epoch 6/40
8100/8100 [==============================] - 1s - loss: 0.3941 - acc: 0.8396 - val_loss: 0.4119 - val_acc: 0.8263
Epoch 7/40
8100/8100 [==============================] - 1s - loss: 0.3928 - acc: 0.8398 - val_loss: 0.4107 - val_acc: 0.8282
Epoch 8/40
8100/8100 [==============================] - 1s - loss: 0.3912 - acc: 0.8410 - val_loss: 0.4100 - val_acc: 0.8268
Epoch 9/40
8100/8100 [==============================] - 1s - loss: 0.3905 - acc: 0.8406 - val_loss: 0.4105 - val_acc: 0.8268
Epoch 10/40
8100/8100 [==============================] - 1s - loss: 0.3894 - acc: 0.8412 - val_loss: 0.4126 - val_acc: 0.8287
Epoch 11/40
8100/8100 [==============================] - 1s - loss: 0.3897 - acc: 0.8425 - val_loss: 0.4126 - val_acc: 0.8263
Epoch 12/40
8100/8100 [==============================] - 1s - loss: 0.3885 - acc: 0.8420 - val_loss: 0.4128 - val_acc: 0.8238
Epoch 13/40
8100/8100 [==============================] - 1s - loss: 0.3883 - acc: 0.8402 - val_loss: 0.4132 - val_acc: 0.8253
Epoch 14/40
8100/8100 [==============================] - 1s - loss: 0.3871 - acc: 0.8431 - val_loss: 0.4130 - val_acc: 0.8233
Epoch 15/40
8100/8100 [==============================] - 1s - loss: 0.3864 - acc: 0.8432 - val_loss: 0.4130 - val_acc: 0.8223
Epoch 16/40
8100/8100 [==============================] - 1s - loss: 0.3864 - acc: 0.8410 - val_loss: 0.4128 - val_acc: 0.8223
Epoch 17/40
8100/8100 [==============================] - 1s - loss: 0.3854 - acc: 0.8423 - val_loss: 0.4134 - val_acc: 0.8238
Epoch 18/40
8100/8100 [==============================] - 1s - loss: 0.3851 - acc: 0.8447 - val_loss: 0.4126 - val_acc: 0.8228
Epoch 19/40
8100/8100 [==============================] - 1s - loss: 0.3843 - acc: 0.8426 - val_loss: 0.4128 - val_acc: 0.8233
Epoch 20/40
8100/8100 [==============================] - 1s - loss: 0.3837 - acc: 0.8444 - val_loss: 0.4135 - val_acc: 0.8238
Epoch 21/40
8100/8100 [==============================] - 1s - loss: 0.3835 - acc: 0.8448 - val_loss: 0.4135 - val_acc: 0.8218
Epoch 22/40
8100/8100 [==============================] - 1s - loss: 0.3829 - acc: 0.8443 - val_loss: 0.4138 - val_acc: 0.8218
Epoch 23/40
8100/8100 [==============================] - 1s - loss: 0.3827 - acc: 0.8437 - val_loss: 0.4137 - val_acc: 0.8248
Epoch 24/40
8100/8100 [==============================] - 1s - loss: 0.3822 - acc: 0.8446 - val_loss: 0.4132 - val_acc: 0.8233
Epoch 25/40
8100/8100 [==============================] - 1s - loss: 0.3815 - acc: 0.8444 - val_loss: 0.4131 - val_acc: 0.8233
Epoch 26/40
8100/8100 [==============================] - 1s - loss: 0.3813 - acc: 0.8437 - val_loss: 0.4130 - val_acc: 0.8223
Epoch 27/40
8100/8100 [==============================] - 1s - loss: 0.3810 - acc: 0.8451 - val_loss: 0.4126 - val_acc: 0.8223
Epoch 28/40
8100/8100 [==============================] - 1s - loss: 0.3804 - acc: 0.8449 - val_loss: 0.4124 - val_acc: 0.8238
Epoch 29/40
8100/8100 [==============================] - 1s - loss: 0.3800 - acc: 0.8449 - val_loss: 0.4124 - val_acc: 0.8238
Epoch 30/40
8100/8100 [==============================] - 1s - loss: 0.3796 - acc: 0.8464 - val_loss: 0.4121 - val_acc: 0.8213
Epoch 31/40
8100/8100 [==============================] - 1s - loss: 0.3790 - acc: 0.8458 - val_loss: 0.4122 - val_acc: 0.8208
Epoch 32/40
8100/8100 [==============================] - 1s - loss: 0.3788 - acc: 0.8458 - val_loss: 0.4121 - val_acc: 0.8253
Epoch 33/40
8100/8100 [==============================] - 1s - loss: 0.3785 - acc: 0.8463 - val_loss: 0.4118 - val_acc: 0.8223
Epoch 34/40
8100/8100 [==============================] - 1s - loss: 0.3778 - acc: 0.8458 - val_loss: 0.4118 - val_acc: 0.8243
Epoch 35/40
8100/8100 [==============================] - 1s - loss: 0.3775 - acc: 0.8459 - val_loss: 0.4119 - val_acc: 0.8268
Epoch 36/40
8100/8100 [==============================] - 1s - loss: 0.3773 - acc: 0.8468 - val_loss: 0.4118 - val_acc: 0.8263
Epoch 37/40
8100/8100 [==============================] - 1s - loss: 0.3768 - acc: 0.8464 - val_loss: 0.4119 - val_acc: 0.8268
Epoch 38/40
8100/8100 [==============================] - 1s - loss: 0.3763 - acc: 0.8473 - val_loss: 0.4122 - val_acc: 0.8243
Epoch 39/40
8100/8100 [==============================] - 1s - loss: 0.3760 - acc: 0.8475 - val_loss: 0.4125 - val_acc: 0.8218
Epoch 40/40
8100/8100 [==============================] - 1s - loss: 0.3755 - acc: 0.8479 - val_loss: 0.4127 - val_acc: 0.8238
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-27.h5
chunk number 27
prepare data
start training
Train on 8095 samples, validate on 2024 samples
Epoch 1/40
8095/8095 [==============================] - 1s - loss: 0.3852 - acc: 0.8429 - val_loss: 0.4054 - val_acc: 0.8266
Epoch 2/40
8095/8095 [==============================] - 1s - loss: 0.3842 - acc: 0.8442 - val_loss: 0.4044 - val_acc: 0.8305
Epoch 3/40
8095/8095 [==============================] - 1s - loss: 0.3824 - acc: 0.8455 - val_loss: 0.4057 - val_acc: 0.8286
Epoch 4/40
8095/8095 [==============================] - 1s - loss: 0.3811 - acc: 0.8473 - val_loss: 0.4073 - val_acc: 0.8266
Epoch 5/40
8095/8095 [==============================] - 1s - loss: 0.3802 - acc: 0.8485 - val_loss: 0.4073 - val_acc: 0.8251
Epoch 6/40
8095/8095 [==============================] - 1s - loss: 0.3794 - acc: 0.8494 - val_loss: 0.4075 - val_acc: 0.8256
Epoch 7/40
8095/8095 [==============================] - 1s - loss: 0.3787 - acc: 0.8502 - val_loss: 0.4087 - val_acc: 0.8241
Epoch 8/40
8095/8095 [==============================] - 1s - loss: 0.3777 - acc: 0.8498 - val_loss: 0.4083 - val_acc: 0.8221
Epoch 9/40
8095/8095 [==============================] - 1s - loss: 0.3768 - acc: 0.8494 - val_loss: 0.4076 - val_acc: 0.8226
Epoch 10/40
8095/8095 [==============================] - 1s - loss: 0.3764 - acc: 0.8492 - val_loss: 0.4082 - val_acc: 0.8231
Epoch 11/40
8095/8095 [==============================] - 1s - loss: 0.3757 - acc: 0.8493 - val_loss: 0.4077 - val_acc: 0.8216
Epoch 12/40
8095/8095 [==============================] - 1s - loss: 0.3750 - acc: 0.8502 - val_loss: 0.4074 - val_acc: 0.8241
Epoch 13/40
8095/8095 [==============================] - 1s - loss: 0.3746 - acc: 0.8508 - val_loss: 0.4079 - val_acc: 0.8211
Epoch 14/40
8095/8095 [==============================] - 1s - loss: 0.3739 - acc: 0.8498 - val_loss: 0.4072 - val_acc: 0.8221
Epoch 15/40
8095/8095 [==============================] - 1s - loss: 0.3732 - acc: 0.8504 - val_loss: 0.4064 - val_acc: 0.8221
Epoch 16/40
8095/8095 [==============================] - 1s - loss: 0.3725 - acc: 0.8511 - val_loss: 0.4062 - val_acc: 0.8226
Epoch 17/40
8095/8095 [==============================] - 1s - loss: 0.3718 - acc: 0.8505 - val_loss: 0.4051 - val_acc: 0.8226
Epoch 18/40
8095/8095 [==============================] - 1s - loss: 0.3711 - acc: 0.8521 - val_loss: 0.4050 - val_acc: 0.8236
Epoch 19/40
8095/8095 [==============================] - 1s - loss: 0.3705 - acc: 0.8527 - val_loss: 0.4053 - val_acc: 0.8211
Epoch 20/40
8095/8095 [==============================] - 1s - loss: 0.3699 - acc: 0.8526 - val_loss: 0.4050 - val_acc: 0.8241
Epoch 21/40
8095/8095 [==============================] - 1s - loss: 0.3693 - acc: 0.8526 - val_loss: 0.4058 - val_acc: 0.8221
Epoch 22/40
8095/8095 [==============================] - 1s - loss: 0.3688 - acc: 0.8529 - val_loss: 0.4048 - val_acc: 0.8231
Epoch 23/40
8095/8095 [==============================] - 1s - loss: 0.3683 - acc: 0.8525 - val_loss: 0.4057 - val_acc: 0.8221
Epoch 24/40
8095/8095 [==============================] - 1s - loss: 0.3679 - acc: 0.8526 - val_loss: 0.4048 - val_acc: 0.8241
Epoch 25/40
8095/8095 [==============================] - 1s - loss: 0.3674 - acc: 0.8527 - val_loss: 0.4064 - val_acc: 0.8226
Epoch 26/40
8095/8095 [==============================] - 1s - loss: 0.3671 - acc: 0.8530 - val_loss: 0.4054 - val_acc: 0.8256
Epoch 27/40
8095/8095 [==============================] - 1s - loss: 0.3666 - acc: 0.8529 - val_loss: 0.4067 - val_acc: 0.8211
Epoch 28/40
8095/8095 [==============================] - 1s - loss: 0.3663 - acc: 0.8541 - val_loss: 0.4055 - val_acc: 0.8261
Epoch 29/40
8095/8095 [==============================] - 1s - loss: 0.3661 - acc: 0.8530 - val_loss: 0.4076 - val_acc: 0.8211
Epoch 30/40
8095/8095 [==============================] - 1s - loss: 0.3668 - acc: 0.8539 - val_loss: 0.4054 - val_acc: 0.8261
Epoch 31/40
8095/8095 [==============================] - 1s - loss: 0.3670 - acc: 0.8535 - val_loss: 0.4095 - val_acc: 0.8221
Epoch 32/40
8095/8095 [==============================] - 1s - loss: 0.3685 - acc: 0.8518 - val_loss: 0.4053 - val_acc: 0.8295
Epoch 33/40
8095/8095 [==============================] - 1s - loss: 0.3658 - acc: 0.8540 - val_loss: 0.4066 - val_acc: 0.8226
Epoch 34/40
8095/8095 [==============================] - 1s - loss: 0.3645 - acc: 0.8546 - val_loss: 0.4049 - val_acc: 0.8271
Epoch 35/40
8095/8095 [==============================] - 1s - loss: 0.3631 - acc: 0.8553 - val_loss: 0.4054 - val_acc: 0.8231
Epoch 36/40
8095/8095 [==============================] - 1s - loss: 0.3624 - acc: 0.8557 - val_loss: 0.4056 - val_acc: 0.8266
Epoch 37/40
8095/8095 [==============================] - 1s - loss: 0.3618 - acc: 0.8562 - val_loss: 0.4064 - val_acc: 0.8256
Epoch 38/40
8095/8095 [==============================] - 1s - loss: 0.3614 - acc: 0.8558 - val_loss: 0.4074 - val_acc: 0.8226
Epoch 39/40
8095/8095 [==============================] - 1s - loss: 0.3611 - acc: 0.8557 - val_loss: 0.4075 - val_acc: 0.8251
Epoch 40/40
8095/8095 [==============================] - 1s - loss: 0.3612 - acc: 0.8557 - val_loss: 0.4098 - val_acc: 0.8221
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-28.h5
chunk number 28
prepare data
start training
Train on 8135 samples, validate on 2034 samples
Epoch 1/40
8135/8135 [==============================] - 2s - loss: 0.4136 - acc: 0.8301 - val_loss: 0.3984 - val_acc: 0.8255
Epoch 2/40
8135/8135 [==============================] - 2s - loss: 0.4074 - acc: 0.8333 - val_loss: 0.3942 - val_acc: 0.8314
Epoch 3/40
8135/8135 [==============================] - 2s - loss: 0.4052 - acc: 0.8329 - val_loss: 0.3935 - val_acc: 0.8294
Epoch 4/40
8135/8135 [==============================] - 2s - loss: 0.4030 - acc: 0.8338 - val_loss: 0.3958 - val_acc: 0.8319
Epoch 5/40
8135/8135 [==============================] - 2s - loss: 0.4016 - acc: 0.8344 - val_loss: 0.3920 - val_acc: 0.8304
Epoch 6/40
8135/8135 [==============================] - 2s - loss: 0.3970 - acc: 0.8353 - val_loss: 0.3944 - val_acc: 0.8304
Epoch 7/40
8135/8135 [==============================] - 2s - loss: 0.3964 - acc: 0.8356 - val_loss: 0.3928 - val_acc: 0.8358
Epoch 8/40
8135/8135 [==============================] - 2s - loss: 0.3962 - acc: 0.8371 - val_loss: 0.3900 - val_acc: 0.8299
Epoch 9/40
8135/8135 [==============================] - 2s - loss: 0.3935 - acc: 0.8371 - val_loss: 0.3886 - val_acc: 0.8314
Epoch 10/40
8135/8135 [==============================] - 2s - loss: 0.3932 - acc: 0.8374 - val_loss: 0.3879 - val_acc: 0.8333
Epoch 11/40
8135/8135 [==============================] - 2s - loss: 0.3932 - acc: 0.8354 - val_loss: 0.3884 - val_acc: 0.8343
Epoch 12/40
8135/8135 [==============================] - 2s - loss: 0.3918 - acc: 0.8371 - val_loss: 0.3899 - val_acc: 0.8368
Epoch 13/40
8135/8135 [==============================] - 2s - loss: 0.3917 - acc: 0.8388 - val_loss: 0.3900 - val_acc: 0.8378
Epoch 14/40
8135/8135 [==============================] - 2s - loss: 0.3920 - acc: 0.8372 - val_loss: 0.3900 - val_acc: 0.8358
Epoch 15/40
8135/8135 [==============================] - 2s - loss: 0.3908 - acc: 0.8396 - val_loss: 0.3878 - val_acc: 0.8387
Epoch 16/40
8135/8135 [==============================] - 2s - loss: 0.3893 - acc: 0.8384 - val_loss: 0.3864 - val_acc: 0.8373
Epoch 17/40
8135/8135 [==============================] - 2s - loss: 0.3882 - acc: 0.8395 - val_loss: 0.3863 - val_acc: 0.8358
Epoch 18/40
8135/8135 [==============================] - 2s - loss: 0.3876 - acc: 0.8387 - val_loss: 0.3863 - val_acc: 0.8382
Epoch 19/40
8135/8135 [==============================] - 2s - loss: 0.3871 - acc: 0.8396 - val_loss: 0.3874 - val_acc: 0.8338
Epoch 20/40
8135/8135 [==============================] - 2s - loss: 0.3864 - acc: 0.8382 - val_loss: 0.3870 - val_acc: 0.8358
Epoch 21/40
8135/8135 [==============================] - 2s - loss: 0.3858 - acc: 0.8388 - val_loss: 0.3868 - val_acc: 0.8338
Epoch 22/40
8135/8135 [==============================] - 2s - loss: 0.3854 - acc: 0.8388 - val_loss: 0.3867 - val_acc: 0.8368
Epoch 23/40
8135/8135 [==============================] - 2s - loss: 0.3846 - acc: 0.8409 - val_loss: 0.3868 - val_acc: 0.8368
Epoch 24/40
8135/8135 [==============================] - 2s - loss: 0.3844 - acc: 0.8407 - val_loss: 0.3881 - val_acc: 0.8358
Epoch 25/40
8135/8135 [==============================] - 2s - loss: 0.3855 - acc: 0.8395 - val_loss: 0.3942 - val_acc: 0.8333
Epoch 26/40
8135/8135 [==============================] - 2s - loss: 0.3905 - acc: 0.8396 - val_loss: 0.3872 - val_acc: 0.8392
Epoch 27/40
8135/8135 [==============================] - 2s - loss: 0.3838 - acc: 0.8392 - val_loss: 0.3882 - val_acc: 0.8373
Epoch 28/40
8135/8135 [==============================] - 2s - loss: 0.3842 - acc: 0.8393 - val_loss: 0.3943 - val_acc: 0.8368
Epoch 29/40
8135/8135 [==============================] - 2s - loss: 0.3891 - acc: 0.8400 - val_loss: 0.3862 - val_acc: 0.8363
Epoch 30/40
8135/8135 [==============================] - 2s - loss: 0.3816 - acc: 0.8411 - val_loss: 0.3864 - val_acc: 0.8368
Epoch 31/40
8135/8135 [==============================] - 2s - loss: 0.3823 - acc: 0.8413 - val_loss: 0.3957 - val_acc: 0.8319
Epoch 32/40
8135/8135 [==============================] - 2s - loss: 0.3912 - acc: 0.8371 - val_loss: 0.3861 - val_acc: 0.8378
Epoch 33/40
8135/8135 [==============================] - 2s - loss: 0.3815 - acc: 0.8418 - val_loss: 0.3854 - val_acc: 0.8378
Epoch 34/40
8135/8135 [==============================] - 2s - loss: 0.3795 - acc: 0.8415 - val_loss: 0.3950 - val_acc: 0.8343
Epoch 35/40
8135/8135 [==============================] - 2s - loss: 0.3882 - acc: 0.8382 - val_loss: 0.3915 - val_acc: 0.8358
Epoch 36/40
8135/8135 [==============================] - 2s - loss: 0.3845 - acc: 0.8398 - val_loss: 0.3913 - val_acc: 0.8343
Epoch 37/40
8135/8135 [==============================] - 2s - loss: 0.3840 - acc: 0.8414 - val_loss: 0.3855 - val_acc: 0.8402
Epoch 38/40
8135/8135 [==============================] - 2s - loss: 0.3802 - acc: 0.8414 - val_loss: 0.3856 - val_acc: 0.8397
Epoch 39/40
8135/8135 [==============================] - 2s - loss: 0.3776 - acc: 0.8419 - val_loss: 0.3865 - val_acc: 0.8373
Epoch 40/40
8135/8135 [==============================] - 2s - loss: 0.3778 - acc: 0.8427 - val_loss: 0.3867 - val_acc: 0.8353
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-29.h5
chunk number 29
prepare data
start training
Train on 8105 samples, validate on 2027 samples
Epoch 1/40
8105/8105 [==============================] - 1s - loss: 0.4239 - acc: 0.8253 - val_loss: 0.4466 - val_acc: 0.8150
Epoch 2/40
8105/8105 [==============================] - 1s - loss: 0.4441 - acc: 0.8146 - val_loss: 0.4691 - val_acc: 0.8061
Epoch 3/40
8105/8105 [==============================] - 1s - loss: 0.4688 - acc: 0.8026 - val_loss: 0.4323 - val_acc: 0.8229
Epoch 4/40
8105/8105 [==============================] - 1s - loss: 0.4237 - acc: 0.8233 - val_loss: 0.4246 - val_acc: 0.8115
Epoch 5/40
8105/8105 [==============================] - 1s - loss: 0.4077 - acc: 0.8283 - val_loss: 0.4454 - val_acc: 0.7913
Epoch 6/40
8105/8105 [==============================] - 1s - loss: 0.4302 - acc: 0.8134 - val_loss: 0.4168 - val_acc: 0.8194
Epoch 7/40
8105/8105 [==============================] - 1s - loss: 0.4031 - acc: 0.8318 - val_loss: 0.4405 - val_acc: 0.8150
Epoch 8/40
8105/8105 [==============================] - 1s - loss: 0.4314 - acc: 0.8163 - val_loss: 0.4309 - val_acc: 0.8170
Epoch 9/40
8105/8105 [==============================] - 1s - loss: 0.4204 - acc: 0.8225 - val_loss: 0.4113 - val_acc: 0.8229
Epoch 10/40
8105/8105 [==============================] - 1s - loss: 0.3954 - acc: 0.8359 - val_loss: 0.4348 - val_acc: 0.8111
Epoch 11/40
8105/8105 [==============================] - 1s - loss: 0.4188 - acc: 0.8255 - val_loss: 0.4175 - val_acc: 0.8194
Epoch 12/40
8105/8105 [==============================] - 1s - loss: 0.4000 - acc: 0.8332 - val_loss: 0.4140 - val_acc: 0.8254
Epoch 13/40
8105/8105 [==============================] - 1s - loss: 0.3952 - acc: 0.8345 - val_loss: 0.4239 - val_acc: 0.8283
Epoch 14/40
8105/8105 [==============================] - 1s - loss: 0.4072 - acc: 0.8265 - val_loss: 0.4188 - val_acc: 0.8254
Epoch 15/40
8105/8105 [==============================] - 1s - loss: 0.3978 - acc: 0.8350 - val_loss: 0.4174 - val_acc: 0.8214
Epoch 16/40
8105/8105 [==============================] - 1s - loss: 0.3919 - acc: 0.8381 - val_loss: 0.4285 - val_acc: 0.8180
Epoch 17/40
8105/8105 [==============================] - 1s - loss: 0.4044 - acc: 0.8339 - val_loss: 0.4109 - val_acc: 0.8268
Epoch 18/40
8105/8105 [==============================] - 1s - loss: 0.3862 - acc: 0.8405 - val_loss: 0.4153 - val_acc: 0.8328
Epoch 19/40
8105/8105 [==============================] - 1s - loss: 0.3941 - acc: 0.8357 - val_loss: 0.4115 - val_acc: 0.8308
Epoch 20/40
8105/8105 [==============================] - 1s - loss: 0.3907 - acc: 0.8381 - val_loss: 0.4072 - val_acc: 0.8318
Epoch 21/40
8105/8105 [==============================] - 1s - loss: 0.3860 - acc: 0.8396 - val_loss: 0.4146 - val_acc: 0.8244
Epoch 22/40
8105/8105 [==============================] - 1s - loss: 0.3940 - acc: 0.8378 - val_loss: 0.4046 - val_acc: 0.8333
Epoch 23/40
8105/8105 [==============================] - 1s - loss: 0.3837 - acc: 0.8410 - val_loss: 0.4091 - val_acc: 0.8303
Epoch 24/40
8105/8105 [==============================] - 1s - loss: 0.3886 - acc: 0.8402 - val_loss: 0.4064 - val_acc: 0.8328
Epoch 25/40
8105/8105 [==============================] - 1s - loss: 0.3847 - acc: 0.8410 - val_loss: 0.4075 - val_acc: 0.8308
Epoch 26/40
8105/8105 [==============================] - 1s - loss: 0.3840 - acc: 0.8406 - val_loss: 0.4105 - val_acc: 0.8263
Epoch 27/40
8105/8105 [==============================] - 1s - loss: 0.3863 - acc: 0.8408 - val_loss: 0.4057 - val_acc: 0.8318
Epoch 28/40
8105/8105 [==============================] - 1s - loss: 0.3815 - acc: 0.8410 - val_loss: 0.4081 - val_acc: 0.8323
Epoch 29/40
8105/8105 [==============================] - 1s - loss: 0.3843 - acc: 0.8423 - val_loss: 0.4045 - val_acc: 0.8352
Epoch 30/40
8105/8105 [==============================] - 1s - loss: 0.3794 - acc: 0.8412 - val_loss: 0.4087 - val_acc: 0.8303
Epoch 31/40
8105/8105 [==============================] - 1s - loss: 0.3828 - acc: 0.8418 - val_loss: 0.4045 - val_acc: 0.8308
Epoch 32/40
8105/8105 [==============================] - 1s - loss: 0.3789 - acc: 0.8428 - val_loss: 0.4046 - val_acc: 0.8328
Epoch 33/40
8105/8105 [==============================] - 1s - loss: 0.3803 - acc: 0.8432 - val_loss: 0.4035 - val_acc: 0.8328
Epoch 34/40
8105/8105 [==============================] - 1s - loss: 0.3794 - acc: 0.8439 - val_loss: 0.4020 - val_acc: 0.8308
Epoch 35/40
8105/8105 [==============================] - 1s - loss: 0.3777 - acc: 0.8437 - val_loss: 0.4028 - val_acc: 0.8303
Epoch 36/40
8105/8105 [==============================] - 1s - loss: 0.3789 - acc: 0.8453 - val_loss: 0.4004 - val_acc: 0.8333
Epoch 37/40
8105/8105 [==============================] - 1s - loss: 0.3769 - acc: 0.8429 - val_loss: 0.4013 - val_acc: 0.8328
Epoch 38/40
8105/8105 [==============================] - 1s - loss: 0.3781 - acc: 0.8421 - val_loss: 0.3992 - val_acc: 0.8342
Epoch 39/40
8105/8105 [==============================] - 1s - loss: 0.3758 - acc: 0.8422 - val_loss: 0.4007 - val_acc: 0.8347
Epoch 40/40
8105/8105 [==============================] - 1s - loss: 0.3773 - acc: 0.8455 - val_loss: 0.3990 - val_acc: 0.8347
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-30.h5
chunk number 30
prepare data
start training
Train on 8185 samples, validate on 2047 samples
Epoch 1/40
8185/8185 [==============================] - 1s - loss: 0.3834 - acc: 0.8461 - val_loss: 0.3930 - val_acc: 0.8466
Epoch 2/40
8185/8185 [==============================] - 1s - loss: 0.3841 - acc: 0.8441 - val_loss: 0.3918 - val_acc: 0.8456
Epoch 3/40
8185/8185 [==============================] - 1s - loss: 0.3824 - acc: 0.8454 - val_loss: 0.3923 - val_acc: 0.8466
Epoch 4/40
8185/8185 [==============================] - 1s - loss: 0.3824 - acc: 0.8457 - val_loss: 0.3913 - val_acc: 0.8471
Epoch 5/40
8185/8185 [==============================] - 1s - loss: 0.3806 - acc: 0.8462 - val_loss: 0.3920 - val_acc: 0.8471
Epoch 6/40
8185/8185 [==============================] - 1s - loss: 0.3804 - acc: 0.8450 - val_loss: 0.3917 - val_acc: 0.8456
Epoch 7/40
8185/8185 [==============================] - 1s - loss: 0.3793 - acc: 0.8453 - val_loss: 0.3921 - val_acc: 0.8447
Epoch 8/40
8185/8185 [==============================] - 1s - loss: 0.3790 - acc: 0.8458 - val_loss: 0.3919 - val_acc: 0.8461
Epoch 9/40
8185/8185 [==============================] - 1s - loss: 0.3777 - acc: 0.8465 - val_loss: 0.3924 - val_acc: 0.8437
Epoch 10/40
8185/8185 [==============================] - 1s - loss: 0.3772 - acc: 0.8467 - val_loss: 0.3920 - val_acc: 0.8451
Epoch 11/40
8185/8185 [==============================] - 1s - loss: 0.3758 - acc: 0.8476 - val_loss: 0.3924 - val_acc: 0.8442
Epoch 12/40
8185/8185 [==============================] - 1s - loss: 0.3755 - acc: 0.8478 - val_loss: 0.3919 - val_acc: 0.8447
Epoch 13/40
8185/8185 [==============================] - 1s - loss: 0.3742 - acc: 0.8479 - val_loss: 0.3920 - val_acc: 0.8456
Epoch 14/40
8185/8185 [==============================] - 1s - loss: 0.3738 - acc: 0.8480 - val_loss: 0.3923 - val_acc: 0.8461
Epoch 15/40
8185/8185 [==============================] - 1s - loss: 0.3728 - acc: 0.8479 - val_loss: 0.3930 - val_acc: 0.8456
Epoch 16/40
8185/8185 [==============================] - 1s - loss: 0.3725 - acc: 0.8479 - val_loss: 0.3924 - val_acc: 0.8461
Epoch 17/40
8185/8185 [==============================] - 1s - loss: 0.3719 - acc: 0.8483 - val_loss: 0.3927 - val_acc: 0.8447
Epoch 18/40
8185/8185 [==============================] - 1s - loss: 0.3712 - acc: 0.8490 - val_loss: 0.3938 - val_acc: 0.8427
Epoch 19/40
8185/8185 [==============================] - 1s - loss: 0.3710 - acc: 0.8487 - val_loss: 0.3930 - val_acc: 0.8461
Epoch 20/40
8185/8185 [==============================] - 1s - loss: 0.3700 - acc: 0.8492 - val_loss: 0.3932 - val_acc: 0.8456
Epoch 21/40
8185/8185 [==============================] - 1s - loss: 0.3695 - acc: 0.8495 - val_loss: 0.3942 - val_acc: 0.8442
Epoch 22/40
8185/8185 [==============================] - 1s - loss: 0.3691 - acc: 0.8496 - val_loss: 0.3938 - val_acc: 0.8466
Epoch 23/40
8185/8185 [==============================] - 1s - loss: 0.3683 - acc: 0.8505 - val_loss: 0.3940 - val_acc: 0.8461
Epoch 24/40
8185/8185 [==============================] - 1s - loss: 0.3679 - acc: 0.8503 - val_loss: 0.3947 - val_acc: 0.8456
Epoch 25/40
8185/8185 [==============================] - 1s - loss: 0.3676 - acc: 0.8512 - val_loss: 0.3942 - val_acc: 0.8437
Epoch 26/40
8185/8185 [==============================] - 1s - loss: 0.3671 - acc: 0.8508 - val_loss: 0.3941 - val_acc: 0.8447
Epoch 27/40
8185/8185 [==============================] - 1s - loss: 0.3665 - acc: 0.8513 - val_loss: 0.3944 - val_acc: 0.8447
Epoch 28/40
8185/8185 [==============================] - 1s - loss: 0.3663 - acc: 0.8516 - val_loss: 0.3941 - val_acc: 0.8442
Epoch 29/40
8185/8185 [==============================] - 1s - loss: 0.3658 - acc: 0.8514 - val_loss: 0.3943 - val_acc: 0.8442
Epoch 30/40
8185/8185 [==============================] - 1s - loss: 0.3653 - acc: 0.8514 - val_loss: 0.3946 - val_acc: 0.8432
Epoch 31/40
8185/8185 [==============================] - 1s - loss: 0.3650 - acc: 0.8520 - val_loss: 0.3947 - val_acc: 0.8447
Epoch 32/40
8185/8185 [==============================] - 1s - loss: 0.3647 - acc: 0.8523 - val_loss: 0.3951 - val_acc: 0.8422
Epoch 33/40
8185/8185 [==============================] - 1s - loss: 0.3642 - acc: 0.8513 - val_loss: 0.3952 - val_acc: 0.8437
Epoch 34/40
8185/8185 [==============================] - 1s - loss: 0.3637 - acc: 0.8525 - val_loss: 0.3954 - val_acc: 0.8437
Epoch 35/40
8185/8185 [==============================] - 1s - loss: 0.3634 - acc: 0.8523 - val_loss: 0.3958 - val_acc: 0.8437
Epoch 36/40
8185/8185 [==============================] - 1s - loss: 0.3631 - acc: 0.8535 - val_loss: 0.3958 - val_acc: 0.8422
Epoch 37/40
8185/8185 [==============================] - 1s - loss: 0.3628 - acc: 0.8538 - val_loss: 0.3961 - val_acc: 0.8432
Epoch 38/40
8185/8185 [==============================] - 1s - loss: 0.3624 - acc: 0.8540 - val_loss: 0.3959 - val_acc: 0.8437
Epoch 39/40
8185/8185 [==============================] - 1s - loss: 0.3620 - acc: 0.8544 - val_loss: 0.3960 - val_acc: 0.8422
Epoch 40/40
8185/8185 [==============================] - 1s - loss: 0.3617 - acc: 0.8545 - val_loss: 0.3960 - val_acc: 0.8427
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-31.h5
chunk number 31
prepare data
start training
Train on 8079 samples, validate on 2020 samples
Epoch 1/40
8079/8079 [==============================] - 1s - loss: 0.3892 - acc: 0.8411 - val_loss: 0.3850 - val_acc: 0.8455
Epoch 2/40
8079/8079 [==============================] - 1s - loss: 0.3872 - acc: 0.8411 - val_loss: 0.3863 - val_acc: 0.8520
Epoch 3/40
8079/8079 [==============================] - 1s - loss: 0.3858 - acc: 0.8423 - val_loss: 0.3861 - val_acc: 0.8495
Epoch 4/40
8079/8079 [==============================] - 1s - loss: 0.3842 - acc: 0.8435 - val_loss: 0.3844 - val_acc: 0.8495
Epoch 5/40
8079/8079 [==============================] - 1s - loss: 0.3824 - acc: 0.8444 - val_loss: 0.3833 - val_acc: 0.8475
Epoch 6/40
8079/8079 [==============================] - 1s - loss: 0.3818 - acc: 0.8440 - val_loss: 0.3838 - val_acc: 0.8485
Epoch 7/40
8079/8079 [==============================] - 1s - loss: 0.3811 - acc: 0.8433 - val_loss: 0.3851 - val_acc: 0.8495
Epoch 8/40
8079/8079 [==============================] - 1s - loss: 0.3800 - acc: 0.8444 - val_loss: 0.3863 - val_acc: 0.8480
Epoch 9/40
8079/8079 [==============================] - 1s - loss: 0.3792 - acc: 0.8453 - val_loss: 0.3870 - val_acc: 0.8500
Epoch 10/40
8079/8079 [==============================] - 1s - loss: 0.3786 - acc: 0.8452 - val_loss: 0.3867 - val_acc: 0.8510
Epoch 11/40
8079/8079 [==============================] - 1s - loss: 0.3778 - acc: 0.8468 - val_loss: 0.3854 - val_acc: 0.8480
Epoch 12/40
8079/8079 [==============================] - 1s - loss: 0.3769 - acc: 0.8470 - val_loss: 0.3844 - val_acc: 0.8485
Epoch 13/40
8079/8079 [==============================] - 1s - loss: 0.3761 - acc: 0.8475 - val_loss: 0.3843 - val_acc: 0.8500
Epoch 14/40
8079/8079 [==============================] - 1s - loss: 0.3755 - acc: 0.8470 - val_loss: 0.3841 - val_acc: 0.8510
Epoch 15/40
8079/8079 [==============================] - 1s - loss: 0.3749 - acc: 0.8485 - val_loss: 0.3846 - val_acc: 0.8515
Epoch 16/40
8079/8079 [==============================] - 1s - loss: 0.3744 - acc: 0.8484 - val_loss: 0.3852 - val_acc: 0.8510
Epoch 17/40
8079/8079 [==============================] - 1s - loss: 0.3739 - acc: 0.8492 - val_loss: 0.3851 - val_acc: 0.8505
Epoch 18/40
8079/8079 [==============================] - 1s - loss: 0.3733 - acc: 0.8490 - val_loss: 0.3848 - val_acc: 0.8510
Epoch 19/40
8079/8079 [==============================] - 1s - loss: 0.3727 - acc: 0.8496 - val_loss: 0.3848 - val_acc: 0.8480
Epoch 20/40
8079/8079 [==============================] - 1s - loss: 0.3722 - acc: 0.8495 - val_loss: 0.3846 - val_acc: 0.8485
Epoch 21/40
8079/8079 [==============================] - 1s - loss: 0.3716 - acc: 0.8495 - val_loss: 0.3849 - val_acc: 0.8505
Epoch 22/40
8079/8079 [==============================] - 1s - loss: 0.3711 - acc: 0.8489 - val_loss: 0.3855 - val_acc: 0.8495
Epoch 23/40
8079/8079 [==============================] - 1s - loss: 0.3706 - acc: 0.8500 - val_loss: 0.3858 - val_acc: 0.8485
Epoch 24/40
8079/8079 [==============================] - 1s - loss: 0.3701 - acc: 0.8502 - val_loss: 0.3860 - val_acc: 0.8485
Epoch 25/40
8079/8079 [==============================] - 1s - loss: 0.3696 - acc: 0.8511 - val_loss: 0.3860 - val_acc: 0.8485
Epoch 26/40
8079/8079 [==============================] - 1s - loss: 0.3692 - acc: 0.8511 - val_loss: 0.3858 - val_acc: 0.8500
Epoch 27/40
8079/8079 [==============================] - 1s - loss: 0.3688 - acc: 0.8517 - val_loss: 0.3855 - val_acc: 0.8520
Epoch 28/40
8079/8079 [==============================] - 1s - loss: 0.3684 - acc: 0.8517 - val_loss: 0.3852 - val_acc: 0.8495
Epoch 29/40
8079/8079 [==============================] - 1s - loss: 0.3679 - acc: 0.8513 - val_loss: 0.3850 - val_acc: 0.8505
Epoch 30/40
8079/8079 [==============================] - 1s - loss: 0.3676 - acc: 0.8515 - val_loss: 0.3847 - val_acc: 0.8485
Epoch 31/40
8079/8079 [==============================] - 1s - loss: 0.3672 - acc: 0.8518 - val_loss: 0.3853 - val_acc: 0.8500
Epoch 32/40
8079/8079 [==============================] - 1s - loss: 0.3670 - acc: 0.8517 - val_loss: 0.3855 - val_acc: 0.8485
Epoch 33/40
8079/8079 [==============================] - 1s - loss: 0.3676 - acc: 0.8489 - val_loss: 0.3884 - val_acc: 0.8530
Epoch 34/40
8079/8079 [==============================] - 1s - loss: 0.3704 - acc: 0.8512 - val_loss: 0.3860 - val_acc: 0.8485
Epoch 35/40
8079/8079 [==============================] - 1s - loss: 0.3670 - acc: 0.8485 - val_loss: 0.3858 - val_acc: 0.8495
Epoch 36/40
8079/8079 [==============================] - 1s - loss: 0.3659 - acc: 0.8525 - val_loss: 0.3847 - val_acc: 0.8485
Epoch 37/40
8079/8079 [==============================] - 1s - loss: 0.3651 - acc: 0.8516 - val_loss: 0.3849 - val_acc: 0.8490
Epoch 38/40
8079/8079 [==============================] - 1s - loss: 0.3648 - acc: 0.8525 - val_loss: 0.3844 - val_acc: 0.8505
Epoch 39/40
8079/8079 [==============================] - 1s - loss: 0.3648 - acc: 0.8504 - val_loss: 0.3863 - val_acc: 0.8535
Epoch 40/40
8079/8079 [==============================] - 1s - loss: 0.3660 - acc: 0.8531 - val_loss: 0.3855 - val_acc: 0.8500
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-32.h5
chunk number 32
prepare data
start training
Train on 8130 samples, validate on 2033 samples
Epoch 1/40
8130/8130 [==============================] - 1s - loss: 0.3873 - acc: 0.8437 - val_loss: 0.3695 - val_acc: 0.8598
Epoch 2/40
8130/8130 [==============================] - 1s - loss: 0.3860 - acc: 0.8432 - val_loss: 0.3702 - val_acc: 0.8564
Epoch 3/40
8130/8130 [==============================] - 1s - loss: 0.3850 - acc: 0.8427 - val_loss: 0.3713 - val_acc: 0.8529
Epoch 4/40
8130/8130 [==============================] - 1s - loss: 0.3815 - acc: 0.8454 - val_loss: 0.3661 - val_acc: 0.8598
Epoch 5/40
8130/8130 [==============================] - 1s - loss: 0.3765 - acc: 0.8498 - val_loss: 0.3686 - val_acc: 0.8554
Epoch 6/40
8130/8130 [==============================] - 1s - loss: 0.3799 - acc: 0.8469 - val_loss: 0.3652 - val_acc: 0.8559
Epoch 7/40
8130/8130 [==============================] - 1s - loss: 0.3751 - acc: 0.8486 - val_loss: 0.3735 - val_acc: 0.8549
Epoch 8/40
8130/8130 [==============================] - 1s - loss: 0.3800 - acc: 0.8467 - val_loss: 0.3675 - val_acc: 0.8569
Epoch 9/40
8130/8130 [==============================] - 1s - loss: 0.3754 - acc: 0.8491 - val_loss: 0.3677 - val_acc: 0.8578
Epoch 10/40
8130/8130 [==============================] - 1s - loss: 0.3752 - acc: 0.8490 - val_loss: 0.3696 - val_acc: 0.8583
Epoch 11/40
8130/8130 [==============================] - 1s - loss: 0.3745 - acc: 0.8494 - val_loss: 0.3635 - val_acc: 0.8593
Epoch 12/40
8130/8130 [==============================] - 1s - loss: 0.3703 - acc: 0.8508 - val_loss: 0.3637 - val_acc: 0.8569
Epoch 13/40
8130/8130 [==============================] - 1s - loss: 0.3726 - acc: 0.8499 - val_loss: 0.3613 - val_acc: 0.8603
Epoch 14/40
8130/8130 [==============================] - 1s - loss: 0.3691 - acc: 0.8499 - val_loss: 0.3649 - val_acc: 0.8593
Epoch 15/40
8130/8130 [==============================] - 1s - loss: 0.3711 - acc: 0.8492 - val_loss: 0.3635 - val_acc: 0.8569
Epoch 16/40
8130/8130 [==============================] - 1s - loss: 0.3690 - acc: 0.8498 - val_loss: 0.3639 - val_acc: 0.8598
Epoch 17/40
8130/8130 [==============================] - 1s - loss: 0.3678 - acc: 0.8506 - val_loss: 0.3664 - val_acc: 0.8598
Epoch 18/40
8130/8130 [==============================] - 1s - loss: 0.3687 - acc: 0.8507 - val_loss: 0.3632 - val_acc: 0.8583
Epoch 19/40
8130/8130 [==============================] - 1s - loss: 0.3660 - acc: 0.8503 - val_loss: 0.3641 - val_acc: 0.8574
Epoch 20/40
8130/8130 [==============================] - 1s - loss: 0.3667 - acc: 0.8514 - val_loss: 0.3642 - val_acc: 0.8603
Epoch 21/40
8130/8130 [==============================] - 1s - loss: 0.3661 - acc: 0.8501 - val_loss: 0.3639 - val_acc: 0.8598
Epoch 22/40
8130/8130 [==============================] - 1s - loss: 0.3642 - acc: 0.8517 - val_loss: 0.3658 - val_acc: 0.8583
Epoch 23/40
8130/8130 [==============================] - 1s - loss: 0.3651 - acc: 0.8517 - val_loss: 0.3650 - val_acc: 0.8608
Epoch 24/40
8130/8130 [==============================] - 1s - loss: 0.3637 - acc: 0.8530 - val_loss: 0.3635 - val_acc: 0.8618
Epoch 25/40
8130/8130 [==============================] - 1s - loss: 0.3627 - acc: 0.8524 - val_loss: 0.3633 - val_acc: 0.8583
Epoch 26/40
8130/8130 [==============================] - 1s - loss: 0.3631 - acc: 0.8512 - val_loss: 0.3620 - val_acc: 0.8652
Epoch 27/40
8130/8130 [==============================] - 1s - loss: 0.3618 - acc: 0.8530 - val_loss: 0.3623 - val_acc: 0.8647
Epoch 28/40
8130/8130 [==============================] - 1s - loss: 0.3611 - acc: 0.8541 - val_loss: 0.3634 - val_acc: 0.8583
Epoch 29/40
8130/8130 [==============================] - 1s - loss: 0.3613 - acc: 0.8528 - val_loss: 0.3633 - val_acc: 0.8613
Epoch 30/40
8130/8130 [==============================] - 1s - loss: 0.3603 - acc: 0.8541 - val_loss: 0.3626 - val_acc: 0.8608
Epoch 31/40
8130/8130 [==============================] - 1s - loss: 0.3595 - acc: 0.8554 - val_loss: 0.3625 - val_acc: 0.8588
Epoch 32/40
8130/8130 [==============================] - 1s - loss: 0.3596 - acc: 0.8526 - val_loss: 0.3628 - val_acc: 0.8608
Epoch 33/40
8130/8130 [==============================] - 1s - loss: 0.3591 - acc: 0.8540 - val_loss: 0.3628 - val_acc: 0.8583
Epoch 34/40
8130/8130 [==============================] - 1s - loss: 0.3582 - acc: 0.8551 - val_loss: 0.3634 - val_acc: 0.8569
Epoch 35/40
8130/8130 [==============================] - 1s - loss: 0.3578 - acc: 0.8560 - val_loss: 0.3640 - val_acc: 0.8578
Epoch 36/40
8130/8130 [==============================] - 1s - loss: 0.3576 - acc: 0.8550 - val_loss: 0.3631 - val_acc: 0.8583
Epoch 37/40
8130/8130 [==============================] - 1s - loss: 0.3575 - acc: 0.8544 - val_loss: 0.3634 - val_acc: 0.8583
Epoch 38/40
8130/8130 [==============================] - 1s - loss: 0.3568 - acc: 0.8550 - val_loss: 0.3630 - val_acc: 0.8578
Epoch 39/40
8130/8130 [==============================] - 1s - loss: 0.3561 - acc: 0.8563 - val_loss: 0.3638 - val_acc: 0.8578
Epoch 40/40
8130/8130 [==============================] - 1s - loss: 0.3556 - acc: 0.8571 - val_loss: 0.3642 - val_acc: 0.8564
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-33.h5
chunk number 33
prepare data
start training
Train on 8214 samples, validate on 2054 samples
Epoch 1/40
8214/8214 [==============================] - 1s - loss: 0.3878 - acc: 0.8425 - val_loss: 0.3704 - val_acc: 0.8525
Epoch 2/40
8214/8214 [==============================] - 1s - loss: 0.3886 - acc: 0.8399 - val_loss: 0.3685 - val_acc: 0.8515
Epoch 3/40
8214/8214 [==============================] - 1s - loss: 0.3857 - acc: 0.8412 - val_loss: 0.3719 - val_acc: 0.8539
Epoch 4/40
8214/8214 [==============================] - 1s - loss: 0.3857 - acc: 0.8420 - val_loss: 0.3675 - val_acc: 0.8520
Epoch 5/40
8214/8214 [==============================] - 1s - loss: 0.3814 - acc: 0.8440 - val_loss: 0.3685 - val_acc: 0.8520
Epoch 6/40
8214/8214 [==============================] - 1s - loss: 0.3823 - acc: 0.8439 - val_loss: 0.3679 - val_acc: 0.8549
Epoch 7/40
8214/8214 [==============================] - 1s - loss: 0.3794 - acc: 0.8447 - val_loss: 0.3706 - val_acc: 0.8544
Epoch 8/40
8214/8214 [==============================] - 1s - loss: 0.3805 - acc: 0.8431 - val_loss: 0.3681 - val_acc: 0.8559
Epoch 9/40
8214/8214 [==============================] - 1s - loss: 0.3782 - acc: 0.8448 - val_loss: 0.3678 - val_acc: 0.8544
Epoch 10/40
8214/8214 [==============================] - 1s - loss: 0.3772 - acc: 0.8451 - val_loss: 0.3704 - val_acc: 0.8539
Epoch 11/40
8214/8214 [==============================] - 1s - loss: 0.3774 - acc: 0.8439 - val_loss: 0.3671 - val_acc: 0.8559
Epoch 12/40
8214/8214 [==============================] - 1s - loss: 0.3749 - acc: 0.8470 - val_loss: 0.3674 - val_acc: 0.8549
Epoch 13/40
8214/8214 [==============================] - 1s - loss: 0.3750 - acc: 0.8471 - val_loss: 0.3692 - val_acc: 0.8525
Epoch 14/40
8214/8214 [==============================] - 1s - loss: 0.3749 - acc: 0.8462 - val_loss: 0.3674 - val_acc: 0.8578
Epoch 15/40
8214/8214 [==============================] - 1s - loss: 0.3732 - acc: 0.8471 - val_loss: 0.3674 - val_acc: 0.8574
Epoch 16/40
8214/8214 [==============================] - 1s - loss: 0.3726 - acc: 0.8478 - val_loss: 0.3694 - val_acc: 0.8520
Epoch 17/40
8214/8214 [==============================] - 1s - loss: 0.3725 - acc: 0.8484 - val_loss: 0.3678 - val_acc: 0.8549
Epoch 18/40
8214/8214 [==============================] - 1s - loss: 0.3714 - acc: 0.8487 - val_loss: 0.3688 - val_acc: 0.8549
Epoch 19/40
8214/8214 [==============================] - 1s - loss: 0.3704 - acc: 0.8492 - val_loss: 0.3710 - val_acc: 0.8544
Epoch 20/40
8214/8214 [==============================] - 1s - loss: 0.3707 - acc: 0.8498 - val_loss: 0.3692 - val_acc: 0.8564
Epoch 21/40
8214/8214 [==============================] - 1s - loss: 0.3707 - acc: 0.8486 - val_loss: 0.3709 - val_acc: 0.8539
Epoch 22/40
8214/8214 [==============================] - 1s - loss: 0.3693 - acc: 0.8506 - val_loss: 0.3706 - val_acc: 0.8559
Epoch 23/40
8214/8214 [==============================] - 1s - loss: 0.3686 - acc: 0.8510 - val_loss: 0.3702 - val_acc: 0.8559
Epoch 24/40
8214/8214 [==============================] - 1s - loss: 0.3686 - acc: 0.8496 - val_loss: 0.3726 - val_acc: 0.8544
Epoch 25/40
8214/8214 [==============================] - 1s - loss: 0.3686 - acc: 0.8499 - val_loss: 0.3709 - val_acc: 0.8554
Epoch 26/40
8214/8214 [==============================] - 1s - loss: 0.3684 - acc: 0.8504 - val_loss: 0.3727 - val_acc: 0.8544
Epoch 27/40
8214/8214 [==============================] - 1s - loss: 0.3672 - acc: 0.8527 - val_loss: 0.3712 - val_acc: 0.8530
Epoch 28/40
8214/8214 [==============================] - 1s - loss: 0.3665 - acc: 0.8515 - val_loss: 0.3716 - val_acc: 0.8525
Epoch 29/40
8214/8214 [==============================] - 1s - loss: 0.3659 - acc: 0.8522 - val_loss: 0.3724 - val_acc: 0.8554
Epoch 30/40
8214/8214 [==============================] - 1s - loss: 0.3656 - acc: 0.8521 - val_loss: 0.3715 - val_acc: 0.8535
Epoch 31/40
8214/8214 [==============================] - 1s - loss: 0.3657 - acc: 0.8517 - val_loss: 0.3741 - val_acc: 0.8530
Epoch 32/40
8214/8214 [==============================] - 1s - loss: 0.3658 - acc: 0.8524 - val_loss: 0.3718 - val_acc: 0.8554
Epoch 33/40
8214/8214 [==============================] - 1s - loss: 0.3666 - acc: 0.8510 - val_loss: 0.3747 - val_acc: 0.8549
Epoch 34/40
8214/8214 [==============================] - 1s - loss: 0.3655 - acc: 0.8518 - val_loss: 0.3717 - val_acc: 0.8544
Epoch 35/40
8214/8214 [==============================] - 1s - loss: 0.3651 - acc: 0.8517 - val_loss: 0.3737 - val_acc: 0.8544
Epoch 36/40
8214/8214 [==============================] - 1s - loss: 0.3640 - acc: 0.8537 - val_loss: 0.3717 - val_acc: 0.8530
Epoch 37/40
8214/8214 [==============================] - 1s - loss: 0.3634 - acc: 0.8534 - val_loss: 0.3732 - val_acc: 0.8535
Epoch 38/40
8214/8214 [==============================] - 1s - loss: 0.3628 - acc: 0.8534 - val_loss: 0.3719 - val_acc: 0.8525
Epoch 39/40
8214/8214 [==============================] - 1s - loss: 0.3625 - acc: 0.8528 - val_loss: 0.3739 - val_acc: 0.8520
Epoch 40/40
8214/8214 [==============================] - 1s - loss: 0.3622 - acc: 0.8534 - val_loss: 0.3725 - val_acc: 0.8544
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-34.h5
chunk number 34
prepare data
start training
Train on 8084 samples, validate on 2021 samples
Epoch 1/40
8084/8084 [==============================] - 2s - loss: 0.4107 - acc: 0.8304 - val_loss: 0.4108 - val_acc: 0.8258
Epoch 2/40
8084/8084 [==============================] - 2s - loss: 0.4034 - acc: 0.8294 - val_loss: 0.4009 - val_acc: 0.8382
Epoch 3/40
8084/8084 [==============================] - 2s - loss: 0.4051 - acc: 0.8352 - val_loss: 0.3944 - val_acc: 0.8402
Epoch 4/40
8084/8084 [==============================] - 2s - loss: 0.3898 - acc: 0.8404 - val_loss: 0.4301 - val_acc: 0.8110
Epoch 5/40
8084/8084 [==============================] - 2s - loss: 0.4130 - acc: 0.8243 - val_loss: 0.3905 - val_acc: 0.8387
Epoch 6/40
8084/8084 [==============================] - 2s - loss: 0.3947 - acc: 0.8405 - val_loss: 0.3911 - val_acc: 0.8387
Epoch 7/40
8084/8084 [==============================] - 2s - loss: 0.3985 - acc: 0.8391 - val_loss: 0.3873 - val_acc: 0.8382
Epoch 8/40
8084/8084 [==============================] - 2s - loss: 0.3863 - acc: 0.8397 - val_loss: 0.4197 - val_acc: 0.8090
Epoch 9/40
8084/8084 [==============================] - 2s - loss: 0.4062 - acc: 0.8179 - val_loss: 0.3818 - val_acc: 0.8446
Epoch 10/40
8084/8084 [==============================] - 2s - loss: 0.3875 - acc: 0.8419 - val_loss: 0.3943 - val_acc: 0.8377
Epoch 11/40
8084/8084 [==============================] - 2s - loss: 0.3984 - acc: 0.8384 - val_loss: 0.3853 - val_acc: 0.8466
Epoch 12/40
8084/8084 [==============================] - 2s - loss: 0.3823 - acc: 0.8451 - val_loss: 0.4125 - val_acc: 0.8189
Epoch 13/40
8084/8084 [==============================] - 2s - loss: 0.3977 - acc: 0.8302 - val_loss: 0.3876 - val_acc: 0.8441
Epoch 14/40
8084/8084 [==============================] - 2s - loss: 0.3796 - acc: 0.8466 - val_loss: 0.3883 - val_acc: 0.8431
Epoch 15/40
8084/8084 [==============================] - 2s - loss: 0.3871 - acc: 0.8434 - val_loss: 0.3838 - val_acc: 0.8486
Epoch 16/40
8084/8084 [==============================] - 2s - loss: 0.3805 - acc: 0.8476 - val_loss: 0.3931 - val_acc: 0.8323
Epoch 17/40
8084/8084 [==============================] - 2s - loss: 0.3827 - acc: 0.8413 - val_loss: 0.3842 - val_acc: 0.8427
Epoch 18/40
8084/8084 [==============================] - 2s - loss: 0.3766 - acc: 0.8470 - val_loss: 0.3820 - val_acc: 0.8397
Epoch 19/40
8084/8084 [==============================] - 2s - loss: 0.3801 - acc: 0.8455 - val_loss: 0.3823 - val_acc: 0.8387
Epoch 20/40
8084/8084 [==============================] - 2s - loss: 0.3785 - acc: 0.8469 - val_loss: 0.3864 - val_acc: 0.8417
Epoch 21/40
8084/8084 [==============================] - 2s - loss: 0.3779 - acc: 0.8457 - val_loss: 0.3818 - val_acc: 0.8441
Epoch 22/40
8084/8084 [==============================] - 2s - loss: 0.3736 - acc: 0.8470 - val_loss: 0.3779 - val_acc: 0.8466
Epoch 23/40
8084/8084 [==============================] - 2s - loss: 0.3748 - acc: 0.8472 - val_loss: 0.3795 - val_acc: 0.8456
Epoch 24/40
8084/8084 [==============================] - 2s - loss: 0.3749 - acc: 0.8487 - val_loss: 0.3842 - val_acc: 0.8402
Epoch 25/40
8084/8084 [==============================] - 2s - loss: 0.3738 - acc: 0.8472 - val_loss: 0.3819 - val_acc: 0.8407
Epoch 26/40
8084/8084 [==============================] - 2s - loss: 0.3707 - acc: 0.8490 - val_loss: 0.3777 - val_acc: 0.8446
Epoch 27/40
8084/8084 [==============================] - 2s - loss: 0.3707 - acc: 0.8497 - val_loss: 0.3790 - val_acc: 0.8451
Epoch 28/40
8084/8084 [==============================] - 2s - loss: 0.3702 - acc: 0.8490 - val_loss: 0.3792 - val_acc: 0.8446
Epoch 29/40
8084/8084 [==============================] - 2s - loss: 0.3687 - acc: 0.8516 - val_loss: 0.3759 - val_acc: 0.8456
Epoch 30/40
8084/8084 [==============================] - 2s - loss: 0.3669 - acc: 0.8506 - val_loss: 0.3740 - val_acc: 0.8451
Epoch 31/40
8084/8084 [==============================] - 2s - loss: 0.3677 - acc: 0.8507 - val_loss: 0.3747 - val_acc: 0.8441
Epoch 32/40
8084/8084 [==============================] - 2s - loss: 0.3658 - acc: 0.8514 - val_loss: 0.3786 - val_acc: 0.8422
Epoch 33/40
8084/8084 [==============================] - 2s - loss: 0.3651 - acc: 0.8523 - val_loss: 0.3786 - val_acc: 0.8392
Epoch 34/40
8084/8084 [==============================] - 2s - loss: 0.3652 - acc: 0.8524 - val_loss: 0.3798 - val_acc: 0.8412
Epoch 35/40
8084/8084 [==============================] - 2s - loss: 0.3642 - acc: 0.8534 - val_loss: 0.3784 - val_acc: 0.8412
Epoch 36/40
8084/8084 [==============================] - 2s - loss: 0.3628 - acc: 0.8539 - val_loss: 0.3741 - val_acc: 0.8441
Epoch 37/40
8084/8084 [==============================] - 2s - loss: 0.3620 - acc: 0.8534 - val_loss: 0.3720 - val_acc: 0.8471
Epoch 38/40
8084/8084 [==============================] - 2s - loss: 0.3631 - acc: 0.8533 - val_loss: 0.3763 - val_acc: 0.8427
Epoch 39/40
8084/8084 [==============================] - 2s - loss: 0.3615 - acc: 0.8543 - val_loss: 0.3762 - val_acc: 0.8417
Epoch 40/40
8084/8084 [==============================] - 2s - loss: 0.3604 - acc: 0.8538 - val_loss: 0.3789 - val_acc: 0.8436
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-35.h5
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers/model-final.h5
