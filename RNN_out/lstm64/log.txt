chunk number 0
prepare data
start training
Epoch 1/40
10171/10171 [==============================] - 0s - loss: 0.7151 - acc: 0.3781
Epoch 2/40
10171/10171 [==============================] - 0s - loss: 0.6936 - acc: 0.4927
Epoch 3/40
10171/10171 [==============================] - 0s - loss: 0.6802 - acc: 0.6530
Epoch 4/40
10171/10171 [==============================] - 0s - loss: 0.6715 - acc: 0.6574
Epoch 5/40
10171/10171 [==============================] - 0s - loss: 0.6649 - acc: 0.6573
Epoch 6/40
10171/10171 [==============================] - 0s - loss: 0.6594 - acc: 0.6572
Epoch 7/40
10171/10171 [==============================] - 0s - loss: 0.6545 - acc: 0.6572
Epoch 8/40
10171/10171 [==============================] - 0s - loss: 0.6507 - acc: 0.6572
Epoch 9/40
10171/10171 [==============================] - 0s - loss: 0.6484 - acc: 0.6572
Epoch 10/40
10171/10171 [==============================] - 0s - loss: 0.6479 - acc: 0.6572
Epoch 11/40
10171/10171 [==============================] - 0s - loss: 0.6484 - acc: 0.6572
Epoch 12/40
10171/10171 [==============================] - 0s - loss: 0.6488 - acc: 0.6572
Epoch 13/40
10171/10171 [==============================] - 0s - loss: 0.6484 - acc: 0.6572
Epoch 14/40
10171/10171 [==============================] - 0s - loss: 0.6473 - acc: 0.6572
Epoch 15/40
10171/10171 [==============================] - 0s - loss: 0.6457 - acc: 0.6572
Epoch 16/40
10171/10171 [==============================] - 0s - loss: 0.6441 - acc: 0.6572
Epoch 17/40
10171/10171 [==============================] - 0s - loss: 0.6429 - acc: 0.6572
Epoch 18/40
10171/10171 [==============================] - 0s - loss: 0.6422 - acc: 0.6572
Epoch 19/40
10171/10171 [==============================] - 0s - loss: 0.6420 - acc: 0.6572
Epoch 20/40
10171/10171 [==============================] - 0s - loss: 0.6420 - acc: 0.6572
Epoch 21/40
10171/10171 [==============================] - 0s - loss: 0.6422 - acc: 0.6572
Epoch 22/40
10171/10171 [==============================] - 0s - loss: 0.6421 - acc: 0.6572
Epoch 23/40
10171/10171 [==============================] - 0s - loss: 0.6416 - acc: 0.6572
Epoch 24/40
10171/10171 [==============================] - 0s - loss: 0.6408 - acc: 0.6572
Epoch 25/40
10171/10171 [==============================] - 0s - loss: 0.6397 - acc: 0.6572
Epoch 26/40
10171/10171 [==============================] - 0s - loss: 0.6382 - acc: 0.6574
Epoch 27/40
10171/10171 [==============================] - 0s - loss: 0.6365 - acc: 0.6579
Epoch 28/40
10171/10171 [==============================] - 0s - loss: 0.6342 - acc: 0.6585
Epoch 29/40
10171/10171 [==============================] - 0s - loss: 0.6310 - acc: 0.6593
Epoch 30/40
10171/10171 [==============================] - 0s - loss: 0.6260 - acc: 0.6634
Epoch 31/40
10171/10171 [==============================] - 0s - loss: 0.6187 - acc: 0.6730
Epoch 32/40
10171/10171 [==============================] - 0s - loss: 0.6107 - acc: 0.6886
Epoch 33/40
10171/10171 [==============================] - 0s - loss: 0.6077 - acc: 0.6938
Epoch 34/40
10171/10171 [==============================] - 0s - loss: 0.6041 - acc: 0.6892
Epoch 35/40
10171/10171 [==============================] - 0s - loss: 0.5916 - acc: 0.7058
Epoch 36/40
10171/10171 [==============================] - 0s - loss: 0.5780 - acc: 0.7199
Epoch 37/40
10171/10171 [==============================] - 0s - loss: 0.5701 - acc: 0.7188
Epoch 38/40
10171/10171 [==============================] - 0s - loss: 0.5624 - acc: 0.7272
Epoch 39/40
10171/10171 [==============================] - 0s - loss: 0.5566 - acc: 0.7278
Epoch 40/40
10171/10171 [==============================] - 0s - loss: 0.5543 - acc: 0.7287
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64/model-1.h5
chunk number 1
prepare data
start training
Epoch 1/40
10201/10201 [==============================] - 0s - loss: 0.5483 - acc: 0.7335
Epoch 2/40
10201/10201 [==============================] - 0s - loss: 0.5408 - acc: 0.7415
Epoch 3/40
10201/10201 [==============================] - 0s - loss: 0.5384 - acc: 0.7465
Epoch 4/40
10201/10201 [==============================] - 0s - loss: 0.5401 - acc: 0.7417
Epoch 5/40
10201/10201 [==============================] - 0s - loss: 0.5361 - acc: 0.7437
Epoch 6/40
10201/10201 [==============================] - 0s - loss: 0.5285 - acc: 0.7530
Epoch 7/40
10201/10201 [==============================] - 0s - loss: 0.5265 - acc: 0.7556
Epoch 8/40
10201/10201 [==============================] - 0s - loss: 0.5275 - acc: 0.7502
Epoch 9/40
10201/10201 [==============================] - 0s - loss: 0.5209 - acc: 0.7599
Epoch 10/40
10201/10201 [==============================] - 0s - loss: 0.5157 - acc: 0.7615
Epoch 11/40
10201/10201 [==============================] - 0s - loss: 0.5145 - acc: 0.7628
Epoch 12/40
10201/10201 [==============================] - 0s - loss: 0.5133 - acc: 0.7640
Epoch 13/40
10201/10201 [==============================] - 0s - loss: 0.5099 - acc: 0.7644
Epoch 14/40
10201/10201 [==============================] - 0s - loss: 0.5066 - acc: 0.7674
Epoch 15/40
10201/10201 [==============================] - 0s - loss: 0.5075 - acc: 0.7706
Epoch 16/40
10201/10201 [==============================] - 0s - loss: 0.5061 - acc: 0.7702
Epoch 17/40
10201/10201 [==============================] - 0s - loss: 0.5048 - acc: 0.7700
Epoch 18/40
10201/10201 [==============================] - 0s - loss: 0.5026 - acc: 0.7751
Epoch 19/40
10201/10201 [==============================] - 0s - loss: 0.5012 - acc: 0.7761
Epoch 20/40
10201/10201 [==============================] - 0s - loss: 0.5008 - acc: 0.7711
Epoch 21/40
10201/10201 [==============================] - 0s - loss: 0.4986 - acc: 0.7757
Epoch 22/40
10201/10201 [==============================] - 0s - loss: 0.4974 - acc: 0.7783
Epoch 23/40
10201/10201 [==============================] - 0s - loss: 0.4958 - acc: 0.7777
Epoch 24/40
10201/10201 [==============================] - 0s - loss: 0.4959 - acc: 0.7756
Epoch 25/40
10201/10201 [==============================] - 0s - loss: 0.4950 - acc: 0.7783
Epoch 26/40
10201/10201 [==============================] - 0s - loss: 0.4937 - acc: 0.7790
Epoch 27/40
10201/10201 [==============================] - 0s - loss: 0.4934 - acc: 0.7784
Epoch 28/40
10201/10201 [==============================] - 0s - loss: 0.4925 - acc: 0.7806
Epoch 29/40
10201/10201 [==============================] - 0s - loss: 0.4915 - acc: 0.7797
Epoch 30/40
10201/10201 [==============================] - 0s - loss: 0.4904 - acc: 0.7802
Epoch 31/40
10201/10201 [==============================] - 0s - loss: 0.4896 - acc: 0.7808
Epoch 32/40
10201/10201 [==============================] - 0s - loss: 0.4889 - acc: 0.7802
Epoch 33/40
10201/10201 [==============================] - 0s - loss: 0.4877 - acc: 0.7810
Epoch 34/40
10201/10201 [==============================] - 0s - loss: 0.4871 - acc: 0.7814
Epoch 35/40
10201/10201 [==============================] - 0s - loss: 0.4867 - acc: 0.7811
Epoch 36/40
10201/10201 [==============================] - 0s - loss: 0.4858 - acc: 0.7802
Epoch 37/40
10201/10201 [==============================] - 0s - loss: 0.4851 - acc: 0.7808
Epoch 38/40
10201/10201 [==============================] - 0s - loss: 0.4848 - acc: 0.7799
Epoch 39/40
10201/10201 [==============================] - 0s - loss: 0.4842 - acc: 0.7797
Epoch 40/40
10201/10201 [==============================] - 0s - loss: 0.4835 - acc: 0.7802
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64/model-2.h5
chunk number 2
prepare data
start training
Epoch 1/40
10228/10228 [==============================] - 0s - loss: 0.4716 - acc: 0.7901
Epoch 2/40
10228/10228 [==============================] - 0s - loss: 0.4718 - acc: 0.7888
Epoch 3/40
10228/10228 [==============================] - 0s - loss: 0.4688 - acc: 0.7899
Epoch 4/40
10228/10228 [==============================] - 0s - loss: 0.4710 - acc: 0.7896
Epoch 5/40
10228/10228 [==============================] - 0s - loss: 0.4668 - acc: 0.7910
Epoch 6/40
10228/10228 [==============================] - 0s - loss: 0.4680 - acc: 0.7913
Epoch 7/40
10228/10228 [==============================] - 0s - loss: 0.4660 - acc: 0.7912
Epoch 8/40
10228/10228 [==============================] - 0s - loss: 0.4652 - acc: 0.7918
Epoch 9/40
10228/10228 [==============================] - 0s - loss: 0.4652 - acc: 0.7917
Epoch 10/40
10228/10228 [==============================] - 0s - loss: 0.4631 - acc: 0.7918
Epoch 11/40
10228/10228 [==============================] - 0s - loss: 0.4641 - acc: 0.7931
Epoch 12/40
10228/10228 [==============================] - 0s - loss: 0.4617 - acc: 0.7929
Epoch 13/40
10228/10228 [==============================] - 0s - loss: 0.4615 - acc: 0.7942
Epoch 14/40
10228/10228 [==============================] - 0s - loss: 0.4610 - acc: 0.7944
Epoch 15/40
10228/10228 [==============================] - 0s - loss: 0.4592 - acc: 0.7948
Epoch 16/40
10228/10228 [==============================] - 0s - loss: 0.4595 - acc: 0.7951
Epoch 17/40
10228/10228 [==============================] - 0s - loss: 0.4580 - acc: 0.7942
Epoch 18/40
10228/10228 [==============================] - 0s - loss: 0.4577 - acc: 0.7956
Epoch 19/40
10228/10228 [==============================] - 0s - loss: 0.4571 - acc: 0.7944
Epoch 20/40
10228/10228 [==============================] - 0s - loss: 0.4560 - acc: 0.7952
Epoch 21/40
10228/10228 [==============================] - 0s - loss: 0.4560 - acc: 0.7983
Epoch 22/40
10228/10228 [==============================] - 0s - loss: 0.4548 - acc: 0.7963
Epoch 23/40
10228/10228 [==============================] - 0s - loss: 0.4543 - acc: 0.7966
Epoch 24/40
10228/10228 [==============================] - 0s - loss: 0.4538 - acc: 0.7986
Epoch 25/40
10228/10228 [==============================] - 0s - loss: 0.4527 - acc: 0.7977
Epoch 26/40
10228/10228 [==============================] - 0s - loss: 0.4524 - acc: 0.7982
Epoch 27/40
10228/10228 [==============================] - 0s - loss: 0.4517 - acc: 0.8002
Epoch 28/40
10228/10228 [==============================] - 0s - loss: 0.4509 - acc: 0.7999
Epoch 29/40
10228/10228 [==============================] - 0s - loss: 0.4505 - acc: 0.7994
Epoch 30/40
10228/10228 [==============================] - 0s - loss: 0.4498 - acc: 0.7998
Epoch 31/40
10228/10228 [==============================] - 0s - loss: 0.4491 - acc: 0.7997
Epoch 32/40
10228/10228 [==============================] - 0s - loss: 0.4486 - acc: 0.8004
Epoch 33/40
10228/10228 [==============================] - 0s - loss: 0.4479 - acc: 0.8000
Epoch 34/40
10228/10228 [==============================] - 0s - loss: 0.4473 - acc: 0.8005
Epoch 35/40
10228/10228 [==============================] - 0s - loss: 0.4468 - acc: 0.8012
Epoch 36/40
10228/10228 [==============================] - 0s - loss: 0.4461 - acc: 0.8013
Epoch 37/40
10228/10228 [==============================] - 0s - loss: 0.4454 - acc: 0.8015
Epoch 38/40
10228/10228 [==============================] - 0s - loss: 0.4449 - acc: 0.8021
Epoch 39/40
10228/10228 [==============================] - 0s - loss: 0.4443 - acc: 0.8020
Epoch 40/40
10228/10228 [==============================] - 0s - loss: 0.4437 - acc: 0.8028
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64/model-3.h5
chunk number 3
prepare data
start training
Epoch 1/40
10209/10209 [==============================] - 0s - loss: 0.4659 - acc: 0.7912
Epoch 2/40
10209/10209 [==============================] - 0s - loss: 0.4652 - acc: 0.7909
Epoch 3/40
10209/10209 [==============================] - 0s - loss: 0.4644 - acc: 0.7919
Epoch 4/40
10209/10209 [==============================] - 0s - loss: 0.4636 - acc: 0.7920
Epoch 5/40
10209/10209 [==============================] - 0s - loss: 0.4627 - acc: 0.7919
Epoch 6/40
10209/10209 [==============================] - 0s - loss: 0.4620 - acc: 0.7914
Epoch 7/40
10209/10209 [==============================] - 0s - loss: 0.4614 - acc: 0.7914
Epoch 8/40
10209/10209 [==============================] - 0s - loss: 0.4606 - acc: 0.7918
Epoch 9/40
10209/10209 [==============================] - 0s - loss: 0.4599 - acc: 0.7923
Epoch 10/40
10209/10209 [==============================] - 0s - loss: 0.4592 - acc: 0.7925
Epoch 11/40
10209/10209 [==============================] - 0s - loss: 0.4583 - acc: 0.7927
Epoch 12/40
10209/10209 [==============================] - 0s - loss: 0.4576 - acc: 0.7940
Epoch 13/40
10209/10209 [==============================] - 0s - loss: 0.4569 - acc: 0.7936
Epoch 14/40
10209/10209 [==============================] - 0s - loss: 0.4563 - acc: 0.7953
Epoch 15/40
10209/10209 [==============================] - 0s - loss: 0.4559 - acc: 0.7947
Epoch 16/40
10209/10209 [==============================] - 0s - loss: 0.4559 - acc: 0.7966
Epoch 17/40
10209/10209 [==============================] - 0s - loss: 0.4574 - acc: 0.7944
Epoch 18/40
10209/10209 [==============================] - 0s - loss: 0.4586 - acc: 0.7957
Epoch 19/40
10209/10209 [==============================] - 0s - loss: 0.4596 - acc: 0.7947
Epoch 20/40
10209/10209 [==============================] - 0s - loss: 0.4529 - acc: 0.7973
Epoch 21/40
10209/10209 [==============================] - 0s - loss: 0.4528 - acc: 0.7985
Epoch 22/40
10209/10209 [==============================] - 0s - loss: 0.4565 - acc: 0.7943
Epoch 23/40
10209/10209 [==============================] - 0s - loss: 0.4516 - acc: 0.7999
Epoch 24/40
10209/10209 [==============================] - 0s - loss: 0.4502 - acc: 0.7995
Epoch 25/40
10209/10209 [==============================] - 0s - loss: 0.4526 - acc: 0.7948
Epoch 26/40
10209/10209 [==============================] - 0s - loss: 0.4496 - acc: 0.8007
Epoch 27/40
10209/10209 [==============================] - 0s - loss: 0.4479 - acc: 0.7999
Epoch 28/40
10209/10209 [==============================] - 0s - loss: 0.4493 - acc: 0.7967
Epoch 29/40
10209/10209 [==============================] - 0s - loss: 0.4478 - acc: 0.8013
Epoch 30/40
10209/10209 [==============================] - 0s - loss: 0.4459 - acc: 0.8008
Epoch 31/40
10209/10209 [==============================] - 0s - loss: 0.4460 - acc: 0.7984
Epoch 32/40
10209/10209 [==============================] - 0s - loss: 0.4458 - acc: 0.8036
Epoch 33/40
10209/10209 [==============================] - 0s - loss: 0.4445 - acc: 0.7999
Epoch 34/40
10209/10209 [==============================] - 0s - loss: 0.4432 - acc: 0.8026
Epoch 35/40
10209/10209 [==============================] - 0s - loss: 0.4431 - acc: 0.8048
Epoch 36/40
10209/10209 [==============================] - 0s - loss: 0.4433 - acc: 0.8013
Epoch 37/40
10209/10209 [==============================] - 0s - loss: 0.4421 - acc: 0.8059
Epoch 38/40
10209/10209 [==============================] - 0s - loss: 0.4408 - acc: 0.8035
Epoch 39/40
10209/10209 [==============================] - 0s - loss: 0.4397 - acc: 0.8056
Epoch 40/40
10209/10209 [==============================] - 0s - loss: 0.4391 - acc: 0.8065
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64/model-4.h5
chunk number 4
prepare data
start training
Epoch 1/40
10060/10060 [==============================] - 0s - loss: 0.5359 - acc: 0.8003
Epoch 2/40
10060/10060 [==============================] - 1s - loss: 0.7788 - acc: 0.5359
Epoch 3/40
10060/10060 [==============================] - 1s - loss: 0.5263 - acc: 0.7985
Epoch 4/40
10060/10060 [==============================] - 1s - loss: 0.6165 - acc: 0.7782
Epoch 5/40
10060/10060 [==============================] - 1s - loss: 0.5716 - acc: 0.7836
Epoch 6/40
10060/10060 [==============================] - 1s - loss: 0.4876 - acc: 0.7844
Epoch 7/40
10060/10060 [==============================] - 1s - loss: 0.5118 - acc: 0.7353
Epoch 8/40
10060/10060 [==============================] - 1s - loss: 0.5173 - acc: 0.7383
Epoch 9/40
10060/10060 [==============================] - 1s - loss: 0.4773 - acc: 0.7822
Epoch 10/40
10060/10060 [==============================] - 1s - loss: 0.4744 - acc: 0.7899
Epoch 11/40
10060/10060 [==============================] - 1s - loss: 0.4859 - acc: 0.7852
Epoch 12/40
10060/10060 [==============================] - 1s - loss: 0.4834 - acc: 0.7838
Epoch 13/40
10060/10060 [==============================] - 1s - loss: 0.4709 - acc: 0.7910
Epoch 14/40
10060/10060 [==============================] - 1s - loss: 0.4632 - acc: 0.7959
Epoch 15/40
10060/10060 [==============================] - 1s - loss: 0.4669 - acc: 0.7895
Epoch 16/40
10060/10060 [==============================] - 1s - loss: 0.4706 - acc: 0.7928
Epoch 17/40
10060/10060 [==============================] - 1s - loss: 0.4672 - acc: 0.7936
Epoch 18/40
10060/10060 [==============================] - 1s - loss: 0.4651 - acc: 0.7937
Epoch 19/40
10060/10060 [==============================] - 1s - loss: 0.4665 - acc: 0.7925
Epoch 20/40
10060/10060 [==============================] - 1s - loss: 0.4676 - acc: 0.7921
Epoch 21/40
10060/10060 [==============================] - 1s - loss: 0.4663 - acc: 0.7915
Epoch 22/40
10060/10060 [==============================] - 1s - loss: 0.4626 - acc: 0.7930
Epoch 23/40
10060/10060 [==============================] - 1s - loss: 0.4588 - acc: 0.7953
Epoch 24/40
10060/10060 [==============================] - 1s - loss: 0.4580 - acc: 0.7956
Epoch 25/40
10060/10060 [==============================] - 1s - loss: 0.4595 - acc: 0.7950
Epoch 26/40
10060/10060 [==============================] - 1s - loss: 0.4582 - acc: 0.7948
Epoch 27/40
10060/10060 [==============================] - 1s - loss: 0.4538 - acc: 0.7965
Epoch 28/40
10060/10060 [==============================] - 1s - loss: 0.4517 - acc: 0.7966
Epoch 29/40
10060/10060 [==============================] - 1s - loss: 0.4530 - acc: 0.7966
Epoch 30/40
10060/10060 [==============================] - 1s - loss: 0.4540 - acc: 0.7972
Epoch 31/40
10060/10060 [==============================] - 1s - loss: 0.4531 - acc: 0.7971
Epoch 32/40
10060/10060 [==============================] - 1s - loss: 0.4512 - acc: 0.7961
Epoch 33/40
10060/10060 [==============================] - 1s - loss: 0.4505 - acc: 0.7966
Epoch 34/40
10060/10060 [==============================] - 1s - loss: 0.4506 - acc: 0.7964
Epoch 35/40
10060/10060 [==============================] - 1s - loss: 0.4499 - acc: 0.7962
Epoch 36/40
10060/10060 [==============================] - 1s - loss: 0.4491 - acc: 0.7998
Epoch 37/40
10060/10060 [==============================] - 1s - loss: 0.4492 - acc: 0.8005
Epoch 38/40
10060/10060 [==============================] - 1s - loss: 0.4491 - acc: 0.8002
Epoch 39/40
10060/10060 [==============================] - 1s - loss: 0.4479 - acc: 0.7996
Epoch 40/40
10060/10060 [==============================] - 1s - loss: 0.4468 - acc: 0.7992Using Theano backend.
Using gpu device 0: Tesla P100-PCIE-16GB (CNMeM is enabled with initial size: 95.0% of memory, cuDNN 5105)
/users/phwindis/.local/lib/python3.5/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.
  warnings.warn(warn)

saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64/model-5.h5
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64/model-final.h5
