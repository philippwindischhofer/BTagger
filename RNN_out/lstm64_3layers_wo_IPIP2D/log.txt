chunk number 0
prepare data
start training
Train on 8136 samples, validate on 2035 samples
Epoch 1/40
8136/8136 [==============================] - 2s - loss: 0.6933 - acc: 0.4774 - val_loss: 0.6650 - val_acc: 0.6629
Epoch 2/40
8136/8136 [==============================] - 1s - loss: 0.6688 - acc: 0.6557 - val_loss: 0.6493 - val_acc: 0.6629
Epoch 3/40
8136/8136 [==============================] - 1s - loss: 0.6543 - acc: 0.6557 - val_loss: 0.6402 - val_acc: 0.6629
Epoch 4/40
8136/8136 [==============================] - 1s - loss: 0.6451 - acc: 0.6557 - val_loss: 0.6344 - val_acc: 0.6629
Epoch 5/40
8136/8136 [==============================] - 1s - loss: 0.6380 - acc: 0.6557 - val_loss: 0.6321 - val_acc: 0.6629
Epoch 6/40
8136/8136 [==============================] - 1s - loss: 0.6341 - acc: 0.6557 - val_loss: 0.6298 - val_acc: 0.6614
Epoch 7/40
8136/8136 [==============================] - 1s - loss: 0.6310 - acc: 0.6555 - val_loss: 0.6238 - val_acc: 0.6614
Epoch 8/40
8136/8136 [==============================] - 1s - loss: 0.6252 - acc: 0.6556 - val_loss: 0.6190 - val_acc: 0.6609
Epoch 9/40
8136/8136 [==============================] - 1s - loss: 0.6207 - acc: 0.6561 - val_loss: 0.6149 - val_acc: 0.6624
Epoch 10/40
8136/8136 [==============================] - 1s - loss: 0.6162 - acc: 0.6573 - val_loss: 0.6104 - val_acc: 0.6624
Epoch 11/40
8136/8136 [==============================] - 1s - loss: 0.6102 - acc: 0.6645 - val_loss: 0.6091 - val_acc: 0.6688
Epoch 12/40
8136/8136 [==============================] - 1s - loss: 0.6074 - acc: 0.6783 - val_loss: 0.6021 - val_acc: 0.6747
Epoch 13/40
8136/8136 [==============================] - 1s - loss: 0.6007 - acc: 0.6817 - val_loss: 0.5987 - val_acc: 0.6762
Epoch 14/40
8136/8136 [==============================] - 1s - loss: 0.5975 - acc: 0.6852 - val_loss: 0.5916 - val_acc: 0.6924
Epoch 15/40
8136/8136 [==============================] - 1s - loss: 0.5892 - acc: 0.6922 - val_loss: 0.5833 - val_acc: 0.6998
Epoch 16/40
8136/8136 [==============================] - 1s - loss: 0.5810 - acc: 0.6967 - val_loss: 0.5730 - val_acc: 0.7160
Epoch 17/40
8136/8136 [==============================] - 1s - loss: 0.5730 - acc: 0.7174 - val_loss: 0.5573 - val_acc: 0.7209
Epoch 18/40
8136/8136 [==============================] - 1s - loss: 0.5583 - acc: 0.7227 - val_loss: 0.5424 - val_acc: 0.7459
Epoch 19/40
8136/8136 [==============================] - 1s - loss: 0.5462 - acc: 0.7397 - val_loss: 0.5326 - val_acc: 0.7577
Epoch 20/40
8136/8136 [==============================] - 1s - loss: 0.5368 - acc: 0.7531 - val_loss: 0.5285 - val_acc: 0.7553
Epoch 21/40
8136/8136 [==============================] - 1s - loss: 0.5328 - acc: 0.7482 - val_loss: 0.5661 - val_acc: 0.7140
Epoch 22/40
8136/8136 [==============================] - 1s - loss: 0.5674 - acc: 0.7088 - val_loss: 0.5561 - val_acc: 0.7455
Epoch 23/40
8136/8136 [==============================] - 1s - loss: 0.5631 - acc: 0.7355 - val_loss: 0.5755 - val_acc: 0.7386
Epoch 24/40
8136/8136 [==============================] - 1s - loss: 0.5847 - acc: 0.7264 - val_loss: 0.5308 - val_acc: 0.7587
Epoch 25/40
8136/8136 [==============================] - 1s - loss: 0.5340 - acc: 0.7471 - val_loss: 0.5402 - val_acc: 0.7479
Epoch 26/40
8136/8136 [==============================] - 1s - loss: 0.5396 - acc: 0.7421 - val_loss: 0.5388 - val_acc: 0.7494
Epoch 27/40
8136/8136 [==============================] - 1s - loss: 0.5375 - acc: 0.7464 - val_loss: 0.5178 - val_acc: 0.7641
Epoch 28/40
8136/8136 [==============================] - 1s - loss: 0.5165 - acc: 0.7633 - val_loss: 0.5324 - val_acc: 0.7518
Epoch 29/40
8136/8136 [==============================] - 1s - loss: 0.5350 - acc: 0.7420 - val_loss: 0.5342 - val_acc: 0.7499
Epoch 30/40
8136/8136 [==============================] - 1s - loss: 0.5369 - acc: 0.7400 - val_loss: 0.5226 - val_acc: 0.7612
Epoch 31/40
8136/8136 [==============================] - 1s - loss: 0.5227 - acc: 0.7521 - val_loss: 0.5192 - val_acc: 0.7641
Epoch 32/40
8136/8136 [==============================] - 1s - loss: 0.5158 - acc: 0.7694 - val_loss: 0.5321 - val_acc: 0.7582
Epoch 33/40
8136/8136 [==============================] - 1s - loss: 0.5281 - acc: 0.7607 - val_loss: 0.5243 - val_acc: 0.7631
Epoch 34/40
8136/8136 [==============================] - 1s - loss: 0.5201 - acc: 0.7667 - val_loss: 0.5132 - val_acc: 0.7666
Epoch 35/40
8136/8136 [==============================] - 1s - loss: 0.5096 - acc: 0.7693 - val_loss: 0.5180 - val_acc: 0.7631
Epoch 36/40
8136/8136 [==============================] - 1s - loss: 0.5162 - acc: 0.7597 - val_loss: 0.5190 - val_acc: 0.7627
Epoch 37/40
8136/8136 [==============================] - 1s - loss: 0.5165 - acc: 0.7606 - val_loss: 0.5113 - val_acc: 0.7666
Epoch 38/40
8136/8136 [==============================] - 1s - loss: 0.5056 - acc: 0.7710 - val_loss: 0.5130 - val_acc: 0.7700
Epoch 39/40
8136/8136 [==============================] - 1s - loss: 0.5069 - acc: 0.7697 - val_loss: 0.5153 - val_acc: 0.7676
Epoch 40/40
8136/8136 [==============================] - 1s - loss: 0.5096 - acc: 0.7652 - val_loss: 0.5068 - val_acc: 0.7686
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-1.h5
chunk number 1
prepare data
start training
Train on 8160 samples, validate on 2041 samples
Epoch 1/40
8160/8160 [==============================] - 1s - loss: 0.5121 - acc: 0.7686 - val_loss: 0.5099 - val_acc: 0.7687
Epoch 2/40
8160/8160 [==============================] - 1s - loss: 0.5173 - acc: 0.7661 - val_loss: 0.5085 - val_acc: 0.7678
Epoch 3/40
8160/8160 [==============================] - 1s - loss: 0.5158 - acc: 0.7663 - val_loss: 0.5029 - val_acc: 0.7771
Epoch 4/40
8160/8160 [==============================] - 1s - loss: 0.5097 - acc: 0.7702 - val_loss: 0.5074 - val_acc: 0.7668
Epoch 5/40
8160/8160 [==============================] - 1s - loss: 0.5140 - acc: 0.7674 - val_loss: 0.5028 - val_acc: 0.7712
Epoch 6/40
8160/8160 [==============================] - 1s - loss: 0.5092 - acc: 0.7706 - val_loss: 0.5019 - val_acc: 0.7717
Epoch 7/40
8160/8160 [==============================] - 1s - loss: 0.5077 - acc: 0.7718 - val_loss: 0.5041 - val_acc: 0.7697
Epoch 8/40
8160/8160 [==============================] - 1s - loss: 0.5098 - acc: 0.7690 - val_loss: 0.4995 - val_acc: 0.7741
Epoch 9/40
8160/8160 [==============================] - 1s - loss: 0.5054 - acc: 0.7710 - val_loss: 0.4984 - val_acc: 0.7746
Epoch 10/40
8160/8160 [==============================] - 1s - loss: 0.5049 - acc: 0.7717 - val_loss: 0.4993 - val_acc: 0.7746
Epoch 11/40
8160/8160 [==============================] - 1s - loss: 0.5060 - acc: 0.7721 - val_loss: 0.4954 - val_acc: 0.7815
Epoch 12/40
8160/8160 [==============================] - 1s - loss: 0.5022 - acc: 0.7712 - val_loss: 0.4968 - val_acc: 0.7800
Epoch 13/40
8160/8160 [==============================] - 1s - loss: 0.5036 - acc: 0.7707 - val_loss: 0.4950 - val_acc: 0.7805
Epoch 14/40
8160/8160 [==============================] - 1s - loss: 0.5020 - acc: 0.7724 - val_loss: 0.4932 - val_acc: 0.7795
Epoch 15/40
8160/8160 [==============================] - 1s - loss: 0.5003 - acc: 0.7730 - val_loss: 0.4944 - val_acc: 0.7800
Epoch 16/40
8160/8160 [==============================] - 1s - loss: 0.5014 - acc: 0.7716 - val_loss: 0.4915 - val_acc: 0.7776
Epoch 17/40
8160/8160 [==============================] - 1s - loss: 0.4984 - acc: 0.7745 - val_loss: 0.4924 - val_acc: 0.7805
Epoch 18/40
8160/8160 [==============================] - 1s - loss: 0.4990 - acc: 0.7730 - val_loss: 0.4913 - val_acc: 0.7776
Epoch 19/40
8160/8160 [==============================] - 1s - loss: 0.4977 - acc: 0.7744 - val_loss: 0.4903 - val_acc: 0.7785
Epoch 20/40
8160/8160 [==============================] - 1s - loss: 0.4963 - acc: 0.7757 - val_loss: 0.4908 - val_acc: 0.7810
Epoch 21/40
8160/8160 [==============================] - 1s - loss: 0.4966 - acc: 0.7748 - val_loss: 0.4887 - val_acc: 0.7790
Epoch 22/40
8160/8160 [==============================] - 1s - loss: 0.4946 - acc: 0.7761 - val_loss: 0.4890 - val_acc: 0.7766
Epoch 23/40
8160/8160 [==============================] - 1s - loss: 0.4950 - acc: 0.7750 - val_loss: 0.4875 - val_acc: 0.7785
Epoch 24/40
8160/8160 [==============================] - 1s - loss: 0.4931 - acc: 0.7765 - val_loss: 0.4883 - val_acc: 0.7825
Epoch 25/40
8160/8160 [==============================] - 1s - loss: 0.4934 - acc: 0.7749 - val_loss: 0.4862 - val_acc: 0.7795
Epoch 26/40
8160/8160 [==============================] - 1s - loss: 0.4916 - acc: 0.7744 - val_loss: 0.4858 - val_acc: 0.7805
Epoch 27/40
8160/8160 [==============================] - 1s - loss: 0.4916 - acc: 0.7749 - val_loss: 0.4847 - val_acc: 0.7810
Epoch 28/40
8160/8160 [==============================] - 1s - loss: 0.4902 - acc: 0.7760 - val_loss: 0.4852 - val_acc: 0.7829
Epoch 29/40
8160/8160 [==============================] - 1s - loss: 0.4901 - acc: 0.7755 - val_loss: 0.4836 - val_acc: 0.7825
Epoch 30/40
8160/8160 [==============================] - 1s - loss: 0.4886 - acc: 0.7751 - val_loss: 0.4834 - val_acc: 0.7790
Epoch 31/40
8160/8160 [==============================] - 1s - loss: 0.4885 - acc: 0.7745 - val_loss: 0.4823 - val_acc: 0.7805
Epoch 32/40
8160/8160 [==============================] - 1s - loss: 0.4870 - acc: 0.7751 - val_loss: 0.4825 - val_acc: 0.7839
Epoch 33/40
8160/8160 [==============================] - 1s - loss: 0.4869 - acc: 0.7775 - val_loss: 0.4811 - val_acc: 0.7785
Epoch 34/40
8160/8160 [==============================] - 1s - loss: 0.4856 - acc: 0.7754 - val_loss: 0.4807 - val_acc: 0.7815
Epoch 35/40
8160/8160 [==============================] - 1s - loss: 0.4851 - acc: 0.7755 - val_loss: 0.4804 - val_acc: 0.7820
Epoch 36/40
8160/8160 [==============================] - 1s - loss: 0.4842 - acc: 0.7773 - val_loss: 0.4795 - val_acc: 0.7820
Epoch 37/40
8160/8160 [==============================] - 1s - loss: 0.4832 - acc: 0.7768 - val_loss: 0.4788 - val_acc: 0.7810
Epoch 38/40
8160/8160 [==============================] - 1s - loss: 0.4827 - acc: 0.7770 - val_loss: 0.4779 - val_acc: 0.7820
Epoch 39/40
8160/8160 [==============================] - 1s - loss: 0.4816 - acc: 0.7787 - val_loss: 0.4774 - val_acc: 0.7829
Epoch 40/40
8160/8160 [==============================] - 1s - loss: 0.4809 - acc: 0.7787 - val_loss: 0.4767 - val_acc: 0.7820
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-2.h5
chunk number 2
prepare data
start training
Train on 8182 samples, validate on 2046 samples
Epoch 1/40
8182/8182 [==============================] - 1s - loss: 0.4663 - acc: 0.7911 - val_loss: 0.4704 - val_acc: 0.7859
Epoch 2/40
8182/8182 [==============================] - 1s - loss: 0.4663 - acc: 0.7893 - val_loss: 0.4694 - val_acc: 0.7874
Epoch 3/40
8182/8182 [==============================] - 1s - loss: 0.4644 - acc: 0.7906 - val_loss: 0.4670 - val_acc: 0.7898
Epoch 4/40
8182/8182 [==============================] - 1s - loss: 0.4624 - acc: 0.7899 - val_loss: 0.4659 - val_acc: 0.7864
Epoch 5/40
8182/8182 [==============================] - 1s - loss: 0.4620 - acc: 0.7920 - val_loss: 0.4654 - val_acc: 0.7903
Epoch 6/40
8182/8182 [==============================] - 1s - loss: 0.4604 - acc: 0.7928 - val_loss: 0.4633 - val_acc: 0.7869
Epoch 7/40
8182/8182 [==============================] - 1s - loss: 0.4585 - acc: 0.7911 - val_loss: 0.4625 - val_acc: 0.7893
Epoch 8/40
8182/8182 [==============================] - 1s - loss: 0.4568 - acc: 0.7909 - val_loss: 0.4614 - val_acc: 0.7893
Epoch 9/40
8182/8182 [==============================] - 1s - loss: 0.4554 - acc: 0.7950 - val_loss: 0.4606 - val_acc: 0.7933
Epoch 10/40
8182/8182 [==============================] - 1s - loss: 0.4548 - acc: 0.7955 - val_loss: 0.4617 - val_acc: 0.7889
Epoch 11/40
8182/8182 [==============================] - 1s - loss: 0.4542 - acc: 0.7972 - val_loss: 0.4628 - val_acc: 0.7893
Epoch 12/40
8182/8182 [==============================] - 1s - loss: 0.4566 - acc: 0.7927 - val_loss: 0.4656 - val_acc: 0.7918
Epoch 13/40
8182/8182 [==============================] - 1s - loss: 0.4569 - acc: 0.7944 - val_loss: 0.4710 - val_acc: 0.7889
Epoch 14/40
8182/8182 [==============================] - 1s - loss: 0.4658 - acc: 0.7909 - val_loss: 0.4570 - val_acc: 0.7898
Epoch 15/40
8182/8182 [==============================] - 1s - loss: 0.4493 - acc: 0.7977 - val_loss: 0.4712 - val_acc: 0.7864
Epoch 16/40
8182/8182 [==============================] - 1s - loss: 0.4628 - acc: 0.7895 - val_loss: 0.4976 - val_acc: 0.7830
Epoch 17/40
8182/8182 [==============================] - 1s - loss: 0.4926 - acc: 0.7828 - val_loss: 0.4842 - val_acc: 0.7830
Epoch 18/40
8182/8182 [==============================] - 1s - loss: 0.4782 - acc: 0.7854 - val_loss: 0.4798 - val_acc: 0.7859
Epoch 19/40
8182/8182 [==============================] - 1s - loss: 0.4712 - acc: 0.7892 - val_loss: 0.4576 - val_acc: 0.7952
Epoch 20/40
8182/8182 [==============================] - 1s - loss: 0.4485 - acc: 0.7959 - val_loss: 0.4667 - val_acc: 0.7859
Epoch 21/40
8182/8182 [==============================] - 1s - loss: 0.4587 - acc: 0.7899 - val_loss: 0.4589 - val_acc: 0.7933
Epoch 22/40
8182/8182 [==============================] - 1s - loss: 0.4499 - acc: 0.7948 - val_loss: 0.4681 - val_acc: 0.7923
Epoch 23/40
8182/8182 [==============================] - 1s - loss: 0.4587 - acc: 0.7963 - val_loss: 0.4565 - val_acc: 0.7942
Epoch 24/40
8182/8182 [==============================] - 1s - loss: 0.4476 - acc: 0.7964 - val_loss: 0.4604 - val_acc: 0.7889
Epoch 25/40
8182/8182 [==============================] - 1s - loss: 0.4526 - acc: 0.7947 - val_loss: 0.4544 - val_acc: 0.7942
Epoch 26/40
8182/8182 [==============================] - 1s - loss: 0.4450 - acc: 0.8002 - val_loss: 0.4631 - val_acc: 0.7893
Epoch 27/40
8182/8182 [==============================] - 1s - loss: 0.4524 - acc: 0.7953 - val_loss: 0.4553 - val_acc: 0.7913
Epoch 28/40
8182/8182 [==============================] - 1s - loss: 0.4476 - acc: 0.7972 - val_loss: 0.4558 - val_acc: 0.7933
Epoch 29/40
8182/8182 [==============================] - 1s - loss: 0.4479 - acc: 0.7976 - val_loss: 0.4581 - val_acc: 0.7952
Epoch 30/40
8182/8182 [==============================] - 1s - loss: 0.4460 - acc: 0.7996 - val_loss: 0.4521 - val_acc: 0.7918
Epoch 31/40
8182/8182 [==============================] - 1s - loss: 0.4406 - acc: 0.8031 - val_loss: 0.4538 - val_acc: 0.7957
Epoch 32/40
8182/8182 [==============================] - 1s - loss: 0.4438 - acc: 0.7987 - val_loss: 0.4509 - val_acc: 0.7937
Epoch 33/40
8182/8182 [==============================] - 1s - loss: 0.4393 - acc: 0.8022 - val_loss: 0.4532 - val_acc: 0.7991
Epoch 34/40
8182/8182 [==============================] - 1s - loss: 0.4413 - acc: 0.8010 - val_loss: 0.4528 - val_acc: 0.7952
Epoch 35/40
8182/8182 [==============================] - 1s - loss: 0.4431 - acc: 0.7994 - val_loss: 0.4487 - val_acc: 0.7962
Epoch 36/40
8182/8182 [==============================] - 1s - loss: 0.4377 - acc: 0.8040 - val_loss: 0.4543 - val_acc: 0.7937
Epoch 37/40
8182/8182 [==============================] - 1s - loss: 0.4425 - acc: 0.8013 - val_loss: 0.4537 - val_acc: 0.7962
Epoch 38/40
8182/8182 [==============================] - 1s - loss: 0.4445 - acc: 0.7976 - val_loss: 0.4475 - val_acc: 0.8011
Epoch 39/40
8182/8182 [==============================] - 1s - loss: 0.4367 - acc: 0.8035 - val_loss: 0.4582 - val_acc: 0.7933
Epoch 40/40
8182/8182 [==============================] - 1s - loss: 0.4456 - acc: 0.8007 - val_loss: 0.4569 - val_acc: 0.7977
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-3.h5
chunk number 3
prepare data
start training
Train on 8167 samples, validate on 2042 samples
Epoch 1/40
8167/8167 [==============================] - 1s - loss: 0.4818 - acc: 0.7805 - val_loss: 0.4558 - val_acc: 0.7914
Epoch 2/40
8167/8167 [==============================] - 1s - loss: 0.4738 - acc: 0.7838 - val_loss: 0.4732 - val_acc: 0.7801
Epoch 3/40
8167/8167 [==============================] - 1s - loss: 0.4840 - acc: 0.7757 - val_loss: 0.4534 - val_acc: 0.7889
Epoch 4/40
8167/8167 [==============================] - 1s - loss: 0.4687 - acc: 0.7851 - val_loss: 0.4563 - val_acc: 0.7889
Epoch 5/40
8167/8167 [==============================] - 1s - loss: 0.4746 - acc: 0.7813 - val_loss: 0.4546 - val_acc: 0.7904
Epoch 6/40
8167/8167 [==============================] - 1s - loss: 0.4662 - acc: 0.7882 - val_loss: 0.4633 - val_acc: 0.7943
Epoch 7/40
8167/8167 [==============================] - 1s - loss: 0.4710 - acc: 0.7868 - val_loss: 0.4554 - val_acc: 0.7914
Epoch 8/40
8167/8167 [==============================] - 1s - loss: 0.4681 - acc: 0.7862 - val_loss: 0.4556 - val_acc: 0.7914
Epoch 9/40
8167/8167 [==============================] - 1s - loss: 0.4690 - acc: 0.7847 - val_loss: 0.4566 - val_acc: 0.7948
Epoch 10/40
8167/8167 [==============================] - 1s - loss: 0.4645 - acc: 0.7896 - val_loss: 0.4583 - val_acc: 0.7924
Epoch 11/40
8167/8167 [==============================] - 1s - loss: 0.4661 - acc: 0.7887 - val_loss: 0.4516 - val_acc: 0.7968
Epoch 12/40
8167/8167 [==============================] - 1s - loss: 0.4646 - acc: 0.7868 - val_loss: 0.4511 - val_acc: 0.7958
Epoch 13/40
8167/8167 [==============================] - 1s - loss: 0.4640 - acc: 0.7885 - val_loss: 0.4553 - val_acc: 0.7953
Epoch 14/40
8167/8167 [==============================] - 1s - loss: 0.4621 - acc: 0.7907 - val_loss: 0.4551 - val_acc: 0.7968
Epoch 15/40
8167/8167 [==============================] - 1s - loss: 0.4614 - acc: 0.7903 - val_loss: 0.4497 - val_acc: 0.7977
Epoch 16/40
8167/8167 [==============================] - 1s - loss: 0.4616 - acc: 0.7923 - val_loss: 0.4490 - val_acc: 0.7982
Epoch 17/40
8167/8167 [==============================] - 1s - loss: 0.4598 - acc: 0.7939 - val_loss: 0.4549 - val_acc: 0.7968
Epoch 18/40
8167/8167 [==============================] - 1s - loss: 0.4608 - acc: 0.7915 - val_loss: 0.4495 - val_acc: 0.8026
Epoch 19/40
8167/8167 [==============================] - 1s - loss: 0.4574 - acc: 0.7965 - val_loss: 0.4485 - val_acc: 0.8012
Epoch 20/40
8167/8167 [==============================] - 1s - loss: 0.4590 - acc: 0.7966 - val_loss: 0.4495 - val_acc: 0.7987
Epoch 21/40
8167/8167 [==============================] - 1s - loss: 0.4563 - acc: 0.7971 - val_loss: 0.4534 - val_acc: 0.7977
Epoch 22/40
8167/8167 [==============================] - 1s - loss: 0.4575 - acc: 0.7961 - val_loss: 0.4483 - val_acc: 0.7997
Epoch 23/40
8167/8167 [==============================] - 1s - loss: 0.4563 - acc: 0.7996 - val_loss: 0.4484 - val_acc: 0.8002
Epoch 24/40
8167/8167 [==============================] - 1s - loss: 0.4552 - acc: 0.7992 - val_loss: 0.4536 - val_acc: 0.7997
Epoch 25/40
8167/8167 [==============================] - 1s - loss: 0.4561 - acc: 0.7974 - val_loss: 0.4485 - val_acc: 0.8007
Epoch 26/40
8167/8167 [==============================] - 1s - loss: 0.4539 - acc: 0.7981 - val_loss: 0.4482 - val_acc: 0.8026
Epoch 27/40
8167/8167 [==============================] - 1s - loss: 0.4539 - acc: 0.7991 - val_loss: 0.4518 - val_acc: 0.8012
Epoch 28/40
8167/8167 [==============================] - 1s - loss: 0.4536 - acc: 0.7991 - val_loss: 0.4483 - val_acc: 0.8007
Epoch 29/40
8167/8167 [==============================] - 1s - loss: 0.4520 - acc: 0.7997 - val_loss: 0.4474 - val_acc: 0.7992
Epoch 30/40
8167/8167 [==============================] - 1s - loss: 0.4520 - acc: 0.7987 - val_loss: 0.4498 - val_acc: 0.8036
Epoch 31/40
8167/8167 [==============================] - 1s - loss: 0.4515 - acc: 0.8000 - val_loss: 0.4469 - val_acc: 0.8031
Epoch 32/40
8167/8167 [==============================] - 1s - loss: 0.4503 - acc: 0.8008 - val_loss: 0.4457 - val_acc: 0.8046
Epoch 33/40
8167/8167 [==============================] - 1s - loss: 0.4502 - acc: 0.8008 - val_loss: 0.4476 - val_acc: 0.8046
Epoch 34/40
8167/8167 [==============================] - 1s - loss: 0.4498 - acc: 0.8026 - val_loss: 0.4456 - val_acc: 0.8046
Epoch 35/40
8167/8167 [==============================] - 1s - loss: 0.4489 - acc: 0.8024 - val_loss: 0.4450 - val_acc: 0.8036
Epoch 36/40
8167/8167 [==============================] - 1s - loss: 0.4485 - acc: 0.8018 - val_loss: 0.4464 - val_acc: 0.8046
Epoch 37/40
8167/8167 [==============================] - 1s - loss: 0.4482 - acc: 0.8037 - val_loss: 0.4440 - val_acc: 0.8056
Epoch 38/40
8167/8167 [==============================] - 1s - loss: 0.4476 - acc: 0.8032 - val_loss: 0.4442 - val_acc: 0.8071
Epoch 39/40
8167/8167 [==============================] - 1s - loss: 0.4470 - acc: 0.8024 - val_loss: 0.4455 - val_acc: 0.8075
Epoch 40/40
8167/8167 [==============================] - 1s - loss: 0.4467 - acc: 0.8036 - val_loss: 0.4437 - val_acc: 0.8056
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-4.h5
chunk number 4
prepare data
start training
Train on 8048 samples, validate on 2012 samples
Epoch 1/40
8048/8048 [==============================] - 2s - loss: 0.5572 - acc: 0.7494 - val_loss: 0.5562 - val_acc: 0.7127
Epoch 2/40
8048/8048 [==============================] - 2s - loss: 0.5653 - acc: 0.7156 - val_loss: 0.4798 - val_acc: 0.8017
Epoch 3/40
8048/8048 [==============================] - 2s - loss: 0.4977 - acc: 0.7873 - val_loss: 0.4886 - val_acc: 0.7932
Epoch 4/40
8048/8048 [==============================] - 2s - loss: 0.5051 - acc: 0.7833 - val_loss: 0.4786 - val_acc: 0.7977
Epoch 5/40
8048/8048 [==============================] - 2s - loss: 0.4873 - acc: 0.7849 - val_loss: 0.4959 - val_acc: 0.7908
Epoch 6/40
8048/8048 [==============================] - 2s - loss: 0.5046 - acc: 0.7808 - val_loss: 0.4841 - val_acc: 0.7962
Epoch 7/40
8048/8048 [==============================] - 2s - loss: 0.4957 - acc: 0.7822 - val_loss: 0.4761 - val_acc: 0.7957
Epoch 8/40
8048/8048 [==============================] - 2s - loss: 0.4938 - acc: 0.7786 - val_loss: 0.4751 - val_acc: 0.7932
Epoch 9/40
8048/8048 [==============================] - 2s - loss: 0.4961 - acc: 0.7751 - val_loss: 0.4701 - val_acc: 0.7947
Epoch 10/40
8048/8048 [==============================] - 2s - loss: 0.4910 - acc: 0.7801 - val_loss: 0.4650 - val_acc: 0.7942
Epoch 11/40
8048/8048 [==============================] - 2s - loss: 0.4842 - acc: 0.7848 - val_loss: 0.4671 - val_acc: 0.7888
Epoch 12/40
8048/8048 [==============================] - 2s - loss: 0.4832 - acc: 0.7822 - val_loss: 0.4662 - val_acc: 0.7878
Epoch 13/40
8048/8048 [==============================] - 2s - loss: 0.4825 - acc: 0.7782 - val_loss: 0.4632 - val_acc: 0.7942
Epoch 14/40
8048/8048 [==============================] - 2s - loss: 0.4796 - acc: 0.7780 - val_loss: 0.4624 - val_acc: 0.7942
Epoch 15/40
8048/8048 [==============================] - 2s - loss: 0.4804 - acc: 0.7803 - val_loss: 0.4603 - val_acc: 0.8007
Epoch 16/40
8048/8048 [==============================] - 2s - loss: 0.4778 - acc: 0.7827 - val_loss: 0.4570 - val_acc: 0.7922
Epoch 17/40
8048/8048 [==============================] - 2s - loss: 0.4733 - acc: 0.7860 - val_loss: 0.4601 - val_acc: 0.7927
Epoch 18/40
8048/8048 [==============================] - 2s - loss: 0.4753 - acc: 0.7818 - val_loss: 0.4568 - val_acc: 0.7967
Epoch 19/40
8048/8048 [==============================] - 2s - loss: 0.4721 - acc: 0.7847 - val_loss: 0.4552 - val_acc: 0.7987
Epoch 20/40
8048/8048 [==============================] - 2s - loss: 0.4715 - acc: 0.7857 - val_loss: 0.4550 - val_acc: 0.7967
Epoch 21/40
8048/8048 [==============================] - 2s - loss: 0.4713 - acc: 0.7862 - val_loss: 0.4550 - val_acc: 0.7947
Epoch 22/40
8048/8048 [==============================] - 2s - loss: 0.4692 - acc: 0.7869 - val_loss: 0.4577 - val_acc: 0.7927
Epoch 23/40
8048/8048 [==============================] - 2s - loss: 0.4708 - acc: 0.7853 - val_loss: 0.4550 - val_acc: 0.7947
Epoch 24/40
8048/8048 [==============================] - 2s - loss: 0.4688 - acc: 0.7867 - val_loss: 0.4516 - val_acc: 0.8002
Epoch 25/40
8048/8048 [==============================] - 2s - loss: 0.4673 - acc: 0.7875 - val_loss: 0.4509 - val_acc: 0.7997
Epoch 26/40
8048/8048 [==============================] - 2s - loss: 0.4662 - acc: 0.7904 - val_loss: 0.4504 - val_acc: 0.7972
Epoch 27/40
8048/8048 [==============================] - 2s - loss: 0.4639 - acc: 0.7899 - val_loss: 0.4511 - val_acc: 0.7992
Epoch 28/40
8048/8048 [==============================] - 2s - loss: 0.4636 - acc: 0.7922 - val_loss: 0.4507 - val_acc: 0.8022
Epoch 29/40
8048/8048 [==============================] - 2s - loss: 0.4629 - acc: 0.7915 - val_loss: 0.4508 - val_acc: 0.7987
Epoch 30/40
8048/8048 [==============================] - 2s - loss: 0.4629 - acc: 0.7893 - val_loss: 0.4497 - val_acc: 0.7997
Epoch 31/40
8048/8048 [==============================] - 2s - loss: 0.4616 - acc: 0.7914 - val_loss: 0.4479 - val_acc: 0.7997
Epoch 32/40
8048/8048 [==============================] - 2s - loss: 0.4597 - acc: 0.7939 - val_loss: 0.4474 - val_acc: 0.7997
Epoch 33/40
8048/8048 [==============================] - 2s - loss: 0.4587 - acc: 0.7966 - val_loss: 0.4466 - val_acc: 0.8017
Epoch 34/40
8048/8048 [==============================] - 2s - loss: 0.4581 - acc: 0.7965 - val_loss: 0.4470 - val_acc: 0.8012
Epoch 35/40
8048/8048 [==============================] - 2s - loss: 0.4576 - acc: 0.7954 - val_loss: 0.4488 - val_acc: 0.8027
Epoch 36/40
8048/8048 [==============================] - 2s - loss: 0.4573 - acc: 0.7970 - val_loss: 0.4485 - val_acc: 0.8007
Epoch 37/40
8048/8048 [==============================] - 2s - loss: 0.4568 - acc: 0.7963 - val_loss: 0.4477 - val_acc: 0.8032
Epoch 38/40
8048/8048 [==============================] - 2s - loss: 0.4565 - acc: 0.7944 - val_loss: 0.4476 - val_acc: 0.8027
Epoch 39/40
8048/8048 [==============================] - 2s - loss: 0.4553 - acc: 0.7956 - val_loss: 0.4474 - val_acc: 0.8042
Epoch 40/40
8048/8048 [==============================] - 2s - loss: 0.4546 - acc: 0.7961 - val_loss: 0.4456 - val_acc: 0.8037
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-5.h5
chunk number 5
prepare data
start training
Train on 8117 samples, validate on 2030 samples
Epoch 1/40
8117/8117 [==============================] - 1s - loss: 0.4839 - acc: 0.7870 - val_loss: 0.4566 - val_acc: 0.7916
Epoch 2/40
8117/8117 [==============================] - 1s - loss: 0.4710 - acc: 0.7875 - val_loss: 0.4686 - val_acc: 0.7847
Epoch 3/40
8117/8117 [==============================] - 1s - loss: 0.4867 - acc: 0.7806 - val_loss: 0.4533 - val_acc: 0.7931
Epoch 4/40
8117/8117 [==============================] - 1s - loss: 0.4699 - acc: 0.7890 - val_loss: 0.4502 - val_acc: 0.8015
Epoch 5/40
8117/8117 [==============================] - 1s - loss: 0.4589 - acc: 0.7968 - val_loss: 0.4756 - val_acc: 0.7867
Epoch 6/40
8117/8117 [==============================] - 1s - loss: 0.4791 - acc: 0.7846 - val_loss: 0.4453 - val_acc: 0.8079
Epoch 7/40
8117/8117 [==============================] - 1s - loss: 0.4569 - acc: 0.7983 - val_loss: 0.4550 - val_acc: 0.7985
Epoch 8/40
8117/8117 [==============================] - 1s - loss: 0.4699 - acc: 0.7870 - val_loss: 0.4537 - val_acc: 0.8020
Epoch 9/40
8117/8117 [==============================] - 1s - loss: 0.4683 - acc: 0.7888 - val_loss: 0.4446 - val_acc: 0.8039
Epoch 10/40
8117/8117 [==============================] - 1s - loss: 0.4556 - acc: 0.8003 - val_loss: 0.4584 - val_acc: 0.7951
Epoch 11/40
8117/8117 [==============================] - 1s - loss: 0.4626 - acc: 0.7924 - val_loss: 0.4501 - val_acc: 0.7985
Epoch 12/40
8117/8117 [==============================] - 1s - loss: 0.4554 - acc: 0.7981 - val_loss: 0.4420 - val_acc: 0.8034
Epoch 13/40
8117/8117 [==============================] - 1s - loss: 0.4535 - acc: 0.8004 - val_loss: 0.4454 - val_acc: 0.7990
Epoch 14/40
8117/8117 [==============================] - 1s - loss: 0.4588 - acc: 0.7975 - val_loss: 0.4425 - val_acc: 0.8005
Epoch 15/40
8117/8117 [==============================] - 1s - loss: 0.4541 - acc: 0.7973 - val_loss: 0.4461 - val_acc: 0.8049
Epoch 16/40
8117/8117 [==============================] - 1s - loss: 0.4520 - acc: 0.7992 - val_loss: 0.4525 - val_acc: 0.7975
Epoch 17/40
8117/8117 [==============================] - 1s - loss: 0.4558 - acc: 0.7968 - val_loss: 0.4423 - val_acc: 0.8010
Epoch 18/40
8117/8117 [==============================] - 1s - loss: 0.4499 - acc: 0.8007 - val_loss: 0.4416 - val_acc: 0.8059
Epoch 19/40
8117/8117 [==============================] - 1s - loss: 0.4525 - acc: 0.8004 - val_loss: 0.4402 - val_acc: 0.8079
Epoch 20/40
8117/8117 [==============================] - 1s - loss: 0.4514 - acc: 0.7988 - val_loss: 0.4393 - val_acc: 0.8049
Epoch 21/40
8117/8117 [==============================] - 1s - loss: 0.4470 - acc: 0.8017 - val_loss: 0.4472 - val_acc: 0.8039
Epoch 22/40
8117/8117 [==============================] - 1s - loss: 0.4510 - acc: 0.8012 - val_loss: 0.4402 - val_acc: 0.8054
Epoch 23/40
8117/8117 [==============================] - 1s - loss: 0.4469 - acc: 0.8015 - val_loss: 0.4370 - val_acc: 0.8123
Epoch 24/40
8117/8117 [==============================] - 1s - loss: 0.4482 - acc: 0.7996 - val_loss: 0.4366 - val_acc: 0.8143
Epoch 25/40
8117/8117 [==============================] - 1s - loss: 0.4481 - acc: 0.7998 - val_loss: 0.4380 - val_acc: 0.8044
Epoch 26/40
8117/8117 [==============================] - 1s - loss: 0.4457 - acc: 0.8014 - val_loss: 0.4429 - val_acc: 0.8015
Epoch 27/40
8117/8117 [==============================] - 1s - loss: 0.4478 - acc: 0.8026 - val_loss: 0.4369 - val_acc: 0.8044
Epoch 28/40
8117/8117 [==============================] - 1s - loss: 0.4444 - acc: 0.8019 - val_loss: 0.4357 - val_acc: 0.8113
Epoch 29/40
8117/8117 [==============================] - 1s - loss: 0.4459 - acc: 0.8019 - val_loss: 0.4358 - val_acc: 0.8064
Epoch 30/40
8117/8117 [==============================] - 1s - loss: 0.4443 - acc: 0.8013 - val_loss: 0.4398 - val_acc: 0.8044
Epoch 31/40
8117/8117 [==============================] - 1s - loss: 0.4443 - acc: 0.8034 - val_loss: 0.4400 - val_acc: 0.8044
Epoch 32/40
8117/8117 [==============================] - 1s - loss: 0.4440 - acc: 0.8026 - val_loss: 0.4364 - val_acc: 0.8059
Epoch 33/40
8117/8117 [==============================] - 1s - loss: 0.4432 - acc: 0.8037 - val_loss: 0.4364 - val_acc: 0.8079
Epoch 34/40
8117/8117 [==============================] - 1s - loss: 0.4437 - acc: 0.8031 - val_loss: 0.4378 - val_acc: 0.8064
Epoch 35/40
8117/8117 [==============================] - 1s - loss: 0.4420 - acc: 0.8036 - val_loss: 0.4404 - val_acc: 0.8034
Epoch 36/40
8117/8117 [==============================] - 1s - loss: 0.4427 - acc: 0.8037 - val_loss: 0.4368 - val_acc: 0.8094
Epoch 37/40
8117/8117 [==============================] - 1s - loss: 0.4412 - acc: 0.8052 - val_loss: 0.4355 - val_acc: 0.8094
Epoch 38/40
8117/8117 [==============================] - 1s - loss: 0.4418 - acc: 0.8051 - val_loss: 0.4358 - val_acc: 0.8064
Epoch 39/40
8117/8117 [==============================] - 1s - loss: 0.4405 - acc: 0.8047 - val_loss: 0.4381 - val_acc: 0.8074
Epoch 40/40
8117/8117 [==============================] - 1s - loss: 0.4411 - acc: 0.8066 - val_loss: 0.4359 - val_acc: 0.8059
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-6.h5
chunk number 6
prepare data
start training
Train on 8182 samples, validate on 2046 samples
Epoch 1/40
8182/8182 [==============================] - 1s - loss: 0.4542 - acc: 0.8002 - val_loss: 0.4559 - val_acc: 0.7962
Epoch 2/40
8182/8182 [==============================] - 1s - loss: 0.4466 - acc: 0.8013 - val_loss: 0.4644 - val_acc: 0.7903
Epoch 3/40
8182/8182 [==============================] - 1s - loss: 0.4565 - acc: 0.7953 - val_loss: 0.4568 - val_acc: 0.7962
Epoch 4/40
8182/8182 [==============================] - 1s - loss: 0.4480 - acc: 0.8011 - val_loss: 0.4550 - val_acc: 0.8030
Epoch 5/40
8182/8182 [==============================] - 1s - loss: 0.4446 - acc: 0.8049 - val_loss: 0.4631 - val_acc: 0.7967
Epoch 6/40
8182/8182 [==============================] - 1s - loss: 0.4519 - acc: 0.8054 - val_loss: 0.4501 - val_acc: 0.8035
Epoch 7/40
8182/8182 [==============================] - 1s - loss: 0.4414 - acc: 0.8060 - val_loss: 0.4537 - val_acc: 0.7967
Epoch 8/40
8182/8182 [==============================] - 1s - loss: 0.4467 - acc: 0.8026 - val_loss: 0.4525 - val_acc: 0.7981
Epoch 9/40
8182/8182 [==============================] - 1s - loss: 0.4457 - acc: 0.8032 - val_loss: 0.4477 - val_acc: 0.8035
Epoch 10/40
8182/8182 [==============================] - 1s - loss: 0.4398 - acc: 0.8052 - val_loss: 0.4547 - val_acc: 0.8001
Epoch 11/40
8182/8182 [==============================] - 1s - loss: 0.4454 - acc: 0.8069 - val_loss: 0.4478 - val_acc: 0.8065
Epoch 12/40
8182/8182 [==============================] - 1s - loss: 0.4393 - acc: 0.8070 - val_loss: 0.4483 - val_acc: 0.8016
Epoch 13/40
8182/8182 [==============================] - 1s - loss: 0.4407 - acc: 0.8062 - val_loss: 0.4490 - val_acc: 0.8040
Epoch 14/40
8182/8182 [==============================] - 1s - loss: 0.4413 - acc: 0.8049 - val_loss: 0.4460 - val_acc: 0.8035
Epoch 15/40
8182/8182 [==============================] - 1s - loss: 0.4371 - acc: 0.8080 - val_loss: 0.4505 - val_acc: 0.8011
Epoch 16/40
8182/8182 [==============================] - 1s - loss: 0.4405 - acc: 0.8055 - val_loss: 0.4468 - val_acc: 0.8040
Epoch 17/40
8182/8182 [==============================] - 1s - loss: 0.4369 - acc: 0.8077 - val_loss: 0.4470 - val_acc: 0.8040
Epoch 18/40
8182/8182 [==============================] - 1s - loss: 0.4376 - acc: 0.8079 - val_loss: 0.4473 - val_acc: 0.8065
Epoch 19/40
8182/8182 [==============================] - 1s - loss: 0.4377 - acc: 0.8076 - val_loss: 0.4461 - val_acc: 0.8045
Epoch 20/40
8182/8182 [==============================] - 1s - loss: 0.4351 - acc: 0.8087 - val_loss: 0.4492 - val_acc: 0.8035
Epoch 21/40
8182/8182 [==============================] - 1s - loss: 0.4371 - acc: 0.8077 - val_loss: 0.4463 - val_acc: 0.8040
Epoch 22/40
8182/8182 [==============================] - 1s - loss: 0.4344 - acc: 0.8098 - val_loss: 0.4468 - val_acc: 0.8045
Epoch 23/40
8182/8182 [==============================] - 1s - loss: 0.4354 - acc: 0.8090 - val_loss: 0.4464 - val_acc: 0.8060
Epoch 24/40
8182/8182 [==============================] - 1s - loss: 0.4346 - acc: 0.8088 - val_loss: 0.4470 - val_acc: 0.8040
Epoch 25/40
8182/8182 [==============================] - 1s - loss: 0.4339 - acc: 0.8087 - val_loss: 0.4481 - val_acc: 0.8045
Epoch 26/40
8182/8182 [==============================] - 1s - loss: 0.4344 - acc: 0.8090 - val_loss: 0.4462 - val_acc: 0.8040
Epoch 27/40
8182/8182 [==============================] - 1s - loss: 0.4329 - acc: 0.8092 - val_loss: 0.4467 - val_acc: 0.8035
Epoch 28/40
8182/8182 [==============================] - 1s - loss: 0.4337 - acc: 0.8090 - val_loss: 0.4461 - val_acc: 0.8030
Epoch 29/40
8182/8182 [==============================] - 1s - loss: 0.4322 - acc: 0.8098 - val_loss: 0.4476 - val_acc: 0.8045
Epoch 30/40
8182/8182 [==============================] - 1s - loss: 0.4327 - acc: 0.8091 - val_loss: 0.4466 - val_acc: 0.8040
Epoch 31/40
8182/8182 [==============================] - 1s - loss: 0.4317 - acc: 0.8109 - val_loss: 0.4461 - val_acc: 0.8050
Epoch 32/40
8182/8182 [==============================] - 1s - loss: 0.4317 - acc: 0.8101 - val_loss: 0.4460 - val_acc: 0.8040
Epoch 33/40
8182/8182 [==============================] - 1s - loss: 0.4313 - acc: 0.8098 - val_loss: 0.4467 - val_acc: 0.8055
Epoch 34/40
8182/8182 [==============================] - 1s - loss: 0.4308 - acc: 0.8117 - val_loss: 0.4472 - val_acc: 0.8035
Epoch 35/40
8182/8182 [==============================] - 1s - loss: 0.4308 - acc: 0.8109 - val_loss: 0.4461 - val_acc: 0.8040
Epoch 36/40
8182/8182 [==============================] - 1s - loss: 0.4302 - acc: 0.8130 - val_loss: 0.4462 - val_acc: 0.8060
Epoch 37/40
8182/8182 [==============================] - 1s - loss: 0.4303 - acc: 0.8113 - val_loss: 0.4465 - val_acc: 0.8035
Epoch 38/40
8182/8182 [==============================] - 1s - loss: 0.4296 - acc: 0.8128 - val_loss: 0.4473 - val_acc: 0.8021
Epoch 39/40
8182/8182 [==============================] - 1s - loss: 0.4297 - acc: 0.8124 - val_loss: 0.4464 - val_acc: 0.8045
Epoch 40/40
8182/8182 [==============================] - 1s - loss: 0.4291 - acc: 0.8128 - val_loss: 0.4464 - val_acc: 0.8050
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-7.h5
chunk number 7
prepare data
start training
Train on 8116 samples, validate on 2030 samples
Epoch 1/40
8116/8116 [==============================] - 1s - loss: 0.4751 - acc: 0.7873 - val_loss: 0.4937 - val_acc: 0.7695
Epoch 2/40
8116/8116 [==============================] - 1s - loss: 0.4641 - acc: 0.7926 - val_loss: 0.4916 - val_acc: 0.7709
Epoch 3/40
8116/8116 [==============================] - 1s - loss: 0.4620 - acc: 0.7904 - val_loss: 0.4911 - val_acc: 0.7857
Epoch 4/40
8116/8116 [==============================] - 1s - loss: 0.4614 - acc: 0.7947 - val_loss: 0.4875 - val_acc: 0.7916
Epoch 5/40
8116/8116 [==============================] - 1s - loss: 0.4578 - acc: 0.7952 - val_loss: 0.4858 - val_acc: 0.7744
Epoch 6/40
8116/8116 [==============================] - 1s - loss: 0.4550 - acc: 0.7962 - val_loss: 0.4848 - val_acc: 0.7768
Epoch 7/40
8116/8116 [==============================] - 1s - loss: 0.4537 - acc: 0.7958 - val_loss: 0.4825 - val_acc: 0.7828
Epoch 8/40
8116/8116 [==============================] - 1s - loss: 0.4537 - acc: 0.7958 - val_loss: 0.4828 - val_acc: 0.7842
Epoch 9/40
8116/8116 [==============================] - 1s - loss: 0.4540 - acc: 0.7940 - val_loss: 0.4811 - val_acc: 0.7744
Epoch 10/40
8116/8116 [==============================] - 1s - loss: 0.4482 - acc: 0.7981 - val_loss: 0.4866 - val_acc: 0.7690
Epoch 11/40
8116/8116 [==============================] - 1s - loss: 0.4520 - acc: 0.7963 - val_loss: 0.4802 - val_acc: 0.7813
Epoch 12/40
8116/8116 [==============================] - 1s - loss: 0.4455 - acc: 0.7990 - val_loss: 0.4837 - val_acc: 0.7862
Epoch 13/40
8116/8116 [==============================] - 1s - loss: 0.4494 - acc: 0.7962 - val_loss: 0.4806 - val_acc: 0.7867
Epoch 14/40
8116/8116 [==============================] - 1s - loss: 0.4442 - acc: 0.7979 - val_loss: 0.4846 - val_acc: 0.7788
Epoch 15/40
8116/8116 [==============================] - 1s - loss: 0.4470 - acc: 0.8019 - val_loss: 0.4812 - val_acc: 0.7798
Epoch 16/40
8116/8116 [==============================] - 1s - loss: 0.4437 - acc: 0.8017 - val_loss: 0.4811 - val_acc: 0.7823
Epoch 17/40
8116/8116 [==============================] - 1s - loss: 0.4445 - acc: 0.7983 - val_loss: 0.4812 - val_acc: 0.7823
Epoch 18/40
8116/8116 [==============================] - 1s - loss: 0.4440 - acc: 0.7976 - val_loss: 0.4812 - val_acc: 0.7803
Epoch 19/40
8116/8116 [==============================] - 1s - loss: 0.4420 - acc: 0.8037 - val_loss: 0.4834 - val_acc: 0.7798
Epoch 20/40
8116/8116 [==============================] - 1s - loss: 0.4435 - acc: 0.8046 - val_loss: 0.4808 - val_acc: 0.7842
Epoch 21/40
8116/8116 [==============================] - 1s - loss: 0.4408 - acc: 0.8024 - val_loss: 0.4818 - val_acc: 0.7847
Epoch 22/40
8116/8116 [==============================] - 1s - loss: 0.4424 - acc: 0.7997 - val_loss: 0.4804 - val_acc: 0.7823
Epoch 23/40
8116/8116 [==============================] - 1s - loss: 0.4399 - acc: 0.8030 - val_loss: 0.4824 - val_acc: 0.7773
Epoch 24/40
8116/8116 [==============================] - 1s - loss: 0.4413 - acc: 0.8040 - val_loss: 0.4801 - val_acc: 0.7798
Epoch 25/40
8116/8116 [==============================] - 1s - loss: 0.4391 - acc: 0.8059 - val_loss: 0.4799 - val_acc: 0.7872
Epoch 26/40
8116/8116 [==============================] - 1s - loss: 0.4400 - acc: 0.8030 - val_loss: 0.4801 - val_acc: 0.7803
Epoch 27/40
8116/8116 [==============================] - 1s - loss: 0.4384 - acc: 0.8042 - val_loss: 0.4827 - val_acc: 0.7798
Epoch 28/40
8116/8116 [==============================] - 1s - loss: 0.4388 - acc: 0.8052 - val_loss: 0.4818 - val_acc: 0.7803
Epoch 29/40
8116/8116 [==============================] - 1s - loss: 0.4375 - acc: 0.8059 - val_loss: 0.4814 - val_acc: 0.7813
Epoch 30/40
8116/8116 [==============================] - 1s - loss: 0.4378 - acc: 0.8043 - val_loss: 0.4810 - val_acc: 0.7803
Epoch 31/40
8116/8116 [==============================] - 1s - loss: 0.4369 - acc: 0.8057 - val_loss: 0.4817 - val_acc: 0.7798
Epoch 32/40
8116/8116 [==============================] - 1s - loss: 0.4370 - acc: 0.8045 - val_loss: 0.4806 - val_acc: 0.7778
Epoch 33/40
8116/8116 [==============================] - 1s - loss: 0.4362 - acc: 0.8057 - val_loss: 0.4803 - val_acc: 0.7808
Epoch 34/40
8116/8116 [==============================] - 1s - loss: 0.4363 - acc: 0.8056 - val_loss: 0.4800 - val_acc: 0.7803
Epoch 35/40
8116/8116 [==============================] - 1s - loss: 0.4355 - acc: 0.8073 - val_loss: 0.4805 - val_acc: 0.7764
Epoch 36/40
8116/8116 [==============================] - 1s - loss: 0.4355 - acc: 0.8059 - val_loss: 0.4799 - val_acc: 0.7783
Epoch 37/40
8116/8116 [==============================] - 1s - loss: 0.4347 - acc: 0.8069 - val_loss: 0.4799 - val_acc: 0.7798
Epoch 38/40
8116/8116 [==============================] - 1s - loss: 0.4348 - acc: 0.8080 - val_loss: 0.4795 - val_acc: 0.7803
Epoch 39/40
8116/8116 [==============================] - 1s - loss: 0.4341 - acc: 0.8067 - val_loss: 0.4800 - val_acc: 0.7768
Epoch 40/40
8116/8116 [==============================] - 1s - loss: 0.4342 - acc: 0.8066 - val_loss: 0.4793 - val_acc: 0.7798
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-8.h5
chunk number 8
prepare data
start training
Train on 8086 samples, validate on 2022 samples
Epoch 1/40
8086/8086 [==============================] - 2s - loss: 0.4923 - acc: 0.7717 - val_loss: 0.4763 - val_acc: 0.7864
Epoch 2/40
8086/8086 [==============================] - 2s - loss: 0.4660 - acc: 0.7844 - val_loss: 0.4881 - val_acc: 0.7829
Epoch 3/40
8086/8086 [==============================] - 2s - loss: 0.4752 - acc: 0.7791 - val_loss: 0.4695 - val_acc: 0.7992
Epoch 4/40
8086/8086 [==============================] - 2s - loss: 0.4654 - acc: 0.7917 - val_loss: 0.4748 - val_acc: 0.7923
Epoch 5/40
8086/8086 [==============================] - 2s - loss: 0.4730 - acc: 0.7865 - val_loss: 0.4617 - val_acc: 0.7918
Epoch 6/40
8086/8086 [==============================] - 2s - loss: 0.4556 - acc: 0.7930 - val_loss: 0.4798 - val_acc: 0.7859
Epoch 7/40
8086/8086 [==============================] - 2s - loss: 0.4734 - acc: 0.7910 - val_loss: 0.4583 - val_acc: 0.7918
Epoch 8/40
8086/8086 [==============================] - 2s - loss: 0.4557 - acc: 0.7932 - val_loss: 0.4642 - val_acc: 0.7987
Epoch 9/40
8086/8086 [==============================] - 2s - loss: 0.4637 - acc: 0.7879 - val_loss: 0.4614 - val_acc: 0.7987
Epoch 10/40
8086/8086 [==============================] - 2s - loss: 0.4598 - acc: 0.7910 - val_loss: 0.4602 - val_acc: 0.7953
Epoch 11/40
8086/8086 [==============================] - 2s - loss: 0.4542 - acc: 0.7920 - val_loss: 0.4712 - val_acc: 0.7908
Epoch 12/40
8086/8086 [==============================] - 2s - loss: 0.4600 - acc: 0.7931 - val_loss: 0.4610 - val_acc: 0.7982
Epoch 13/40
8086/8086 [==============================] - 2s - loss: 0.4510 - acc: 0.7947 - val_loss: 0.4651 - val_acc: 0.7898
Epoch 14/40
8086/8086 [==============================] - 2s - loss: 0.4572 - acc: 0.7889 - val_loss: 0.4623 - val_acc: 0.7943
Epoch 15/40
8086/8086 [==============================] - 2s - loss: 0.4521 - acc: 0.7935 - val_loss: 0.4664 - val_acc: 0.7948
Epoch 16/40
8086/8086 [==============================] - 2s - loss: 0.4508 - acc: 0.7929 - val_loss: 0.4689 - val_acc: 0.7913
Epoch 17/40
8086/8086 [==============================] - 2s - loss: 0.4529 - acc: 0.7919 - val_loss: 0.4609 - val_acc: 0.7928
Epoch 18/40
8086/8086 [==============================] - 2s - loss: 0.4479 - acc: 0.7942 - val_loss: 0.4602 - val_acc: 0.7908
Epoch 19/40
8086/8086 [==============================] - 2s - loss: 0.4510 - acc: 0.7925 - val_loss: 0.4612 - val_acc: 0.7933
Epoch 20/40
8086/8086 [==============================] - 2s - loss: 0.4449 - acc: 0.7969 - val_loss: 0.4687 - val_acc: 0.7898
Epoch 21/40
8086/8086 [==============================] - 2s - loss: 0.4495 - acc: 0.7968 - val_loss: 0.4614 - val_acc: 0.7972
Epoch 22/40
8086/8086 [==============================] - 2s - loss: 0.4435 - acc: 0.7985 - val_loss: 0.4605 - val_acc: 0.7957
Epoch 23/40
8086/8086 [==============================] - 2s - loss: 0.4470 - acc: 0.7958 - val_loss: 0.4598 - val_acc: 0.7982
Epoch 24/40
8086/8086 [==============================] - 2s - loss: 0.4427 - acc: 0.7989 - val_loss: 0.4642 - val_acc: 0.7962
Epoch 25/40
8086/8086 [==============================] - 2s - loss: 0.4454 - acc: 0.7976 - val_loss: 0.4599 - val_acc: 0.7987
Epoch 26/40
8086/8086 [==============================] - 2s - loss: 0.4415 - acc: 0.7990 - val_loss: 0.4603 - val_acc: 0.7972
Epoch 27/40
8086/8086 [==============================] - 2s - loss: 0.4434 - acc: 0.7980 - val_loss: 0.4605 - val_acc: 0.8022
Epoch 28/40
8086/8086 [==============================] - 2s - loss: 0.4409 - acc: 0.8010 - val_loss: 0.4626 - val_acc: 0.7972
Epoch 29/40
8086/8086 [==============================] - 2s - loss: 0.4419 - acc: 0.7984 - val_loss: 0.4599 - val_acc: 0.7987
Epoch 30/40
8086/8086 [==============================] - 2s - loss: 0.4396 - acc: 0.8011 - val_loss: 0.4594 - val_acc: 0.7967
Epoch 31/40
8086/8086 [==============================] - 2s - loss: 0.4404 - acc: 0.7987 - val_loss: 0.4604 - val_acc: 0.7982
Epoch 32/40
8086/8086 [==============================] - 2s - loss: 0.4389 - acc: 0.8010 - val_loss: 0.4612 - val_acc: 0.7962
Epoch 33/40
8086/8086 [==============================] - 2s - loss: 0.4390 - acc: 0.8001 - val_loss: 0.4596 - val_acc: 0.7987
Epoch 34/40
8086/8086 [==============================] - 2s - loss: 0.4391 - acc: 0.8018 - val_loss: 0.4594 - val_acc: 0.7972
Epoch 35/40
8086/8086 [==============================] - 2s - loss: 0.4379 - acc: 0.8036 - val_loss: 0.4607 - val_acc: 0.7972
Epoch 36/40
8086/8086 [==============================] - 2s - loss: 0.4384 - acc: 0.8009 - val_loss: 0.4577 - val_acc: 0.8017
Epoch 37/40
8086/8086 [==============================] - 2s - loss: 0.4370 - acc: 0.8037 - val_loss: 0.4572 - val_acc: 0.8022
Epoch 38/40
8086/8086 [==============================] - 2s - loss: 0.4370 - acc: 0.8032 - val_loss: 0.4587 - val_acc: 0.8012
Epoch 39/40
8086/8086 [==============================] - 2s - loss: 0.4365 - acc: 0.8041 - val_loss: 0.4582 - val_acc: 0.8007
Epoch 40/40
8086/8086 [==============================] - 2s - loss: 0.4357 - acc: 0.8044 - val_loss: 0.4573 - val_acc: 0.7982
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-9.h5
chunk number 9
prepare data
start training
Train on 8220 samples, validate on 2055 samples
Epoch 1/40
8220/8220 [==============================] - 2s - loss: 0.4580 - acc: 0.7959 - val_loss: 0.4483 - val_acc: 0.8039
Epoch 2/40
8220/8220 [==============================] - 2s - loss: 0.4515 - acc: 0.8006 - val_loss: 0.4553 - val_acc: 0.7976
Epoch 3/40
8220/8220 [==============================] - 2s - loss: 0.4552 - acc: 0.7973 - val_loss: 0.4460 - val_acc: 0.7995
Epoch 4/40
8220/8220 [==============================] - 2s - loss: 0.4490 - acc: 0.8026 - val_loss: 0.4471 - val_acc: 0.7981
Epoch 5/40
8220/8220 [==============================] - 2s - loss: 0.4503 - acc: 0.7999 - val_loss: 0.4459 - val_acc: 0.7966
Epoch 6/40
8220/8220 [==============================] - 2s - loss: 0.4463 - acc: 0.8021 - val_loss: 0.4494 - val_acc: 0.8029
Epoch 7/40
8220/8220 [==============================] - 2s - loss: 0.4470 - acc: 0.8022 - val_loss: 0.4437 - val_acc: 0.8019
Epoch 8/40
8220/8220 [==============================] - 2s - loss: 0.4443 - acc: 0.8023 - val_loss: 0.4424 - val_acc: 0.8010
Epoch 9/40
8220/8220 [==============================] - 2s - loss: 0.4443 - acc: 0.8027 - val_loss: 0.4426 - val_acc: 0.8058
Epoch 10/40
8220/8220 [==============================] - 2s - loss: 0.4435 - acc: 0.8039 - val_loss: 0.4439 - val_acc: 0.8073
Epoch 11/40
8220/8220 [==============================] - 2s - loss: 0.4437 - acc: 0.8016 - val_loss: 0.4412 - val_acc: 0.8083
Epoch 12/40
8220/8220 [==============================] - 2s - loss: 0.4414 - acc: 0.8035 - val_loss: 0.4404 - val_acc: 0.8078
Epoch 13/40
8220/8220 [==============================] - 2s - loss: 0.4413 - acc: 0.8038 - val_loss: 0.4417 - val_acc: 0.8088
Epoch 14/40
8220/8220 [==============================] - 2s - loss: 0.4408 - acc: 0.8029 - val_loss: 0.4426 - val_acc: 0.8117
Epoch 15/40
8220/8220 [==============================] - 2s - loss: 0.4405 - acc: 0.8036 - val_loss: 0.4414 - val_acc: 0.8107
Epoch 16/40
8220/8220 [==============================] - 2s - loss: 0.4392 - acc: 0.8044 - val_loss: 0.4417 - val_acc: 0.8107
Epoch 17/40
8220/8220 [==============================] - 2s - loss: 0.4391 - acc: 0.8036 - val_loss: 0.4425 - val_acc: 0.8107
Epoch 18/40
8220/8220 [==============================] - 2s - loss: 0.4384 - acc: 0.8039 - val_loss: 0.4432 - val_acc: 0.8092
Epoch 19/40
8220/8220 [==============================] - 2s - loss: 0.4381 - acc: 0.8046 - val_loss: 0.4435 - val_acc: 0.8097
Epoch 20/40
8220/8220 [==============================] - 2s - loss: 0.4378 - acc: 0.8044 - val_loss: 0.4429 - val_acc: 0.8088
Epoch 21/40
8220/8220 [==============================] - 2s - loss: 0.4368 - acc: 0.8040 - val_loss: 0.4430 - val_acc: 0.8112
Epoch 22/40
8220/8220 [==============================] - 2s - loss: 0.4368 - acc: 0.8038 - val_loss: 0.4421 - val_acc: 0.8107
Epoch 23/40
8220/8220 [==============================] - 2s - loss: 0.4367 - acc: 0.8050 - val_loss: 0.4432 - val_acc: 0.8117
Epoch 24/40
8220/8220 [==============================] - 2s - loss: 0.4363 - acc: 0.8066 - val_loss: 0.4421 - val_acc: 0.8073
Epoch 25/40
8220/8220 [==============================] - 2s - loss: 0.4358 - acc: 0.8069 - val_loss: 0.4427 - val_acc: 0.8102
Epoch 26/40
8220/8220 [==============================] - 2s - loss: 0.4351 - acc: 0.8061 - val_loss: 0.4429 - val_acc: 0.8092
Epoch 27/40
8220/8220 [==============================] - 2s - loss: 0.4348 - acc: 0.8064 - val_loss: 0.4429 - val_acc: 0.8068
Epoch 28/40
8220/8220 [==============================] - 2s - loss: 0.4346 - acc: 0.8080 - val_loss: 0.4434 - val_acc: 0.8097
Epoch 29/40
8220/8220 [==============================] - 2s - loss: 0.4341 - acc: 0.8088 - val_loss: 0.4428 - val_acc: 0.8097
Epoch 30/40
8220/8220 [==============================] - 2s - loss: 0.4336 - acc: 0.8084 - val_loss: 0.4429 - val_acc: 0.8102
Epoch 31/40
8220/8220 [==============================] - 2s - loss: 0.4334 - acc: 0.8085 - val_loss: 0.4437 - val_acc: 0.8092
Epoch 32/40
8220/8220 [==============================] - 2s - loss: 0.4331 - acc: 0.8085 - val_loss: 0.4437 - val_acc: 0.8102
Epoch 33/40
8220/8220 [==============================] - 2s - loss: 0.4328 - acc: 0.8088 - val_loss: 0.4440 - val_acc: 0.8097
Epoch 34/40
8220/8220 [==============================] - 2s - loss: 0.4325 - acc: 0.8090 - val_loss: 0.4444 - val_acc: 0.8092
Epoch 35/40
8220/8220 [==============================] - 2s - loss: 0.4323 - acc: 0.8092 - val_loss: 0.4434 - val_acc: 0.8092
Epoch 36/40
8220/8220 [==============================] - 2s - loss: 0.4320 - acc: 0.8091 - val_loss: 0.4447 - val_acc: 0.8092
Epoch 37/40
8220/8220 [==============================] - 2s - loss: 0.4320 - acc: 0.8095 - val_loss: 0.4425 - val_acc: 0.8083
Epoch 38/40
8220/8220 [==============================] - 2s - loss: 0.4321 - acc: 0.8079 - val_loss: 0.4440 - val_acc: 0.8068
Epoch 39/40
8220/8220 [==============================] - 2s - loss: 0.4312 - acc: 0.8099 - val_loss: 0.4439 - val_acc: 0.8063
Epoch 40/40
8220/8220 [==============================] - 2s - loss: 0.4307 - acc: 0.8092 - val_loss: 0.4435 - val_acc: 0.8083
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-10.h5
chunk number 10
prepare data
start training
Train on 8132 samples, validate on 2033 samples
Epoch 1/40
8132/8132 [==============================] - 1s - loss: 0.4883 - acc: 0.7834 - val_loss: 0.4508 - val_acc: 0.7983
Epoch 2/40
8132/8132 [==============================] - 1s - loss: 0.4610 - acc: 0.7912 - val_loss: 0.4826 - val_acc: 0.7796
Epoch 3/40
8132/8132 [==============================] - 1s - loss: 0.4893 - acc: 0.7748 - val_loss: 0.4655 - val_acc: 0.7959
Epoch 4/40
8132/8132 [==============================] - 1s - loss: 0.4723 - acc: 0.7864 - val_loss: 0.4433 - val_acc: 0.8072
Epoch 5/40
8132/8132 [==============================] - 1s - loss: 0.4510 - acc: 0.7999 - val_loss: 0.4580 - val_acc: 0.7949
Epoch 6/40
8132/8132 [==============================] - 1s - loss: 0.4655 - acc: 0.7880 - val_loss: 0.4611 - val_acc: 0.7949
Epoch 7/40
8132/8132 [==============================] - 1s - loss: 0.4689 - acc: 0.7839 - val_loss: 0.4469 - val_acc: 0.8091
Epoch 8/40
8132/8132 [==============================] - 1s - loss: 0.4564 - acc: 0.7932 - val_loss: 0.4492 - val_acc: 0.8023
Epoch 9/40
8132/8132 [==============================] - 1s - loss: 0.4590 - acc: 0.7935 - val_loss: 0.4551 - val_acc: 0.7969
Epoch 10/40
8132/8132 [==============================] - 1s - loss: 0.4633 - acc: 0.7935 - val_loss: 0.4521 - val_acc: 0.7954
Epoch 11/40
8132/8132 [==============================] - 1s - loss: 0.4586 - acc: 0.7959 - val_loss: 0.4472 - val_acc: 0.8003
Epoch 12/40
8132/8132 [==============================] - 1s - loss: 0.4509 - acc: 0.7993 - val_loss: 0.4519 - val_acc: 0.7983
Epoch 13/40
8132/8132 [==============================] - 1s - loss: 0.4515 - acc: 0.7978 - val_loss: 0.4530 - val_acc: 0.7939
Epoch 14/40
8132/8132 [==============================] - 1s - loss: 0.4509 - acc: 0.7971 - val_loss: 0.4468 - val_acc: 0.7924
Epoch 15/40
8132/8132 [==============================] - 1s - loss: 0.4456 - acc: 0.7997 - val_loss: 0.4472 - val_acc: 0.7934
Epoch 16/40
8132/8132 [==============================] - 1s - loss: 0.4476 - acc: 0.7976 - val_loss: 0.4460 - val_acc: 0.7973
Epoch 17/40
8132/8132 [==============================] - 1s - loss: 0.4478 - acc: 0.7973 - val_loss: 0.4417 - val_acc: 0.8008
Epoch 18/40
8132/8132 [==============================] - 1s - loss: 0.4435 - acc: 0.8007 - val_loss: 0.4435 - val_acc: 0.8052
Epoch 19/40
8132/8132 [==============================] - 1s - loss: 0.4446 - acc: 0.8024 - val_loss: 0.4442 - val_acc: 0.8028
Epoch 20/40
8132/8132 [==============================] - 1s - loss: 0.4453 - acc: 0.8015 - val_loss: 0.4389 - val_acc: 0.8047
Epoch 21/40
8132/8132 [==============================] - 1s - loss: 0.4416 - acc: 0.8013 - val_loss: 0.4392 - val_acc: 0.8032
Epoch 22/40
8132/8132 [==============================] - 1s - loss: 0.4426 - acc: 0.8002 - val_loss: 0.4392 - val_acc: 0.8047
Epoch 23/40
8132/8132 [==============================] - 1s - loss: 0.4414 - acc: 0.8034 - val_loss: 0.4406 - val_acc: 0.8062
Epoch 24/40
8132/8132 [==============================] - 1s - loss: 0.4401 - acc: 0.8053 - val_loss: 0.4439 - val_acc: 0.8047
Epoch 25/40
8132/8132 [==============================] - 1s - loss: 0.4416 - acc: 0.8044 - val_loss: 0.4400 - val_acc: 0.8028
Epoch 26/40
8132/8132 [==============================] - 1s - loss: 0.4389 - acc: 0.8052 - val_loss: 0.4375 - val_acc: 0.8062
Epoch 27/40
8132/8132 [==============================] - 1s - loss: 0.4388 - acc: 0.8061 - val_loss: 0.4372 - val_acc: 0.8047
Epoch 28/40
8132/8132 [==============================] - 1s - loss: 0.4393 - acc: 0.8055 - val_loss: 0.4372 - val_acc: 0.8042
Epoch 29/40
8132/8132 [==============================] - 1s - loss: 0.4379 - acc: 0.8061 - val_loss: 0.4398 - val_acc: 0.8047
Epoch 30/40
8132/8132 [==============================] - 1s - loss: 0.4383 - acc: 0.8036 - val_loss: 0.4396 - val_acc: 0.8042
Epoch 31/40
8132/8132 [==============================] - 1s - loss: 0.4372 - acc: 0.8055 - val_loss: 0.4391 - val_acc: 0.7998
Epoch 32/40
8132/8132 [==============================] - 1s - loss: 0.4368 - acc: 0.8057 - val_loss: 0.4396 - val_acc: 0.8028
Epoch 33/40
8132/8132 [==============================] - 1s - loss: 0.4369 - acc: 0.8072 - val_loss: 0.4389 - val_acc: 0.8042
Epoch 34/40
8132/8132 [==============================] - 1s - loss: 0.4354 - acc: 0.8064 - val_loss: 0.4397 - val_acc: 0.8052
Epoch 35/40
8132/8132 [==============================] - 1s - loss: 0.4354 - acc: 0.8067 - val_loss: 0.4387 - val_acc: 0.8037
Epoch 36/40
8132/8132 [==============================] - 1s - loss: 0.4348 - acc: 0.8079 - val_loss: 0.4370 - val_acc: 0.8072
Epoch 37/40
8132/8132 [==============================] - 1s - loss: 0.4344 - acc: 0.8082 - val_loss: 0.4369 - val_acc: 0.8062
Epoch 38/40
8132/8132 [==============================] - 1s - loss: 0.4345 - acc: 0.8085 - val_loss: 0.4373 - val_acc: 0.8072
Epoch 39/40
8132/8132 [==============================] - 1s - loss: 0.4340 - acc: 0.8080 - val_loss: 0.4384 - val_acc: 0.8057
Epoch 40/40
8132/8132 [==============================] - 1s - loss: 0.4343 - acc: 0.8079 - val_loss: 0.4375 - val_acc: 0.8077
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-11.h5
chunk number 11
prepare data
start training
Train on 8186 samples, validate on 2047 samples
Epoch 1/40
8186/8186 [==============================] - 1s - loss: 0.4568 - acc: 0.7960 - val_loss: 0.4506 - val_acc: 0.7978
Epoch 2/40
8186/8186 [==============================] - 1s - loss: 0.4506 - acc: 0.7992 - val_loss: 0.4562 - val_acc: 0.7948
Epoch 3/40
8186/8186 [==============================] - 1s - loss: 0.4541 - acc: 0.7966 - val_loss: 0.4461 - val_acc: 0.7987
Epoch 4/40
8186/8186 [==============================] - 1s - loss: 0.4480 - acc: 0.8014 - val_loss: 0.4450 - val_acc: 0.7968
Epoch 5/40
8186/8186 [==============================] - 1s - loss: 0.4502 - acc: 0.8023 - val_loss: 0.4432 - val_acc: 0.7943
Epoch 6/40
8186/8186 [==============================] - 1s - loss: 0.4468 - acc: 0.8020 - val_loss: 0.4464 - val_acc: 0.7953
Epoch 7/40
8186/8186 [==============================] - 1s - loss: 0.4462 - acc: 0.8039 - val_loss: 0.4468 - val_acc: 0.7968
Epoch 8/40
8186/8186 [==============================] - 1s - loss: 0.4460 - acc: 0.8045 - val_loss: 0.4433 - val_acc: 0.7919
Epoch 9/40
8186/8186 [==============================] - 1s - loss: 0.4439 - acc: 0.8027 - val_loss: 0.4430 - val_acc: 0.7919
Epoch 10/40
8186/8186 [==============================] - 1s - loss: 0.4440 - acc: 0.8045 - val_loss: 0.4428 - val_acc: 0.7973
Epoch 11/40
8186/8186 [==============================] - 1s - loss: 0.4414 - acc: 0.8069 - val_loss: 0.4457 - val_acc: 0.7948
Epoch 12/40
8186/8186 [==============================] - 1s - loss: 0.4426 - acc: 0.8026 - val_loss: 0.4430 - val_acc: 0.8007
Epoch 13/40
8186/8186 [==============================] - 1s - loss: 0.4404 - acc: 0.8048 - val_loss: 0.4422 - val_acc: 0.7982
Epoch 14/40
8186/8186 [==============================] - 1s - loss: 0.4405 - acc: 0.8065 - val_loss: 0.4427 - val_acc: 0.7997
Epoch 15/40
8186/8186 [==============================] - 1s - loss: 0.4401 - acc: 0.8052 - val_loss: 0.4434 - val_acc: 0.8026
Epoch 16/40
8186/8186 [==============================] - 1s - loss: 0.4394 - acc: 0.8069 - val_loss: 0.4441 - val_acc: 0.8012
Epoch 17/40
8186/8186 [==============================] - 1s - loss: 0.4395 - acc: 0.8055 - val_loss: 0.4421 - val_acc: 0.8036
Epoch 18/40
8186/8186 [==============================] - 1s - loss: 0.4380 - acc: 0.8056 - val_loss: 0.4419 - val_acc: 0.8031
Epoch 19/40
8186/8186 [==============================] - 1s - loss: 0.4384 - acc: 0.8082 - val_loss: 0.4417 - val_acc: 0.7987
Epoch 20/40
8186/8186 [==============================] - 1s - loss: 0.4375 - acc: 0.8076 - val_loss: 0.4427 - val_acc: 0.8002
Epoch 21/40
8186/8186 [==============================] - 1s - loss: 0.4373 - acc: 0.8065 - val_loss: 0.4424 - val_acc: 0.8002
Epoch 22/40
8186/8186 [==============================] - 1s - loss: 0.4368 - acc: 0.8082 - val_loss: 0.4416 - val_acc: 0.8017
Epoch 23/40
8186/8186 [==============================] - 1s - loss: 0.4366 - acc: 0.8096 - val_loss: 0.4409 - val_acc: 0.8031
Epoch 24/40
8186/8186 [==============================] - 1s - loss: 0.4361 - acc: 0.8103 - val_loss: 0.4408 - val_acc: 0.8026
Epoch 25/40
8186/8186 [==============================] - 1s - loss: 0.4355 - acc: 0.8085 - val_loss: 0.4408 - val_acc: 0.8031
Epoch 26/40
8186/8186 [==============================] - 1s - loss: 0.4354 - acc: 0.8080 - val_loss: 0.4403 - val_acc: 0.8051
Epoch 27/40
8186/8186 [==============================] - 1s - loss: 0.4347 - acc: 0.8098 - val_loss: 0.4404 - val_acc: 0.8046
Epoch 28/40
8186/8186 [==============================] - 1s - loss: 0.4346 - acc: 0.8098 - val_loss: 0.4404 - val_acc: 0.8036
Epoch 29/40
8186/8186 [==============================] - 1s - loss: 0.4341 - acc: 0.8094 - val_loss: 0.4404 - val_acc: 0.8031
Epoch 30/40
8186/8186 [==============================] - 1s - loss: 0.4340 - acc: 0.8096 - val_loss: 0.4395 - val_acc: 0.8017
Epoch 31/40
8186/8186 [==============================] - 1s - loss: 0.4335 - acc: 0.8104 - val_loss: 0.4394 - val_acc: 0.8046
Epoch 32/40
8186/8186 [==============================] - 1s - loss: 0.4333 - acc: 0.8100 - val_loss: 0.4396 - val_acc: 0.8026
Epoch 33/40
8186/8186 [==============================] - 1s - loss: 0.4328 - acc: 0.8109 - val_loss: 0.4403 - val_acc: 0.8031
Epoch 34/40
8186/8186 [==============================] - 1s - loss: 0.4326 - acc: 0.8115 - val_loss: 0.4400 - val_acc: 0.7997
Epoch 35/40
8186/8186 [==============================] - 1s - loss: 0.4321 - acc: 0.8115 - val_loss: 0.4396 - val_acc: 0.7992
Epoch 36/40
8186/8186 [==============================] - 1s - loss: 0.4319 - acc: 0.8124 - val_loss: 0.4394 - val_acc: 0.7992
Epoch 37/40
8186/8186 [==============================] - 1s - loss: 0.4315 - acc: 0.8111 - val_loss: 0.4397 - val_acc: 0.8007
Epoch 38/40
8186/8186 [==============================] - 1s - loss: 0.4313 - acc: 0.8109 - val_loss: 0.4401 - val_acc: 0.7992
Epoch 39/40
8186/8186 [==============================] - 1s - loss: 0.4308 - acc: 0.8120 - val_loss: 0.4404 - val_acc: 0.8002
Epoch 40/40
8186/8186 [==============================] - 1s - loss: 0.4307 - acc: 0.8124 - val_loss: 0.4402 - val_acc: 0.7992
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-12.h5
chunk number 12
prepare data
start training
Train on 8205 samples, validate on 2052 samples
Epoch 1/40
8205/8205 [==============================] - 1s - loss: 0.4356 - acc: 0.8039 - val_loss: 0.4607 - val_acc: 0.8026
Epoch 2/40
8205/8205 [==============================] - 1s - loss: 0.4353 - acc: 0.8048 - val_loss: 0.4607 - val_acc: 0.8031
Epoch 3/40
8205/8205 [==============================] - 1s - loss: 0.4347 - acc: 0.8050 - val_loss: 0.4607 - val_acc: 0.8017
Epoch 4/40
8205/8205 [==============================] - 1s - loss: 0.4341 - acc: 0.8051 - val_loss: 0.4613 - val_acc: 0.8007
Epoch 5/40
8205/8205 [==============================] - 1s - loss: 0.4335 - acc: 0.8056 - val_loss: 0.4619 - val_acc: 0.7992
Epoch 6/40
8205/8205 [==============================] - 1s - loss: 0.4330 - acc: 0.8056 - val_loss: 0.4615 - val_acc: 0.7997
Epoch 7/40
8205/8205 [==============================] - 1s - loss: 0.4323 - acc: 0.8055 - val_loss: 0.4612 - val_acc: 0.8007
Epoch 8/40
8205/8205 [==============================] - 1s - loss: 0.4317 - acc: 0.8056 - val_loss: 0.4616 - val_acc: 0.8021
Epoch 9/40
8205/8205 [==============================] - 1s - loss: 0.4311 - acc: 0.8063 - val_loss: 0.4620 - val_acc: 0.8017
Epoch 10/40
8205/8205 [==============================] - 1s - loss: 0.4306 - acc: 0.8074 - val_loss: 0.4615 - val_acc: 0.8007
Epoch 11/40
8205/8205 [==============================] - 1s - loss: 0.4301 - acc: 0.8072 - val_loss: 0.4613 - val_acc: 0.8012
Epoch 12/40
8205/8205 [==============================] - 1s - loss: 0.4295 - acc: 0.8072 - val_loss: 0.4617 - val_acc: 0.8002
Epoch 13/40
8205/8205 [==============================] - 1s - loss: 0.4290 - acc: 0.8084 - val_loss: 0.4619 - val_acc: 0.8002
Epoch 14/40
8205/8205 [==============================] - 1s - loss: 0.4285 - acc: 0.8088 - val_loss: 0.4616 - val_acc: 0.8002
Epoch 15/40
8205/8205 [==============================] - 1s - loss: 0.4281 - acc: 0.8078 - val_loss: 0.4618 - val_acc: 0.7973
Epoch 16/40
8205/8205 [==============================] - 1s - loss: 0.4276 - acc: 0.8085 - val_loss: 0.4623 - val_acc: 0.7987
Epoch 17/40
8205/8205 [==============================] - 1s - loss: 0.4272 - acc: 0.8095 - val_loss: 0.4620 - val_acc: 0.7978
Epoch 18/40
8205/8205 [==============================] - 1s - loss: 0.4268 - acc: 0.8098 - val_loss: 0.4618 - val_acc: 0.7982
Epoch 19/40
8205/8205 [==============================] - 1s - loss: 0.4264 - acc: 0.8101 - val_loss: 0.4621 - val_acc: 0.7997
Epoch 20/40
8205/8205 [==============================] - 1s - loss: 0.4260 - acc: 0.8113 - val_loss: 0.4622 - val_acc: 0.7997
Epoch 21/40
8205/8205 [==============================] - 1s - loss: 0.4255 - acc: 0.8113 - val_loss: 0.4620 - val_acc: 0.7992
Epoch 22/40
8205/8205 [==============================] - 1s - loss: 0.4251 - acc: 0.8106 - val_loss: 0.4622 - val_acc: 0.7987
Epoch 23/40
8205/8205 [==============================] - 1s - loss: 0.4247 - acc: 0.8115 - val_loss: 0.4623 - val_acc: 0.7992
Epoch 24/40
8205/8205 [==============================] - 1s - loss: 0.4243 - acc: 0.8122 - val_loss: 0.4622 - val_acc: 0.7992
Epoch 25/40
8205/8205 [==============================] - 1s - loss: 0.4239 - acc: 0.8124 - val_loss: 0.4624 - val_acc: 0.7987
Epoch 26/40
8205/8205 [==============================] - 1s - loss: 0.4235 - acc: 0.8122 - val_loss: 0.4628 - val_acc: 0.7982
Epoch 27/40
8205/8205 [==============================] - 1s - loss: 0.4231 - acc: 0.8128 - val_loss: 0.4629 - val_acc: 0.7987
Epoch 28/40
8205/8205 [==============================] - 1s - loss: 0.4228 - acc: 0.8132 - val_loss: 0.4632 - val_acc: 0.7987
Epoch 29/40
8205/8205 [==============================] - 1s - loss: 0.4224 - acc: 0.8134 - val_loss: 0.4637 - val_acc: 0.7968
Epoch 30/40
8205/8205 [==============================] - 1s - loss: 0.4221 - acc: 0.8135 - val_loss: 0.4637 - val_acc: 0.7968
Epoch 31/40
8205/8205 [==============================] - 1s - loss: 0.4218 - acc: 0.8132 - val_loss: 0.4640 - val_acc: 0.7963
Epoch 32/40
8205/8205 [==============================] - 1s - loss: 0.4215 - acc: 0.8126 - val_loss: 0.4641 - val_acc: 0.7963
Epoch 33/40
8205/8205 [==============================] - 1s - loss: 0.4212 - acc: 0.8129 - val_loss: 0.4637 - val_acc: 0.7929
Epoch 34/40
8205/8205 [==============================] - 1s - loss: 0.4208 - acc: 0.8132 - val_loss: 0.4638 - val_acc: 0.7939
Epoch 35/40
8205/8205 [==============================] - 1s - loss: 0.4205 - acc: 0.8133 - val_loss: 0.4635 - val_acc: 0.7943
Epoch 36/40
8205/8205 [==============================] - 1s - loss: 0.4202 - acc: 0.8138 - val_loss: 0.4633 - val_acc: 0.7934
Epoch 37/40
8205/8205 [==============================] - 1s - loss: 0.4198 - acc: 0.8139 - val_loss: 0.4634 - val_acc: 0.7958
Epoch 38/40
8205/8205 [==============================] - 1s - loss: 0.4195 - acc: 0.8143 - val_loss: 0.4630 - val_acc: 0.7948
Epoch 39/40
8205/8205 [==============================] - 1s - loss: 0.4192 - acc: 0.8146 - val_loss: 0.4633 - val_acc: 0.7958
Epoch 40/40
8205/8205 [==============================] - 1s - loss: 0.4189 - acc: 0.8147 - val_loss: 0.4631 - val_acc: 0.7943
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-13.h5
chunk number 13
prepare data
start training
Train on 8166 samples, validate on 2042 samples
Epoch 1/40
8166/8166 [==============================] - 1s - loss: 0.4469 - acc: 0.8028 - val_loss: 0.4304 - val_acc: 0.8129
Epoch 2/40
8166/8166 [==============================] - 1s - loss: 0.4473 - acc: 0.8008 - val_loss: 0.4245 - val_acc: 0.8159
Epoch 3/40
8166/8166 [==============================] - 1s - loss: 0.4418 - acc: 0.8036 - val_loss: 0.4237 - val_acc: 0.8188
Epoch 4/40
8166/8166 [==============================] - 1s - loss: 0.4427 - acc: 0.8017 - val_loss: 0.4259 - val_acc: 0.8159
Epoch 5/40
8166/8166 [==============================] - 1s - loss: 0.4405 - acc: 0.8019 - val_loss: 0.4310 - val_acc: 0.8119
Epoch 6/40
8166/8166 [==============================] - 1s - loss: 0.4407 - acc: 0.8021 - val_loss: 0.4325 - val_acc: 0.8119
Epoch 7/40
8166/8166 [==============================] - 1s - loss: 0.4407 - acc: 0.8016 - val_loss: 0.4290 - val_acc: 0.8154
Epoch 8/40
8166/8166 [==============================] - 1s - loss: 0.4395 - acc: 0.8030 - val_loss: 0.4273 - val_acc: 0.8188
Epoch 9/40
8166/8166 [==============================] - 1s - loss: 0.4388 - acc: 0.8032 - val_loss: 0.4277 - val_acc: 0.8164
Epoch 10/40
8166/8166 [==============================] - 1s - loss: 0.4370 - acc: 0.8035 - val_loss: 0.4291 - val_acc: 0.8154
Epoch 11/40
8166/8166 [==============================] - 1s - loss: 0.4377 - acc: 0.8031 - val_loss: 0.4255 - val_acc: 0.8159
Epoch 12/40
8166/8166 [==============================] - 1s - loss: 0.4364 - acc: 0.8037 - val_loss: 0.4235 - val_acc: 0.8139
Epoch 13/40
8166/8166 [==============================] - 1s - loss: 0.4368 - acc: 0.8038 - val_loss: 0.4243 - val_acc: 0.8168
Epoch 14/40
8166/8166 [==============================] - 1s - loss: 0.4358 - acc: 0.8032 - val_loss: 0.4266 - val_acc: 0.8149
Epoch 15/40
8166/8166 [==============================] - 1s - loss: 0.4356 - acc: 0.8047 - val_loss: 0.4240 - val_acc: 0.8164
Epoch 16/40
8166/8166 [==============================] - 1s - loss: 0.4341 - acc: 0.8037 - val_loss: 0.4226 - val_acc: 0.8164
Epoch 17/40
8166/8166 [==============================] - 1s - loss: 0.4343 - acc: 0.8044 - val_loss: 0.4237 - val_acc: 0.8188
Epoch 18/40
8166/8166 [==============================] - 1s - loss: 0.4334 - acc: 0.8060 - val_loss: 0.4254 - val_acc: 0.8198
Epoch 19/40
8166/8166 [==============================] - 1s - loss: 0.4336 - acc: 0.8058 - val_loss: 0.4225 - val_acc: 0.8193
Epoch 20/40
8166/8166 [==============================] - 1s - loss: 0.4330 - acc: 0.8064 - val_loss: 0.4213 - val_acc: 0.8208
Epoch 21/40
8166/8166 [==============================] - 1s - loss: 0.4331 - acc: 0.8066 - val_loss: 0.4233 - val_acc: 0.8208
Epoch 22/40
8166/8166 [==============================] - 1s - loss: 0.4322 - acc: 0.8076 - val_loss: 0.4232 - val_acc: 0.8198
Epoch 23/40
8166/8166 [==============================] - 1s - loss: 0.4319 - acc: 0.8076 - val_loss: 0.4204 - val_acc: 0.8178
Epoch 24/40
8166/8166 [==============================] - 1s - loss: 0.4316 - acc: 0.8080 - val_loss: 0.4207 - val_acc: 0.8193
Epoch 25/40
8166/8166 [==============================] - 1s - loss: 0.4309 - acc: 0.8075 - val_loss: 0.4226 - val_acc: 0.8183
Epoch 26/40
8166/8166 [==============================] - 1s - loss: 0.4310 - acc: 0.8082 - val_loss: 0.4215 - val_acc: 0.8168
Epoch 27/40
8166/8166 [==============================] - 1s - loss: 0.4304 - acc: 0.8087 - val_loss: 0.4220 - val_acc: 0.8178
Epoch 28/40
8166/8166 [==============================] - 1s - loss: 0.4301 - acc: 0.8082 - val_loss: 0.4246 - val_acc: 0.8183
Epoch 29/40
8166/8166 [==============================] - 1s - loss: 0.4299 - acc: 0.8088 - val_loss: 0.4241 - val_acc: 0.8193
Epoch 30/40
8166/8166 [==============================] - 1s - loss: 0.4294 - acc: 0.8097 - val_loss: 0.4227 - val_acc: 0.8178
Epoch 31/40
8166/8166 [==============================] - 1s - loss: 0.4292 - acc: 0.8091 - val_loss: 0.4237 - val_acc: 0.8183
Epoch 32/40
8166/8166 [==============================] - 1s - loss: 0.4288 - acc: 0.8099 - val_loss: 0.4242 - val_acc: 0.8188
Epoch 33/40
8166/8166 [==============================] - 1s - loss: 0.4286 - acc: 0.8102 - val_loss: 0.4229 - val_acc: 0.8173
Epoch 34/40
8166/8166 [==============================] - 1s - loss: 0.4283 - acc: 0.8102 - val_loss: 0.4236 - val_acc: 0.8188
Epoch 35/40
8166/8166 [==============================] - 1s - loss: 0.4278 - acc: 0.8098 - val_loss: 0.4243 - val_acc: 0.8173
Epoch 36/40
8166/8166 [==============================] - 1s - loss: 0.4276 - acc: 0.8107 - val_loss: 0.4225 - val_acc: 0.8188
Epoch 37/40
8166/8166 [==============================] - 1s - loss: 0.4272 - acc: 0.8108 - val_loss: 0.4225 - val_acc: 0.8164
Epoch 38/40
8166/8166 [==============================] - 1s - loss: 0.4269 - acc: 0.8107 - val_loss: 0.4232 - val_acc: 0.8183
Epoch 39/40
8166/8166 [==============================] - 1s - loss: 0.4266 - acc: 0.8110 - val_loss: 0.4221 - val_acc: 0.8154
Epoch 40/40
8166/8166 [==============================] - 1s - loss: 0.4264 - acc: 0.8113 - val_loss: 0.4235 - val_acc: 0.8188
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-14.h5
chunk number 14
prepare data
start training
Train on 8130 samples, validate on 2033 samples
Epoch 1/40
8130/8130 [==============================] - 1s - loss: 0.4414 - acc: 0.8009 - val_loss: 0.4308 - val_acc: 0.8096
Epoch 2/40
8130/8130 [==============================] - 1s - loss: 0.4406 - acc: 0.8018 - val_loss: 0.4308 - val_acc: 0.8121
Epoch 3/40
8130/8130 [==============================] - 1s - loss: 0.4399 - acc: 0.8020 - val_loss: 0.4311 - val_acc: 0.8062
Epoch 4/40
8130/8130 [==============================] - 1s - loss: 0.4394 - acc: 0.8036 - val_loss: 0.4305 - val_acc: 0.8116
Epoch 5/40
8130/8130 [==============================] - 1s - loss: 0.4383 - acc: 0.8016 - val_loss: 0.4303 - val_acc: 0.8082
Epoch 6/40
8130/8130 [==============================] - 1s - loss: 0.4376 - acc: 0.8038 - val_loss: 0.4308 - val_acc: 0.8077
Epoch 7/40
8130/8130 [==============================] - 1s - loss: 0.4371 - acc: 0.8052 - val_loss: 0.4313 - val_acc: 0.8116
Epoch 8/40
8130/8130 [==============================] - 1s - loss: 0.4365 - acc: 0.8038 - val_loss: 0.4310 - val_acc: 0.8101
Epoch 9/40
8130/8130 [==============================] - 1s - loss: 0.4357 - acc: 0.8059 - val_loss: 0.4305 - val_acc: 0.8096
Epoch 10/40
8130/8130 [==============================] - 1s - loss: 0.4350 - acc: 0.8060 - val_loss: 0.4300 - val_acc: 0.8116
Epoch 11/40
8130/8130 [==============================] - 1s - loss: 0.4344 - acc: 0.8075 - val_loss: 0.4295 - val_acc: 0.8121
Epoch 12/40
8130/8130 [==============================] - 1s - loss: 0.4336 - acc: 0.8087 - val_loss: 0.4296 - val_acc: 0.8111
Epoch 13/40
8130/8130 [==============================] - 1s - loss: 0.4331 - acc: 0.8085 - val_loss: 0.4304 - val_acc: 0.8136
Epoch 14/40
8130/8130 [==============================] - 1s - loss: 0.4326 - acc: 0.8077 - val_loss: 0.4302 - val_acc: 0.8126
Epoch 15/40
8130/8130 [==============================] - 1s - loss: 0.4320 - acc: 0.8080 - val_loss: 0.4298 - val_acc: 0.8106
Epoch 16/40
8130/8130 [==============================] - 1s - loss: 0.4315 - acc: 0.8080 - val_loss: 0.4300 - val_acc: 0.8170
Epoch 17/40
8130/8130 [==============================] - 1s - loss: 0.4310 - acc: 0.8090 - val_loss: 0.4299 - val_acc: 0.8155
Epoch 18/40
8130/8130 [==============================] - 1s - loss: 0.4304 - acc: 0.8093 - val_loss: 0.4300 - val_acc: 0.8151
Epoch 19/40
8130/8130 [==============================] - 1s - loss: 0.4300 - acc: 0.8095 - val_loss: 0.4305 - val_acc: 0.8175
Epoch 20/40
8130/8130 [==============================] - 1s - loss: 0.4296 - acc: 0.8097 - val_loss: 0.4304 - val_acc: 0.8141
Epoch 21/40
8130/8130 [==============================] - 1s - loss: 0.4292 - acc: 0.8106 - val_loss: 0.4305 - val_acc: 0.8155
Epoch 22/40
8130/8130 [==============================] - 1s - loss: 0.4288 - acc: 0.8106 - val_loss: 0.4311 - val_acc: 0.8170
Epoch 23/40
8130/8130 [==============================] - 1s - loss: 0.4284 - acc: 0.8105 - val_loss: 0.4306 - val_acc: 0.8141
Epoch 24/40
8130/8130 [==============================] - 1s - loss: 0.4281 - acc: 0.8107 - val_loss: 0.4311 - val_acc: 0.8160
Epoch 25/40
8130/8130 [==============================] - 1s - loss: 0.4277 - acc: 0.8111 - val_loss: 0.4312 - val_acc: 0.8160
Epoch 26/40
8130/8130 [==============================] - 1s - loss: 0.4273 - acc: 0.8098 - val_loss: 0.4314 - val_acc: 0.8170
Epoch 27/40
8130/8130 [==============================] - 1s - loss: 0.4270 - acc: 0.8102 - val_loss: 0.4318 - val_acc: 0.8165
Epoch 28/40
8130/8130 [==============================] - 1s - loss: 0.4267 - acc: 0.8105 - val_loss: 0.4311 - val_acc: 0.8155
Epoch 29/40
8130/8130 [==============================] - 1s - loss: 0.4264 - acc: 0.8121 - val_loss: 0.4315 - val_acc: 0.8155
Epoch 30/40
8130/8130 [==============================] - 1s - loss: 0.4260 - acc: 0.8106 - val_loss: 0.4311 - val_acc: 0.8175
Epoch 31/40
8130/8130 [==============================] - 1s - loss: 0.4256 - acc: 0.8123 - val_loss: 0.4314 - val_acc: 0.8195
Epoch 32/40
8130/8130 [==============================] - 1s - loss: 0.4252 - acc: 0.8113 - val_loss: 0.4313 - val_acc: 0.8195
Epoch 33/40
8130/8130 [==============================] - 1s - loss: 0.4249 - acc: 0.8118 - val_loss: 0.4312 - val_acc: 0.8190
Epoch 34/40
8130/8130 [==============================] - 1s - loss: 0.4245 - acc: 0.8124 - val_loss: 0.4317 - val_acc: 0.8175
Epoch 35/40
8130/8130 [==============================] - 1s - loss: 0.4242 - acc: 0.8124 - val_loss: 0.4313 - val_acc: 0.8185
Epoch 36/40
8130/8130 [==============================] - 1s - loss: 0.4239 - acc: 0.8135 - val_loss: 0.4323 - val_acc: 0.8190
Epoch 37/40
8130/8130 [==============================] - 1s - loss: 0.4236 - acc: 0.8123 - val_loss: 0.4313 - val_acc: 0.8175
Epoch 38/40
8130/8130 [==============================] - 1s - loss: 0.4235 - acc: 0.8135 - val_loss: 0.4336 - val_acc: 0.8190
Epoch 39/40
8130/8130 [==============================] - 1s - loss: 0.4239 - acc: 0.8134 - val_loss: 0.4320 - val_acc: 0.8126
Epoch 40/40
8130/8130 [==============================] - 1s - loss: 0.4251 - acc: 0.8122 - val_loss: 0.4382 - val_acc: 0.8165
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-15.h5
chunk number 15
prepare data
start training
Train on 8086 samples, validate on 2022 samples
Epoch 1/40
8086/8086 [==============================] - 1s - loss: 0.4467 - acc: 0.8058 - val_loss: 0.4360 - val_acc: 0.8220
Epoch 2/40
8086/8086 [==============================] - 1s - loss: 0.4418 - acc: 0.8074 - val_loss: 0.4334 - val_acc: 0.8205
Epoch 3/40
8086/8086 [==============================] - 1s - loss: 0.4383 - acc: 0.8072 - val_loss: 0.4340 - val_acc: 0.8200
Epoch 4/40
8086/8086 [==============================] - 1s - loss: 0.4413 - acc: 0.8072 - val_loss: 0.4309 - val_acc: 0.8170
Epoch 5/40
8086/8086 [==============================] - 1s - loss: 0.4352 - acc: 0.8113 - val_loss: 0.4349 - val_acc: 0.8140
Epoch 6/40
8086/8086 [==============================] - 1s - loss: 0.4381 - acc: 0.8072 - val_loss: 0.4282 - val_acc: 0.8131
Epoch 7/40
8086/8086 [==============================] - 1s - loss: 0.4361 - acc: 0.8086 - val_loss: 0.4281 - val_acc: 0.8145
Epoch 8/40
8086/8086 [==============================] - 1s - loss: 0.4358 - acc: 0.8074 - val_loss: 0.4312 - val_acc: 0.8150
Epoch 9/40
8086/8086 [==============================] - 1s - loss: 0.4349 - acc: 0.8078 - val_loss: 0.4292 - val_acc: 0.8180
Epoch 10/40
8086/8086 [==============================] - 1s - loss: 0.4325 - acc: 0.8087 - val_loss: 0.4294 - val_acc: 0.8140
Epoch 11/40
8086/8086 [==============================] - 1s - loss: 0.4345 - acc: 0.8104 - val_loss: 0.4279 - val_acc: 0.8195
Epoch 12/40
8086/8086 [==============================] - 1s - loss: 0.4307 - acc: 0.8097 - val_loss: 0.4286 - val_acc: 0.8185
Epoch 13/40
8086/8086 [==============================] - 1s - loss: 0.4314 - acc: 0.8104 - val_loss: 0.4262 - val_acc: 0.8155
Epoch 14/40
8086/8086 [==============================] - 1s - loss: 0.4312 - acc: 0.8109 - val_loss: 0.4253 - val_acc: 0.8160
Epoch 15/40
8086/8086 [==============================] - 1s - loss: 0.4293 - acc: 0.8113 - val_loss: 0.4293 - val_acc: 0.8225
Epoch 16/40
8086/8086 [==============================] - 1s - loss: 0.4308 - acc: 0.8105 - val_loss: 0.4264 - val_acc: 0.8185
Epoch 17/40
8086/8086 [==============================] - 1s - loss: 0.4291 - acc: 0.8121 - val_loss: 0.4266 - val_acc: 0.8190
Epoch 18/40
8086/8086 [==============================] - 1s - loss: 0.4279 - acc: 0.8128 - val_loss: 0.4298 - val_acc: 0.8205
Epoch 19/40
8086/8086 [==============================] - 1s - loss: 0.4288 - acc: 0.8110 - val_loss: 0.4262 - val_acc: 0.8185
Epoch 20/40
8086/8086 [==============================] - 1s - loss: 0.4273 - acc: 0.8138 - val_loss: 0.4262 - val_acc: 0.8175
Epoch 21/40
8086/8086 [==============================] - 1s - loss: 0.4263 - acc: 0.8131 - val_loss: 0.4294 - val_acc: 0.8170
Epoch 22/40
8086/8086 [==============================] - 1s - loss: 0.4269 - acc: 0.8118 - val_loss: 0.4269 - val_acc: 0.8175
Epoch 23/40
8086/8086 [==============================] - 1s - loss: 0.4256 - acc: 0.8150 - val_loss: 0.4275 - val_acc: 0.8170
Epoch 24/40
8086/8086 [==============================] - 1s - loss: 0.4253 - acc: 0.8145 - val_loss: 0.4303 - val_acc: 0.8165
Epoch 25/40
8086/8086 [==============================] - 1s - loss: 0.4258 - acc: 0.8116 - val_loss: 0.4267 - val_acc: 0.8155
Epoch 26/40
8086/8086 [==============================] - 1s - loss: 0.4247 - acc: 0.8147 - val_loss: 0.4269 - val_acc: 0.8185
Epoch 27/40
8086/8086 [==============================] - 1s - loss: 0.4239 - acc: 0.8149 - val_loss: 0.4288 - val_acc: 0.8165
Epoch 28/40
8086/8086 [==============================] - 1s - loss: 0.4241 - acc: 0.8141 - val_loss: 0.4269 - val_acc: 0.8170
Epoch 29/40
8086/8086 [==============================] - 1s - loss: 0.4234 - acc: 0.8156 - val_loss: 0.4282 - val_acc: 0.8165
Epoch 30/40
8086/8086 [==============================] - 1s - loss: 0.4227 - acc: 0.8166 - val_loss: 0.4286 - val_acc: 0.8160
Epoch 31/40
8086/8086 [==============================] - 1s - loss: 0.4224 - acc: 0.8165 - val_loss: 0.4268 - val_acc: 0.8170
Epoch 32/40
8086/8086 [==============================] - 1s - loss: 0.4226 - acc: 0.8159 - val_loss: 0.4301 - val_acc: 0.8165
Epoch 33/40
8086/8086 [==============================] - 1s - loss: 0.4226 - acc: 0.8155 - val_loss: 0.4273 - val_acc: 0.8170
Epoch 34/40
8086/8086 [==============================] - 1s - loss: 0.4221 - acc: 0.8171 - val_loss: 0.4298 - val_acc: 0.8165
Epoch 35/40
8086/8086 [==============================] - 1s - loss: 0.4211 - acc: 0.8160 - val_loss: 0.4283 - val_acc: 0.8195
Epoch 36/40
8086/8086 [==============================] - 1s - loss: 0.4204 - acc: 0.8175 - val_loss: 0.4288 - val_acc: 0.8180
Epoch 37/40
8086/8086 [==============================] - 1s - loss: 0.4199 - acc: 0.8168 - val_loss: 0.4295 - val_acc: 0.8155
Epoch 38/40
8086/8086 [==============================] - 1s - loss: 0.4197 - acc: 0.8162 - val_loss: 0.4281 - val_acc: 0.8170
Epoch 39/40
8086/8086 [==============================] - 1s - loss: 0.4197 - acc: 0.8183 - val_loss: 0.4312 - val_acc: 0.8126
Epoch 40/40
8086/8086 [==============================] - 1s - loss: 0.4198 - acc: 0.8163 - val_loss: 0.4285 - val_acc: 0.8160
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-16.h5
chunk number 16
prepare data
start training
Train on 8113 samples, validate on 2029 samples
Epoch 1/40
8113/8113 [==============================] - 1s - loss: 0.4439 - acc: 0.8002 - val_loss: 0.4513 - val_acc: 0.8014
Epoch 2/40
8113/8113 [==============================] - 1s - loss: 0.4560 - acc: 0.7971 - val_loss: 0.4631 - val_acc: 0.7925
Epoch 3/40
8113/8113 [==============================] - 1s - loss: 0.4615 - acc: 0.7956 - val_loss: 0.4442 - val_acc: 0.8058
Epoch 4/40
8113/8113 [==============================] - 1s - loss: 0.4433 - acc: 0.8043 - val_loss: 0.4509 - val_acc: 0.8014
Epoch 5/40
8113/8113 [==============================] - 1s - loss: 0.4508 - acc: 0.7979 - val_loss: 0.4507 - val_acc: 0.7979
Epoch 6/40
8113/8113 [==============================] - 1s - loss: 0.4460 - acc: 0.8035 - val_loss: 0.4517 - val_acc: 0.7965
Epoch 7/40
8113/8113 [==============================] - 1s - loss: 0.4453 - acc: 0.8019 - val_loss: 0.4436 - val_acc: 0.8034
Epoch 8/40
8113/8113 [==============================] - 1s - loss: 0.4401 - acc: 0.8057 - val_loss: 0.4418 - val_acc: 0.8093
Epoch 9/40
8113/8113 [==============================] - 1s - loss: 0.4397 - acc: 0.8089 - val_loss: 0.4414 - val_acc: 0.8053
Epoch 10/40
8113/8113 [==============================] - 1s - loss: 0.4387 - acc: 0.8065 - val_loss: 0.4371 - val_acc: 0.8107
Epoch 11/40
8113/8113 [==============================] - 1s - loss: 0.4363 - acc: 0.8073 - val_loss: 0.4389 - val_acc: 0.8083
Epoch 12/40
8113/8113 [==============================] - 1s - loss: 0.4377 - acc: 0.8056 - val_loss: 0.4395 - val_acc: 0.8088
Epoch 13/40
8113/8113 [==============================] - 1s - loss: 0.4347 - acc: 0.8097 - val_loss: 0.4419 - val_acc: 0.8078
Epoch 14/40
8113/8113 [==============================] - 1s - loss: 0.4348 - acc: 0.8093 - val_loss: 0.4423 - val_acc: 0.8088
Epoch 15/40
8113/8113 [==============================] - 1s - loss: 0.4345 - acc: 0.8122 - val_loss: 0.4423 - val_acc: 0.8083
Epoch 16/40
8113/8113 [==============================] - 1s - loss: 0.4320 - acc: 0.8124 - val_loss: 0.4447 - val_acc: 0.8053
Epoch 17/40
8113/8113 [==============================] - 1s - loss: 0.4322 - acc: 0.8099 - val_loss: 0.4437 - val_acc: 0.8107
Epoch 18/40
8113/8113 [==============================] - 1s - loss: 0.4316 - acc: 0.8131 - val_loss: 0.4425 - val_acc: 0.8112
Epoch 19/40
8113/8113 [==============================] - 1s - loss: 0.4318 - acc: 0.8109 - val_loss: 0.4421 - val_acc: 0.8117
Epoch 20/40
8113/8113 [==============================] - 1s - loss: 0.4296 - acc: 0.8125 - val_loss: 0.4423 - val_acc: 0.8068
Epoch 21/40
8113/8113 [==============================] - 1s - loss: 0.4295 - acc: 0.8110 - val_loss: 0.4405 - val_acc: 0.8093
Epoch 22/40
8113/8113 [==============================] - 1s - loss: 0.4288 - acc: 0.8131 - val_loss: 0.4406 - val_acc: 0.8088
Epoch 23/40
8113/8113 [==============================] - 1s - loss: 0.4287 - acc: 0.8117 - val_loss: 0.4417 - val_acc: 0.8098
Epoch 24/40
8113/8113 [==============================] - 1s - loss: 0.4286 - acc: 0.8122 - val_loss: 0.4399 - val_acc: 0.8107
Epoch 25/40
8113/8113 [==============================] - 1s - loss: 0.4269 - acc: 0.8128 - val_loss: 0.4400 - val_acc: 0.8107
Epoch 26/40
8113/8113 [==============================] - 1s - loss: 0.4273 - acc: 0.8130 - val_loss: 0.4409 - val_acc: 0.8107
Epoch 27/40
8113/8113 [==============================] - 1s - loss: 0.4271 - acc: 0.8118 - val_loss: 0.4417 - val_acc: 0.8103
Epoch 28/40
8113/8113 [==============================] - 1s - loss: 0.4260 - acc: 0.8140 - val_loss: 0.4421 - val_acc: 0.8083
Epoch 29/40
8113/8113 [==============================] - 1s - loss: 0.4260 - acc: 0.8149 - val_loss: 0.4412 - val_acc: 0.8078
Epoch 30/40
8113/8113 [==============================] - 1s - loss: 0.4253 - acc: 0.8167 - val_loss: 0.4400 - val_acc: 0.8073
Epoch 31/40
8113/8113 [==============================] - 1s - loss: 0.4248 - acc: 0.8155 - val_loss: 0.4395 - val_acc: 0.8103
Epoch 32/40
8113/8113 [==============================] - 1s - loss: 0.4245 - acc: 0.8154 - val_loss: 0.4398 - val_acc: 0.8107
Epoch 33/40
8113/8113 [==============================] - 1s - loss: 0.4242 - acc: 0.8154 - val_loss: 0.4389 - val_acc: 0.8132
Epoch 34/40
8113/8113 [==============================] - 1s - loss: 0.4238 - acc: 0.8162 - val_loss: 0.4385 - val_acc: 0.8103
Epoch 35/40
8113/8113 [==============================] - 1s - loss: 0.4234 - acc: 0.8165 - val_loss: 0.4387 - val_acc: 0.8063
Epoch 36/40
8113/8113 [==============================] - 1s - loss: 0.4231 - acc: 0.8161 - val_loss: 0.4393 - val_acc: 0.8078
Epoch 37/40
8113/8113 [==============================] - 1s - loss: 0.4227 - acc: 0.8173 - val_loss: 0.4400 - val_acc: 0.8078
Epoch 38/40
8113/8113 [==============================] - 1s - loss: 0.4225 - acc: 0.8168 - val_loss: 0.4393 - val_acc: 0.8053
Epoch 39/40
8113/8113 [==============================] - 1s - loss: 0.4219 - acc: 0.8170 - val_loss: 0.4387 - val_acc: 0.8058
Epoch 40/40
8113/8113 [==============================] - 1s - loss: 0.4217 - acc: 0.8170 - val_loss: 0.4386 - val_acc: 0.8058
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-17.h5
chunk number 17
prepare data
start training
Train on 8134 samples, validate on 2034 samples
Epoch 1/40
8134/8134 [==============================] - 1s - loss: 0.4516 - acc: 0.7970 - val_loss: 0.4419 - val_acc: 0.8033
Epoch 2/40
8134/8134 [==============================] - 1s - loss: 0.4491 - acc: 0.7968 - val_loss: 0.4389 - val_acc: 0.8058
Epoch 3/40
8134/8134 [==============================] - 1s - loss: 0.4462 - acc: 0.7996 - val_loss: 0.4350 - val_acc: 0.8117
Epoch 4/40
8134/8134 [==============================] - 1s - loss: 0.4421 - acc: 0.8055 - val_loss: 0.4394 - val_acc: 0.8063
Epoch 5/40
8134/8134 [==============================] - 1s - loss: 0.4455 - acc: 0.8030 - val_loss: 0.4338 - val_acc: 0.8127
Epoch 6/40
8134/8134 [==============================] - 1s - loss: 0.4383 - acc: 0.8072 - val_loss: 0.4395 - val_acc: 0.8127
Epoch 7/40
8134/8134 [==============================] - 1s - loss: 0.4422 - acc: 0.8062 - val_loss: 0.4371 - val_acc: 0.8127
Epoch 8/40
8134/8134 [==============================] - 1s - loss: 0.4387 - acc: 0.8085 - val_loss: 0.4389 - val_acc: 0.8063
Epoch 9/40
8134/8134 [==============================] - 1s - loss: 0.4409 - acc: 0.8032 - val_loss: 0.4363 - val_acc: 0.8102
Epoch 10/40
8134/8134 [==============================] - 1s - loss: 0.4378 - acc: 0.8042 - val_loss: 0.4354 - val_acc: 0.8137
Epoch 11/40
8134/8134 [==============================] - 1s - loss: 0.4360 - acc: 0.8085 - val_loss: 0.4364 - val_acc: 0.8151
Epoch 12/40
8134/8134 [==============================] - 1s - loss: 0.4367 - acc: 0.8071 - val_loss: 0.4341 - val_acc: 0.8142
Epoch 13/40
8134/8134 [==============================] - 1s - loss: 0.4336 - acc: 0.8092 - val_loss: 0.4367 - val_acc: 0.8058
Epoch 14/40
8134/8134 [==============================] - 1s - loss: 0.4355 - acc: 0.8072 - val_loss: 0.4347 - val_acc: 0.8127
Epoch 15/40
8134/8134 [==============================] - 1s - loss: 0.4321 - acc: 0.8104 - val_loss: 0.4370 - val_acc: 0.8132
Epoch 16/40
8134/8134 [==============================] - 1s - loss: 0.4342 - acc: 0.8080 - val_loss: 0.4340 - val_acc: 0.8127
Epoch 17/40
8134/8134 [==============================] - 1s - loss: 0.4308 - acc: 0.8108 - val_loss: 0.4341 - val_acc: 0.8117
Epoch 18/40
8134/8134 [==============================] - 1s - loss: 0.4316 - acc: 0.8099 - val_loss: 0.4317 - val_acc: 0.8127
Epoch 19/40
8134/8134 [==============================] - 1s - loss: 0.4290 - acc: 0.8113 - val_loss: 0.4331 - val_acc: 0.8107
Epoch 20/40
8134/8134 [==============================] - 1s - loss: 0.4302 - acc: 0.8130 - val_loss: 0.4319 - val_acc: 0.8112
Epoch 21/40
8134/8134 [==============================] - 1s - loss: 0.4282 - acc: 0.8140 - val_loss: 0.4335 - val_acc: 0.8078
Epoch 22/40
8134/8134 [==============================] - 1s - loss: 0.4292 - acc: 0.8135 - val_loss: 0.4331 - val_acc: 0.8083
Epoch 23/40
8134/8134 [==============================] - 1s - loss: 0.4275 - acc: 0.8146 - val_loss: 0.4344 - val_acc: 0.8063
Epoch 24/40
8134/8134 [==============================] - 1s - loss: 0.4281 - acc: 0.8140 - val_loss: 0.4330 - val_acc: 0.8107
Epoch 25/40
8134/8134 [==============================] - 1s - loss: 0.4262 - acc: 0.8146 - val_loss: 0.4337 - val_acc: 0.8107
Epoch 26/40
8134/8134 [==============================] - 1s - loss: 0.4265 - acc: 0.8146 - val_loss: 0.4343 - val_acc: 0.8127
Epoch 27/40
8134/8134 [==============================] - 1s - loss: 0.4258 - acc: 0.8150 - val_loss: 0.4349 - val_acc: 0.8102
Epoch 28/40
8134/8134 [==============================] - 1s - loss: 0.4254 - acc: 0.8146 - val_loss: 0.4355 - val_acc: 0.8117
Epoch 29/40
8134/8134 [==============================] - 1s - loss: 0.4254 - acc: 0.8147 - val_loss: 0.4351 - val_acc: 0.8122
Epoch 30/40
8134/8134 [==============================] - 1s - loss: 0.4244 - acc: 0.8174 - val_loss: 0.4353 - val_acc: 0.8102
Epoch 31/40
8134/8134 [==============================] - 1s - loss: 0.4245 - acc: 0.8168 - val_loss: 0.4346 - val_acc: 0.8122
Epoch 32/40
8134/8134 [==============================] - 1s - loss: 0.4235 - acc: 0.8172 - val_loss: 0.4346 - val_acc: 0.8107
Epoch 33/40
8134/8134 [==============================] - 1s - loss: 0.4235 - acc: 0.8168 - val_loss: 0.4347 - val_acc: 0.8117
Epoch 34/40
8134/8134 [==============================] - 1s - loss: 0.4231 - acc: 0.8185 - val_loss: 0.4344 - val_acc: 0.8107
Epoch 35/40
8134/8134 [==============================] - 1s - loss: 0.4224 - acc: 0.8190 - val_loss: 0.4347 - val_acc: 0.8112
Epoch 36/40
8134/8134 [==============================] - 1s - loss: 0.4224 - acc: 0.8187 - val_loss: 0.4346 - val_acc: 0.8132
Epoch 37/40
8134/8134 [==============================] - 1s - loss: 0.4217 - acc: 0.8195 - val_loss: 0.4347 - val_acc: 0.8122
Epoch 38/40
8134/8134 [==============================] - 1s - loss: 0.4212 - acc: 0.8180 - val_loss: 0.4352 - val_acc: 0.8107
Epoch 39/40
8134/8134 [==============================] - 1s - loss: 0.4213 - acc: 0.8183 - val_loss: 0.4352 - val_acc: 0.8127
Epoch 40/40
8134/8134 [==============================] - 1s - loss: 0.4207 - acc: 0.8171 - val_loss: 0.4352 - val_acc: 0.8127
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-18.h5
chunk number 18
prepare data
start training
Train on 8221 samples, validate on 2056 samples
Epoch 1/40
8221/8221 [==============================] - 1s - loss: 0.4426 - acc: 0.8053 - val_loss: 0.4424 - val_acc: 0.8016
Epoch 2/40
8221/8221 [==============================] - 1s - loss: 0.4414 - acc: 0.8027 - val_loss: 0.4439 - val_acc: 0.7991
Epoch 3/40
8221/8221 [==============================] - 1s - loss: 0.4401 - acc: 0.8065 - val_loss: 0.4427 - val_acc: 0.8016
Epoch 4/40
8221/8221 [==============================] - 1s - loss: 0.4380 - acc: 0.8057 - val_loss: 0.4418 - val_acc: 0.8011
Epoch 5/40
8221/8221 [==============================] - 1s - loss: 0.4365 - acc: 0.8055 - val_loss: 0.4434 - val_acc: 0.7977
Epoch 6/40
8221/8221 [==============================] - 1s - loss: 0.4352 - acc: 0.8072 - val_loss: 0.4434 - val_acc: 0.7977
Epoch 7/40
8221/8221 [==============================] - 1s - loss: 0.4335 - acc: 0.8072 - val_loss: 0.4438 - val_acc: 0.8006
Epoch 8/40
8221/8221 [==============================] - 1s - loss: 0.4326 - acc: 0.8065 - val_loss: 0.4463 - val_acc: 0.7991
Epoch 9/40
8221/8221 [==============================] - 1s - loss: 0.4321 - acc: 0.8070 - val_loss: 0.4445 - val_acc: 0.7991
Epoch 10/40
8221/8221 [==============================] - 1s - loss: 0.4316 - acc: 0.8078 - val_loss: 0.4470 - val_acc: 0.7962
Epoch 11/40
8221/8221 [==============================] - 1s - loss: 0.4311 - acc: 0.8095 - val_loss: 0.4451 - val_acc: 0.8016
Epoch 12/40
8221/8221 [==============================] - 1s - loss: 0.4303 - acc: 0.8085 - val_loss: 0.4456 - val_acc: 0.8011
Epoch 13/40
8221/8221 [==============================] - 1s - loss: 0.4298 - acc: 0.8079 - val_loss: 0.4461 - val_acc: 0.8016
Epoch 14/40
8221/8221 [==============================] - 1s - loss: 0.4295 - acc: 0.8089 - val_loss: 0.4440 - val_acc: 0.8006
Epoch 15/40
8221/8221 [==============================] - 1s - loss: 0.4290 - acc: 0.8084 - val_loss: 0.4444 - val_acc: 0.8016
Epoch 16/40
8221/8221 [==============================] - 1s - loss: 0.4285 - acc: 0.8101 - val_loss: 0.4435 - val_acc: 0.8016
Epoch 17/40
8221/8221 [==============================] - 1s - loss: 0.4280 - acc: 0.8110 - val_loss: 0.4425 - val_acc: 0.8016
Epoch 18/40
8221/8221 [==============================] - 1s - loss: 0.4275 - acc: 0.8111 - val_loss: 0.4435 - val_acc: 0.8006
Epoch 19/40
8221/8221 [==============================] - 1s - loss: 0.4271 - acc: 0.8096 - val_loss: 0.4418 - val_acc: 0.8025
Epoch 20/40
8221/8221 [==============================] - 1s - loss: 0.4265 - acc: 0.8104 - val_loss: 0.4419 - val_acc: 0.8025
Epoch 21/40
8221/8221 [==============================] - 1s - loss: 0.4259 - acc: 0.8107 - val_loss: 0.4406 - val_acc: 0.8030
Epoch 22/40
8221/8221 [==============================] - 1s - loss: 0.4254 - acc: 0.8107 - val_loss: 0.4401 - val_acc: 0.8011
Epoch 23/40
8221/8221 [==============================] - 1s - loss: 0.4250 - acc: 0.8118 - val_loss: 0.4403 - val_acc: 0.8025
Epoch 24/40
8221/8221 [==============================] - 1s - loss: 0.4247 - acc: 0.8123 - val_loss: 0.4395 - val_acc: 0.8006
Epoch 25/40
8221/8221 [==============================] - 1s - loss: 0.4244 - acc: 0.8117 - val_loss: 0.4397 - val_acc: 0.8030
Epoch 26/40
8221/8221 [==============================] - 1s - loss: 0.4239 - acc: 0.8111 - val_loss: 0.4392 - val_acc: 0.8030
Epoch 27/40
8221/8221 [==============================] - 1s - loss: 0.4235 - acc: 0.8110 - val_loss: 0.4396 - val_acc: 0.8040
Epoch 28/40
8221/8221 [==============================] - 1s - loss: 0.4231 - acc: 0.8117 - val_loss: 0.4398 - val_acc: 0.8035
Epoch 29/40
8221/8221 [==============================] - 1s - loss: 0.4226 - acc: 0.8122 - val_loss: 0.4401 - val_acc: 0.8045
Epoch 30/40
8221/8221 [==============================] - 1s - loss: 0.4222 - acc: 0.8123 - val_loss: 0.4406 - val_acc: 0.8011
Epoch 31/40
8221/8221 [==============================] - 1s - loss: 0.4218 - acc: 0.8134 - val_loss: 0.4405 - val_acc: 0.8011
Epoch 32/40
8221/8221 [==============================] - 1s - loss: 0.4215 - acc: 0.8128 - val_loss: 0.4423 - val_acc: 0.7996
Epoch 33/40
8221/8221 [==============================] - 1s - loss: 0.4215 - acc: 0.8138 - val_loss: 0.4417 - val_acc: 0.8050
Epoch 34/40
8221/8221 [==============================] - 1s - loss: 0.4231 - acc: 0.8101 - val_loss: 0.4542 - val_acc: 0.7918
Epoch 35/40
8221/8221 [==============================] - 1s - loss: 0.4325 - acc: 0.8087 - val_loss: 0.4565 - val_acc: 0.7928
Epoch 36/40
8221/8221 [==============================] - 1s - loss: 0.4498 - acc: 0.7943 - val_loss: 0.4557 - val_acc: 0.7933
Epoch 37/40
8221/8221 [==============================] - 1s - loss: 0.4362 - acc: 0.8111 - val_loss: 0.4502 - val_acc: 0.7957
Epoch 38/40
8221/8221 [==============================] - 1s - loss: 0.4329 - acc: 0.8115 - val_loss: 0.4430 - val_acc: 0.7986
Epoch 39/40
8221/8221 [==============================] - 1s - loss: 0.4355 - acc: 0.8012 - val_loss: 0.4389 - val_acc: 0.7977
Epoch 40/40
8221/8221 [==============================] - 1s - loss: 0.4269 - acc: 0.8081 - val_loss: 0.4577 - val_acc: 0.7884
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-19.h5
chunk number 19
prepare data
start training
Train on 8180 samples, validate on 2046 samples
Epoch 1/40
8180/8180 [==============================] - 2s - loss: 0.5098 - acc: 0.7708 - val_loss: 0.4527 - val_acc: 0.7937
Epoch 2/40
8180/8180 [==============================] - 2s - loss: 0.4696 - acc: 0.7902 - val_loss: 0.4757 - val_acc: 0.7854
Epoch 3/40
8180/8180 [==============================] - 2s - loss: 0.4920 - acc: 0.7836 - val_loss: 0.4520 - val_acc: 0.7849
Epoch 4/40
8180/8180 [==============================] - 2s - loss: 0.4655 - acc: 0.7897 - val_loss: 0.4641 - val_acc: 0.7801
Epoch 5/40
8180/8180 [==============================] - 2s - loss: 0.4717 - acc: 0.7897 - val_loss: 0.4667 - val_acc: 0.7786
Epoch 6/40
8180/8180 [==============================] - 2s - loss: 0.4711 - acc: 0.7842 - val_loss: 0.4575 - val_acc: 0.7825
Epoch 7/40
8180/8180 [==============================] - 2s - loss: 0.4639 - acc: 0.7930 - val_loss: 0.4567 - val_acc: 0.7986
Epoch 8/40
8180/8180 [==============================] - 2s - loss: 0.4662 - acc: 0.7969 - val_loss: 0.4455 - val_acc: 0.8006
Epoch 9/40
8180/8180 [==============================] - 2s - loss: 0.4556 - acc: 0.7983 - val_loss: 0.4439 - val_acc: 0.7996
Epoch 10/40
8180/8180 [==============================] - 2s - loss: 0.4519 - acc: 0.8001 - val_loss: 0.4481 - val_acc: 0.7986
Epoch 11/40
8180/8180 [==============================] - 2s - loss: 0.4551 - acc: 0.8001 - val_loss: 0.4408 - val_acc: 0.8079
Epoch 12/40
8180/8180 [==============================] - 2s - loss: 0.4495 - acc: 0.8031 - val_loss: 0.4367 - val_acc: 0.8084
Epoch 13/40
8180/8180 [==============================] - 2s - loss: 0.4494 - acc: 0.8024 - val_loss: 0.4386 - val_acc: 0.8133
Epoch 14/40
8180/8180 [==============================] - 2s - loss: 0.4503 - acc: 0.8032 - val_loss: 0.4436 - val_acc: 0.8006
Epoch 15/40
8180/8180 [==============================] - 2s - loss: 0.4503 - acc: 0.8018 - val_loss: 0.4381 - val_acc: 0.8035
Epoch 16/40
8180/8180 [==============================] - 2s - loss: 0.4447 - acc: 0.8045 - val_loss: 0.4335 - val_acc: 0.8118
Epoch 17/40
8180/8180 [==============================] - 2s - loss: 0.4472 - acc: 0.8027 - val_loss: 0.4348 - val_acc: 0.8060
Epoch 18/40
8180/8180 [==============================] - 2s - loss: 0.4474 - acc: 0.8028 - val_loss: 0.4390 - val_acc: 0.8021
Epoch 19/40
8180/8180 [==============================] - 2s - loss: 0.4465 - acc: 0.8040 - val_loss: 0.4395 - val_acc: 0.7991
Epoch 20/40
8180/8180 [==============================] - 2s - loss: 0.4451 - acc: 0.8040 - val_loss: 0.4346 - val_acc: 0.8079
Epoch 21/40
8180/8180 [==============================] - 2s - loss: 0.4422 - acc: 0.8073 - val_loss: 0.4350 - val_acc: 0.8128
Epoch 22/40
8180/8180 [==============================] - 2s - loss: 0.4441 - acc: 0.8062 - val_loss: 0.4351 - val_acc: 0.8074
Epoch 23/40
8180/8180 [==============================] - 2s - loss: 0.4417 - acc: 0.8089 - val_loss: 0.4379 - val_acc: 0.8035
Epoch 24/40
8180/8180 [==============================] - 2s - loss: 0.4432 - acc: 0.8071 - val_loss: 0.4339 - val_acc: 0.8084
Epoch 25/40
8180/8180 [==============================] - 2s - loss: 0.4409 - acc: 0.8090 - val_loss: 0.4317 - val_acc: 0.8167
Epoch 26/40
8180/8180 [==============================] - 2s - loss: 0.4417 - acc: 0.8039 - val_loss: 0.4307 - val_acc: 0.8084
Epoch 27/40
8180/8180 [==============================] - 2s - loss: 0.4399 - acc: 0.8076 - val_loss: 0.4333 - val_acc: 0.8060
Epoch 28/40
8180/8180 [==============================] - 2s - loss: 0.4399 - acc: 0.8086 - val_loss: 0.4317 - val_acc: 0.8050
Epoch 29/40
8180/8180 [==============================] - 2s - loss: 0.4386 - acc: 0.8075 - val_loss: 0.4304 - val_acc: 0.8109
Epoch 30/40
8180/8180 [==============================] - 2s - loss: 0.4390 - acc: 0.8065 - val_loss: 0.4312 - val_acc: 0.8094
Epoch 31/40
8180/8180 [==============================] - 2s - loss: 0.4381 - acc: 0.8056 - val_loss: 0.4329 - val_acc: 0.8030
Epoch 32/40
8180/8180 [==============================] - 2s - loss: 0.4379 - acc: 0.8083 - val_loss: 0.4304 - val_acc: 0.8074
Epoch 33/40
8180/8180 [==============================] - 2s - loss: 0.4364 - acc: 0.8083 - val_loss: 0.4296 - val_acc: 0.8113
Epoch 34/40
8180/8180 [==============================] - 2s - loss: 0.4366 - acc: 0.8082 - val_loss: 0.4309 - val_acc: 0.8089
Epoch 35/40
8180/8180 [==============================] - 2s - loss: 0.4360 - acc: 0.8084 - val_loss: 0.4322 - val_acc: 0.8055
Epoch 36/40
8180/8180 [==============================] - 2s - loss: 0.4356 - acc: 0.8089 - val_loss: 0.4306 - val_acc: 0.8109
Epoch 37/40
8180/8180 [==============================] - 2s - loss: 0.4346 - acc: 0.8094 - val_loss: 0.4305 - val_acc: 0.8104
Epoch 38/40
8180/8180 [==============================] - 2s - loss: 0.4345 - acc: 0.8092 - val_loss: 0.4315 - val_acc: 0.8074
Epoch 39/40
8180/8180 [==============================] - 2s - loss: 0.4339 - acc: 0.8100 - val_loss: 0.4315 - val_acc: 0.8060
Epoch 40/40
8180/8180 [==============================] - 2s - loss: 0.4333 - acc: 0.8097 - val_loss: 0.4310 - val_acc: 0.8069
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-20.h5
chunk number 20
prepare data
start training
Train on 8123 samples, validate on 2031 samples
Epoch 1/40
8123/8123 [==============================] - 1s - loss: 0.4465 - acc: 0.8048 - val_loss: 0.4433 - val_acc: 0.7962
Epoch 2/40
8123/8123 [==============================] - 1s - loss: 0.4386 - acc: 0.8040 - val_loss: 0.4517 - val_acc: 0.7957
Epoch 3/40
8123/8123 [==============================] - 1s - loss: 0.4426 - acc: 0.8016 - val_loss: 0.4368 - val_acc: 0.8060
Epoch 4/40
8123/8123 [==============================] - 1s - loss: 0.4335 - acc: 0.8098 - val_loss: 0.4385 - val_acc: 0.8060
Epoch 5/40
8123/8123 [==============================] - 1s - loss: 0.4392 - acc: 0.8071 - val_loss: 0.4346 - val_acc: 0.8114
Epoch 6/40
8123/8123 [==============================] - 1s - loss: 0.4337 - acc: 0.8088 - val_loss: 0.4399 - val_acc: 0.8035
Epoch 7/40
8123/8123 [==============================] - 1s - loss: 0.4335 - acc: 0.8118 - val_loss: 0.4425 - val_acc: 0.8001
Epoch 8/40
8123/8123 [==============================] - 1s - loss: 0.4347 - acc: 0.8114 - val_loss: 0.4356 - val_acc: 0.8109
Epoch 9/40
8123/8123 [==============================] - 1s - loss: 0.4308 - acc: 0.8120 - val_loss: 0.4364 - val_acc: 0.8031
Epoch 10/40
8123/8123 [==============================] - 1s - loss: 0.4328 - acc: 0.8115 - val_loss: 0.4348 - val_acc: 0.8060
Epoch 11/40
8123/8123 [==============================] - 1s - loss: 0.4289 - acc: 0.8118 - val_loss: 0.4383 - val_acc: 0.8070
Epoch 12/40
8123/8123 [==============================] - 1s - loss: 0.4295 - acc: 0.8128 - val_loss: 0.4386 - val_acc: 0.8050
Epoch 13/40
8123/8123 [==============================] - 1s - loss: 0.4294 - acc: 0.8139 - val_loss: 0.4345 - val_acc: 0.8060
Epoch 14/40
8123/8123 [==============================] - 1s - loss: 0.4274 - acc: 0.8139 - val_loss: 0.4341 - val_acc: 0.8060
Epoch 15/40
8123/8123 [==============================] - 1s - loss: 0.4282 - acc: 0.8118 - val_loss: 0.4337 - val_acc: 0.8095
Epoch 16/40
8123/8123 [==============================] - 1s - loss: 0.4256 - acc: 0.8160 - val_loss: 0.4375 - val_acc: 0.8045
Epoch 17/40
8123/8123 [==============================] - 1s - loss: 0.4269 - acc: 0.8134 - val_loss: 0.4350 - val_acc: 0.8050
Epoch 18/40
8123/8123 [==============================] - 1s - loss: 0.4247 - acc: 0.8148 - val_loss: 0.4358 - val_acc: 0.8035
Epoch 19/40
8123/8123 [==============================] - 1s - loss: 0.4257 - acc: 0.8141 - val_loss: 0.4359 - val_acc: 0.8031
Epoch 20/40
8123/8123 [==============================] - 1s - loss: 0.4244 - acc: 0.8145 - val_loss: 0.4380 - val_acc: 0.8031
Epoch 21/40
8123/8123 [==============================] - 1s - loss: 0.4247 - acc: 0.8145 - val_loss: 0.4364 - val_acc: 0.8055
Epoch 22/40
8123/8123 [==============================] - 1s - loss: 0.4235 - acc: 0.8156 - val_loss: 0.4347 - val_acc: 0.8011
Epoch 23/40
8123/8123 [==============================] - 1s - loss: 0.4234 - acc: 0.8142 - val_loss: 0.4346 - val_acc: 0.8050
Epoch 24/40
8123/8123 [==============================] - 1s - loss: 0.4228 - acc: 0.8158 - val_loss: 0.4365 - val_acc: 0.8070
Epoch 25/40
8123/8123 [==============================] - 1s - loss: 0.4226 - acc: 0.8157 - val_loss: 0.4361 - val_acc: 0.8065
Epoch 26/40
8123/8123 [==============================] - 1s - loss: 0.4220 - acc: 0.8169 - val_loss: 0.4354 - val_acc: 0.8040
Epoch 27/40
8123/8123 [==============================] - 1s - loss: 0.4221 - acc: 0.8155 - val_loss: 0.4353 - val_acc: 0.8031
Epoch 28/40
8123/8123 [==============================] - 1s - loss: 0.4213 - acc: 0.8153 - val_loss: 0.4363 - val_acc: 0.8070
Epoch 29/40
8123/8123 [==============================] - 1s - loss: 0.4213 - acc: 0.8160 - val_loss: 0.4353 - val_acc: 0.8070
Epoch 30/40
8123/8123 [==============================] - 1s - loss: 0.4205 - acc: 0.8173 - val_loss: 0.4347 - val_acc: 0.8080
Epoch 31/40
8123/8123 [==============================] - 1s - loss: 0.4206 - acc: 0.8155 - val_loss: 0.4348 - val_acc: 0.8065
Epoch 32/40
8123/8123 [==============================] - 1s - loss: 0.4200 - acc: 0.8157 - val_loss: 0.4361 - val_acc: 0.8055
Epoch 33/40
8123/8123 [==============================] - 1s - loss: 0.4200 - acc: 0.8168 - val_loss: 0.4357 - val_acc: 0.8065
Epoch 34/40
8123/8123 [==============================] - 1s - loss: 0.4193 - acc: 0.8176 - val_loss: 0.4353 - val_acc: 0.8080
Epoch 35/40
8123/8123 [==============================] - 1s - loss: 0.4193 - acc: 0.8166 - val_loss: 0.4350 - val_acc: 0.8075
Epoch 36/40
8123/8123 [==============================] - 1s - loss: 0.4186 - acc: 0.8183 - val_loss: 0.4355 - val_acc: 0.8070
Epoch 37/40
8123/8123 [==============================] - 1s - loss: 0.4186 - acc: 0.8168 - val_loss: 0.4344 - val_acc: 0.8119
Epoch 38/40
8123/8123 [==============================] - 1s - loss: 0.4181 - acc: 0.8182 - val_loss: 0.4343 - val_acc: 0.8109
Epoch 39/40
8123/8123 [==============================] - 1s - loss: 0.4180 - acc: 0.8176 - val_loss: 0.4348 - val_acc: 0.8085
Epoch 40/40
8123/8123 [==============================] - 1s - loss: 0.4176 - acc: 0.8182 - val_loss: 0.4349 - val_acc: 0.8080
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-21.h5
chunk number 21
prepare data
start training
Train on 8140 samples, validate on 2036 samples
Epoch 1/40
8140/8140 [==============================] - 2s - loss: 0.4812 - acc: 0.7862 - val_loss: 0.4964 - val_acc: 0.7908
Epoch 2/40
8140/8140 [==============================] - 2s - loss: 0.4849 - acc: 0.7885 - val_loss: 0.4799 - val_acc: 0.7917
Epoch 3/40
8140/8140 [==============================] - 2s - loss: 0.4587 - acc: 0.7941 - val_loss: 0.4854 - val_acc: 0.7726
Epoch 4/40
8140/8140 [==============================] - 2s - loss: 0.4629 - acc: 0.7893 - val_loss: 0.4866 - val_acc: 0.7721
Epoch 5/40
8140/8140 [==============================] - 2s - loss: 0.4644 - acc: 0.7845 - val_loss: 0.4857 - val_acc: 0.7893
Epoch 6/40
8140/8140 [==============================] - 2s - loss: 0.4668 - acc: 0.7945 - val_loss: 0.4771 - val_acc: 0.7947
Epoch 7/40
8140/8140 [==============================] - 2s - loss: 0.4560 - acc: 0.8004 - val_loss: 0.4703 - val_acc: 0.7947
Epoch 8/40
8140/8140 [==============================] - 2s - loss: 0.4477 - acc: 0.8026 - val_loss: 0.4677 - val_acc: 0.7908
Epoch 9/40
8140/8140 [==============================] - 2s - loss: 0.4471 - acc: 0.8055 - val_loss: 0.4640 - val_acc: 0.7927
Epoch 10/40
8140/8140 [==============================] - 2s - loss: 0.4483 - acc: 0.7936 - val_loss: 0.4592 - val_acc: 0.7967
Epoch 11/40
8140/8140 [==============================] - 2s - loss: 0.4440 - acc: 0.7937 - val_loss: 0.4555 - val_acc: 0.7972
Epoch 12/40
8140/8140 [==============================] - 2s - loss: 0.4381 - acc: 0.8031 - val_loss: 0.4574 - val_acc: 0.8050
Epoch 13/40
8140/8140 [==============================] - 2s - loss: 0.4399 - acc: 0.8010 - val_loss: 0.4574 - val_acc: 0.8006
Epoch 14/40
8140/8140 [==============================] - 2s - loss: 0.4386 - acc: 0.8009 - val_loss: 0.4517 - val_acc: 0.8001
Epoch 15/40
8140/8140 [==============================] - 2s - loss: 0.4331 - acc: 0.8029 - val_loss: 0.4513 - val_acc: 0.8001
Epoch 16/40
8140/8140 [==============================] - 2s - loss: 0.4314 - acc: 0.8033 - val_loss: 0.4547 - val_acc: 0.7981
Epoch 17/40
8140/8140 [==============================] - 2s - loss: 0.4336 - acc: 0.8050 - val_loss: 0.4550 - val_acc: 0.7981
Epoch 18/40
8140/8140 [==============================] - 2s - loss: 0.4338 - acc: 0.8058 - val_loss: 0.4511 - val_acc: 0.7991
Epoch 19/40
8140/8140 [==============================] - 2s - loss: 0.4299 - acc: 0.8080 - val_loss: 0.4493 - val_acc: 0.7991
Epoch 20/40
8140/8140 [==============================] - 2s - loss: 0.4300 - acc: 0.8076 - val_loss: 0.4515 - val_acc: 0.7991
Epoch 21/40
8140/8140 [==============================] - 2s - loss: 0.4311 - acc: 0.8074 - val_loss: 0.4504 - val_acc: 0.7991
Epoch 22/40
8140/8140 [==============================] - 2s - loss: 0.4296 - acc: 0.8081 - val_loss: 0.4468 - val_acc: 0.7942
Epoch 23/40
8140/8140 [==============================] - 2s - loss: 0.4268 - acc: 0.8091 - val_loss: 0.4477 - val_acc: 0.8021
Epoch 24/40
8140/8140 [==============================] - 2s - loss: 0.4285 - acc: 0.8107 - val_loss: 0.4486 - val_acc: 0.7976
Epoch 25/40
8140/8140 [==============================] - 2s - loss: 0.4268 - acc: 0.8080 - val_loss: 0.4491 - val_acc: 0.7957
Epoch 26/40
8140/8140 [==============================] - 2s - loss: 0.4255 - acc: 0.8102 - val_loss: 0.4467 - val_acc: 0.8011
Epoch 27/40
8140/8140 [==============================] - 2s - loss: 0.4251 - acc: 0.8109 - val_loss: 0.4470 - val_acc: 0.8021
Epoch 28/40
8140/8140 [==============================] - 2s - loss: 0.4252 - acc: 0.8106 - val_loss: 0.4484 - val_acc: 0.8035
Epoch 29/40
8140/8140 [==============================] - 2s - loss: 0.4239 - acc: 0.8098 - val_loss: 0.4488 - val_acc: 0.8021
Epoch 30/40
8140/8140 [==============================] - 2s - loss: 0.4233 - acc: 0.8107 - val_loss: 0.4460 - val_acc: 0.7991
Epoch 31/40
8140/8140 [==============================] - 2s - loss: 0.4237 - acc: 0.8109 - val_loss: 0.4451 - val_acc: 0.8006
Epoch 32/40
8140/8140 [==============================] - 2s - loss: 0.4223 - acc: 0.8111 - val_loss: 0.4476 - val_acc: 0.7976
Epoch 33/40
8140/8140 [==============================] - 2s - loss: 0.4225 - acc: 0.8107 - val_loss: 0.4461 - val_acc: 0.7996
Epoch 34/40
8140/8140 [==============================] - 2s - loss: 0.4213 - acc: 0.8106 - val_loss: 0.4462 - val_acc: 0.8021
Epoch 35/40
8140/8140 [==============================] - 2s - loss: 0.4221 - acc: 0.8131 - val_loss: 0.4461 - val_acc: 0.8006
Epoch 36/40
8140/8140 [==============================] - 2s - loss: 0.4198 - acc: 0.8117 - val_loss: 0.4495 - val_acc: 0.7991
Epoch 37/40
8140/8140 [==============================] - 2s - loss: 0.4212 - acc: 0.8133 - val_loss: 0.4471 - val_acc: 0.8011
Epoch 38/40
8140/8140 [==============================] - 2s - loss: 0.4195 - acc: 0.8129 - val_loss: 0.4472 - val_acc: 0.8050
Epoch 39/40
8140/8140 [==============================] - 2s - loss: 0.4199 - acc: 0.8143 - val_loss: 0.4487 - val_acc: 0.8011
Epoch 40/40
8140/8140 [==============================] - 2s - loss: 0.4196 - acc: 0.8128 - val_loss: 0.4478 - val_acc: 0.8006
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-22.h5
chunk number 22
prepare data
start training
Train on 8040 samples, validate on 2010 samples
Epoch 1/40
8040/8040 [==============================] - 1s - loss: 0.4492 - acc: 0.7964 - val_loss: 0.4131 - val_acc: 0.8154
Epoch 2/40
8040/8040 [==============================] - 1s - loss: 0.4458 - acc: 0.7995 - val_loss: 0.4146 - val_acc: 0.8174
Epoch 3/40
8040/8040 [==============================] - 1s - loss: 0.4452 - acc: 0.7990 - val_loss: 0.4172 - val_acc: 0.8164
Epoch 4/40
8040/8040 [==============================] - 1s - loss: 0.4414 - acc: 0.8014 - val_loss: 0.4234 - val_acc: 0.8154
Epoch 5/40
8040/8040 [==============================] - 1s - loss: 0.4435 - acc: 0.8021 - val_loss: 0.4188 - val_acc: 0.8224
Epoch 6/40
8040/8040 [==============================] - 1s - loss: 0.4402 - acc: 0.8035 - val_loss: 0.4175 - val_acc: 0.8164
Epoch 7/40
8040/8040 [==============================] - 1s - loss: 0.4415 - acc: 0.8009 - val_loss: 0.4159 - val_acc: 0.8204
Epoch 8/40
8040/8040 [==============================] - 1s - loss: 0.4386 - acc: 0.8021 - val_loss: 0.4195 - val_acc: 0.8169
Epoch 9/40
8040/8040 [==============================] - 1s - loss: 0.4394 - acc: 0.8032 - val_loss: 0.4175 - val_acc: 0.8199
Epoch 10/40
8040/8040 [==============================] - 1s - loss: 0.4381 - acc: 0.8029 - val_loss: 0.4149 - val_acc: 0.8234
Epoch 11/40
8040/8040 [==============================] - 1s - loss: 0.4374 - acc: 0.8005 - val_loss: 0.4145 - val_acc: 0.8229
Epoch 12/40
8040/8040 [==============================] - 1s - loss: 0.4367 - acc: 0.8011 - val_loss: 0.4151 - val_acc: 0.8219
Epoch 13/40
8040/8040 [==============================] - 1s - loss: 0.4348 - acc: 0.8046 - val_loss: 0.4163 - val_acc: 0.8199
Epoch 14/40
8040/8040 [==============================] - 1s - loss: 0.4350 - acc: 0.8066 - val_loss: 0.4147 - val_acc: 0.8244
Epoch 15/40
8040/8040 [==============================] - 1s - loss: 0.4338 - acc: 0.8025 - val_loss: 0.4152 - val_acc: 0.8249
Epoch 16/40
8040/8040 [==============================] - 1s - loss: 0.4342 - acc: 0.8025 - val_loss: 0.4156 - val_acc: 0.8199
Epoch 17/40
8040/8040 [==============================] - 1s - loss: 0.4330 - acc: 0.8040 - val_loss: 0.4161 - val_acc: 0.8199
Epoch 18/40
8040/8040 [==============================] - 1s - loss: 0.4329 - acc: 0.8047 - val_loss: 0.4149 - val_acc: 0.8234
Epoch 19/40
8040/8040 [==============================] - 1s - loss: 0.4322 - acc: 0.8042 - val_loss: 0.4149 - val_acc: 0.8224
Epoch 20/40
8040/8040 [==============================] - 1s - loss: 0.4318 - acc: 0.8050 - val_loss: 0.4161 - val_acc: 0.8154
Epoch 21/40
8040/8040 [==============================] - 1s - loss: 0.4315 - acc: 0.8052 - val_loss: 0.4156 - val_acc: 0.8169
Epoch 22/40
8040/8040 [==============================] - 1s - loss: 0.4307 - acc: 0.8055 - val_loss: 0.4155 - val_acc: 0.8179
Epoch 23/40
8040/8040 [==============================] - 1s - loss: 0.4310 - acc: 0.8041 - val_loss: 0.4161 - val_acc: 0.8154
Epoch 24/40
8040/8040 [==============================] - 1s - loss: 0.4300 - acc: 0.8056 - val_loss: 0.4174 - val_acc: 0.8154
Epoch 25/40
8040/8040 [==============================] - 1s - loss: 0.4303 - acc: 0.8050 - val_loss: 0.4160 - val_acc: 0.8159
Epoch 26/40
8040/8040 [==============================] - 1s - loss: 0.4295 - acc: 0.8056 - val_loss: 0.4157 - val_acc: 0.8164
Epoch 27/40
8040/8040 [==============================] - 1s - loss: 0.4289 - acc: 0.8073 - val_loss: 0.4163 - val_acc: 0.8159
Epoch 28/40
8040/8040 [==============================] - 1s - loss: 0.4289 - acc: 0.8072 - val_loss: 0.4151 - val_acc: 0.8179
Epoch 29/40
8040/8040 [==============================] - 1s - loss: 0.4281 - acc: 0.8080 - val_loss: 0.4151 - val_acc: 0.8189
Epoch 30/40
8040/8040 [==============================] - 1s - loss: 0.4282 - acc: 0.8076 - val_loss: 0.4159 - val_acc: 0.8169
Epoch 31/40
8040/8040 [==============================] - 1s - loss: 0.4278 - acc: 0.8086 - val_loss: 0.4160 - val_acc: 0.8184
Epoch 32/40
8040/8040 [==============================] - 1s - loss: 0.4273 - acc: 0.8085 - val_loss: 0.4160 - val_acc: 0.8164
Epoch 33/40
8040/8040 [==============================] - 1s - loss: 0.4273 - acc: 0.8085 - val_loss: 0.4167 - val_acc: 0.8164
Epoch 34/40
8040/8040 [==============================] - 1s - loss: 0.4266 - acc: 0.8095 - val_loss: 0.4169 - val_acc: 0.8159
Epoch 35/40
8040/8040 [==============================] - 1s - loss: 0.4263 - acc: 0.8097 - val_loss: 0.4162 - val_acc: 0.8164
Epoch 36/40
8040/8040 [==============================] - 1s - loss: 0.4262 - acc: 0.8083 - val_loss: 0.4167 - val_acc: 0.8154
Epoch 37/40
8040/8040 [==============================] - 1s - loss: 0.4256 - acc: 0.8103 - val_loss: 0.4170 - val_acc: 0.8134
Epoch 38/40
8040/8040 [==============================] - 1s - loss: 0.4254 - acc: 0.8103 - val_loss: 0.4165 - val_acc: 0.8139
Epoch 39/40
8040/8040 [==============================] - 1s - loss: 0.4253 - acc: 0.8101 - val_loss: 0.4174 - val_acc: 0.8104
Epoch 40/40
8040/8040 [==============================] - 1s - loss: 0.4247 - acc: 0.8114 - val_loss: 0.4174 - val_acc: 0.8124
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-23.h5
chunk number 23
prepare data
start training
Train on 8117 samples, validate on 2030 samples
Epoch 1/40
8117/8117 [==============================] - 1s - loss: 0.4228 - acc: 0.8163 - val_loss: 0.4451 - val_acc: 0.8039
Epoch 2/40
8117/8117 [==============================] - 1s - loss: 0.4222 - acc: 0.8167 - val_loss: 0.4446 - val_acc: 0.8044
Epoch 3/40
8117/8117 [==============================] - 1s - loss: 0.4211 - acc: 0.8168 - val_loss: 0.4458 - val_acc: 0.8039
Epoch 4/40
8117/8117 [==============================] - 1s - loss: 0.4212 - acc: 0.8161 - val_loss: 0.4448 - val_acc: 0.8089
Epoch 5/40
8117/8117 [==============================] - 1s - loss: 0.4202 - acc: 0.8168 - val_loss: 0.4449 - val_acc: 0.8084
Epoch 6/40
8117/8117 [==============================] - 1s - loss: 0.4202 - acc: 0.8169 - val_loss: 0.4451 - val_acc: 0.8034
Epoch 7/40
8117/8117 [==============================] - 1s - loss: 0.4193 - acc: 0.8191 - val_loss: 0.4453 - val_acc: 0.8044
Epoch 8/40
8117/8117 [==============================] - 1s - loss: 0.4191 - acc: 0.8179 - val_loss: 0.4446 - val_acc: 0.8059
Epoch 9/40
8117/8117 [==============================] - 1s - loss: 0.4184 - acc: 0.8190 - val_loss: 0.4446 - val_acc: 0.8054
Epoch 10/40
8117/8117 [==============================] - 1s - loss: 0.4177 - acc: 0.8180 - val_loss: 0.4456 - val_acc: 0.8039
Epoch 11/40
8117/8117 [==============================] - 1s - loss: 0.4173 - acc: 0.8189 - val_loss: 0.4453 - val_acc: 0.8039
Epoch 12/40
8117/8117 [==============================] - 1s - loss: 0.4164 - acc: 0.8187 - val_loss: 0.4454 - val_acc: 0.8069
Epoch 13/40
8117/8117 [==============================] - 1s - loss: 0.4162 - acc: 0.8172 - val_loss: 0.4457 - val_acc: 0.8034
Epoch 14/40
8117/8117 [==============================] - 1s - loss: 0.4156 - acc: 0.8189 - val_loss: 0.4452 - val_acc: 0.8044
Epoch 15/40
8117/8117 [==============================] - 1s - loss: 0.4150 - acc: 0.8194 - val_loss: 0.4441 - val_acc: 0.8069
Epoch 16/40
8117/8117 [==============================] - 1s - loss: 0.4147 - acc: 0.8174 - val_loss: 0.4436 - val_acc: 0.8059
Epoch 17/40
8117/8117 [==============================] - 1s - loss: 0.4140 - acc: 0.8182 - val_loss: 0.4429 - val_acc: 0.8064
Epoch 18/40
8117/8117 [==============================] - 1s - loss: 0.4135 - acc: 0.8193 - val_loss: 0.4424 - val_acc: 0.8044
Epoch 19/40
8117/8117 [==============================] - 1s - loss: 0.4132 - acc: 0.8184 - val_loss: 0.4432 - val_acc: 0.8054
Epoch 20/40
8117/8117 [==============================] - 1s - loss: 0.4126 - acc: 0.8191 - val_loss: 0.4435 - val_acc: 0.8049
Epoch 21/40
8117/8117 [==============================] - 1s - loss: 0.4120 - acc: 0.8196 - val_loss: 0.4439 - val_acc: 0.8044
Epoch 22/40
8117/8117 [==============================] - 1s - loss: 0.4116 - acc: 0.8199 - val_loss: 0.4451 - val_acc: 0.8030
Epoch 23/40
8117/8117 [==============================] - 1s - loss: 0.4111 - acc: 0.8214 - val_loss: 0.4451 - val_acc: 0.8034
Epoch 24/40
8117/8117 [==============================] - 1s - loss: 0.4106 - acc: 0.8215 - val_loss: 0.4454 - val_acc: 0.8034
Epoch 25/40
8117/8117 [==============================] - 1s - loss: 0.4102 - acc: 0.8217 - val_loss: 0.4463 - val_acc: 0.8030
Epoch 26/40
8117/8117 [==============================] - 1s - loss: 0.4098 - acc: 0.8222 - val_loss: 0.4461 - val_acc: 0.8030
Epoch 27/40
8117/8117 [==============================] - 1s - loss: 0.4094 - acc: 0.8232 - val_loss: 0.4468 - val_acc: 0.8020
Epoch 28/40
8117/8117 [==============================] - 1s - loss: 0.4089 - acc: 0.8233 - val_loss: 0.4466 - val_acc: 0.8034
Epoch 29/40
8117/8117 [==============================] - 1s - loss: 0.4084 - acc: 0.8237 - val_loss: 0.4466 - val_acc: 0.8020
Epoch 30/40
8117/8117 [==============================] - 1s - loss: 0.4080 - acc: 0.8243 - val_loss: 0.4470 - val_acc: 0.8010
Epoch 31/40
8117/8117 [==============================] - 1s - loss: 0.4076 - acc: 0.8244 - val_loss: 0.4467 - val_acc: 0.8044
Epoch 32/40
8117/8117 [==============================] - 1s - loss: 0.4073 - acc: 0.8243 - val_loss: 0.4483 - val_acc: 0.8015
Epoch 33/40
8117/8117 [==============================] - 1s - loss: 0.4076 - acc: 0.8241 - val_loss: 0.4476 - val_acc: 0.8054
Epoch 34/40
8117/8117 [==============================] - 1s - loss: 0.4092 - acc: 0.8232 - val_loss: 0.4509 - val_acc: 0.8039
Epoch 35/40
8117/8117 [==============================] - 1s - loss: 0.4102 - acc: 0.8241 - val_loss: 0.4484 - val_acc: 0.8049
Epoch 36/40
8117/8117 [==============================] - 1s - loss: 0.4099 - acc: 0.8230 - val_loss: 0.4488 - val_acc: 0.8039
Epoch 37/40
8117/8117 [==============================] - 1s - loss: 0.4061 - acc: 0.8284 - val_loss: 0.4492 - val_acc: 0.8030
Epoch 38/40
8117/8117 [==============================] - 1s - loss: 0.4058 - acc: 0.8279 - val_loss: 0.4486 - val_acc: 0.8054
Epoch 39/40
8117/8117 [==============================] - 1s - loss: 0.4081 - acc: 0.8247 - val_loss: 0.4509 - val_acc: 0.8025
Epoch 40/40
8117/8117 [==============================] - 1s - loss: 0.4070 - acc: 0.8273 - val_loss: 0.4487 - val_acc: 0.8034
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-24.h5
chunk number 24
prepare data
start training
Train on 8140 samples, validate on 2035 samples
Epoch 1/40
8140/8140 [==============================] - 1s - loss: 0.4451 - acc: 0.8052 - val_loss: 0.4436 - val_acc: 0.8064
Epoch 2/40
8140/8140 [==============================] - 1s - loss: 0.4394 - acc: 0.8076 - val_loss: 0.4517 - val_acc: 0.7971
Epoch 3/40
8140/8140 [==============================] - 1s - loss: 0.4419 - acc: 0.8074 - val_loss: 0.4431 - val_acc: 0.8064
Epoch 4/40
8140/8140 [==============================] - 1s - loss: 0.4369 - acc: 0.8090 - val_loss: 0.4408 - val_acc: 0.8079
Epoch 5/40
8140/8140 [==============================] - 1s - loss: 0.4390 - acc: 0.8069 - val_loss: 0.4415 - val_acc: 0.8039
Epoch 6/40
8140/8140 [==============================] - 1s - loss: 0.4355 - acc: 0.8091 - val_loss: 0.4482 - val_acc: 0.8010
Epoch 7/40
8140/8140 [==============================] - 1s - loss: 0.4367 - acc: 0.8096 - val_loss: 0.4453 - val_acc: 0.8079
Epoch 8/40
8140/8140 [==============================] - 1s - loss: 0.4348 - acc: 0.8103 - val_loss: 0.4420 - val_acc: 0.8064
Epoch 9/40
8140/8140 [==============================] - 1s - loss: 0.4349 - acc: 0.8096 - val_loss: 0.4430 - val_acc: 0.8039
Epoch 10/40
8140/8140 [==============================] - 1s - loss: 0.4340 - acc: 0.8100 - val_loss: 0.4481 - val_acc: 0.8049
Epoch 11/40
8140/8140 [==============================] - 1s - loss: 0.4335 - acc: 0.8102 - val_loss: 0.4489 - val_acc: 0.8044
Epoch 12/40
8140/8140 [==============================] - 1s - loss: 0.4331 - acc: 0.8101 - val_loss: 0.4447 - val_acc: 0.8015
Epoch 13/40
8140/8140 [==============================] - 1s - loss: 0.4323 - acc: 0.8102 - val_loss: 0.4447 - val_acc: 0.8015
Epoch 14/40
8140/8140 [==============================] - 1s - loss: 0.4320 - acc: 0.8115 - val_loss: 0.4489 - val_acc: 0.8044
Epoch 15/40
8140/8140 [==============================] - 1s - loss: 0.4313 - acc: 0.8095 - val_loss: 0.4488 - val_acc: 0.8049
Epoch 16/40
8140/8140 [==============================] - 1s - loss: 0.4306 - acc: 0.8090 - val_loss: 0.4449 - val_acc: 0.8059
Epoch 17/40
8140/8140 [==============================] - 1s - loss: 0.4301 - acc: 0.8117 - val_loss: 0.4454 - val_acc: 0.8098
Epoch 18/40
8140/8140 [==============================] - 1s - loss: 0.4293 - acc: 0.8117 - val_loss: 0.4492 - val_acc: 0.8034
Epoch 19/40
8140/8140 [==============================] - 1s - loss: 0.4294 - acc: 0.8111 - val_loss: 0.4465 - val_acc: 0.8069
Epoch 20/40
8140/8140 [==============================] - 1s - loss: 0.4283 - acc: 0.8115 - val_loss: 0.4436 - val_acc: 0.8088
Epoch 21/40
8140/8140 [==============================] - 1s - loss: 0.4287 - acc: 0.8117 - val_loss: 0.4456 - val_acc: 0.8074
Epoch 22/40
8140/8140 [==============================] - 1s - loss: 0.4276 - acc: 0.8120 - val_loss: 0.4471 - val_acc: 0.8039
Epoch 23/40
8140/8140 [==============================] - 1s - loss: 0.4277 - acc: 0.8131 - val_loss: 0.4432 - val_acc: 0.8088
Epoch 24/40
8140/8140 [==============================] - 1s - loss: 0.4271 - acc: 0.8122 - val_loss: 0.4434 - val_acc: 0.8088
Epoch 25/40
8140/8140 [==============================] - 1s - loss: 0.4266 - acc: 0.8125 - val_loss: 0.4466 - val_acc: 0.8054
Epoch 26/40
8140/8140 [==============================] - 1s - loss: 0.4266 - acc: 0.8125 - val_loss: 0.4440 - val_acc: 0.8108
Epoch 27/40
8140/8140 [==============================] - 1s - loss: 0.4257 - acc: 0.8127 - val_loss: 0.4437 - val_acc: 0.8118
Epoch 28/40
8140/8140 [==============================] - 1s - loss: 0.4255 - acc: 0.8134 - val_loss: 0.4478 - val_acc: 0.8054
Epoch 29/40
8140/8140 [==============================] - 1s - loss: 0.4254 - acc: 0.8124 - val_loss: 0.4452 - val_acc: 0.8103
Epoch 30/40
8140/8140 [==============================] - 1s - loss: 0.4247 - acc: 0.8136 - val_loss: 0.4455 - val_acc: 0.8103
Epoch 31/40
8140/8140 [==============================] - 1s - loss: 0.4243 - acc: 0.8134 - val_loss: 0.4483 - val_acc: 0.8088
Epoch 32/40
8140/8140 [==============================] - 1s - loss: 0.4244 - acc: 0.8130 - val_loss: 0.4446 - val_acc: 0.8108
Epoch 33/40
8140/8140 [==============================] - 1s - loss: 0.4240 - acc: 0.8136 - val_loss: 0.4471 - val_acc: 0.8093
Epoch 34/40
8140/8140 [==============================] - 1s - loss: 0.4234 - acc: 0.8131 - val_loss: 0.4460 - val_acc: 0.8118
Epoch 35/40
8140/8140 [==============================] - 1s - loss: 0.4229 - acc: 0.8140 - val_loss: 0.4452 - val_acc: 0.8118
Epoch 36/40
8140/8140 [==============================] - 1s - loss: 0.4228 - acc: 0.8154 - val_loss: 0.4481 - val_acc: 0.8069
Epoch 37/40
8140/8140 [==============================] - 1s - loss: 0.4228 - acc: 0.8149 - val_loss: 0.4442 - val_acc: 0.8118
Epoch 38/40
8140/8140 [==============================] - 1s - loss: 0.4229 - acc: 0.8149 - val_loss: 0.4499 - val_acc: 0.8020
Epoch 39/40
8140/8140 [==============================] - 1s - loss: 0.4231 - acc: 0.8157 - val_loss: 0.4434 - val_acc: 0.8118
Epoch 40/40
8140/8140 [==============================] - 1s - loss: 0.4233 - acc: 0.8143 - val_loss: 0.4511 - val_acc: 0.7980
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-25.h5
chunk number 25
prepare data
start training
Train on 8058 samples, validate on 2015 samples
Epoch 1/40
8058/8058 [==============================] - 1s - loss: 0.4350 - acc: 0.8076 - val_loss: 0.4404 - val_acc: 0.8089
Epoch 2/40
8058/8058 [==============================] - 1s - loss: 0.4319 - acc: 0.8126 - val_loss: 0.4375 - val_acc: 0.8094
Epoch 3/40
8058/8058 [==============================] - 1s - loss: 0.4296 - acc: 0.8122 - val_loss: 0.4357 - val_acc: 0.8089
Epoch 4/40
8058/8058 [==============================] - 1s - loss: 0.4281 - acc: 0.8119 - val_loss: 0.4363 - val_acc: 0.8099
Epoch 5/40
8058/8058 [==============================] - 1s - loss: 0.4271 - acc: 0.8147 - val_loss: 0.4353 - val_acc: 0.8109
Epoch 6/40
8058/8058 [==============================] - 1s - loss: 0.4255 - acc: 0.8141 - val_loss: 0.4359 - val_acc: 0.8139
Epoch 7/40
8058/8058 [==============================] - 1s - loss: 0.4246 - acc: 0.8153 - val_loss: 0.4374 - val_acc: 0.8089
Epoch 8/40
8058/8058 [==============================] - 1s - loss: 0.4236 - acc: 0.8145 - val_loss: 0.4375 - val_acc: 0.8089
Epoch 9/40
8058/8058 [==============================] - 1s - loss: 0.4226 - acc: 0.8171 - val_loss: 0.4378 - val_acc: 0.8089
Epoch 10/40
8058/8058 [==============================] - 1s - loss: 0.4220 - acc: 0.8186 - val_loss: 0.4387 - val_acc: 0.8079
Epoch 11/40
8058/8058 [==============================] - 1s - loss: 0.4212 - acc: 0.8186 - val_loss: 0.4398 - val_acc: 0.8055
Epoch 12/40
8058/8058 [==============================] - 1s - loss: 0.4209 - acc: 0.8165 - val_loss: 0.4408 - val_acc: 0.8040
Epoch 13/40
8058/8058 [==============================] - 1s - loss: 0.4206 - acc: 0.8171 - val_loss: 0.4408 - val_acc: 0.8065
Epoch 14/40
8058/8058 [==============================] - 1s - loss: 0.4199 - acc: 0.8173 - val_loss: 0.4404 - val_acc: 0.8065
Epoch 15/40
8058/8058 [==============================] - 1s - loss: 0.4191 - acc: 0.8173 - val_loss: 0.4397 - val_acc: 0.8074
Epoch 16/40
8058/8058 [==============================] - 1s - loss: 0.4187 - acc: 0.8176 - val_loss: 0.4389 - val_acc: 0.8079
Epoch 17/40
8058/8058 [==============================] - 1s - loss: 0.4183 - acc: 0.8170 - val_loss: 0.4386 - val_acc: 0.8079
Epoch 18/40
8058/8058 [==============================] - 1s - loss: 0.4176 - acc: 0.8187 - val_loss: 0.4384 - val_acc: 0.8079
Epoch 19/40
8058/8058 [==============================] - 1s - loss: 0.4170 - acc: 0.8202 - val_loss: 0.4388 - val_acc: 0.8074
Epoch 20/40
8058/8058 [==============================] - 1s - loss: 0.4167 - acc: 0.8199 - val_loss: 0.4393 - val_acc: 0.8065
Epoch 21/40
8058/8058 [==============================] - 1s - loss: 0.4164 - acc: 0.8208 - val_loss: 0.4396 - val_acc: 0.8074
Epoch 22/40
8058/8058 [==============================] - 1s - loss: 0.4159 - acc: 0.8210 - val_loss: 0.4397 - val_acc: 0.8069
Epoch 23/40
8058/8058 [==============================] - 1s - loss: 0.4155 - acc: 0.8218 - val_loss: 0.4398 - val_acc: 0.8074
Epoch 24/40
8058/8058 [==============================] - 1s - loss: 0.4152 - acc: 0.8219 - val_loss: 0.4394 - val_acc: 0.8074
Epoch 25/40
8058/8058 [==============================] - 1s - loss: 0.4148 - acc: 0.8223 - val_loss: 0.4392 - val_acc: 0.8089
Epoch 26/40
8058/8058 [==============================] - 1s - loss: 0.4144 - acc: 0.8228 - val_loss: 0.4393 - val_acc: 0.8084
Epoch 27/40
8058/8058 [==============================] - 1s - loss: 0.4140 - acc: 0.8227 - val_loss: 0.4396 - val_acc: 0.8099
Epoch 28/40
8058/8058 [==============================] - 1s - loss: 0.4136 - acc: 0.8234 - val_loss: 0.4401 - val_acc: 0.8084
Epoch 29/40
8058/8058 [==============================] - 1s - loss: 0.4132 - acc: 0.8230 - val_loss: 0.4406 - val_acc: 0.8094
Epoch 30/40
8058/8058 [==============================] - 1s - loss: 0.4129 - acc: 0.8232 - val_loss: 0.4407 - val_acc: 0.8094
Epoch 31/40
8058/8058 [==============================] - 1s - loss: 0.4126 - acc: 0.8240 - val_loss: 0.4409 - val_acc: 0.8094
Epoch 32/40
8058/8058 [==============================] - 1s - loss: 0.4123 - acc: 0.8232 - val_loss: 0.4405 - val_acc: 0.8094
Epoch 33/40
8058/8058 [==============================] - 1s - loss: 0.4122 - acc: 0.8217 - val_loss: 0.4413 - val_acc: 0.8065
Epoch 34/40
8058/8058 [==============================] - 1s - loss: 0.4127 - acc: 0.8228 - val_loss: 0.4411 - val_acc: 0.8074
Epoch 35/40
8058/8058 [==============================] - 1s - loss: 0.4139 - acc: 0.8208 - val_loss: 0.4452 - val_acc: 0.8015
Epoch 36/40
8058/8058 [==============================] - 1s - loss: 0.4172 - acc: 0.8184 - val_loss: 0.4448 - val_acc: 0.8074
Epoch 37/40
8058/8058 [==============================] - 1s - loss: 0.4186 - acc: 0.8182 - val_loss: 0.4429 - val_acc: 0.8030
Epoch 38/40
8058/8058 [==============================] - 1s - loss: 0.4145 - acc: 0.8213 - val_loss: 0.4400 - val_acc: 0.8074
Epoch 39/40
8058/8058 [==============================] - 1s - loss: 0.4107 - acc: 0.8239 - val_loss: 0.4417 - val_acc: 0.8109
Epoch 40/40
8058/8058 [==============================] - 1s - loss: 0.4130 - acc: 0.8228 - val_loss: 0.4456 - val_acc: 0.7995
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-26.h5
chunk number 26
prepare data
start training
Train on 8100 samples, validate on 2026 samples
Epoch 1/40
8100/8100 [==============================] - 1s - loss: 0.4491 - acc: 0.8011 - val_loss: 0.4326 - val_acc: 0.8055
Epoch 2/40
8100/8100 [==============================] - 1s - loss: 0.4456 - acc: 0.8016 - val_loss: 0.4384 - val_acc: 0.8070
Epoch 3/40
8100/8100 [==============================] - 1s - loss: 0.4502 - acc: 0.8009 - val_loss: 0.4347 - val_acc: 0.8065
Epoch 4/40
8100/8100 [==============================] - 1s - loss: 0.4437 - acc: 0.8025 - val_loss: 0.4375 - val_acc: 0.8070
Epoch 5/40
8100/8100 [==============================] - 1s - loss: 0.4460 - acc: 0.8007 - val_loss: 0.4356 - val_acc: 0.8070
Epoch 6/40
8100/8100 [==============================] - 1s - loss: 0.4438 - acc: 0.8038 - val_loss: 0.4347 - val_acc: 0.8065
Epoch 7/40
8100/8100 [==============================] - 1s - loss: 0.4431 - acc: 0.8040 - val_loss: 0.4351 - val_acc: 0.8045
Epoch 8/40
8100/8100 [==============================] - 1s - loss: 0.4420 - acc: 0.8043 - val_loss: 0.4340 - val_acc: 0.8050
Epoch 9/40
8100/8100 [==============================] - 1s - loss: 0.4408 - acc: 0.8038 - val_loss: 0.4331 - val_acc: 0.8100
Epoch 10/40
8100/8100 [==============================] - 1s - loss: 0.4401 - acc: 0.8065 - val_loss: 0.4321 - val_acc: 0.8080
Epoch 11/40
8100/8100 [==============================] - 1s - loss: 0.4384 - acc: 0.8068 - val_loss: 0.4346 - val_acc: 0.8055
Epoch 12/40
8100/8100 [==============================] - 1s - loss: 0.4391 - acc: 0.8037 - val_loss: 0.4321 - val_acc: 0.8085
Epoch 13/40
8100/8100 [==============================] - 1s - loss: 0.4365 - acc: 0.8070 - val_loss: 0.4321 - val_acc: 0.8119
Epoch 14/40
8100/8100 [==============================] - 1s - loss: 0.4379 - acc: 0.8068 - val_loss: 0.4318 - val_acc: 0.8119
Epoch 15/40
8100/8100 [==============================] - 1s - loss: 0.4350 - acc: 0.8084 - val_loss: 0.4326 - val_acc: 0.8115
Epoch 16/40
8100/8100 [==============================] - 1s - loss: 0.4355 - acc: 0.8086 - val_loss: 0.4309 - val_acc: 0.8100
Epoch 17/40
8100/8100 [==============================] - 1s - loss: 0.4346 - acc: 0.8079 - val_loss: 0.4303 - val_acc: 0.8119
Epoch 18/40
8100/8100 [==============================] - 1s - loss: 0.4334 - acc: 0.8094 - val_loss: 0.4328 - val_acc: 0.8129
Epoch 19/40
8100/8100 [==============================] - 1s - loss: 0.4345 - acc: 0.8090 - val_loss: 0.4308 - val_acc: 0.8115
Epoch 20/40
8100/8100 [==============================] - 1s - loss: 0.4328 - acc: 0.8095 - val_loss: 0.4313 - val_acc: 0.8139
Epoch 21/40
8100/8100 [==============================] - 1s - loss: 0.4324 - acc: 0.8089 - val_loss: 0.4323 - val_acc: 0.8144
Epoch 22/40
8100/8100 [==============================] - 1s - loss: 0.4326 - acc: 0.8106 - val_loss: 0.4309 - val_acc: 0.8164
Epoch 23/40
8100/8100 [==============================] - 1s - loss: 0.4311 - acc: 0.8109 - val_loss: 0.4305 - val_acc: 0.8154
Epoch 24/40
8100/8100 [==============================] - 1s - loss: 0.4314 - acc: 0.8084 - val_loss: 0.4309 - val_acc: 0.8144
Epoch 25/40
8100/8100 [==============================] - 1s - loss: 0.4309 - acc: 0.8107 - val_loss: 0.4296 - val_acc: 0.8159
Epoch 26/40
8100/8100 [==============================] - 1s - loss: 0.4298 - acc: 0.8115 - val_loss: 0.4291 - val_acc: 0.8159
Epoch 27/40
8100/8100 [==============================] - 1s - loss: 0.4302 - acc: 0.8117 - val_loss: 0.4307 - val_acc: 0.8159
Epoch 28/40
8100/8100 [==============================] - 1s - loss: 0.4297 - acc: 0.8140 - val_loss: 0.4297 - val_acc: 0.8159
Epoch 29/40
8100/8100 [==============================] - 1s - loss: 0.4286 - acc: 0.8122 - val_loss: 0.4298 - val_acc: 0.8149
Epoch 30/40
8100/8100 [==============================] - 1s - loss: 0.4286 - acc: 0.8109 - val_loss: 0.4309 - val_acc: 0.8144
Epoch 31/40
8100/8100 [==============================] - 1s - loss: 0.4287 - acc: 0.8127 - val_loss: 0.4295 - val_acc: 0.8154
Epoch 32/40
8100/8100 [==============================] - 1s - loss: 0.4280 - acc: 0.8117 - val_loss: 0.4298 - val_acc: 0.8159
Epoch 33/40
8100/8100 [==============================] - 1s - loss: 0.4273 - acc: 0.8133 - val_loss: 0.4299 - val_acc: 0.8159
Epoch 34/40
8100/8100 [==============================] - 1s - loss: 0.4271 - acc: 0.8138 - val_loss: 0.4294 - val_acc: 0.8159
Epoch 35/40
8100/8100 [==============================] - 1s - loss: 0.4270 - acc: 0.8121 - val_loss: 0.4304 - val_acc: 0.8149
Epoch 36/40
8100/8100 [==============================] - 1s - loss: 0.4266 - acc: 0.8131 - val_loss: 0.4303 - val_acc: 0.8139
Epoch 37/40
8100/8100 [==============================] - 1s - loss: 0.4260 - acc: 0.8140 - val_loss: 0.4304 - val_acc: 0.8129
Epoch 38/40
8100/8100 [==============================] - 1s - loss: 0.4257 - acc: 0.8140 - val_loss: 0.4308 - val_acc: 0.8119
Epoch 39/40
8100/8100 [==============================] - 1s - loss: 0.4255 - acc: 0.8137 - val_loss: 0.4300 - val_acc: 0.8139
Epoch 40/40
8100/8100 [==============================] - 1s - loss: 0.4254 - acc: 0.8136 - val_loss: 0.4309 - val_acc: 0.8159
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-27.h5
chunk number 27
prepare data
start training
Train on 8095 samples, validate on 2024 samples
Epoch 1/40
8095/8095 [==============================] - 1s - loss: 0.4305 - acc: 0.8184 - val_loss: 0.4202 - val_acc: 0.8197
Epoch 2/40
8095/8095 [==============================] - 1s - loss: 0.4289 - acc: 0.8187 - val_loss: 0.4213 - val_acc: 0.8177
Epoch 3/40
8095/8095 [==============================] - 1s - loss: 0.4278 - acc: 0.8187 - val_loss: 0.4213 - val_acc: 0.8172
Epoch 4/40
8095/8095 [==============================] - 1s - loss: 0.4265 - acc: 0.8172 - val_loss: 0.4217 - val_acc: 0.8167
Epoch 5/40
8095/8095 [==============================] - 1s - loss: 0.4256 - acc: 0.8177 - val_loss: 0.4230 - val_acc: 0.8182
Epoch 6/40
8095/8095 [==============================] - 1s - loss: 0.4253 - acc: 0.8164 - val_loss: 0.4234 - val_acc: 0.8172
Epoch 7/40
8095/8095 [==============================] - 1s - loss: 0.4247 - acc: 0.8174 - val_loss: 0.4238 - val_acc: 0.8162
Epoch 8/40
8095/8095 [==============================] - 1s - loss: 0.4245 - acc: 0.8182 - val_loss: 0.4237 - val_acc: 0.8187
Epoch 9/40
8095/8095 [==============================] - 1s - loss: 0.4236 - acc: 0.8172 - val_loss: 0.4231 - val_acc: 0.8182
Epoch 10/40
8095/8095 [==============================] - 1s - loss: 0.4228 - acc: 0.8182 - val_loss: 0.4227 - val_acc: 0.8172
Epoch 11/40
8095/8095 [==============================] - 1s - loss: 0.4222 - acc: 0.8175 - val_loss: 0.4228 - val_acc: 0.8172
Epoch 12/40
8095/8095 [==============================] - 1s - loss: 0.4215 - acc: 0.8182 - val_loss: 0.4232 - val_acc: 0.8192
Epoch 13/40
8095/8095 [==============================] - 1s - loss: 0.4211 - acc: 0.8187 - val_loss: 0.4226 - val_acc: 0.8162
Epoch 14/40
8095/8095 [==============================] - 1s - loss: 0.4205 - acc: 0.8199 - val_loss: 0.4221 - val_acc: 0.8172
Epoch 15/40
8095/8095 [==============================] - 1s - loss: 0.4202 - acc: 0.8201 - val_loss: 0.4217 - val_acc: 0.8162
Epoch 16/40
8095/8095 [==============================] - 1s - loss: 0.4196 - acc: 0.8189 - val_loss: 0.4212 - val_acc: 0.8177
Epoch 17/40
8095/8095 [==============================] - 1s - loss: 0.4188 - acc: 0.8193 - val_loss: 0.4215 - val_acc: 0.8177
Epoch 18/40
8095/8095 [==============================] - 1s - loss: 0.4184 - acc: 0.8195 - val_loss: 0.4224 - val_acc: 0.8177
Epoch 19/40
8095/8095 [==============================] - 1s - loss: 0.4179 - acc: 0.8206 - val_loss: 0.4227 - val_acc: 0.8177
Epoch 20/40
8095/8095 [==============================] - 1s - loss: 0.4174 - acc: 0.8201 - val_loss: 0.4225 - val_acc: 0.8177
Epoch 21/40
8095/8095 [==============================] - 1s - loss: 0.4169 - acc: 0.8206 - val_loss: 0.4220 - val_acc: 0.8177
Epoch 22/40
8095/8095 [==============================] - 1s - loss: 0.4164 - acc: 0.8209 - val_loss: 0.4211 - val_acc: 0.8172
Epoch 23/40
8095/8095 [==============================] - 1s - loss: 0.4159 - acc: 0.8217 - val_loss: 0.4210 - val_acc: 0.8202
Epoch 24/40
8095/8095 [==============================] - 1s - loss: 0.4154 - acc: 0.8214 - val_loss: 0.4204 - val_acc: 0.8172
Epoch 25/40
8095/8095 [==============================] - 1s - loss: 0.4149 - acc: 0.8222 - val_loss: 0.4205 - val_acc: 0.8192
Epoch 26/40
8095/8095 [==============================] - 1s - loss: 0.4144 - acc: 0.8224 - val_loss: 0.4201 - val_acc: 0.8182
Epoch 27/40
8095/8095 [==============================] - 1s - loss: 0.4140 - acc: 0.8215 - val_loss: 0.4201 - val_acc: 0.8172
Epoch 28/40
8095/8095 [==============================] - 1s - loss: 0.4135 - acc: 0.8226 - val_loss: 0.4202 - val_acc: 0.8177
Epoch 29/40
8095/8095 [==============================] - 1s - loss: 0.4131 - acc: 0.8225 - val_loss: 0.4204 - val_acc: 0.8177
Epoch 30/40
8095/8095 [==============================] - 1s - loss: 0.4127 - acc: 0.8219 - val_loss: 0.4206 - val_acc: 0.8177
Epoch 31/40
8095/8095 [==============================] - 1s - loss: 0.4123 - acc: 0.8225 - val_loss: 0.4205 - val_acc: 0.8162
Epoch 32/40
8095/8095 [==============================] - 1s - loss: 0.4119 - acc: 0.8220 - val_loss: 0.4212 - val_acc: 0.8182
Epoch 33/40
8095/8095 [==============================] - 1s - loss: 0.4119 - acc: 0.8217 - val_loss: 0.4216 - val_acc: 0.8137
Epoch 34/40
8095/8095 [==============================] - 1s - loss: 0.4128 - acc: 0.8227 - val_loss: 0.4283 - val_acc: 0.8187
Epoch 35/40
8095/8095 [==============================] - 1s - loss: 0.4184 - acc: 0.8156 - val_loss: 0.4286 - val_acc: 0.8157
Epoch 36/40
8095/8095 [==============================] - 1s - loss: 0.4227 - acc: 0.8125 - val_loss: 0.4289 - val_acc: 0.8216
Epoch 37/40
8095/8095 [==============================] - 1s - loss: 0.4181 - acc: 0.8172 - val_loss: 0.4220 - val_acc: 0.8187
Epoch 38/40
8095/8095 [==============================] - 1s - loss: 0.4115 - acc: 0.8216 - val_loss: 0.4227 - val_acc: 0.8167
Epoch 39/40
8095/8095 [==============================] - 1s - loss: 0.4125 - acc: 0.8232 - val_loss: 0.4294 - val_acc: 0.8216
Epoch 40/40
8095/8095 [==============================] - 1s - loss: 0.4191 - acc: 0.8145 - val_loss: 0.4228 - val_acc: 0.8187
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-28.h5
chunk number 28
prepare data
start training
Train on 8135 samples, validate on 2034 samples
Epoch 1/40
8135/8135 [==============================] - 2s - loss: 0.4557 - acc: 0.7984 - val_loss: 0.4492 - val_acc: 0.8097
Epoch 2/40
8135/8135 [==============================] - 2s - loss: 0.4519 - acc: 0.7998 - val_loss: 0.4515 - val_acc: 0.8102
Epoch 3/40
8135/8135 [==============================] - 2s - loss: 0.4531 - acc: 0.8017 - val_loss: 0.4467 - val_acc: 0.8107
Epoch 4/40
8135/8135 [==============================] - 2s - loss: 0.4458 - acc: 0.8017 - val_loss: 0.4511 - val_acc: 0.8102
Epoch 5/40
8135/8135 [==============================] - 2s - loss: 0.4476 - acc: 0.8002 - val_loss: 0.4453 - val_acc: 0.8024
Epoch 6/40
8135/8135 [==============================] - 2s - loss: 0.4443 - acc: 0.8025 - val_loss: 0.4459 - val_acc: 0.8033
Epoch 7/40
8135/8135 [==============================] - 2s - loss: 0.4437 - acc: 0.8023 - val_loss: 0.4509 - val_acc: 0.8058
Epoch 8/40
8135/8135 [==============================] - 2s - loss: 0.4417 - acc: 0.8034 - val_loss: 0.4503 - val_acc: 0.8029
Epoch 9/40
8135/8135 [==============================] - 2s - loss: 0.4386 - acc: 0.8034 - val_loss: 0.4475 - val_acc: 0.8063
Epoch 10/40
8135/8135 [==============================] - 2s - loss: 0.4401 - acc: 0.8036 - val_loss: 0.4463 - val_acc: 0.8073
Epoch 11/40
8135/8135 [==============================] - 2s - loss: 0.4375 - acc: 0.8052 - val_loss: 0.4520 - val_acc: 0.8019
Epoch 12/40
8135/8135 [==============================] - 2s - loss: 0.4393 - acc: 0.8069 - val_loss: 0.4453 - val_acc: 0.8043
Epoch 13/40
8135/8135 [==============================] - 2s - loss: 0.4351 - acc: 0.8049 - val_loss: 0.4453 - val_acc: 0.8004
Epoch 14/40
8135/8135 [==============================] - 2s - loss: 0.4366 - acc: 0.8047 - val_loss: 0.4546 - val_acc: 0.8009
Epoch 15/40
8135/8135 [==============================] - 2s - loss: 0.4370 - acc: 0.8065 - val_loss: 0.4484 - val_acc: 0.8038
Epoch 16/40
8135/8135 [==============================] - 2s - loss: 0.4330 - acc: 0.8076 - val_loss: 0.4476 - val_acc: 0.8014
Epoch 17/40
8135/8135 [==============================] - 2s - loss: 0.4345 - acc: 0.8052 - val_loss: 0.4492 - val_acc: 0.8024
Epoch 18/40
8135/8135 [==============================] - 2s - loss: 0.4314 - acc: 0.8066 - val_loss: 0.4520 - val_acc: 0.8024
Epoch 19/40
8135/8135 [==============================] - 2s - loss: 0.4325 - acc: 0.8079 - val_loss: 0.4475 - val_acc: 0.8038
Epoch 20/40
8135/8135 [==============================] - 2s - loss: 0.4320 - acc: 0.8068 - val_loss: 0.4481 - val_acc: 0.8038
Epoch 21/40
8135/8135 [==============================] - 2s - loss: 0.4315 - acc: 0.8076 - val_loss: 0.4539 - val_acc: 0.8009
Epoch 22/40
8135/8135 [==============================] - 2s - loss: 0.4315 - acc: 0.8093 - val_loss: 0.4512 - val_acc: 0.8043
Epoch 23/40
8135/8135 [==============================] - 2s - loss: 0.4293 - acc: 0.8101 - val_loss: 0.4482 - val_acc: 0.8038
Epoch 24/40
8135/8135 [==============================] - 2s - loss: 0.4306 - acc: 0.8079 - val_loss: 0.4485 - val_acc: 0.8058
Epoch 25/40
8135/8135 [==============================] - 2s - loss: 0.4289 - acc: 0.8104 - val_loss: 0.4489 - val_acc: 0.8019
Epoch 26/40
8135/8135 [==============================] - 2s - loss: 0.4287 - acc: 0.8103 - val_loss: 0.4481 - val_acc: 0.8029
Epoch 27/40
8135/8135 [==============================] - 2s - loss: 0.4287 - acc: 0.8085 - val_loss: 0.4509 - val_acc: 0.8043
Epoch 28/40
8135/8135 [==============================] - 2s - loss: 0.4272 - acc: 0.8116 - val_loss: 0.4515 - val_acc: 0.8073
Epoch 29/40
8135/8135 [==============================] - 2s - loss: 0.4271 - acc: 0.8117 - val_loss: 0.4479 - val_acc: 0.8033
Epoch 30/40
8135/8135 [==============================] - 2s - loss: 0.4281 - acc: 0.8085 - val_loss: 0.4509 - val_acc: 0.8053
Epoch 31/40
8135/8135 [==============================] - 2s - loss: 0.4282 - acc: 0.8132 - val_loss: 0.4470 - val_acc: 0.8033
Epoch 32/40
8135/8135 [==============================] - 2s - loss: 0.4250 - acc: 0.8128 - val_loss: 0.4479 - val_acc: 0.8014
Epoch 33/40
8135/8135 [==============================] - 2s - loss: 0.4259 - acc: 0.8117 - val_loss: 0.4553 - val_acc: 0.8029
Epoch 34/40
8135/8135 [==============================] - 2s - loss: 0.4305 - acc: 0.8108 - val_loss: 0.4489 - val_acc: 0.8024
Epoch 35/40
8135/8135 [==============================] - 2s - loss: 0.4251 - acc: 0.8102 - val_loss: 0.4488 - val_acc: 0.8029
Epoch 36/40
8135/8135 [==============================] - 2s - loss: 0.4234 - acc: 0.8114 - val_loss: 0.4543 - val_acc: 0.8033
Epoch 37/40
8135/8135 [==============================] - 2s - loss: 0.4260 - acc: 0.8125 - val_loss: 0.4501 - val_acc: 0.8038
Epoch 38/40
8135/8135 [==============================] - 2s - loss: 0.4247 - acc: 0.8102 - val_loss: 0.4536 - val_acc: 0.8048
Epoch 39/40
8135/8135 [==============================] - 2s - loss: 0.4233 - acc: 0.8146 - val_loss: 0.4499 - val_acc: 0.8053
Epoch 40/40
8135/8135 [==============================] - 2s - loss: 0.4221 - acc: 0.8125 - val_loss: 0.4496 - val_acc: 0.8083
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-29.h5
chunk number 29
prepare data
start training
Train on 8105 samples, validate on 2027 samples
Epoch 1/40
8105/8105 [==============================] - 1s - loss: 0.4377 - acc: 0.8073 - val_loss: 0.4439 - val_acc: 0.8066
Epoch 2/40
8105/8105 [==============================] - 1s - loss: 0.4345 - acc: 0.8094 - val_loss: 0.4455 - val_acc: 0.8022
Epoch 3/40
8105/8105 [==============================] - 1s - loss: 0.4355 - acc: 0.8080 - val_loss: 0.4409 - val_acc: 0.8056
Epoch 4/40
8105/8105 [==============================] - 1s - loss: 0.4326 - acc: 0.8083 - val_loss: 0.4392 - val_acc: 0.8046
Epoch 5/40
8105/8105 [==============================] - 1s - loss: 0.4334 - acc: 0.8097 - val_loss: 0.4375 - val_acc: 0.8091
Epoch 6/40
8105/8105 [==============================] - 1s - loss: 0.4301 - acc: 0.8094 - val_loss: 0.4404 - val_acc: 0.8091
Epoch 7/40
8105/8105 [==============================] - 1s - loss: 0.4303 - acc: 0.8090 - val_loss: 0.4390 - val_acc: 0.8120
Epoch 8/40
8105/8105 [==============================] - 1s - loss: 0.4291 - acc: 0.8099 - val_loss: 0.4357 - val_acc: 0.8135
Epoch 9/40
8105/8105 [==============================] - 1s - loss: 0.4287 - acc: 0.8081 - val_loss: 0.4353 - val_acc: 0.8145
Epoch 10/40
8105/8105 [==============================] - 1s - loss: 0.4279 - acc: 0.8088 - val_loss: 0.4372 - val_acc: 0.8130
Epoch 11/40
8105/8105 [==============================] - 1s - loss: 0.4270 - acc: 0.8106 - val_loss: 0.4376 - val_acc: 0.8130
Epoch 12/40
8105/8105 [==============================] - 1s - loss: 0.4267 - acc: 0.8106 - val_loss: 0.4352 - val_acc: 0.8130
Epoch 13/40
8105/8105 [==============================] - 1s - loss: 0.4254 - acc: 0.8090 - val_loss: 0.4353 - val_acc: 0.8096
Epoch 14/40
8105/8105 [==============================] - 1s - loss: 0.4251 - acc: 0.8105 - val_loss: 0.4373 - val_acc: 0.8115
Epoch 15/40
8105/8105 [==============================] - 1s - loss: 0.4242 - acc: 0.8106 - val_loss: 0.4376 - val_acc: 0.8101
Epoch 16/40
8105/8105 [==============================] - 1s - loss: 0.4235 - acc: 0.8107 - val_loss: 0.4364 - val_acc: 0.8096
Epoch 17/40
8105/8105 [==============================] - 1s - loss: 0.4229 - acc: 0.8115 - val_loss: 0.4368 - val_acc: 0.8101
Epoch 18/40
8105/8105 [==============================] - 1s - loss: 0.4220 - acc: 0.8128 - val_loss: 0.4385 - val_acc: 0.8106
Epoch 19/40
8105/8105 [==============================] - 1s - loss: 0.4219 - acc: 0.8126 - val_loss: 0.4371 - val_acc: 0.8120
Epoch 20/40
8105/8105 [==============================] - 1s - loss: 0.4206 - acc: 0.8125 - val_loss: 0.4370 - val_acc: 0.8111
Epoch 21/40
8105/8105 [==============================] - 1s - loss: 0.4205 - acc: 0.8128 - val_loss: 0.4377 - val_acc: 0.8091
Epoch 22/40
8105/8105 [==============================] - 1s - loss: 0.4198 - acc: 0.8138 - val_loss: 0.4375 - val_acc: 0.8091
Epoch 23/40
8105/8105 [==============================] - 1s - loss: 0.4191 - acc: 0.8134 - val_loss: 0.4369 - val_acc: 0.8106
Epoch 24/40
8105/8105 [==============================] - 1s - loss: 0.4188 - acc: 0.8143 - val_loss: 0.4376 - val_acc: 0.8125
Epoch 25/40
8105/8105 [==============================] - 1s - loss: 0.4180 - acc: 0.8139 - val_loss: 0.4386 - val_acc: 0.8111
Epoch 26/40
8105/8105 [==============================] - 1s - loss: 0.4178 - acc: 0.8139 - val_loss: 0.4378 - val_acc: 0.8106
Epoch 27/40
8105/8105 [==============================] - 1s - loss: 0.4172 - acc: 0.8159 - val_loss: 0.4382 - val_acc: 0.8106
Epoch 28/40
8105/8105 [==============================] - 1s - loss: 0.4167 - acc: 0.8157 - val_loss: 0.4391 - val_acc: 0.8096
Epoch 29/40
8105/8105 [==============================] - 1s - loss: 0.4165 - acc: 0.8137 - val_loss: 0.4384 - val_acc: 0.8101
Epoch 30/40
8105/8105 [==============================] - 1s - loss: 0.4160 - acc: 0.8149 - val_loss: 0.4390 - val_acc: 0.8096
Epoch 31/40
8105/8105 [==============================] - 1s - loss: 0.4155 - acc: 0.8157 - val_loss: 0.4400 - val_acc: 0.8091
Epoch 32/40
8105/8105 [==============================] - 1s - loss: 0.4152 - acc: 0.8144 - val_loss: 0.4397 - val_acc: 0.8091
Epoch 33/40
8105/8105 [==============================] - 1s - loss: 0.4150 - acc: 0.8162 - val_loss: 0.4404 - val_acc: 0.8091
Epoch 34/40
8105/8105 [==============================] - 1s - loss: 0.4144 - acc: 0.8168 - val_loss: 0.4406 - val_acc: 0.8086
Epoch 35/40
8105/8105 [==============================] - 1s - loss: 0.4141 - acc: 0.8162 - val_loss: 0.4403 - val_acc: 0.8096
Epoch 36/40
8105/8105 [==============================] - 1s - loss: 0.4138 - acc: 0.8162 - val_loss: 0.4412 - val_acc: 0.8096
Epoch 37/40
8105/8105 [==============================] - 1s - loss: 0.4134 - acc: 0.8159 - val_loss: 0.4410 - val_acc: 0.8086
Epoch 38/40
8105/8105 [==============================] - 1s - loss: 0.4130 - acc: 0.8167 - val_loss: 0.4411 - val_acc: 0.8091
Epoch 39/40
8105/8105 [==============================] - 1s - loss: 0.4126 - acc: 0.8167 - val_loss: 0.4416 - val_acc: 0.8101
Epoch 40/40
8105/8105 [==============================] - 1s - loss: 0.4124 - acc: 0.8163 - val_loss: 0.4408 - val_acc: 0.8115
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-30.h5
chunk number 30
prepare data
start training
Train on 8185 samples, validate on 2047 samples
Epoch 1/40
8185/8185 [==============================] - 1s - loss: 0.4357 - acc: 0.8121 - val_loss: 0.4273 - val_acc: 0.8158
Epoch 2/40
8185/8185 [==============================] - 1s - loss: 0.4348 - acc: 0.8117 - val_loss: 0.4265 - val_acc: 0.8149
Epoch 3/40
8185/8185 [==============================] - 1s - loss: 0.4331 - acc: 0.8119 - val_loss: 0.4263 - val_acc: 0.8119
Epoch 4/40
8185/8185 [==============================] - 1s - loss: 0.4319 - acc: 0.8120 - val_loss: 0.4265 - val_acc: 0.8061
Epoch 5/40
8185/8185 [==============================] - 1s - loss: 0.4309 - acc: 0.8133 - val_loss: 0.4265 - val_acc: 0.8075
Epoch 6/40
8185/8185 [==============================] - 1s - loss: 0.4299 - acc: 0.8134 - val_loss: 0.4269 - val_acc: 0.8095
Epoch 7/40
8185/8185 [==============================] - 1s - loss: 0.4291 - acc: 0.8127 - val_loss: 0.4269 - val_acc: 0.8080
Epoch 8/40
8185/8185 [==============================] - 1s - loss: 0.4283 - acc: 0.8150 - val_loss: 0.4270 - val_acc: 0.8051
Epoch 9/40
8185/8185 [==============================] - 1s - loss: 0.4277 - acc: 0.8159 - val_loss: 0.4269 - val_acc: 0.8075
Epoch 10/40
8185/8185 [==============================] - 1s - loss: 0.4268 - acc: 0.8151 - val_loss: 0.4265 - val_acc: 0.8109
Epoch 11/40
8185/8185 [==============================] - 1s - loss: 0.4258 - acc: 0.8156 - val_loss: 0.4261 - val_acc: 0.8105
Epoch 12/40
8185/8185 [==============================] - 1s - loss: 0.4250 - acc: 0.8181 - val_loss: 0.4255 - val_acc: 0.8119
Epoch 13/40
8185/8185 [==============================] - 1s - loss: 0.4244 - acc: 0.8175 - val_loss: 0.4252 - val_acc: 0.8149
Epoch 14/40
8185/8185 [==============================] - 1s - loss: 0.4239 - acc: 0.8164 - val_loss: 0.4253 - val_acc: 0.8129
Epoch 15/40
8185/8185 [==============================] - 1s - loss: 0.4233 - acc: 0.8169 - val_loss: 0.4256 - val_acc: 0.8105
Epoch 16/40
8185/8185 [==============================] - 1s - loss: 0.4227 - acc: 0.8169 - val_loss: 0.4257 - val_acc: 0.8090
Epoch 17/40
8185/8185 [==============================] - 1s - loss: 0.4222 - acc: 0.8178 - val_loss: 0.4257 - val_acc: 0.8085
Epoch 18/40
8185/8185 [==============================] - 1s - loss: 0.4215 - acc: 0.8184 - val_loss: 0.4257 - val_acc: 0.8065
Epoch 19/40
8185/8185 [==============================] - 1s - loss: 0.4210 - acc: 0.8192 - val_loss: 0.4255 - val_acc: 0.8100
Epoch 20/40
8185/8185 [==============================] - 1s - loss: 0.4205 - acc: 0.8211 - val_loss: 0.4254 - val_acc: 0.8080
Epoch 21/40
8185/8185 [==============================] - 1s - loss: 0.4200 - acc: 0.8211 - val_loss: 0.4252 - val_acc: 0.8080
Epoch 22/40
8185/8185 [==============================] - 1s - loss: 0.4196 - acc: 0.8216 - val_loss: 0.4249 - val_acc: 0.8095
Epoch 23/40
8185/8185 [==============================] - 1s - loss: 0.4192 - acc: 0.8205 - val_loss: 0.4245 - val_acc: 0.8105
Epoch 24/40
8185/8185 [==============================] - 1s - loss: 0.4186 - acc: 0.8209 - val_loss: 0.4241 - val_acc: 0.8119
Epoch 25/40
8185/8185 [==============================] - 1s - loss: 0.4181 - acc: 0.8205 - val_loss: 0.4239 - val_acc: 0.8119
Epoch 26/40
8185/8185 [==============================] - 1s - loss: 0.4176 - acc: 0.8199 - val_loss: 0.4241 - val_acc: 0.8114
Epoch 27/40
8185/8185 [==============================] - 1s - loss: 0.4172 - acc: 0.8206 - val_loss: 0.4244 - val_acc: 0.8109
Epoch 28/40
8185/8185 [==============================] - 1s - loss: 0.4167 - acc: 0.8206 - val_loss: 0.4246 - val_acc: 0.8105
Epoch 29/40
8185/8185 [==============================] - 1s - loss: 0.4163 - acc: 0.8202 - val_loss: 0.4247 - val_acc: 0.8105
Epoch 30/40
8185/8185 [==============================] - 1s - loss: 0.4158 - acc: 0.8203 - val_loss: 0.4246 - val_acc: 0.8124
Epoch 31/40
8185/8185 [==============================] - 1s - loss: 0.4154 - acc: 0.8199 - val_loss: 0.4248 - val_acc: 0.8114
Epoch 32/40
8185/8185 [==============================] - 1s - loss: 0.4150 - acc: 0.8216 - val_loss: 0.4250 - val_acc: 0.8153
Epoch 33/40
8185/8185 [==============================] - 1s - loss: 0.4147 - acc: 0.8222 - val_loss: 0.4253 - val_acc: 0.8095
Epoch 34/40
8185/8185 [==============================] - 1s - loss: 0.4144 - acc: 0.8215 - val_loss: 0.4250 - val_acc: 0.8149
Epoch 35/40
8185/8185 [==============================] - 1s - loss: 0.4141 - acc: 0.8221 - val_loss: 0.4251 - val_acc: 0.8085
Epoch 36/40
8185/8185 [==============================] - 1s - loss: 0.4139 - acc: 0.8224 - val_loss: 0.4248 - val_acc: 0.8134
Epoch 37/40
8185/8185 [==============================] - 1s - loss: 0.4143 - acc: 0.8213 - val_loss: 0.4270 - val_acc: 0.8100
Epoch 38/40
8185/8185 [==============================] - 1s - loss: 0.4164 - acc: 0.8209 - val_loss: 0.4314 - val_acc: 0.8149
Epoch 39/40
8185/8185 [==============================] - 1s - loss: 0.4235 - acc: 0.8150 - val_loss: 0.4275 - val_acc: 0.8100
Epoch 40/40
8185/8185 [==============================] - 1s - loss: 0.4175 - acc: 0.8197 - val_loss: 0.4253 - val_acc: 0.8105
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-31.h5
chunk number 31
prepare data
start training
Train on 8079 samples, validate on 2020 samples
Epoch 1/40
8079/8079 [==============================] - 1s - loss: 0.4316 - acc: 0.8158 - val_loss: 0.4474 - val_acc: 0.8104
Epoch 2/40
8079/8079 [==============================] - 1s - loss: 0.4297 - acc: 0.8156 - val_loss: 0.4524 - val_acc: 0.8015
Epoch 3/40
8079/8079 [==============================] - 1s - loss: 0.4298 - acc: 0.8128 - val_loss: 0.4501 - val_acc: 0.8050
Epoch 4/40
8079/8079 [==============================] - 1s - loss: 0.4272 - acc: 0.8153 - val_loss: 0.4490 - val_acc: 0.8094
Epoch 5/40
8079/8079 [==============================] - 1s - loss: 0.4271 - acc: 0.8159 - val_loss: 0.4481 - val_acc: 0.8079
Epoch 6/40
8079/8079 [==============================] - 1s - loss: 0.4251 - acc: 0.8179 - val_loss: 0.4476 - val_acc: 0.8074
Epoch 7/40
8079/8079 [==============================] - 1s - loss: 0.4243 - acc: 0.8171 - val_loss: 0.4467 - val_acc: 0.8104
Epoch 8/40
8079/8079 [==============================] - 1s - loss: 0.4234 - acc: 0.8172 - val_loss: 0.4476 - val_acc: 0.8084
Epoch 9/40
8079/8079 [==============================] - 1s - loss: 0.4220 - acc: 0.8188 - val_loss: 0.4490 - val_acc: 0.8064
Epoch 10/40
8079/8079 [==============================] - 1s - loss: 0.4219 - acc: 0.8187 - val_loss: 0.4474 - val_acc: 0.8069
Epoch 11/40
8079/8079 [==============================] - 1s - loss: 0.4206 - acc: 0.8193 - val_loss: 0.4473 - val_acc: 0.8074
Epoch 12/40
8079/8079 [==============================] - 1s - loss: 0.4196 - acc: 0.8192 - val_loss: 0.4489 - val_acc: 0.8050
Epoch 13/40
8079/8079 [==============================] - 1s - loss: 0.4196 - acc: 0.8189 - val_loss: 0.4481 - val_acc: 0.8045
Epoch 14/40
8079/8079 [==============================] - 1s - loss: 0.4185 - acc: 0.8208 - val_loss: 0.4486 - val_acc: 0.8059
Epoch 15/40
8079/8079 [==============================] - 1s - loss: 0.4178 - acc: 0.8198 - val_loss: 0.4503 - val_acc: 0.8054
Epoch 16/40
8079/8079 [==============================] - 1s - loss: 0.4176 - acc: 0.8193 - val_loss: 0.4474 - val_acc: 0.8074
Epoch 17/40
8079/8079 [==============================] - 1s - loss: 0.4167 - acc: 0.8211 - val_loss: 0.4480 - val_acc: 0.8089
Epoch 18/40
8079/8079 [==============================] - 1s - loss: 0.4159 - acc: 0.8219 - val_loss: 0.4491 - val_acc: 0.8084
Epoch 19/40
8079/8079 [==============================] - 1s - loss: 0.4153 - acc: 0.8215 - val_loss: 0.4493 - val_acc: 0.8089
Epoch 20/40
8079/8079 [==============================] - 1s - loss: 0.4150 - acc: 0.8228 - val_loss: 0.4506 - val_acc: 0.8069
Epoch 21/40
8079/8079 [==============================] - 1s - loss: 0.4146 - acc: 0.8214 - val_loss: 0.4492 - val_acc: 0.8079
Epoch 22/40
8079/8079 [==============================] - 1s - loss: 0.4134 - acc: 0.8232 - val_loss: 0.4487 - val_acc: 0.8099
Epoch 23/40
8079/8079 [==============================] - 1s - loss: 0.4129 - acc: 0.8234 - val_loss: 0.4499 - val_acc: 0.8069
Epoch 24/40
8079/8079 [==============================] - 1s - loss: 0.4128 - acc: 0.8235 - val_loss: 0.4499 - val_acc: 0.8084
Epoch 25/40
8079/8079 [==============================] - 1s - loss: 0.4126 - acc: 0.8224 - val_loss: 0.4517 - val_acc: 0.8045
Epoch 26/40
8079/8079 [==============================] - 1s - loss: 0.4133 - acc: 0.8211 - val_loss: 0.4493 - val_acc: 0.8099
Epoch 27/40
8079/8079 [==============================] - 1s - loss: 0.4141 - acc: 0.8223 - val_loss: 0.4531 - val_acc: 0.8059
Epoch 28/40
8079/8079 [==============================] - 1s - loss: 0.4154 - acc: 0.8195 - val_loss: 0.4489 - val_acc: 0.8094
Epoch 29/40
8079/8079 [==============================] - 1s - loss: 0.4116 - acc: 0.8251 - val_loss: 0.4498 - val_acc: 0.8079
Epoch 30/40
8079/8079 [==============================] - 1s - loss: 0.4096 - acc: 0.8252 - val_loss: 0.4495 - val_acc: 0.8074
Epoch 31/40
8079/8079 [==============================] - 1s - loss: 0.4090 - acc: 0.8256 - val_loss: 0.4485 - val_acc: 0.8089
Epoch 32/40
8079/8079 [==============================] - 1s - loss: 0.4101 - acc: 0.8252 - val_loss: 0.4542 - val_acc: 0.8064
Epoch 33/40
8079/8079 [==============================] - 1s - loss: 0.4140 - acc: 0.8197 - val_loss: 0.4512 - val_acc: 0.8074
Epoch 34/40
8079/8079 [==============================] - 1s - loss: 0.4144 - acc: 0.8223 - val_loss: 0.4545 - val_acc: 0.8050
Epoch 35/40
8079/8079 [==============================] - 1s - loss: 0.4125 - acc: 0.8210 - val_loss: 0.4504 - val_acc: 0.8059
Epoch 36/40
8079/8079 [==============================] - 1s - loss: 0.4080 - acc: 0.8260 - val_loss: 0.4514 - val_acc: 0.8059
Epoch 37/40
8079/8079 [==============================] - 1s - loss: 0.4121 - acc: 0.8229 - val_loss: 0.4578 - val_acc: 0.8010
Epoch 38/40
8079/8079 [==============================] - 1s - loss: 0.4163 - acc: 0.8176 - val_loss: 0.4486 - val_acc: 0.8064
Epoch 39/40
8079/8079 [==============================] - 1s - loss: 0.4092 - acc: 0.8260 - val_loss: 0.4487 - val_acc: 0.8050
Epoch 40/40
8079/8079 [==============================] - 1s - loss: 0.4071 - acc: 0.8282 - val_loss: 0.4534 - val_acc: 0.8079
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-32.h5
chunk number 32
prepare data
start training
Train on 8130 samples, validate on 2033 samples
Epoch 1/40
8130/8130 [==============================] - 1s - loss: 0.4353 - acc: 0.8063 - val_loss: 0.4214 - val_acc: 0.8200
Epoch 2/40
8130/8130 [==============================] - 1s - loss: 0.4299 - acc: 0.8124 - val_loss: 0.4221 - val_acc: 0.8146
Epoch 3/40
8130/8130 [==============================] - 1s - loss: 0.4314 - acc: 0.8105 - val_loss: 0.4208 - val_acc: 0.8160
Epoch 4/40
8130/8130 [==============================] - 1s - loss: 0.4280 - acc: 0.8108 - val_loss: 0.4213 - val_acc: 0.8160
Epoch 5/40
8130/8130 [==============================] - 1s - loss: 0.4289 - acc: 0.8087 - val_loss: 0.4177 - val_acc: 0.8165
Epoch 6/40
8130/8130 [==============================] - 1s - loss: 0.4268 - acc: 0.8125 - val_loss: 0.4172 - val_acc: 0.8155
Epoch 7/40
8130/8130 [==============================] - 1s - loss: 0.4257 - acc: 0.8129 - val_loss: 0.4182 - val_acc: 0.8155
Epoch 8/40
8130/8130 [==============================] - 1s - loss: 0.4251 - acc: 0.8125 - val_loss: 0.4180 - val_acc: 0.8151
Epoch 9/40
8130/8130 [==============================] - 1s - loss: 0.4239 - acc: 0.8141 - val_loss: 0.4171 - val_acc: 0.8190
Epoch 10/40
8130/8130 [==============================] - 1s - loss: 0.4242 - acc: 0.8132 - val_loss: 0.4171 - val_acc: 0.8190
Epoch 11/40
8130/8130 [==============================] - 1s - loss: 0.4220 - acc: 0.8133 - val_loss: 0.4186 - val_acc: 0.8165
Epoch 12/40
8130/8130 [==============================] - 1s - loss: 0.4223 - acc: 0.8132 - val_loss: 0.4173 - val_acc: 0.8210
Epoch 13/40
8130/8130 [==============================] - 1s - loss: 0.4216 - acc: 0.8144 - val_loss: 0.4180 - val_acc: 0.8165
Epoch 14/40
8130/8130 [==============================] - 1s - loss: 0.4198 - acc: 0.8155 - val_loss: 0.4187 - val_acc: 0.8175
Epoch 15/40
8130/8130 [==============================] - 1s - loss: 0.4197 - acc: 0.8151 - val_loss: 0.4176 - val_acc: 0.8170
Epoch 16/40
8130/8130 [==============================] - 1s - loss: 0.4192 - acc: 0.8160 - val_loss: 0.4175 - val_acc: 0.8185
Epoch 17/40
8130/8130 [==============================] - 1s - loss: 0.4179 - acc: 0.8167 - val_loss: 0.4178 - val_acc: 0.8180
Epoch 18/40
8130/8130 [==============================] - 1s - loss: 0.4174 - acc: 0.8167 - val_loss: 0.4172 - val_acc: 0.8170
Epoch 19/40
8130/8130 [==============================] - 1s - loss: 0.4170 - acc: 0.8181 - val_loss: 0.4191 - val_acc: 0.8160
Epoch 20/40
8130/8130 [==============================] - 1s - loss: 0.4162 - acc: 0.8185 - val_loss: 0.4187 - val_acc: 0.8141
Epoch 21/40
8130/8130 [==============================] - 1s - loss: 0.4156 - acc: 0.8189 - val_loss: 0.4183 - val_acc: 0.8146
Epoch 22/40
8130/8130 [==============================] - 1s - loss: 0.4150 - acc: 0.8194 - val_loss: 0.4186 - val_acc: 0.8131
Epoch 23/40
8130/8130 [==============================] - 1s - loss: 0.4147 - acc: 0.8194 - val_loss: 0.4176 - val_acc: 0.8180
Epoch 24/40
8130/8130 [==============================] - 1s - loss: 0.4143 - acc: 0.8198 - val_loss: 0.4186 - val_acc: 0.8131
Epoch 25/40
8130/8130 [==============================] - 1s - loss: 0.4137 - acc: 0.8202 - val_loss: 0.4177 - val_acc: 0.8170
Epoch 26/40
8130/8130 [==============================] - 1s - loss: 0.4131 - acc: 0.8213 - val_loss: 0.4184 - val_acc: 0.8151
Epoch 27/40
8130/8130 [==============================] - 1s - loss: 0.4126 - acc: 0.8210 - val_loss: 0.4173 - val_acc: 0.8170
Epoch 28/40
8130/8130 [==============================] - 1s - loss: 0.4124 - acc: 0.8226 - val_loss: 0.4195 - val_acc: 0.8121
Epoch 29/40
8130/8130 [==============================] - 1s - loss: 0.4126 - acc: 0.8200 - val_loss: 0.4183 - val_acc: 0.8136
Epoch 30/40
8130/8130 [==============================] - 1s - loss: 0.4142 - acc: 0.8197 - val_loss: 0.4254 - val_acc: 0.8052
Epoch 31/40
8130/8130 [==============================] - 1s - loss: 0.4204 - acc: 0.8153 - val_loss: 0.4196 - val_acc: 0.8155
Epoch 32/40
8130/8130 [==============================] - 1s - loss: 0.4179 - acc: 0.8177 - val_loss: 0.4176 - val_acc: 0.8126
Epoch 33/40
8130/8130 [==============================] - 1s - loss: 0.4128 - acc: 0.8197 - val_loss: 0.4171 - val_acc: 0.8146
Epoch 34/40
8130/8130 [==============================] - 1s - loss: 0.4115 - acc: 0.8205 - val_loss: 0.4202 - val_acc: 0.8146
Epoch 35/40
8130/8130 [==============================] - 1s - loss: 0.4187 - acc: 0.8148 - val_loss: 0.4307 - val_acc: 0.8042
Epoch 36/40
8130/8130 [==============================] - 1s - loss: 0.4270 - acc: 0.8108 - val_loss: 0.4151 - val_acc: 0.8160
Epoch 37/40
8130/8130 [==============================] - 1s - loss: 0.4124 - acc: 0.8203 - val_loss: 0.4183 - val_acc: 0.8160
Epoch 38/40
8130/8130 [==============================] - 1s - loss: 0.4182 - acc: 0.8191 - val_loss: 0.4241 - val_acc: 0.8111
Epoch 39/40
8130/8130 [==============================] - 1s - loss: 0.4206 - acc: 0.8130 - val_loss: 0.4152 - val_acc: 0.8165
Epoch 40/40
8130/8130 [==============================] - 1s - loss: 0.4089 - acc: 0.8220 - val_loss: 0.4221 - val_acc: 0.8111
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-33.h5
chunk number 33
prepare data
start training
Train on 8214 samples, validate on 2054 samples
Epoch 1/40
8214/8214 [==============================] - 1s - loss: 0.4362 - acc: 0.8050 - val_loss: 0.4416 - val_acc: 0.8062
Epoch 2/40
8214/8214 [==============================] - 1s - loss: 0.4279 - acc: 0.8140 - val_loss: 0.4448 - val_acc: 0.8048
Epoch 3/40
8214/8214 [==============================] - 1s - loss: 0.4314 - acc: 0.8109 - val_loss: 0.4426 - val_acc: 0.8023
Epoch 4/40
8214/8214 [==============================] - 1s - loss: 0.4256 - acc: 0.8164 - val_loss: 0.4462 - val_acc: 0.8072
Epoch 5/40
8214/8214 [==============================] - 1s - loss: 0.4286 - acc: 0.8119 - val_loss: 0.4393 - val_acc: 0.8057
Epoch 6/40
8214/8214 [==============================] - 1s - loss: 0.4236 - acc: 0.8170 - val_loss: 0.4399 - val_acc: 0.8077
Epoch 7/40
8214/8214 [==============================] - 1s - loss: 0.4257 - acc: 0.8137 - val_loss: 0.4383 - val_acc: 0.8033
Epoch 8/40
8214/8214 [==============================] - 1s - loss: 0.4216 - acc: 0.8182 - val_loss: 0.4419 - val_acc: 0.8072
Epoch 9/40
8214/8214 [==============================] - 1s - loss: 0.4234 - acc: 0.8150 - val_loss: 0.4391 - val_acc: 0.8077
Epoch 10/40
8214/8214 [==============================] - 1s - loss: 0.4197 - acc: 0.8181 - val_loss: 0.4392 - val_acc: 0.8067
Epoch 11/40
8214/8214 [==============================] - 1s - loss: 0.4207 - acc: 0.8182 - val_loss: 0.4381 - val_acc: 0.8092
Epoch 12/40
8214/8214 [==============================] - 1s - loss: 0.4180 - acc: 0.8193 - val_loss: 0.4399 - val_acc: 0.8101
Epoch 13/40
8214/8214 [==============================] - 1s - loss: 0.4191 - acc: 0.8191 - val_loss: 0.4385 - val_acc: 0.8067
Epoch 14/40
8214/8214 [==============================] - 1s - loss: 0.4170 - acc: 0.8198 - val_loss: 0.4396 - val_acc: 0.8057
Epoch 15/40
8214/8214 [==============================] - 1s - loss: 0.4179 - acc: 0.8188 - val_loss: 0.4394 - val_acc: 0.8057
Epoch 16/40
8214/8214 [==============================] - 1s - loss: 0.4158 - acc: 0.8190 - val_loss: 0.4408 - val_acc: 0.8067
Epoch 17/40
8214/8214 [==============================] - 1s - loss: 0.4165 - acc: 0.8193 - val_loss: 0.4388 - val_acc: 0.8057
Epoch 18/40
8214/8214 [==============================] - 1s - loss: 0.4148 - acc: 0.8179 - val_loss: 0.4385 - val_acc: 0.8062
Epoch 19/40
8214/8214 [==============================] - 1s - loss: 0.4154 - acc: 0.8203 - val_loss: 0.4384 - val_acc: 0.8077
Epoch 20/40
8214/8214 [==============================] - 1s - loss: 0.4142 - acc: 0.8191 - val_loss: 0.4393 - val_acc: 0.8067
Epoch 21/40
8214/8214 [==============================] - 1s - loss: 0.4145 - acc: 0.8201 - val_loss: 0.4382 - val_acc: 0.8096
Epoch 22/40
8214/8214 [==============================] - 1s - loss: 0.4134 - acc: 0.8195 - val_loss: 0.4385 - val_acc: 0.8062
Epoch 23/40
8214/8214 [==============================] - 1s - loss: 0.4133 - acc: 0.8207 - val_loss: 0.4387 - val_acc: 0.8096
Epoch 24/40
8214/8214 [==============================] - 1s - loss: 0.4126 - acc: 0.8192 - val_loss: 0.4386 - val_acc: 0.8092
Epoch 25/40
8214/8214 [==============================] - 1s - loss: 0.4121 - acc: 0.8192 - val_loss: 0.4389 - val_acc: 0.8057
Epoch 26/40
8214/8214 [==============================] - 1s - loss: 0.4120 - acc: 0.8213 - val_loss: 0.4386 - val_acc: 0.8053
Epoch 27/40
8214/8214 [==============================] - 1s - loss: 0.4110 - acc: 0.8204 - val_loss: 0.4392 - val_acc: 0.8087
Epoch 28/40
8214/8214 [==============================] - 1s - loss: 0.4110 - acc: 0.8208 - val_loss: 0.4391 - val_acc: 0.8072
Epoch 29/40
8214/8214 [==============================] - 1s - loss: 0.4103 - acc: 0.8223 - val_loss: 0.4392 - val_acc: 0.8077
Epoch 30/40
8214/8214 [==============================] - 1s - loss: 0.4098 - acc: 0.8219 - val_loss: 0.4397 - val_acc: 0.8067
Epoch 31/40
8214/8214 [==============================] - 1s - loss: 0.4097 - acc: 0.8221 - val_loss: 0.4395 - val_acc: 0.8072
Epoch 32/40
8214/8214 [==============================] - 1s - loss: 0.4089 - acc: 0.8216 - val_loss: 0.4398 - val_acc: 0.8057
Epoch 33/40
8214/8214 [==============================] - 1s - loss: 0.4084 - acc: 0.8224 - val_loss: 0.4403 - val_acc: 0.8067
Epoch 34/40
8214/8214 [==============================] - 1s - loss: 0.4083 - acc: 0.8229 - val_loss: 0.4405 - val_acc: 0.8072
Epoch 35/40
8214/8214 [==============================] - 1s - loss: 0.4077 - acc: 0.8234 - val_loss: 0.4406 - val_acc: 0.8053
Epoch 36/40
8214/8214 [==============================] - 1s - loss: 0.4071 - acc: 0.8238 - val_loss: 0.4410 - val_acc: 0.8067
Epoch 37/40
8214/8214 [==============================] - 1s - loss: 0.4069 - acc: 0.8240 - val_loss: 0.4417 - val_acc: 0.8057
Epoch 38/40
8214/8214 [==============================] - 1s - loss: 0.4067 - acc: 0.8230 - val_loss: 0.4416 - val_acc: 0.8067
Epoch 39/40
8214/8214 [==============================] - 1s - loss: 0.4062 - acc: 0.8247 - val_loss: 0.4419 - val_acc: 0.8048
Epoch 40/40
8214/8214 [==============================] - 1s - loss: 0.4056 - acc: 0.8246 - val_loss: 0.4417 - val_acc: 0.8048
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-34.h5
chunk number 34
prepare data
start training
Train on 8084 samples, validate on 2021 samples
Epoch 1/40
8084/8084 [==============================] - 2s - loss: 0.4457 - acc: 0.8039 - val_loss: 0.4250 - val_acc: 0.8164
Epoch 2/40
8084/8084 [==============================] - 2s - loss: 0.4432 - acc: 0.8021 - val_loss: 0.4229 - val_acc: 0.8199
Epoch 3/40
8084/8084 [==============================] - 2s - loss: 0.4388 - acc: 0.8041 - val_loss: 0.4247 - val_acc: 0.8209
Epoch 4/40
8084/8084 [==============================] - 2s - loss: 0.4376 - acc: 0.8081 - val_loss: 0.4223 - val_acc: 0.8224
Epoch 5/40
8084/8084 [==============================] - 2s - loss: 0.4315 - acc: 0.8127 - val_loss: 0.4218 - val_acc: 0.8194
Epoch 6/40
8084/8084 [==============================] - 2s - loss: 0.4302 - acc: 0.8111 - val_loss: 0.4200 - val_acc: 0.8184
Epoch 7/40
8084/8084 [==============================] - 2s - loss: 0.4261 - acc: 0.8138 - val_loss: 0.4264 - val_acc: 0.8164
Epoch 8/40
8084/8084 [==============================] - 2s - loss: 0.4327 - acc: 0.8085 - val_loss: 0.4224 - val_acc: 0.8219
Epoch 9/40
8084/8084 [==============================] - 2s - loss: 0.4253 - acc: 0.8157 - val_loss: 0.4209 - val_acc: 0.8219
Epoch 10/40
8084/8084 [==============================] - 2s - loss: 0.4240 - acc: 0.8158 - val_loss: 0.4202 - val_acc: 0.8204
Epoch 11/40
8084/8084 [==============================] - 2s - loss: 0.4245 - acc: 0.8184 - val_loss: 0.4194 - val_acc: 0.8199
Epoch 12/40
8084/8084 [==============================] - 2s - loss: 0.4203 - acc: 0.8184 - val_loss: 0.4223 - val_acc: 0.8169
Epoch 13/40
8084/8084 [==============================] - 2s - loss: 0.4207 - acc: 0.8190 - val_loss: 0.4197 - val_acc: 0.8179
Epoch 14/40
8084/8084 [==============================] - 2s - loss: 0.4181 - acc: 0.8215 - val_loss: 0.4186 - val_acc: 0.8224
Epoch 15/40
8084/8084 [==============================] - 2s - loss: 0.4205 - acc: 0.8190 - val_loss: 0.4190 - val_acc: 0.8189
Epoch 16/40
8084/8084 [==============================] - 2s - loss: 0.4171 - acc: 0.8203 - val_loss: 0.4187 - val_acc: 0.8189
Epoch 17/40
8084/8084 [==============================] - 2s - loss: 0.4171 - acc: 0.8196 - val_loss: 0.4155 - val_acc: 0.8219
Epoch 18/40
8084/8084 [==============================] - 2s - loss: 0.4168 - acc: 0.8224 - val_loss: 0.4150 - val_acc: 0.8238
Epoch 19/40
8084/8084 [==============================] - 2s - loss: 0.4146 - acc: 0.8235 - val_loss: 0.4200 - val_acc: 0.8174
Epoch 20/40
8084/8084 [==============================] - 2s - loss: 0.4166 - acc: 0.8206 - val_loss: 0.4159 - val_acc: 0.8189
Epoch 21/40
8084/8084 [==============================] - 2s - loss: 0.4144 - acc: 0.8235 - val_loss: 0.4156 - val_acc: 0.8204
Epoch 22/40
8084/8084 [==============================] - 2s - loss: 0.4120 - acc: 0.8219 - val_loss: 0.4170 - val_acc: 0.8189
Epoch 23/40
8084/8084 [==============================] - 2s - loss: 0.4127 - acc: 0.8206 - val_loss: 0.4141 - val_acc: 0.8214
Epoch 24/40
8084/8084 [==============================] - 2s - loss: 0.4112 - acc: 0.8220 - val_loss: 0.4139 - val_acc: 0.8224
Epoch 25/40
8084/8084 [==============================] - 2s - loss: 0.4099 - acc: 0.8245 - val_loss: 0.4163 - val_acc: 0.8209
Epoch 26/40
8084/8084 [==============================] - 2s - loss: 0.4107 - acc: 0.8236 - val_loss: 0.4163 - val_acc: 0.8204
Epoch 27/40
8084/8084 [==============================] - 2s - loss: 0.4103 - acc: 0.8238 - val_loss: 0.4203 - val_acc: 0.8199
Epoch 28/40
8084/8084 [==============================] - 2s - loss: 0.4108 - acc: 0.8238 - val_loss: 0.4174 - val_acc: 0.8214
Epoch 29/40
8084/8084 [==============================] - 2s - loss: 0.4082 - acc: 0.8247 - val_loss: 0.4173 - val_acc: 0.8204
Epoch 30/40
8084/8084 [==============================] - 2s - loss: 0.4091 - acc: 0.8237 - val_loss: 0.4227 - val_acc: 0.8189
Epoch 31/40
8084/8084 [==============================] - 2s - loss: 0.4141 - acc: 0.8193 - val_loss: 0.4224 - val_acc: 0.8204
Epoch 32/40
8084/8084 [==============================] - 2s - loss: 0.4160 - acc: 0.8199 - val_loss: 0.4163 - val_acc: 0.8209
Epoch 33/40
8084/8084 [==============================] - 2s - loss: 0.4066 - acc: 0.8250 - val_loss: 0.4234 - val_acc: 0.8219
Epoch 34/40
8084/8084 [==============================] - 2s - loss: 0.4142 - acc: 0.8220 - val_loss: 0.4199 - val_acc: 0.8184
Epoch 35/40
8084/8084 [==============================] - 2s - loss: 0.4126 - acc: 0.8219 - val_loss: 0.4184 - val_acc: 0.8243
Epoch 36/40
8084/8084 [==============================] - 2s - loss: 0.4067 - acc: 0.8261 - val_loss: 0.4200 - val_acc: 0.8229
Epoch 37/40
8084/8084 [==============================] - 2s - loss: 0.4076 - acc: 0.8250 - val_loss: 0.4194 - val_acc: 0.8214
Epoch 38/40
8084/8084 [==============================] - 2s - loss: 0.4083 - acc: 0.8236 - val_loss: 0.4182 - val_acc: 0.8219
Epoch 39/40
8084/8084 [==============================] - 2s - loss: 0.4057 - acc: 0.8267 - val_loss: 0.4169 - val_acc: 0.8179
Epoch 40/40
8084/8084 [==============================] - 2s - loss: 0.4048 - acc: 0.8279 - val_loss: 0.4177 - val_acc: 0.8194
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-35.h5
chunk number 35
prepare data
start training
Train on 8199 samples, validate on 2050 samples
Epoch 1/40
8199/8199 [==============================] - 1s - loss: 0.4396 - acc: 0.8073 - val_loss: 0.4098 - val_acc: 0.8244
Epoch 2/40
8199/8199 [==============================] - 1s - loss: 0.4378 - acc: 0.8096 - val_loss: 0.4077 - val_acc: 0.8220
Epoch 3/40
8199/8199 [==============================] - 1s - loss: 0.4347 - acc: 0.8094 - val_loss: 0.4037 - val_acc: 0.8239
Epoch 4/40
8199/8199 [==============================] - 1s - loss: 0.4311 - acc: 0.8094 - val_loss: 0.4024 - val_acc: 0.8254
Epoch 5/40
8199/8199 [==============================] - 1s - loss: 0.4297 - acc: 0.8107 - val_loss: 0.4036 - val_acc: 0.8234
Epoch 6/40
8199/8199 [==============================] - 1s - loss: 0.4298 - acc: 0.8099 - val_loss: 0.4035 - val_acc: 0.8254
Epoch 7/40
8199/8199 [==============================] - 1s - loss: 0.4290 - acc: 0.8114 - val_loss: 0.4015 - val_acc: 0.8293
Epoch 8/40
8199/8199 [==============================] - 1s - loss: 0.4275 - acc: 0.8123 - val_loss: 0.4006 - val_acc: 0.8288
Epoch 9/40
8199/8199 [==============================] - 1s - loss: 0.4266 - acc: 0.8129 - val_loss: 0.4001 - val_acc: 0.8283
Epoch 10/40
8199/8199 [==============================] - 1s - loss: 0.4255 - acc: 0.8141 - val_loss: 0.4008 - val_acc: 0.8263
Epoch 11/40
8199/8199 [==============================] - 1s - loss: 0.4242 - acc: 0.8152 - val_loss: 0.4015 - val_acc: 0.8259
Epoch 12/40
8199/8199 [==============================] - 1s - loss: 0.4238 - acc: 0.8134 - val_loss: 0.4027 - val_acc: 0.8244
Epoch 13/40
8199/8199 [==============================] - 1s - loss: 0.4237 - acc: 0.8131 - val_loss: 0.4039 - val_acc: 0.8229
Epoch 14/40
8199/8199 [==============================] - 1s - loss: 0.4231 - acc: 0.8146 - val_loss: 0.4043 - val_acc: 0.8234
Epoch 15/40
8199/8199 [==============================] - 1s - loss: 0.4222 - acc: 0.8161 - val_loss: 0.4036 - val_acc: 0.8234
Epoch 16/40
8199/8199 [==============================] - 1s - loss: 0.4213 - acc: 0.8162 - val_loss: 0.4028 - val_acc: 0.8224
Epoch 17/40
8199/8199 [==============================] - 1s - loss: 0.4206 - acc: 0.8175 - val_loss: 0.4021 - val_acc: 0.8224
Epoch 18/40
8199/8199 [==============================] - 1s - loss: 0.4203 - acc: 0.8163 - val_loss: 0.4015 - val_acc: 0.8224
Epoch 19/40
8199/8199 [==============================] - 1s - loss: 0.4200 - acc: 0.8169 - val_loss: 0.4012 - val_acc: 0.8220
Epoch 20/40
8199/8199 [==============================] - 1s - loss: 0.4195 - acc: 0.8173 - val_loss: 0.4017 - val_acc: 0.8205
Epoch 21/40
8199/8199 [==============================] - 1s - loss: 0.4189 - acc: 0.8171 - val_loss: 0.4019 - val_acc: 0.8220
Epoch 22/40
8199/8199 [==============================] - 1s - loss: 0.4184 - acc: 0.8161 - val_loss: 0.4016 - val_acc: 0.8215
Epoch 23/40
8199/8199 [==============================] - 1s - loss: 0.4178 - acc: 0.8174 - val_loss: 0.4016 - val_acc: 0.8220
Epoch 24/40
8199/8199 [==============================] - 1s - loss: 0.4173 - acc: 0.8166 - val_loss: 0.4016 - val_acc: 0.8244
Epoch 25/40
8199/8199 [==============================] - 1s - loss: 0.4168 - acc: 0.8172 - val_loss: 0.4020 - val_acc: 0.8234
Epoch 26/40
8199/8199 [==============================] - 1s - loss: 0.4164 - acc: 0.8179 - val_loss: 0.4027 - val_acc: 0.8229
Epoch 27/40
8199/8199 [==============================] - 1s - loss: 0.4160 - acc: 0.8184 - val_loss: 0.4030 - val_acc: 0.8224
Epoch 28/40
8199/8199 [==============================] - 1s - loss: 0.4157 - acc: 0.8200 - val_loss: 0.4028 - val_acc: 0.8229
Epoch 29/40
8199/8199 [==============================] - 1s - loss: 0.4153 - acc: 0.8191 - val_loss: 0.4025 - val_acc: 0.8229
Epoch 30/40
8199/8199 [==============================] - 1s - loss: 0.4147 - acc: 0.8191 - val_loss: 0.4021 - val_acc: 0.8239
Epoch 31/40
8199/8199 [==============================] - 1s - loss: 0.4142 - acc: 0.8191 - val_loss: 0.4016 - val_acc: 0.8249
Epoch 32/40
8199/8199 [==============================] - 1s - loss: 0.4138 - acc: 0.8185 - val_loss: 0.4016 - val_acc: 0.8244
Epoch 33/40
8199/8199 [==============================] - 1s - loss: 0.4134 - acc: 0.8195 - val_loss: 0.4017 - val_acc: 0.8254
Epoch 34/40
8199/8199 [==============================] - 1s - loss: 0.4130 - acc: 0.8199 - val_loss: 0.4023 - val_acc: 0.8263
Epoch 35/40
8199/8199 [==============================] - 1s - loss: 0.4126 - acc: 0.8202 - val_loss: 0.4025 - val_acc: 0.8244
Epoch 36/40
8199/8199 [==============================] - 1s - loss: 0.4122 - acc: 0.8203 - val_loss: 0.4028 - val_acc: 0.8239
Epoch 37/40
8199/8199 [==============================] - 1s - loss: 0.4118 - acc: 0.8207 - val_loss: 0.4026 - val_acc: 0.8229
Epoch 38/40
8199/8199 [==============================] - 1s - loss: 0.4115 - acc: 0.8213 - val_loss: 0.4032 - val_acc: 0.8234
Epoch 39/40
8199/8199 [==============================] - 1s - loss: 0.4111 - acc: 0.8216 - val_loss: 0.4028 - val_acc: 0.8244
Epoch 40/40
8199/8199 [==============================] - 1s - loss: 0.4107 - acc: 0.8221 - val_loss: 0.4037 - val_acc: 0.8263
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-36.h5
chunk number 36
prepare data
start training
Train on 8126 samples, validate on 2032 samples
Epoch 1/40
8126/8126 [==============================] - 1s - loss: 0.4270 - acc: 0.8142 - val_loss: 0.4519 - val_acc: 0.7923
Epoch 2/40
8126/8126 [==============================] - 1s - loss: 0.4247 - acc: 0.8166 - val_loss: 0.4508 - val_acc: 0.7938
Epoch 3/40
8126/8126 [==============================] - 1s - loss: 0.4231 - acc: 0.8173 - val_loss: 0.4466 - val_acc: 0.7977
Epoch 4/40
8126/8126 [==============================] - 1s - loss: 0.4216 - acc: 0.8163 - val_loss: 0.4462 - val_acc: 0.7997
Epoch 5/40
8126/8126 [==============================] - 1s - loss: 0.4213 - acc: 0.8175 - val_loss: 0.4482 - val_acc: 0.8002
Epoch 6/40
8126/8126 [==============================] - 1s - loss: 0.4212 - acc: 0.8164 - val_loss: 0.4482 - val_acc: 0.7992
Epoch 7/40
8126/8126 [==============================] - 1s - loss: 0.4207 - acc: 0.8179 - val_loss: 0.4464 - val_acc: 0.8041
Epoch 8/40
8126/8126 [==============================] - 1s - loss: 0.4198 - acc: 0.8170 - val_loss: 0.4463 - val_acc: 0.8046
Epoch 9/40
8126/8126 [==============================] - 1s - loss: 0.4190 - acc: 0.8174 - val_loss: 0.4480 - val_acc: 0.8017
Epoch 10/40
8126/8126 [==============================] - 1s - loss: 0.4180 - acc: 0.8198 - val_loss: 0.4487 - val_acc: 0.8002
Epoch 11/40
8126/8126 [==============================] - 1s - loss: 0.4172 - acc: 0.8193 - val_loss: 0.4482 - val_acc: 0.7997
Epoch 12/40
8126/8126 [==============================] - 1s - loss: 0.4165 - acc: 0.8205 - val_loss: 0.4491 - val_acc: 0.7982
Epoch 13/40
8126/8126 [==============================] - 1s - loss: 0.4161 - acc: 0.8191 - val_loss: 0.4510 - val_acc: 0.7933
Epoch 14/40
8126/8126 [==============================] - 1s - loss: 0.4157 - acc: 0.8211 - val_loss: 0.4514 - val_acc: 0.7938
Epoch 15/40
8126/8126 [==============================] - 1s - loss: 0.4152 - acc: 0.8221 - val_loss: 0.4502 - val_acc: 0.7968
Epoch 16/40
8126/8126 [==============================] - 1s - loss: 0.4147 - acc: 0.8209 - val_loss: 0.4502 - val_acc: 0.7968
Epoch 17/40
8126/8126 [==============================] - 1s - loss: 0.4140 - acc: 0.8217 - val_loss: 0.4510 - val_acc: 0.7982
Epoch 18/40
8126/8126 [==============================] - 1s - loss: 0.4135 - acc: 0.8219 - val_loss: 0.4499 - val_acc: 0.7982
Epoch 19/40
8126/8126 [==============================] - 1s - loss: 0.4128 - acc: 0.8219 - val_loss: 0.4489 - val_acc: 0.7977
Epoch 20/40
8126/8126 [==============================] - 1s - loss: 0.4124 - acc: 0.8209 - val_loss: 0.4492 - val_acc: 0.7977
Epoch 21/40
8126/8126 [==============================] - 1s - loss: 0.4118 - acc: 0.8222 - val_loss: 0.4494 - val_acc: 0.7953
Epoch 22/40
8126/8126 [==============================] - 1s - loss: 0.4115 - acc: 0.8224 - val_loss: 0.4485 - val_acc: 0.7977
Epoch 23/40
8126/8126 [==============================] - 1s - loss: 0.4109 - acc: 0.8219 - val_loss: 0.4485 - val_acc: 0.7987
Epoch 24/40
8126/8126 [==============================] - 1s - loss: 0.4104 - acc: 0.8216 - val_loss: 0.4494 - val_acc: 0.7958
Epoch 25/40
8126/8126 [==============================] - 1s - loss: 0.4100 - acc: 0.8214 - val_loss: 0.4496 - val_acc: 0.7958
Epoch 26/40
8126/8126 [==============================] - 1s - loss: 0.4095 - acc: 0.8216 - val_loss: 0.4492 - val_acc: 0.7968
Epoch 27/40
8126/8126 [==============================] - 1s - loss: 0.4090 - acc: 0.8229 - val_loss: 0.4500 - val_acc: 0.7948
Epoch 28/40
8126/8126 [==============================] - 1s - loss: 0.4084 - acc: 0.8245 - val_loss: 0.4507 - val_acc: 0.7928
Epoch 29/40
8126/8126 [==============================] - 1s - loss: 0.4081 - acc: 0.8244 - val_loss: 0.4501 - val_acc: 0.7938
Epoch 30/40
8126/8126 [==============================] - 1s - loss: 0.4076 - acc: 0.8249 - val_loss: 0.4504 - val_acc: 0.7928
Epoch 31/40
8126/8126 [==============================] - 1s - loss: 0.4071 - acc: 0.8246 - val_loss: 0.4508 - val_acc: 0.7933
Epoch 32/40
8126/8126 [==============================] - 1s - loss: 0.4067 - acc: 0.8248 - val_loss: 0.4498 - val_acc: 0.7943
Epoch 33/40
8126/8126 [==============================] - 1s - loss: 0.4062 - acc: 0.8239 - val_loss: 0.4503 - val_acc: 0.7948
Epoch 34/40
8126/8126 [==============================] - 1s - loss: 0.4057 - acc: 0.8246 - val_loss: 0.4503 - val_acc: 0.7968
Epoch 35/40
8126/8126 [==============================] - 1s - loss: 0.4052 - acc: 0.8243 - val_loss: 0.4499 - val_acc: 0.7968
Epoch 36/40
8126/8126 [==============================] - 1s - loss: 0.4048 - acc: 0.8235 - val_loss: 0.4506 - val_acc: 0.7972
Epoch 37/40
8126/8126 [==============================] - 1s - loss: 0.4043 - acc: 0.8250 - val_loss: 0.4505 - val_acc: 0.7992
Epoch 38/40
8126/8126 [==============================] - 1s - loss: 0.4038 - acc: 0.8254 - val_loss: 0.4503 - val_acc: 0.7992
Epoch 39/40
8126/8126 [==============================] - 1s - loss: 0.4033 - acc: 0.8259 - val_loss: 0.4507 - val_acc: 0.8007
Epoch 40/40
8126/8126 [==============================] - 1s - loss: 0.4028 - acc: 0.8256 - val_loss: 0.4503 - val_acc: 0.8002
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-37.h5
chunk number 37
prepare data
start training
Train on 8127 samples, validate on 2032 samples
Epoch 1/40
8127/8127 [==============================] - 1s - loss: 0.4392 - acc: 0.8078 - val_loss: 0.4540 - val_acc: 0.8022
Epoch 2/40
8127/8127 [==============================] - 1s - loss: 0.4378 - acc: 0.8085 - val_loss: 0.4538 - val_acc: 0.8031
Epoch 3/40
8127/8127 [==============================] - 1s - loss: 0.4342 - acc: 0.8099 - val_loss: 0.4527 - val_acc: 0.8007
Epoch 4/40
8127/8127 [==============================] - 1s - loss: 0.4347 - acc: 0.8104 - val_loss: 0.4543 - val_acc: 0.7992
Epoch 5/40
8127/8127 [==============================] - 1s - loss: 0.4311 - acc: 0.8101 - val_loss: 0.4554 - val_acc: 0.7992
Epoch 6/40
8127/8127 [==============================] - 1s - loss: 0.4308 - acc: 0.8105 - val_loss: 0.4494 - val_acc: 0.7992
Epoch 7/40
8127/8127 [==============================] - 1s - loss: 0.4279 - acc: 0.8109 - val_loss: 0.4455 - val_acc: 0.8022
Epoch 8/40
8127/8127 [==============================] - 1s - loss: 0.4275 - acc: 0.8114 - val_loss: 0.4454 - val_acc: 0.8012
Epoch 9/40
8127/8127 [==============================] - 1s - loss: 0.4259 - acc: 0.8112 - val_loss: 0.4477 - val_acc: 0.7987
Epoch 10/40
8127/8127 [==============================] - 1s - loss: 0.4256 - acc: 0.8132 - val_loss: 0.4456 - val_acc: 0.7997
Epoch 11/40
8127/8127 [==============================] - 1s - loss: 0.4240 - acc: 0.8133 - val_loss: 0.4440 - val_acc: 0.7982
Epoch 12/40
8127/8127 [==============================] - 1s - loss: 0.4233 - acc: 0.8142 - val_loss: 0.4451 - val_acc: 0.7987
Epoch 13/40
8127/8127 [==============================] - 1s - loss: 0.4218 - acc: 0.8148 - val_loss: 0.4445 - val_acc: 0.7977
Epoch 14/40
8127/8127 [==============================] - 1s - loss: 0.4212 - acc: 0.8162 - val_loss: 0.4427 - val_acc: 0.7982
Epoch 15/40
8127/8127 [==============================] - 1s - loss: 0.4205 - acc: 0.8168 - val_loss: 0.4430 - val_acc: 0.7968
Epoch 16/40
8127/8127 [==============================] - 1s - loss: 0.4198 - acc: 0.8170 - val_loss: 0.4444 - val_acc: 0.7943
Epoch 17/40
8127/8127 [==============================] - 1s - loss: 0.4194 - acc: 0.8164 - val_loss: 0.4435 - val_acc: 0.7963
Epoch 18/40
8127/8127 [==============================] - 1s - loss: 0.4186 - acc: 0.8173 - val_loss: 0.4440 - val_acc: 0.7968
Epoch 19/40
8127/8127 [==============================] - 1s - loss: 0.4178 - acc: 0.8174 - val_loss: 0.4451 - val_acc: 0.7963
Epoch 20/40
8127/8127 [==============================] - 1s - loss: 0.4172 - acc: 0.8174 - val_loss: 0.4442 - val_acc: 0.7992
Epoch 21/40
8127/8127 [==============================] - 1s - loss: 0.4166 - acc: 0.8185 - val_loss: 0.4433 - val_acc: 0.7997
Epoch 22/40
8127/8127 [==============================] - 1s - loss: 0.4160 - acc: 0.8176 - val_loss: 0.4441 - val_acc: 0.7987
Epoch 23/40
8127/8127 [==============================] - 1s - loss: 0.4154 - acc: 0.8176 - val_loss: 0.4430 - val_acc: 0.8012
Epoch 24/40
8127/8127 [==============================] - 1s - loss: 0.4148 - acc: 0.8191 - val_loss: 0.4424 - val_acc: 0.8012
Epoch 25/40
8127/8127 [==============================] - 1s - loss: 0.4145 - acc: 0.8189 - val_loss: 0.4447 - val_acc: 0.7997
Epoch 26/40
8127/8127 [==============================] - 1s - loss: 0.4144 - acc: 0.8200 - val_loss: 0.4414 - val_acc: 0.8061
Epoch 27/40
8127/8127 [==============================] - 1s - loss: 0.4143 - acc: 0.8181 - val_loss: 0.4440 - val_acc: 0.8027
Epoch 28/40
8127/8127 [==============================] - 1s - loss: 0.4133 - acc: 0.8206 - val_loss: 0.4431 - val_acc: 0.8066
Epoch 29/40
8127/8127 [==============================] - 1s - loss: 0.4125 - acc: 0.8205 - val_loss: 0.4431 - val_acc: 0.8031
Epoch 30/40
8127/8127 [==============================] - 1s - loss: 0.4121 - acc: 0.8208 - val_loss: 0.4435 - val_acc: 0.8017
Epoch 31/40
8127/8127 [==============================] - 1s - loss: 0.4119 - acc: 0.8202 - val_loss: 0.4426 - val_acc: 0.8036
Epoch 32/40
8127/8127 [==============================] - 1s - loss: 0.4114 - acc: 0.8207 - val_loss: 0.4441 - val_acc: 0.8017
Epoch 33/40
8127/8127 [==============================] - 1s - loss: 0.4109 - acc: 0.8217 - val_loss: 0.4442 - val_acc: 0.8031
Epoch 34/40
8127/8127 [==============================] - 1s - loss: 0.4104 - acc: 0.8208 - val_loss: 0.4449 - val_acc: 0.8007
Epoch 35/40
8127/8127 [==============================] - 1s - loss: 0.4100 - acc: 0.8213 - val_loss: 0.4441 - val_acc: 0.8031
Epoch 36/40
8127/8127 [==============================] - 1s - loss: 0.4096 - acc: 0.8210 - val_loss: 0.4454 - val_acc: 0.7977
Epoch 37/40
8127/8127 [==============================] - 1s - loss: 0.4101 - acc: 0.8221 - val_loss: 0.4425 - val_acc: 0.8022
Epoch 38/40
8127/8127 [==============================] - 1s - loss: 0.4123 - acc: 0.8195 - val_loss: 0.4499 - val_acc: 0.7953
Epoch 39/40
8127/8127 [==============================] - 1s - loss: 0.4138 - acc: 0.8196 - val_loss: 0.4431 - val_acc: 0.8002
Epoch 40/40
8127/8127 [==============================] - 1s - loss: 0.4133 - acc: 0.8196 - val_loss: 0.4458 - val_acc: 0.7977
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-38.h5
chunk number 38
prepare data
start training
Train on 8040 samples, validate on 2011 samples
Epoch 1/40
8040/8040 [==============================] - 1s - loss: 0.4426 - acc: 0.8066 - val_loss: 0.4489 - val_acc: 0.8066
Epoch 2/40
8040/8040 [==============================] - 1s - loss: 0.4350 - acc: 0.8092 - val_loss: 0.4519 - val_acc: 0.8066
Epoch 3/40
8040/8040 [==============================] - 1s - loss: 0.4353 - acc: 0.8071 - val_loss: 0.4497 - val_acc: 0.8061
Epoch 4/40
8040/8040 [==============================] - 1s - loss: 0.4334 - acc: 0.8109 - val_loss: 0.4507 - val_acc: 0.8056
Epoch 5/40
8040/8040 [==============================] - 1s - loss: 0.4339 - acc: 0.8087 - val_loss: 0.4508 - val_acc: 0.8051
Epoch 6/40
8040/8040 [==============================] - 1s - loss: 0.4320 - acc: 0.8111 - val_loss: 0.4510 - val_acc: 0.8081
Epoch 7/40
8040/8040 [==============================] - 1s - loss: 0.4314 - acc: 0.8133 - val_loss: 0.4468 - val_acc: 0.8100
Epoch 8/40
8040/8040 [==============================] - 1s - loss: 0.4297 - acc: 0.8121 - val_loss: 0.4473 - val_acc: 0.8081
Epoch 9/40
8040/8040 [==============================] - 1s - loss: 0.4292 - acc: 0.8117 - val_loss: 0.4526 - val_acc: 0.8110
Epoch 10/40
8040/8040 [==============================] - 1s - loss: 0.4295 - acc: 0.8117 - val_loss: 0.4495 - val_acc: 0.8110
Epoch 11/40
8040/8040 [==============================] - 1s - loss: 0.4272 - acc: 0.8127 - val_loss: 0.4496 - val_acc: 0.8046
Epoch 12/40
8040/8040 [==============================] - 1s - loss: 0.4277 - acc: 0.8122 - val_loss: 0.4519 - val_acc: 0.8125
Epoch 13/40
8040/8040 [==============================] - 1s - loss: 0.4264 - acc: 0.8132 - val_loss: 0.4476 - val_acc: 0.8135
Epoch 14/40
8040/8040 [==============================] - 1s - loss: 0.4242 - acc: 0.8163 - val_loss: 0.4464 - val_acc: 0.8091
Epoch 15/40
8040/8040 [==============================] - 1s - loss: 0.4247 - acc: 0.8149 - val_loss: 0.4496 - val_acc: 0.8130
Epoch 16/40
8040/8040 [==============================] - 1s - loss: 0.4244 - acc: 0.8144 - val_loss: 0.4460 - val_acc: 0.8120
Epoch 17/40
8040/8040 [==============================] - 1s - loss: 0.4226 - acc: 0.8164 - val_loss: 0.4462 - val_acc: 0.8100
Epoch 18/40
8040/8040 [==============================] - 1s - loss: 0.4225 - acc: 0.8175 - val_loss: 0.4489 - val_acc: 0.8130
Epoch 19/40
8040/8040 [==============================] - 1s - loss: 0.4229 - acc: 0.8160 - val_loss: 0.4460 - val_acc: 0.8115
Epoch 20/40
8040/8040 [==============================] - 1s - loss: 0.4214 - acc: 0.8179 - val_loss: 0.4449 - val_acc: 0.8125
Epoch 21/40
8040/8040 [==============================] - 1s - loss: 0.4214 - acc: 0.8167 - val_loss: 0.4483 - val_acc: 0.8130
Epoch 22/40
8040/8040 [==============================] - 1s - loss: 0.4208 - acc: 0.8146 - val_loss: 0.4448 - val_acc: 0.8140
Epoch 23/40
8040/8040 [==============================] - 1s - loss: 0.4197 - acc: 0.8163 - val_loss: 0.4462 - val_acc: 0.8120
Epoch 24/40
8040/8040 [==============================] - 1s - loss: 0.4190 - acc: 0.8173 - val_loss: 0.4493 - val_acc: 0.8120
Epoch 25/40
8040/8040 [==============================] - 1s - loss: 0.4195 - acc: 0.8184 - val_loss: 0.4470 - val_acc: 0.8066
Epoch 26/40
8040/8040 [==============================] - 1s - loss: 0.4205 - acc: 0.8155 - val_loss: 0.4497 - val_acc: 0.8125
Epoch 27/40
8040/8040 [==============================] - 1s - loss: 0.4187 - acc: 0.8175 - val_loss: 0.4462 - val_acc: 0.8095
Epoch 28/40
8040/8040 [==============================] - 1s - loss: 0.4172 - acc: 0.8175 - val_loss: 0.4469 - val_acc: 0.8125
Epoch 29/40
8040/8040 [==============================] - 1s - loss: 0.4163 - acc: 0.8197 - val_loss: 0.4485 - val_acc: 0.8130
Epoch 30/40
8040/8040 [==============================] - 1s - loss: 0.4164 - acc: 0.8193 - val_loss: 0.4467 - val_acc: 0.8081
Epoch 31/40
8040/8040 [==============================] - 1s - loss: 0.4182 - acc: 0.8175 - val_loss: 0.4545 - val_acc: 0.8095
Epoch 32/40
8040/8040 [==============================] - 1s - loss: 0.4204 - acc: 0.8154 - val_loss: 0.4471 - val_acc: 0.8091
Epoch 33/40
8040/8040 [==============================] - 1s - loss: 0.4168 - acc: 0.8194 - val_loss: 0.4475 - val_acc: 0.8115
Epoch 34/40
8040/8040 [==============================] - 1s - loss: 0.4143 - acc: 0.8201 - val_loss: 0.4503 - val_acc: 0.8140
Epoch 35/40
8040/8040 [==============================] - 1s - loss: 0.4152 - acc: 0.8175 - val_loss: 0.4488 - val_acc: 0.8066
Epoch 36/40
8040/8040 [==============================] - 1s - loss: 0.4177 - acc: 0.8173 - val_loss: 0.4520 - val_acc: 0.8120
Epoch 37/40
8040/8040 [==============================] - 1s - loss: 0.4166 - acc: 0.8177 - val_loss: 0.4473 - val_acc: 0.8076
Epoch 38/40
8040/8040 [==============================] - 1s - loss: 0.4141 - acc: 0.8187 - val_loss: 0.4475 - val_acc: 0.8095
Epoch 39/40
8040/8040 [==============================] - 1s - loss: 0.4122 - acc: 0.8210 - val_loss: 0.4490 - val_acc: 0.8135
Epoch 40/40
8040/8040 [==============================] - 1s - loss: 0.4124 - acc: 0.8190 - val_loss: 0.4482 - val_acc: 0.8071
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-39.h5
chunk number 39
prepare data
start training
Train on 8109 samples, validate on 2028 samples
Epoch 1/40
8109/8109 [==============================] - 1s - loss: 0.4290 - acc: 0.8119 - val_loss: 0.4561 - val_acc: 0.7890
Epoch 2/40
8109/8109 [==============================] - 1s - loss: 0.4262 - acc: 0.8139 - val_loss: 0.4575 - val_acc: 0.7899
Epoch 3/40
8109/8109 [==============================] - 1s - loss: 0.4263 - acc: 0.8132 - val_loss: 0.4501 - val_acc: 0.7924
Epoch 4/40
8109/8109 [==============================] - 1s - loss: 0.4249 - acc: 0.8140 - val_loss: 0.4501 - val_acc: 0.7919
Epoch 5/40
8109/8109 [==============================] - 1s - loss: 0.4240 - acc: 0.8149 - val_loss: 0.4550 - val_acc: 0.7870
Epoch 6/40
8109/8109 [==============================] - 1s - loss: 0.4237 - acc: 0.8145 - val_loss: 0.4537 - val_acc: 0.7914
Epoch 7/40
8109/8109 [==============================] - 1s - loss: 0.4222 - acc: 0.8161 - val_loss: 0.4519 - val_acc: 0.7909
Epoch 8/40
8109/8109 [==============================] - 1s - loss: 0.4220 - acc: 0.8158 - val_loss: 0.4542 - val_acc: 0.7954
Epoch 9/40
8109/8109 [==============================] - 1s - loss: 0.4203 - acc: 0.8165 - val_loss: 0.4563 - val_acc: 0.7939
Epoch 10/40
8109/8109 [==============================] - 1s - loss: 0.4197 - acc: 0.8191 - val_loss: 0.4527 - val_acc: 0.7944
Epoch 11/40
8109/8109 [==============================] - 1s - loss: 0.4185 - acc: 0.8202 - val_loss: 0.4534 - val_acc: 0.7919
Epoch 12/40
8109/8109 [==============================] - 1s - loss: 0.4172 - acc: 0.8217 - val_loss: 0.4539 - val_acc: 0.7929
Epoch 13/40
8109/8109 [==============================] - 1s - loss: 0.4167 - acc: 0.8212 - val_loss: 0.4518 - val_acc: 0.7949
Epoch 14/40
8109/8109 [==============================] - 1s - loss: 0.4163 - acc: 0.8202 - val_loss: 0.4557 - val_acc: 0.7919
Epoch 15/40
8109/8109 [==============================] - 1s - loss: 0.4158 - acc: 0.8213 - val_loss: 0.4526 - val_acc: 0.7929
Epoch 16/40
8109/8109 [==============================] - 1s - loss: 0.4147 - acc: 0.8217 - val_loss: 0.4551 - val_acc: 0.7904
Epoch 17/40
8109/8109 [==============================] - 1s - loss: 0.4137 - acc: 0.8235 - val_loss: 0.4546 - val_acc: 0.7894
Epoch 18/40
8109/8109 [==============================] - 1s - loss: 0.4129 - acc: 0.8228 - val_loss: 0.4534 - val_acc: 0.7899
Epoch 19/40
8109/8109 [==============================] - 1s - loss: 0.4124 - acc: 0.8227 - val_loss: 0.4562 - val_acc: 0.7880
Epoch 20/40
8109/8109 [==============================] - 1s - loss: 0.4121 - acc: 0.8230 - val_loss: 0.4518 - val_acc: 0.7890
Epoch 21/40
8109/8109 [==============================] - 1s - loss: 0.4124 - acc: 0.8212 - val_loss: 0.4629 - val_acc: 0.7870
Epoch 22/40
8109/8109 [==============================] - 1s - loss: 0.4153 - acc: 0.8202 - val_loss: 0.4510 - val_acc: 0.7875
Epoch 23/40
8109/8109 [==============================] - 1s - loss: 0.4143 - acc: 0.8213 - val_loss: 0.4553 - val_acc: 0.7885
Epoch 24/40
8109/8109 [==============================] - 1s - loss: 0.4105 - acc: 0.8245 - val_loss: 0.4584 - val_acc: 0.7885
Epoch 25/40
8109/8109 [==============================] - 1s - loss: 0.4103 - acc: 0.8237 - val_loss: 0.4516 - val_acc: 0.7894
Epoch 26/40
8109/8109 [==============================] - 1s - loss: 0.4129 - acc: 0.8211 - val_loss: 0.4636 - val_acc: 0.7870
Epoch 27/40
8109/8109 [==============================] - 1s - loss: 0.4122 - acc: 0.8224 - val_loss: 0.4525 - val_acc: 0.7929
Epoch 28/40
8109/8109 [==============================] - 1s - loss: 0.4085 - acc: 0.8254 - val_loss: 0.4533 - val_acc: 0.7904
Epoch 29/40
8109/8109 [==============================] - 1s - loss: 0.4075 - acc: 0.8245 - val_loss: 0.4612 - val_acc: 0.7830
Epoch 30/40
8109/8109 [==============================] - 1s - loss: 0.4091 - acc: 0.8240 - val_loss: 0.4533 - val_acc: 0.7890
Epoch 31/40
8109/8109 [==============================] - 1s - loss: 0.4089 - acc: 0.8229 - val_loss: 0.4609 - val_acc: 0.7865
Epoch 32/40
8109/8109 [==============================] - 1s - loss: 0.4076 - acc: 0.8246 - val_loss: 0.4541 - val_acc: 0.7890
Epoch 33/40
8109/8109 [==============================] - 1s - loss: 0.4063 - acc: 0.8251 - val_loss: 0.4595 - val_acc: 0.7875
Epoch 34/40
8109/8109 [==============================] - 1s - loss: 0.4055 - acc: 0.8282 - val_loss: 0.4550 - val_acc: 0.7904
Epoch 35/40
8109/8109 [==============================] - 1s - loss: 0.4050 - acc: 0.8271 - val_loss: 0.4615 - val_acc: 0.7865
Epoch 36/40
8109/8109 [==============================] - 1s - loss: 0.4052 - acc: 0.8282 - val_loss: 0.4555 - val_acc: 0.7880
Epoch 37/40
8109/8109 [==============================] - 1s - loss: 0.4067 - acc: 0.8259 - val_loss: 0.4690 - val_acc: 0.7855
Epoch 38/40
8109/8109 [==============================] - 1s - loss: 0.4104 - acc: 0.8234 - val_loss: 0.4559 - val_acc: 0.7914
Epoch 39/40
8109/8109 [==============================] - 1s - loss: 0.4110 - acc: 0.8214 - val_loss: 0.4662 - val_acc: 0.7860
Epoch 40/40
8109/8109 [==============================] - 1s - loss: 0.4069 - acc: 0.8267 - val_loss: 0.4568 - val_acc: 0.7909
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-40.h5
chunk number 40
prepare data
start training
Train on 8199 samples, validate on 2050 samples
Epoch 1/40
8199/8199 [==============================] - 2s - loss: 0.4427 - acc: 0.8064 - val_loss: 0.4357 - val_acc: 0.8073
Epoch 2/40
8199/8199 [==============================] - 2s - loss: 0.4447 - acc: 0.8025 - val_loss: 0.4279 - val_acc: 0.8166
Epoch 3/40
8199/8199 [==============================] - 2s - loss: 0.4394 - acc: 0.8068 - val_loss: 0.4258 - val_acc: 0.8239
Epoch 4/40
8199/8199 [==============================] - 2s - loss: 0.4400 - acc: 0.8127 - val_loss: 0.4244 - val_acc: 0.8205
Epoch 5/40
8199/8199 [==============================] - 2s - loss: 0.4363 - acc: 0.8118 - val_loss: 0.4255 - val_acc: 0.8224
Epoch 6/40
8199/8199 [==============================] - 2s - loss: 0.4370 - acc: 0.8121 - val_loss: 0.4233 - val_acc: 0.8220
Epoch 7/40
8199/8199 [==============================] - 2s - loss: 0.4350 - acc: 0.8106 - val_loss: 0.4227 - val_acc: 0.8220
Epoch 8/40
8199/8199 [==============================] - 2s - loss: 0.4334 - acc: 0.8119 - val_loss: 0.4272 - val_acc: 0.8195
Epoch 9/40
8199/8199 [==============================] - 2s - loss: 0.4348 - acc: 0.8107 - val_loss: 0.4227 - val_acc: 0.8215
Epoch 10/40
8199/8199 [==============================] - 2s - loss: 0.4307 - acc: 0.8127 - val_loss: 0.4211 - val_acc: 0.8229
Epoch 11/40
8199/8199 [==============================] - 2s - loss: 0.4325 - acc: 0.8125 - val_loss: 0.4211 - val_acc: 0.8259
Epoch 12/40
8199/8199 [==============================] - 2s - loss: 0.4287 - acc: 0.8134 - val_loss: 0.4254 - val_acc: 0.8176
Epoch 13/40
8199/8199 [==============================] - 2s - loss: 0.4309 - acc: 0.8110 - val_loss: 0.4213 - val_acc: 0.8229
Epoch 14/40
8199/8199 [==============================] - 2s - loss: 0.4284 - acc: 0.8161 - val_loss: 0.4202 - val_acc: 0.8263
Epoch 15/40
8199/8199 [==============================] - 2s - loss: 0.4261 - acc: 0.8164 - val_loss: 0.4226 - val_acc: 0.8259
Epoch 16/40
8199/8199 [==============================] - 2s - loss: 0.4286 - acc: 0.8147 - val_loss: 0.4207 - val_acc: 0.8259
Epoch 17/40
8199/8199 [==============================] - 2s - loss: 0.4260 - acc: 0.8161 - val_loss: 0.4186 - val_acc: 0.8298
Epoch 18/40
8199/8199 [==============================] - 2s - loss: 0.4241 - acc: 0.8157 - val_loss: 0.4186 - val_acc: 0.8283
Epoch 19/40
8199/8199 [==============================] - 2s - loss: 0.4249 - acc: 0.8168 - val_loss: 0.4176 - val_acc: 0.8268
Epoch 20/40
8199/8199 [==============================] - 2s - loss: 0.4243 - acc: 0.8168 - val_loss: 0.4166 - val_acc: 0.8273
Epoch 21/40
8199/8199 [==============================] - 2s - loss: 0.4221 - acc: 0.8189 - val_loss: 0.4174 - val_acc: 0.8268
Epoch 22/40
8199/8199 [==============================] - 2s - loss: 0.4217 - acc: 0.8190 - val_loss: 0.4176 - val_acc: 0.8244
Epoch 23/40
8199/8199 [==============================] - 2s - loss: 0.4225 - acc: 0.8168 - val_loss: 0.4186 - val_acc: 0.8268
Epoch 24/40
8199/8199 [==============================] - 2s - loss: 0.4215 - acc: 0.8207 - val_loss: 0.4171 - val_acc: 0.8249
Epoch 25/40
8199/8199 [==============================] - 2s - loss: 0.4202 - acc: 0.8202 - val_loss: 0.4158 - val_acc: 0.8249
Epoch 26/40
8199/8199 [==============================] - 2s - loss: 0.4209 - acc: 0.8189 - val_loss: 0.4209 - val_acc: 0.8190
Epoch 27/40
8199/8199 [==============================] - 2s - loss: 0.4230 - acc: 0.8199 - val_loss: 0.4170 - val_acc: 0.8224
Epoch 28/40
8199/8199 [==============================] - 2s - loss: 0.4197 - acc: 0.8200 - val_loss: 0.4178 - val_acc: 0.8273
Epoch 29/40
8199/8199 [==============================] - 2s - loss: 0.4182 - acc: 0.8206 - val_loss: 0.4186 - val_acc: 0.8254
Epoch 30/40
8199/8199 [==============================] - 2s - loss: 0.4181 - acc: 0.8205 - val_loss: 0.4192 - val_acc: 0.8263
Epoch 31/40
8199/8199 [==============================] - 2s - loss: 0.4202 - acc: 0.8205 - val_loss: 0.4235 - val_acc: 0.8180
Epoch 32/40
8199/8199 [==============================] - 2s - loss: 0.4232 - acc: 0.8181 - val_loss: 0.4184 - val_acc: 0.8254
Epoch 33/40
8199/8199 [==============================] - 2s - loss: 0.4187 - acc: 0.8207 - val_loss: 0.4179 - val_acc: 0.8244
Epoch 34/40
8199/8199 [==============================] - 2s - loss: 0.4160 - acc: 0.8214 - val_loss: 0.4177 - val_acc: 0.8263
Epoch 35/40
8199/8199 [==============================] - 2s - loss: 0.4154 - acc: 0.8217 - val_loss: 0.4188 - val_acc: 0.8254
Epoch 36/40
8199/8199 [==============================] - 2s - loss: 0.4172 - acc: 0.8206 - val_loss: 0.4206 - val_acc: 0.8215
Epoch 37/40
8199/8199 [==============================] - 2s - loss: 0.4194 - acc: 0.8200 - val_loss: 0.4196 - val_acc: 0.8259
Epoch 38/40
8199/8199 [==============================] - 2s - loss: 0.4179 - acc: 0.8200 - val_loss: 0.4183 - val_acc: 0.8215
Epoch 39/40
8199/8199 [==============================] - 2s - loss: 0.4160 - acc: 0.8219 - val_loss: 0.4166 - val_acc: 0.8298
Epoch 40/40
8199/8199 [==============================] - 2s - loss: 0.4134 - acc: 0.8218 - val_loss: 0.4170 - val_acc: 0.8288
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-41.h5
chunk number 41
prepare data
start training
Train on 8120 samples, validate on 2031 samples
Epoch 1/40
8120/8120 [==============================] - 1s - loss: 0.4350 - acc: 0.8128 - val_loss: 0.4358 - val_acc: 0.8104
Epoch 2/40
8120/8120 [==============================] - 1s - loss: 0.4318 - acc: 0.8103 - val_loss: 0.4302 - val_acc: 0.8134
Epoch 3/40
8120/8120 [==============================] - 1s - loss: 0.4266 - acc: 0.8122 - val_loss: 0.4239 - val_acc: 0.8119
Epoch 4/40
8120/8120 [==============================] - 1s - loss: 0.4265 - acc: 0.8149 - val_loss: 0.4236 - val_acc: 0.8139
Epoch 5/40
8120/8120 [==============================] - 1s - loss: 0.4257 - acc: 0.8156 - val_loss: 0.4245 - val_acc: 0.8163
Epoch 6/40
8120/8120 [==============================] - 1s - loss: 0.4217 - acc: 0.8131 - val_loss: 0.4302 - val_acc: 0.8109
Epoch 7/40
8120/8120 [==============================] - 1s - loss: 0.4242 - acc: 0.8124 - val_loss: 0.4244 - val_acc: 0.8178
Epoch 8/40
8120/8120 [==============================] - 1s - loss: 0.4201 - acc: 0.8174 - val_loss: 0.4245 - val_acc: 0.8114
Epoch 9/40
8120/8120 [==============================] - 1s - loss: 0.4216 - acc: 0.8143 - val_loss: 0.4256 - val_acc: 0.8090
Epoch 10/40
8120/8120 [==============================] - 1s - loss: 0.4213 - acc: 0.8149 - val_loss: 0.4262 - val_acc: 0.8163
Epoch 11/40
8120/8120 [==============================] - 1s - loss: 0.4186 - acc: 0.8185 - val_loss: 0.4298 - val_acc: 0.8154
Epoch 12/40
8120/8120 [==============================] - 1s - loss: 0.4195 - acc: 0.8171 - val_loss: 0.4257 - val_acc: 0.8144
Epoch 13/40
8120/8120 [==============================] - 1s - loss: 0.4165 - acc: 0.8191 - val_loss: 0.4232 - val_acc: 0.8159
Epoch 14/40
8120/8120 [==============================] - 1s - loss: 0.4168 - acc: 0.8202 - val_loss: 0.4234 - val_acc: 0.8168
Epoch 15/40
8120/8120 [==============================] - 1s - loss: 0.4161 - acc: 0.8207 - val_loss: 0.4256 - val_acc: 0.8129
Epoch 16/40
8120/8120 [==============================] - 1s - loss: 0.4153 - acc: 0.8202 - val_loss: 0.4264 - val_acc: 0.8134
Epoch 17/40
8120/8120 [==============================] - 1s - loss: 0.4149 - acc: 0.8206 - val_loss: 0.4240 - val_acc: 0.8168
Epoch 18/40
8120/8120 [==============================] - 1s - loss: 0.4133 - acc: 0.8223 - val_loss: 0.4243 - val_acc: 0.8139
Epoch 19/40
8120/8120 [==============================] - 1s - loss: 0.4135 - acc: 0.8214 - val_loss: 0.4260 - val_acc: 0.8159
Epoch 20/40
8120/8120 [==============================] - 1s - loss: 0.4123 - acc: 0.8213 - val_loss: 0.4266 - val_acc: 0.8139
Epoch 21/40
8120/8120 [==============================] - 1s - loss: 0.4121 - acc: 0.8209 - val_loss: 0.4245 - val_acc: 0.8168
Epoch 22/40
8120/8120 [==============================] - 1s - loss: 0.4111 - acc: 0.8220 - val_loss: 0.4238 - val_acc: 0.8173
Epoch 23/40
8120/8120 [==============================] - 1s - loss: 0.4110 - acc: 0.8218 - val_loss: 0.4252 - val_acc: 0.8144
Epoch 24/40
8120/8120 [==============================] - 1s - loss: 0.4103 - acc: 0.8216 - val_loss: 0.4261 - val_acc: 0.8149
Epoch 25/40
8120/8120 [==============================] - 1s - loss: 0.4101 - acc: 0.8217 - val_loss: 0.4248 - val_acc: 0.8193
Epoch 26/40
8120/8120 [==============================] - 1s - loss: 0.4093 - acc: 0.8212 - val_loss: 0.4249 - val_acc: 0.8178
Epoch 27/40
8120/8120 [==============================] - 1s - loss: 0.4086 - acc: 0.8214 - val_loss: 0.4268 - val_acc: 0.8149
Epoch 28/40
8120/8120 [==============================] - 1s - loss: 0.4086 - acc: 0.8211 - val_loss: 0.4256 - val_acc: 0.8154
Epoch 29/40
8120/8120 [==============================] - 1s - loss: 0.4079 - acc: 0.8228 - val_loss: 0.4241 - val_acc: 0.8163
Epoch 30/40
8120/8120 [==============================] - 1s - loss: 0.4077 - acc: 0.8229 - val_loss: 0.4256 - val_acc: 0.8139
Epoch 31/40
8120/8120 [==============================] - 1s - loss: 0.4070 - acc: 0.8230 - val_loss: 0.4259 - val_acc: 0.8124
Epoch 32/40
8120/8120 [==============================] - 1s - loss: 0.4068 - acc: 0.8232 - val_loss: 0.4244 - val_acc: 0.8173
Epoch 33/40
8120/8120 [==============================] - 1s - loss: 0.4064 - acc: 0.8236 - val_loss: 0.4251 - val_acc: 0.8168
Epoch 34/40
8120/8120 [==============================] - 1s - loss: 0.4056 - acc: 0.8239 - val_loss: 0.4266 - val_acc: 0.8144
Epoch 35/40
8120/8120 [==============================] - 1s - loss: 0.4056 - acc: 0.8229 - val_loss: 0.4253 - val_acc: 0.8178
Epoch 36/40
8120/8120 [==============================] - 1s - loss: 0.4051 - acc: 0.8243 - val_loss: 0.4258 - val_acc: 0.8168
Epoch 37/40
8120/8120 [==============================] - 1s - loss: 0.4046 - acc: 0.8249 - val_loss: 0.4267 - val_acc: 0.8134
Epoch 38/40
8120/8120 [==============================] - 1s - loss: 0.4044 - acc: 0.8229 - val_loss: 0.4250 - val_acc: 0.8178
Epoch 39/40
8120/8120 [==============================] - 1s - loss: 0.4039 - acc: 0.8245 - val_loss: 0.4252 - val_acc: 0.8178
Epoch 40/40
8120/8120 [==============================] - 1s - loss: 0.4035 - acc: 0.8246 - val_loss: 0.4264 - val_acc: 0.8139
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-42.h5
chunk number 42
prepare data
start training
Train on 8105 samples, validate on 2027 samples
Epoch 1/40
8105/8105 [==============================] - 1s - loss: 0.4238 - acc: 0.8171 - val_loss: 0.4321 - val_acc: 0.8125
Epoch 2/40
8105/8105 [==============================] - 1s - loss: 0.4198 - acc: 0.8185 - val_loss: 0.4334 - val_acc: 0.8150
Epoch 3/40
8105/8105 [==============================] - 1s - loss: 0.4202 - acc: 0.8144 - val_loss: 0.4342 - val_acc: 0.8180
Epoch 4/40
8105/8105 [==============================] - 1s - loss: 0.4190 - acc: 0.8163 - val_loss: 0.4333 - val_acc: 0.8150
Epoch 5/40
8105/8105 [==============================] - 1s - loss: 0.4169 - acc: 0.8169 - val_loss: 0.4312 - val_acc: 0.8155
Epoch 6/40
8105/8105 [==============================] - 1s - loss: 0.4159 - acc: 0.8174 - val_loss: 0.4314 - val_acc: 0.8160
Epoch 7/40
8105/8105 [==============================] - 1s - loss: 0.4155 - acc: 0.8178 - val_loss: 0.4318 - val_acc: 0.8106
Epoch 8/40
8105/8105 [==============================] - 1s - loss: 0.4149 - acc: 0.8188 - val_loss: 0.4312 - val_acc: 0.8091
Epoch 9/40
8105/8105 [==============================] - 1s - loss: 0.4142 - acc: 0.8189 - val_loss: 0.4290 - val_acc: 0.8145
Epoch 10/40
8105/8105 [==============================] - 1s - loss: 0.4122 - acc: 0.8185 - val_loss: 0.4293 - val_acc: 0.8140
Epoch 11/40
8105/8105 [==============================] - 1s - loss: 0.4120 - acc: 0.8174 - val_loss: 0.4278 - val_acc: 0.8160
Epoch 12/40
8105/8105 [==============================] - 1s - loss: 0.4105 - acc: 0.8190 - val_loss: 0.4266 - val_acc: 0.8180
Epoch 13/40
8105/8105 [==============================] - 1s - loss: 0.4100 - acc: 0.8204 - val_loss: 0.4258 - val_acc: 0.8185
Epoch 14/40
8105/8105 [==============================] - 1s - loss: 0.4089 - acc: 0.8216 - val_loss: 0.4266 - val_acc: 0.8170
Epoch 15/40
8105/8105 [==============================] - 1s - loss: 0.4083 - acc: 0.8225 - val_loss: 0.4269 - val_acc: 0.8135
Epoch 16/40
8105/8105 [==============================] - 1s - loss: 0.4075 - acc: 0.8228 - val_loss: 0.4270 - val_acc: 0.8130
Epoch 17/40
8105/8105 [==============================] - 1s - loss: 0.4073 - acc: 0.8216 - val_loss: 0.4271 - val_acc: 0.8145
Epoch 18/40
8105/8105 [==============================] - 1s - loss: 0.4061 - acc: 0.8241 - val_loss: 0.4265 - val_acc: 0.8170
Epoch 19/40
8105/8105 [==============================] - 1s - loss: 0.4053 - acc: 0.8238 - val_loss: 0.4257 - val_acc: 0.8180
Epoch 20/40
8105/8105 [==============================] - 1s - loss: 0.4051 - acc: 0.8228 - val_loss: 0.4262 - val_acc: 0.8155
Epoch 21/40
8105/8105 [==============================] - 1s - loss: 0.4042 - acc: 0.8239 - val_loss: 0.4253 - val_acc: 0.8155
Epoch 22/40
8105/8105 [==============================] - 1s - loss: 0.4033 - acc: 0.8259 - val_loss: 0.4253 - val_acc: 0.8130
Epoch 23/40
8105/8105 [==============================] - 1s - loss: 0.4029 - acc: 0.8262 - val_loss: 0.4258 - val_acc: 0.8160
Epoch 24/40
8105/8105 [==============================] - 1s - loss: 0.4024 - acc: 0.8257 - val_loss: 0.4253 - val_acc: 0.8175
Epoch 25/40
8105/8105 [==============================] - 1s - loss: 0.4017 - acc: 0.8263 - val_loss: 0.4249 - val_acc: 0.8170
Epoch 26/40
8105/8105 [==============================] - 1s - loss: 0.4012 - acc: 0.8273 - val_loss: 0.4252 - val_acc: 0.8160
Epoch 27/40
8105/8105 [==============================] - 1s - loss: 0.4008 - acc: 0.8275 - val_loss: 0.4249 - val_acc: 0.8165
Epoch 28/40
8105/8105 [==============================] - 1s - loss: 0.4000 - acc: 0.8269 - val_loss: 0.4250 - val_acc: 0.8160
Epoch 29/40
8105/8105 [==============================] - 1s - loss: 0.3996 - acc: 0.8275 - val_loss: 0.4258 - val_acc: 0.8145
Epoch 30/40
8105/8105 [==============================] - 1s - loss: 0.3994 - acc: 0.8285 - val_loss: 0.4255 - val_acc: 0.8135
Epoch 31/40
8105/8105 [==============================] - 1s - loss: 0.3987 - acc: 0.8289 - val_loss: 0.4256 - val_acc: 0.8155
Epoch 32/40
8105/8105 [==============================] - 1s - loss: 0.3979 - acc: 0.8287 - val_loss: 0.4253 - val_acc: 0.8140
Epoch 33/40
8105/8105 [==============================] - 1s - loss: 0.3974 - acc: 0.8291 - val_loss: 0.4253 - val_acc: 0.8150
Epoch 34/40
8105/8105 [==============================] - 1s - loss: 0.3968 - acc: 0.8291 - val_loss: 0.4253 - val_acc: 0.8165
Epoch 35/40
8105/8105 [==============================] - 1s - loss: 0.3963 - acc: 0.8301 - val_loss: 0.4253 - val_acc: 0.8170
Epoch 36/40
8105/8105 [==============================] - 1s - loss: 0.3958 - acc: 0.8308 - val_loss: 0.4252 - val_acc: 0.8150
Epoch 37/40
8105/8105 [==============================] - 1s - loss: 0.3954 - acc: 0.8301 - val_loss: 0.4256 - val_acc: 0.8165
Epoch 38/40
8105/8105 [==============================] - 1s - loss: 0.3952 - acc: 0.8302 - val_loss: 0.4269 - val_acc: 0.8130
Epoch 39/40
8105/8105 [==============================] - 1s - loss: 0.3966 - acc: 0.8295 - val_loss: 0.4278 - val_acc: 0.8175
Epoch 40/40
8105/8105 [==============================] - 1s - loss: 0.3982 - acc: 0.8276 - val_loss: 0.4321 - val_acc: 0.8061
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-43.h5
chunk number 43
prepare data
start training
Train on 8189 samples, validate on 2048 samples
Epoch 1/40
8189/8189 [==============================] - 1s - loss: 0.4495 - acc: 0.8033 - val_loss: 0.4317 - val_acc: 0.8105
Epoch 2/40
8189/8189 [==============================] - 1s - loss: 0.4400 - acc: 0.8121 - val_loss: 0.4333 - val_acc: 0.8101
Epoch 3/40
8189/8189 [==============================] - 1s - loss: 0.4419 - acc: 0.8146 - val_loss: 0.4340 - val_acc: 0.8125
Epoch 4/40
8189/8189 [==============================] - 1s - loss: 0.4406 - acc: 0.8137 - val_loss: 0.4333 - val_acc: 0.8130
Epoch 5/40
8189/8189 [==============================] - 1s - loss: 0.4391 - acc: 0.8129 - val_loss: 0.4346 - val_acc: 0.8154
Epoch 6/40
8189/8189 [==============================] - 1s - loss: 0.4399 - acc: 0.8121 - val_loss: 0.4331 - val_acc: 0.8130
Epoch 7/40
8189/8189 [==============================] - 1s - loss: 0.4348 - acc: 0.8146 - val_loss: 0.4360 - val_acc: 0.8125
Epoch 8/40
8189/8189 [==============================] - 1s - loss: 0.4358 - acc: 0.8149 - val_loss: 0.4348 - val_acc: 0.8115
Epoch 9/40
8189/8189 [==============================] - 1s - loss: 0.4331 - acc: 0.8163 - val_loss: 0.4339 - val_acc: 0.8115
Epoch 10/40
8189/8189 [==============================] - 1s - loss: 0.4346 - acc: 0.8118 - val_loss: 0.4334 - val_acc: 0.8110
Epoch 11/40
8189/8189 [==============================] - 1s - loss: 0.4326 - acc: 0.8150 - val_loss: 0.4355 - val_acc: 0.8145
Epoch 12/40
8189/8189 [==============================] - 1s - loss: 0.4319 - acc: 0.8171 - val_loss: 0.4342 - val_acc: 0.8154
Epoch 13/40
8189/8189 [==============================] - 1s - loss: 0.4303 - acc: 0.8169 - val_loss: 0.4327 - val_acc: 0.8154
Epoch 14/40
8189/8189 [==============================] - 1s - loss: 0.4303 - acc: 0.8165 - val_loss: 0.4323 - val_acc: 0.8149
Epoch 15/40
8189/8189 [==============================] - 1s - loss: 0.4295 - acc: 0.8162 - val_loss: 0.4339 - val_acc: 0.8164
Epoch 16/40
8189/8189 [==============================] - 1s - loss: 0.4291 - acc: 0.8165 - val_loss: 0.4333 - val_acc: 0.8140
Epoch 17/40
8189/8189 [==============================] - 1s - loss: 0.4274 - acc: 0.8174 - val_loss: 0.4330 - val_acc: 0.8135
Epoch 18/40
8189/8189 [==============================] - 1s - loss: 0.4273 - acc: 0.8169 - val_loss: 0.4336 - val_acc: 0.8140
Epoch 19/40
8189/8189 [==============================] - 1s - loss: 0.4259 - acc: 0.8163 - val_loss: 0.4363 - val_acc: 0.8145
Epoch 20/40
8189/8189 [==============================] - 1s - loss: 0.4268 - acc: 0.8174 - val_loss: 0.4341 - val_acc: 0.8140
Epoch 21/40
8189/8189 [==============================] - 1s - loss: 0.4248 - acc: 0.8173 - val_loss: 0.4337 - val_acc: 0.8154
Epoch 22/40
8189/8189 [==============================] - 1s - loss: 0.4246 - acc: 0.8180 - val_loss: 0.4344 - val_acc: 0.8149
Epoch 23/40
8189/8189 [==============================] - 1s - loss: 0.4239 - acc: 0.8201 - val_loss: 0.4331 - val_acc: 0.8154
Epoch 24/40
8189/8189 [==============================] - 1s - loss: 0.4230 - acc: 0.8189 - val_loss: 0.4329 - val_acc: 0.8154
Epoch 25/40
8189/8189 [==============================] - 1s - loss: 0.4232 - acc: 0.8193 - val_loss: 0.4334 - val_acc: 0.8174
Epoch 26/40
8189/8189 [==============================] - 1s - loss: 0.4219 - acc: 0.8198 - val_loss: 0.4336 - val_acc: 0.8164
Epoch 27/40
8189/8189 [==============================] - 1s - loss: 0.4212 - acc: 0.8216 - val_loss: 0.4338 - val_acc: 0.8140
Epoch 28/40
8189/8189 [==============================] - 1s - loss: 0.4215 - acc: 0.8190 - val_loss: 0.4336 - val_acc: 0.8164
Epoch 29/40
8189/8189 [==============================] - 1s - loss: 0.4205 - acc: 0.8217 - val_loss: 0.4331 - val_acc: 0.8169
Epoch 30/40
8189/8189 [==============================] - 1s - loss: 0.4197 - acc: 0.8221 - val_loss: 0.4332 - val_acc: 0.8159
Epoch 31/40
8189/8189 [==============================] - 1s - loss: 0.4197 - acc: 0.8210 - val_loss: 0.4333 - val_acc: 0.8179
Epoch 32/40
8189/8189 [==============================] - 1s - loss: 0.4191 - acc: 0.8213 - val_loss: 0.4332 - val_acc: 0.8169
Epoch 33/40
8189/8189 [==============================] - 1s - loss: 0.4182 - acc: 0.8211 - val_loss: 0.4336 - val_acc: 0.8145
Epoch 34/40
8189/8189 [==============================] - 1s - loss: 0.4179 - acc: 0.8212 - val_loss: 0.4341 - val_acc: 0.8179
Epoch 35/40
8189/8189 [==============================] - 1s - loss: 0.4180 - acc: 0.8215 - val_loss: 0.4340 - val_acc: 0.8159
Epoch 36/40
8189/8189 [==============================] - 1s - loss: 0.4175 - acc: 0.8210 - val_loss: 0.4339 - val_acc: 0.8179
Epoch 37/40
8189/8189 [==============================] - 1s - loss: 0.4166 - acc: 0.8210 - val_loss: 0.4338 - val_acc: 0.8179
Epoch 38/40
8189/8189 [==============================] - 1s - loss: 0.4159 - acc: 0.8224 - val_loss: 0.4337 - val_acc: 0.8159
Epoch 39/40
8189/8189 [==============================] - 1s - loss: 0.4155 - acc: 0.8213 - val_loss: 0.4337 - val_acc: 0.8174
Epoch 40/40
8189/8189 [==============================] - 1s - loss: 0.4155 - acc: 0.8224 - val_loss: 0.4343 - val_acc: 0.8164
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-44.h5
chunk number 44
prepare data
start training
Train on 8052 samples, validate on 2013 samples
Epoch 1/40
8052/8052 [==============================] - 1s - loss: 0.4390 - acc: 0.8091 - val_loss: 0.4316 - val_acc: 0.8112
Epoch 2/40
8052/8052 [==============================] - 1s - loss: 0.4382 - acc: 0.8101 - val_loss: 0.4269 - val_acc: 0.8077
Epoch 3/40
8052/8052 [==============================] - 1s - loss: 0.4371 - acc: 0.8120 - val_loss: 0.4271 - val_acc: 0.8117
Epoch 4/40
8052/8052 [==============================] - 1s - loss: 0.4347 - acc: 0.8132 - val_loss: 0.4287 - val_acc: 0.8107
Epoch 5/40
8052/8052 [==============================] - 1s - loss: 0.4343 - acc: 0.8127 - val_loss: 0.4264 - val_acc: 0.8122
Epoch 6/40
8052/8052 [==============================] - 1s - loss: 0.4327 - acc: 0.8131 - val_loss: 0.4253 - val_acc: 0.8102
Epoch 7/40
8052/8052 [==============================] - 1s - loss: 0.4322 - acc: 0.8122 - val_loss: 0.4269 - val_acc: 0.8077
Epoch 8/40
8052/8052 [==============================] - 1s - loss: 0.4307 - acc: 0.8143 - val_loss: 0.4287 - val_acc: 0.8082
Epoch 9/40
8052/8052 [==============================] - 1s - loss: 0.4299 - acc: 0.8138 - val_loss: 0.4270 - val_acc: 0.8087
Epoch 10/40
8052/8052 [==============================] - 1s - loss: 0.4288 - acc: 0.8118 - val_loss: 0.4278 - val_acc: 0.8082
Epoch 11/40
8052/8052 [==============================] - 1s - loss: 0.4278 - acc: 0.8146 - val_loss: 0.4296 - val_acc: 0.8117
Epoch 12/40
8052/8052 [==============================] - 1s - loss: 0.4274 - acc: 0.8150 - val_loss: 0.4280 - val_acc: 0.8073
Epoch 13/40
8052/8052 [==============================] - 1s - loss: 0.4265 - acc: 0.8163 - val_loss: 0.4279 - val_acc: 0.8117
Epoch 14/40
8052/8052 [==============================] - 1s - loss: 0.4256 - acc: 0.8163 - val_loss: 0.4290 - val_acc: 0.8122
Epoch 15/40
8052/8052 [==============================] - 1s - loss: 0.4250 - acc: 0.8146 - val_loss: 0.4279 - val_acc: 0.8122
Epoch 16/40
8052/8052 [==============================] - 1s - loss: 0.4239 - acc: 0.8167 - val_loss: 0.4274 - val_acc: 0.8122
Epoch 17/40
8052/8052 [==============================] - 1s - loss: 0.4232 - acc: 0.8169 - val_loss: 0.4284 - val_acc: 0.8122
Epoch 18/40
8052/8052 [==============================] - 1s - loss: 0.4223 - acc: 0.8154 - val_loss: 0.4281 - val_acc: 0.8122
Epoch 19/40
8052/8052 [==============================] - 1s - loss: 0.4215 - acc: 0.8159 - val_loss: 0.4275 - val_acc: 0.8102
Epoch 20/40
8052/8052 [==============================] - 1s - loss: 0.4208 - acc: 0.8171 - val_loss: 0.4279 - val_acc: 0.8127
Epoch 21/40
8052/8052 [==============================] - 1s - loss: 0.4198 - acc: 0.8171 - val_loss: 0.4284 - val_acc: 0.8132
Epoch 22/40
8052/8052 [==============================] - 1s - loss: 0.4190 - acc: 0.8179 - val_loss: 0.4284 - val_acc: 0.8132
Epoch 23/40
8052/8052 [==============================] - 1s - loss: 0.4185 - acc: 0.8187 - val_loss: 0.4296 - val_acc: 0.8132
Epoch 24/40
8052/8052 [==============================] - 1s - loss: 0.4178 - acc: 0.8183 - val_loss: 0.4289 - val_acc: 0.8127
Epoch 25/40
8052/8052 [==============================] - 1s - loss: 0.4170 - acc: 0.8199 - val_loss: 0.4296 - val_acc: 0.8127
Epoch 26/40
8052/8052 [==============================] - 1s - loss: 0.4164 - acc: 0.8194 - val_loss: 0.4289 - val_acc: 0.8117
Epoch 27/40
8052/8052 [==============================] - 1s - loss: 0.4159 - acc: 0.8203 - val_loss: 0.4301 - val_acc: 0.8147
Epoch 28/40
8052/8052 [==============================] - 1s - loss: 0.4156 - acc: 0.8181 - val_loss: 0.4287 - val_acc: 0.8117
Epoch 29/40
8052/8052 [==============================] - 1s - loss: 0.4157 - acc: 0.8187 - val_loss: 0.4319 - val_acc: 0.8117
Epoch 30/40
8052/8052 [==============================] - 1s - loss: 0.4168 - acc: 0.8171 - val_loss: 0.4288 - val_acc: 0.8112
Epoch 31/40
8052/8052 [==============================] - 1s - loss: 0.4160 - acc: 0.8187 - val_loss: 0.4314 - val_acc: 0.8137
Epoch 32/40
8052/8052 [==============================] - 1s - loss: 0.4149 - acc: 0.8192 - val_loss: 0.4282 - val_acc: 0.8127
Epoch 33/40
8052/8052 [==============================] - 1s - loss: 0.4125 - acc: 0.8199 - val_loss: 0.4288 - val_acc: 0.8142
Epoch 34/40
8052/8052 [==============================] - 1s - loss: 0.4114 - acc: 0.8220 - val_loss: 0.4294 - val_acc: 0.8152
Epoch 35/40
8052/8052 [==============================] - 1s - loss: 0.4107 - acc: 0.8231 - val_loss: 0.4294 - val_acc: 0.8117
Epoch 36/40
8052/8052 [==============================] - 1s - loss: 0.4107 - acc: 0.8213 - val_loss: 0.4316 - val_acc: 0.8122
Epoch 37/40
8052/8052 [==============================] - 1s - loss: 0.4110 - acc: 0.8210 - val_loss: 0.4301 - val_acc: 0.8092
Epoch 38/40
8052/8052 [==============================] - 1s - loss: 0.4119 - acc: 0.8195 - val_loss: 0.4370 - val_acc: 0.8142
Epoch 39/40
8052/8052 [==============================] - 1s - loss: 0.4154 - acc: 0.8179 - val_loss: 0.4305 - val_acc: 0.8068
Epoch 40/40
8052/8052 [==============================] - 1s - loss: 0.4161 - acc: 0.8164 - val_loss: 0.4343 - val_acc: 0.8127
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-45.h5
chunk number 45
prepare data
start training
Train on 8124 samples, validate on 2032 samples
Epoch 1/40
8124/8124 [==============================] - 1s - loss: 0.4301 - acc: 0.8083 - val_loss: 0.4308 - val_acc: 0.8130
Epoch 2/40
8124/8124 [==============================] - 1s - loss: 0.4319 - acc: 0.8104 - val_loss: 0.4330 - val_acc: 0.8145
Epoch 3/40
8124/8124 [==============================] - 1s - loss: 0.4296 - acc: 0.8113 - val_loss: 0.4331 - val_acc: 0.8145
Epoch 4/40
8124/8124 [==============================] - 1s - loss: 0.4281 - acc: 0.8135 - val_loss: 0.4266 - val_acc: 0.8179
Epoch 5/40
8124/8124 [==============================] - 1s - loss: 0.4252 - acc: 0.8131 - val_loss: 0.4278 - val_acc: 0.8194
Epoch 6/40
8124/8124 [==============================] - 1s - loss: 0.4228 - acc: 0.8136 - val_loss: 0.4349 - val_acc: 0.8159
Epoch 7/40
8124/8124 [==============================] - 1s - loss: 0.4233 - acc: 0.8135 - val_loss: 0.4307 - val_acc: 0.8174
Epoch 8/40
8124/8124 [==============================] - 1s - loss: 0.4197 - acc: 0.8160 - val_loss: 0.4315 - val_acc: 0.8179
Epoch 9/40
8124/8124 [==============================] - 1s - loss: 0.4210 - acc: 0.8151 - val_loss: 0.4363 - val_acc: 0.8179
Epoch 10/40
8124/8124 [==============================] - 1s - loss: 0.4203 - acc: 0.8140 - val_loss: 0.4325 - val_acc: 0.8150
Epoch 11/40
8124/8124 [==============================] - 1s - loss: 0.4163 - acc: 0.8176 - val_loss: 0.4344 - val_acc: 0.8125
Epoch 12/40
8124/8124 [==============================] - 1s - loss: 0.4201 - acc: 0.8146 - val_loss: 0.4354 - val_acc: 0.8150
Epoch 13/40
8124/8124 [==============================] - 1s - loss: 0.4159 - acc: 0.8178 - val_loss: 0.4358 - val_acc: 0.8159
Epoch 14/40
8124/8124 [==============================] - 1s - loss: 0.4157 - acc: 0.8198 - val_loss: 0.4321 - val_acc: 0.8110
Epoch 15/40
8124/8124 [==============================] - 1s - loss: 0.4157 - acc: 0.8176 - val_loss: 0.4317 - val_acc: 0.8130
Epoch 16/40
8124/8124 [==============================] - 1s - loss: 0.4124 - acc: 0.8200 - val_loss: 0.4379 - val_acc: 0.8169
Epoch 17/40
8124/8124 [==============================] - 1s - loss: 0.4153 - acc: 0.8193 - val_loss: 0.4336 - val_acc: 0.8145
Epoch 18/40
8124/8124 [==============================] - 1s - loss: 0.4163 - acc: 0.8160 - val_loss: 0.4359 - val_acc: 0.8159
Epoch 19/40
8124/8124 [==============================] - 1s - loss: 0.4104 - acc: 0.8203 - val_loss: 0.4388 - val_acc: 0.8150
Epoch 20/40
8124/8124 [==============================] - 1s - loss: 0.4118 - acc: 0.8210 - val_loss: 0.4323 - val_acc: 0.8130
Epoch 21/40
8124/8124 [==============================] - 1s - loss: 0.4122 - acc: 0.8176 - val_loss: 0.4348 - val_acc: 0.8125
Epoch 22/40
8124/8124 [==============================] - 1s - loss: 0.4077 - acc: 0.8224 - val_loss: 0.4378 - val_acc: 0.8130
Epoch 23/40
8124/8124 [==============================] - 1s - loss: 0.4092 - acc: 0.8220 - val_loss: 0.4329 - val_acc: 0.8140
Epoch 24/40
8124/8124 [==============================] - 1s - loss: 0.4116 - acc: 0.8183 - val_loss: 0.4346 - val_acc: 0.8130
Epoch 25/40
8124/8124 [==============================] - 1s - loss: 0.4056 - acc: 0.8219 - val_loss: 0.4386 - val_acc: 0.8110
Epoch 26/40
8124/8124 [==============================] - 1s - loss: 0.4075 - acc: 0.8225 - val_loss: 0.4338 - val_acc: 0.8105
Epoch 27/40
8124/8124 [==============================] - 1s - loss: 0.4097 - acc: 0.8191 - val_loss: 0.4345 - val_acc: 0.8130
Epoch 28/40
8124/8124 [==============================] - 1s - loss: 0.4034 - acc: 0.8236 - val_loss: 0.4401 - val_acc: 0.8120
Epoch 29/40
8124/8124 [==============================] - 1s - loss: 0.4068 - acc: 0.8207 - val_loss: 0.4357 - val_acc: 0.8115
Epoch 30/40
8124/8124 [==============================] - 1s - loss: 0.4106 - acc: 0.8181 - val_loss: 0.4364 - val_acc: 0.8150
Epoch 31/40
8124/8124 [==============================] - 1s - loss: 0.4020 - acc: 0.8240 - val_loss: 0.4417 - val_acc: 0.8120
Epoch 32/40
8124/8124 [==============================] - 1s - loss: 0.4058 - acc: 0.8216 - val_loss: 0.4388 - val_acc: 0.8115
Epoch 33/40
8124/8124 [==============================] - 1s - loss: 0.4151 - acc: 0.8177 - val_loss: 0.4415 - val_acc: 0.8150
Epoch 34/40
8124/8124 [==============================] - 1s - loss: 0.4054 - acc: 0.8227 - val_loss: 0.4378 - val_acc: 0.8145
Epoch 35/40
8124/8124 [==============================] - 1s - loss: 0.4013 - acc: 0.8240 - val_loss: 0.4354 - val_acc: 0.8120
Epoch 36/40
8124/8124 [==============================] - 1s - loss: 0.4061 - acc: 0.8215 - val_loss: 0.4370 - val_acc: 0.8155
Epoch 37/40
8124/8124 [==============================] - 1s - loss: 0.3994 - acc: 0.8247 - val_loss: 0.4420 - val_acc: 0.8115
Epoch 38/40
8124/8124 [==============================] - 1s - loss: 0.4018 - acc: 0.8248 - val_loss: 0.4394 - val_acc: 0.8061
Epoch 39/40
8124/8124 [==============================] - 1s - loss: 0.4101 - acc: 0.8186 - val_loss: 0.4400 - val_acc: 0.8130
Epoch 40/40
8124/8124 [==============================] - 1s - loss: 0.4003 - acc: 0.8256 - val_loss: 0.4428 - val_acc: 0.8140
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-46.h5
chunk number 46
prepare data
start training
Train on 8172 samples, validate on 2043 samples
Epoch 1/40
8172/8172 [==============================] - 1s - loss: 0.4299 - acc: 0.8131 - val_loss: 0.4108 - val_acc: 0.8282
Epoch 2/40
8172/8172 [==============================] - 1s - loss: 0.4256 - acc: 0.8158 - val_loss: 0.4140 - val_acc: 0.8272
Epoch 3/40
8172/8172 [==============================] - 1s - loss: 0.4261 - acc: 0.8149 - val_loss: 0.4083 - val_acc: 0.8287
Epoch 4/40
8172/8172 [==============================] - 1s - loss: 0.4208 - acc: 0.8168 - val_loss: 0.4077 - val_acc: 0.8243
Epoch 5/40
8172/8172 [==============================] - 1s - loss: 0.4219 - acc: 0.8149 - val_loss: 0.4061 - val_acc: 0.8272
Epoch 6/40
8172/8172 [==============================] - 1s - loss: 0.4182 - acc: 0.8175 - val_loss: 0.4074 - val_acc: 0.8331
Epoch 7/40
8172/8172 [==============================] - 1s - loss: 0.4171 - acc: 0.8202 - val_loss: 0.4057 - val_acc: 0.8360
Epoch 8/40
8172/8172 [==============================] - 1s - loss: 0.4151 - acc: 0.8233 - val_loss: 0.4054 - val_acc: 0.8287
Epoch 9/40
8172/8172 [==============================] - 1s - loss: 0.4153 - acc: 0.8227 - val_loss: 0.4058 - val_acc: 0.8292
Epoch 10/40
8172/8172 [==============================] - 1s - loss: 0.4134 - acc: 0.8238 - val_loss: 0.4090 - val_acc: 0.8350
Epoch 11/40
8172/8172 [==============================] - 1s - loss: 0.4132 - acc: 0.8235 - val_loss: 0.4077 - val_acc: 0.8331
Epoch 12/40
8172/8172 [==============================] - 1s - loss: 0.4110 - acc: 0.8237 - val_loss: 0.4066 - val_acc: 0.8287
Epoch 13/40
8172/8172 [==============================] - 1s - loss: 0.4110 - acc: 0.8250 - val_loss: 0.4073 - val_acc: 0.8346
Epoch 14/40
8172/8172 [==============================] - 1s - loss: 0.4089 - acc: 0.8268 - val_loss: 0.4083 - val_acc: 0.8385
Epoch 15/40
8172/8172 [==============================] - 1s - loss: 0.4079 - acc: 0.8289 - val_loss: 0.4059 - val_acc: 0.8336
Epoch 16/40
8172/8172 [==============================] - 1s - loss: 0.4069 - acc: 0.8273 - val_loss: 0.4062 - val_acc: 0.8355
Epoch 17/40
8172/8172 [==============================] - 1s - loss: 0.4054 - acc: 0.8299 - val_loss: 0.4067 - val_acc: 0.8370
Epoch 18/40
8172/8172 [==============================] - 1s - loss: 0.4049 - acc: 0.8282 - val_loss: 0.4042 - val_acc: 0.8365
Epoch 19/40
8172/8172 [==============================] - 1s - loss: 0.4044 - acc: 0.8294 - val_loss: 0.4053 - val_acc: 0.8346
Epoch 20/40
8172/8172 [==============================] - 1s - loss: 0.4028 - acc: 0.8297 - val_loss: 0.4082 - val_acc: 0.8346
Epoch 21/40
8172/8172 [==============================] - 1s - loss: 0.4022 - acc: 0.8300 - val_loss: 0.4060 - val_acc: 0.8355
Epoch 22/40
8172/8172 [==============================] - 1s - loss: 0.4017 - acc: 0.8308 - val_loss: 0.4076 - val_acc: 0.8336
Epoch 23/40
8172/8172 [==============================] - 1s - loss: 0.4001 - acc: 0.8321 - val_loss: 0.4080 - val_acc: 0.8336
Epoch 24/40
8172/8172 [==============================] - 1s - loss: 0.3990 - acc: 0.8332 - val_loss: 0.4062 - val_acc: 0.8341
Epoch 25/40
8172/8172 [==============================] - 1s - loss: 0.3983 - acc: 0.8341 - val_loss: 0.4068 - val_acc: 0.8326
Epoch 26/40
8172/8172 [==============================] - 1s - loss: 0.3973 - acc: 0.8347 - val_loss: 0.4061 - val_acc: 0.8321
Epoch 27/40
8172/8172 [==============================] - 1s - loss: 0.3965 - acc: 0.8371 - val_loss: 0.4056 - val_acc: 0.8316
Epoch 28/40
8172/8172 [==============================] - 1s - loss: 0.3960 - acc: 0.8353 - val_loss: 0.4072 - val_acc: 0.8326
Epoch 29/40
8172/8172 [==============================] - 1s - loss: 0.3954 - acc: 0.8359 - val_loss: 0.4059 - val_acc: 0.8360
Epoch 30/40
8172/8172 [==============================] - 1s - loss: 0.3942 - acc: 0.8370 - val_loss: 0.4066 - val_acc: 0.8326
Epoch 31/40
8172/8172 [==============================] - 1s - loss: 0.3938 - acc: 0.8369 - val_loss: 0.4063 - val_acc: 0.8326
Epoch 32/40
8172/8172 [==============================] - 1s - loss: 0.3941 - acc: 0.8361 - val_loss: 0.4126 - val_acc: 0.8336
Epoch 33/40
8172/8172 [==============================] - 1s - loss: 0.3965 - acc: 0.8320 - val_loss: 0.4070 - val_acc: 0.8346
Epoch 34/40
8172/8172 [==============================] - 1s - loss: 0.3958 - acc: 0.8344 - val_loss: 0.4071 - val_acc: 0.8341
Epoch 35/40
8172/8172 [==============================] - 1s - loss: 0.3915 - acc: 0.8386 - val_loss: 0.4100 - val_acc: 0.8355
Epoch 36/40
8172/8172 [==============================] - 1s - loss: 0.3922 - acc: 0.8370 - val_loss: 0.4080 - val_acc: 0.8321
Epoch 37/40
8172/8172 [==============================] - 1s - loss: 0.3973 - acc: 0.8338 - val_loss: 0.4156 - val_acc: 0.8316
Epoch 38/40
8172/8172 [==============================] - 1s - loss: 0.3935 - acc: 0.8352 - val_loss: 0.4081 - val_acc: 0.8316
Epoch 39/40
8172/8172 [==============================] - 1s - loss: 0.3900 - acc: 0.8384 - val_loss: 0.4077 - val_acc: 0.8350
Epoch 40/40
8172/8172 [==============================] - 1s - loss: 0.3889 - acc: 0.8395 - val_loss: 0.4141 - val_acc: 0.8346
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-47.h5
chunk number 47
prepare data
start training
Train on 8160 samples, validate on 2040 samples
Epoch 1/40
8160/8160 [==============================] - 1s - loss: 0.4479 - acc: 0.8083 - val_loss: 0.4296 - val_acc: 0.8142
Epoch 2/40
8160/8160 [==============================] - 1s - loss: 0.4400 - acc: 0.8121 - val_loss: 0.4323 - val_acc: 0.8123
Epoch 3/40
8160/8160 [==============================] - 1s - loss: 0.4391 - acc: 0.8092 - val_loss: 0.4282 - val_acc: 0.8172
Epoch 4/40
8160/8160 [==============================] - 1s - loss: 0.4342 - acc: 0.8125 - val_loss: 0.4299 - val_acc: 0.8186
Epoch 5/40
8160/8160 [==============================] - 1s - loss: 0.4375 - acc: 0.8136 - val_loss: 0.4269 - val_acc: 0.8176
Epoch 6/40
8160/8160 [==============================] - 1s - loss: 0.4306 - acc: 0.8154 - val_loss: 0.4290 - val_acc: 0.8181
Epoch 7/40
8160/8160 [==============================] - 1s - loss: 0.4319 - acc: 0.8142 - val_loss: 0.4238 - val_acc: 0.8196
Epoch 8/40
8160/8160 [==============================] - 1s - loss: 0.4286 - acc: 0.8151 - val_loss: 0.4245 - val_acc: 0.8235
Epoch 9/40
8160/8160 [==============================] - 1s - loss: 0.4307 - acc: 0.8125 - val_loss: 0.4241 - val_acc: 0.8211
Epoch 10/40
8160/8160 [==============================] - 1s - loss: 0.4279 - acc: 0.8156 - val_loss: 0.4251 - val_acc: 0.8206
Epoch 11/40
8160/8160 [==============================] - 1s - loss: 0.4274 - acc: 0.8156 - val_loss: 0.4232 - val_acc: 0.8186
Epoch 12/40
8160/8160 [==============================] - 1s - loss: 0.4255 - acc: 0.8153 - val_loss: 0.4205 - val_acc: 0.8235
Epoch 13/40
8160/8160 [==============================] - 1s - loss: 0.4240 - acc: 0.8181 - val_loss: 0.4201 - val_acc: 0.8275
Epoch 14/40
8160/8160 [==============================] - 1s - loss: 0.4238 - acc: 0.8181 - val_loss: 0.4198 - val_acc: 0.8191
Epoch 15/40
8160/8160 [==============================] - 1s - loss: 0.4220 - acc: 0.8178 - val_loss: 0.4205 - val_acc: 0.8176
Epoch 16/40
8160/8160 [==============================] - 1s - loss: 0.4213 - acc: 0.8190 - val_loss: 0.4188 - val_acc: 0.8245
Epoch 17/40
8160/8160 [==============================] - 1s - loss: 0.4196 - acc: 0.8196 - val_loss: 0.4197 - val_acc: 0.8255
Epoch 18/40
8160/8160 [==============================] - 1s - loss: 0.4197 - acc: 0.8196 - val_loss: 0.4211 - val_acc: 0.8255
Epoch 19/40
8160/8160 [==============================] - 1s - loss: 0.4182 - acc: 0.8203 - val_loss: 0.4224 - val_acc: 0.8235
Epoch 20/40
8160/8160 [==============================] - 1s - loss: 0.4177 - acc: 0.8210 - val_loss: 0.4203 - val_acc: 0.8245
Epoch 21/40
8160/8160 [==============================] - 1s - loss: 0.4165 - acc: 0.8228 - val_loss: 0.4202 - val_acc: 0.8250
Epoch 22/40
8160/8160 [==============================] - 1s - loss: 0.4164 - acc: 0.8221 - val_loss: 0.4212 - val_acc: 0.8201
Epoch 23/40
8160/8160 [==============================] - 1s - loss: 0.4156 - acc: 0.8217 - val_loss: 0.4220 - val_acc: 0.8225
Epoch 24/40
8160/8160 [==============================] - 1s - loss: 0.4145 - acc: 0.8227 - val_loss: 0.4220 - val_acc: 0.8245
Epoch 25/40
8160/8160 [==============================] - 1s - loss: 0.4139 - acc: 0.8238 - val_loss: 0.4219 - val_acc: 0.8255
Epoch 26/40
8160/8160 [==============================] - 1s - loss: 0.4131 - acc: 0.8246 - val_loss: 0.4221 - val_acc: 0.8225
Epoch 27/40
8160/8160 [==============================] - 1s - loss: 0.4127 - acc: 0.8225 - val_loss: 0.4208 - val_acc: 0.8235
Epoch 28/40
8160/8160 [==============================] - 1s - loss: 0.4119 - acc: 0.8243 - val_loss: 0.4206 - val_acc: 0.8255
Epoch 29/40
8160/8160 [==============================] - 1s - loss: 0.4114 - acc: 0.8252 - val_loss: 0.4223 - val_acc: 0.8250
Epoch 30/40
8160/8160 [==============================] - 1s - loss: 0.4108 - acc: 0.8246 - val_loss: 0.4217 - val_acc: 0.8230
Epoch 31/40
8160/8160 [==============================] - 1s - loss: 0.4097 - acc: 0.8265 - val_loss: 0.4210 - val_acc: 0.8225
Epoch 32/40
8160/8160 [==============================] - 1s - loss: 0.4092 - acc: 0.8268 - val_loss: 0.4221 - val_acc: 0.8230
Epoch 33/40
8160/8160 [==============================] - 1s - loss: 0.4087 - acc: 0.8267 - val_loss: 0.4220 - val_acc: 0.8240
Epoch 34/40
8160/8160 [==============================] - 1s - loss: 0.4078 - acc: 0.8293 - val_loss: 0.4231 - val_acc: 0.8225
Epoch 35/40
8160/8160 [==============================] - 1s - loss: 0.4071 - acc: 0.8289 - val_loss: 0.4235 - val_acc: 0.8225
Epoch 36/40
8160/8160 [==============================] - 1s - loss: 0.4065 - acc: 0.8300 - val_loss: 0.4234 - val_acc: 0.8196
Epoch 37/40
8160/8160 [==============================] - 1s - loss: 0.4059 - acc: 0.8292 - val_loss: 0.4240 - val_acc: 0.8172
Epoch 38/40
8160/8160 [==============================] - 1s - loss: 0.4055 - acc: 0.8293 - val_loss: 0.4240 - val_acc: 0.8216
Epoch 39/40
8160/8160 [==============================] - 1s - loss: 0.4052 - acc: 0.8304 - val_loss: 0.4256 - val_acc: 0.8201
Epoch 40/40
8160/8160 [==============================] - 1s - loss: 0.4054 - acc: 0.8293 - val_loss: 0.4234 - val_acc: 0.8221
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-48.h5
chunk number 48
prepare data
start training
Train on 8160 samples, validate on 2040 samples
Epoch 1/40
8160/8160 [==============================] - 1s - loss: 0.4378 - acc: 0.8105 - val_loss: 0.4234 - val_acc: 0.8191
Epoch 2/40
8160/8160 [==============================] - 1s - loss: 0.4331 - acc: 0.8126 - val_loss: 0.4258 - val_acc: 0.8167
Epoch 3/40
8160/8160 [==============================] - 1s - loss: 0.4321 - acc: 0.8129 - val_loss: 0.4266 - val_acc: 0.8132
Epoch 4/40
8160/8160 [==============================] - 1s - loss: 0.4303 - acc: 0.8150 - val_loss: 0.4263 - val_acc: 0.8118
Epoch 5/40
8160/8160 [==============================] - 1s - loss: 0.4295 - acc: 0.8141 - val_loss: 0.4265 - val_acc: 0.8132
Epoch 6/40
8160/8160 [==============================] - 1s - loss: 0.4279 - acc: 0.8138 - val_loss: 0.4272 - val_acc: 0.8083
Epoch 7/40
8160/8160 [==============================] - 1s - loss: 0.4266 - acc: 0.8158 - val_loss: 0.4267 - val_acc: 0.8103
Epoch 8/40
8160/8160 [==============================] - 1s - loss: 0.4250 - acc: 0.8161 - val_loss: 0.4249 - val_acc: 0.8113
Epoch 9/40
8160/8160 [==============================] - 1s - loss: 0.4232 - acc: 0.8163 - val_loss: 0.4244 - val_acc: 0.8108
Epoch 10/40
8160/8160 [==============================] - 1s - loss: 0.4219 - acc: 0.8167 - val_loss: 0.4253 - val_acc: 0.8108
Epoch 11/40
8160/8160 [==============================] - 1s - loss: 0.4212 - acc: 0.8176 - val_loss: 0.4251 - val_acc: 0.8103
Epoch 12/40
8160/8160 [==============================] - 1s - loss: 0.4199 - acc: 0.8186 - val_loss: 0.4249 - val_acc: 0.8098
Epoch 13/40
8160/8160 [==============================] - 1s - loss: 0.4186 - acc: 0.8188 - val_loss: 0.4262 - val_acc: 0.8093
Epoch 14/40
8160/8160 [==============================] - 1s - loss: 0.4174 - acc: 0.8205 - val_loss: 0.4264 - val_acc: 0.8108
Epoch 15/40
8160/8160 [==============================] - 1s - loss: 0.4164 - acc: 0.8206 - val_loss: 0.4256 - val_acc: 0.8108
Epoch 16/40
8160/8160 [==============================] - 1s - loss: 0.4157 - acc: 0.8210 - val_loss: 0.4258 - val_acc: 0.8108
Epoch 17/40
8160/8160 [==============================] - 1s - loss: 0.4146 - acc: 0.8224 - val_loss: 0.4252 - val_acc: 0.8098
Epoch 18/40
8160/8160 [==============================] - 1s - loss: 0.4138 - acc: 0.8230 - val_loss: 0.4246 - val_acc: 0.8118
Epoch 19/40
8160/8160 [==============================] - 1s - loss: 0.4131 - acc: 0.8217 - val_loss: 0.4252 - val_acc: 0.8127
Epoch 20/40
8160/8160 [==============================] - 1s - loss: 0.4117 - acc: 0.8248 - val_loss: 0.4263 - val_acc: 0.8088
Epoch 21/40
8160/8160 [==============================] - 1s - loss: 0.4107 - acc: 0.8254 - val_loss: 0.4266 - val_acc: 0.8083
Epoch 22/40
8160/8160 [==============================] - 1s - loss: 0.4101 - acc: 0.8255 - val_loss: 0.4267 - val_acc: 0.8069
Epoch 23/40
8160/8160 [==============================] - 1s - loss: 0.4094 - acc: 0.8266 - val_loss: 0.4255 - val_acc: 0.8078
Epoch 24/40
8160/8160 [==============================] - 1s - loss: 0.4086 - acc: 0.8272 - val_loss: 0.4254 - val_acc: 0.8074
Epoch 25/40
8160/8160 [==============================] - 1s - loss: 0.4076 - acc: 0.8272 - val_loss: 0.4259 - val_acc: 0.8088
Epoch 26/40
8160/8160 [==============================] - 1s - loss: 0.4071 - acc: 0.8272 - val_loss: 0.4260 - val_acc: 0.8088
Epoch 27/40
8160/8160 [==============================] - 1s - loss: 0.4068 - acc: 0.8290 - val_loss: 0.4256 - val_acc: 0.8069
Epoch 28/40
8160/8160 [==============================] - 1s - loss: 0.4058 - acc: 0.8293 - val_loss: 0.4251 - val_acc: 0.8108
Epoch 29/40
8160/8160 [==============================] - 1s - loss: 0.4049 - acc: 0.8290 - val_loss: 0.4254 - val_acc: 0.8103
Epoch 30/40
8160/8160 [==============================] - 1s - loss: 0.4041 - acc: 0.8300 - val_loss: 0.4267 - val_acc: 0.8083
Epoch 31/40
8160/8160 [==============================] - 1s - loss: 0.4039 - acc: 0.8294 - val_loss: 0.4274 - val_acc: 0.8103
Epoch 32/40
8160/8160 [==============================] - 1s - loss: 0.4040 - acc: 0.8299 - val_loss: 0.4289 - val_acc: 0.8127
Epoch 33/40
8160/8160 [==============================] - 1s - loss: 0.4047 - acc: 0.8299 - val_loss: 0.4279 - val_acc: 0.8142
Epoch 34/40
8160/8160 [==============================] - 1s - loss: 0.4055 - acc: 0.8270 - val_loss: 0.4294 - val_acc: 0.8132
Epoch 35/40
8160/8160 [==============================] - 1s - loss: 0.4057 - acc: 0.8279 - val_loss: 0.4281 - val_acc: 0.8108
Epoch 36/40
8160/8160 [==============================] - 1s - loss: 0.4041 - acc: 0.8288 - val_loss: 0.4272 - val_acc: 0.8083
Epoch 37/40
8160/8160 [==============================] - 1s - loss: 0.4012 - acc: 0.8305 - val_loss: 0.4271 - val_acc: 0.8083
Epoch 38/40
8160/8160 [==============================] - 1s - loss: 0.4009 - acc: 0.8308 - val_loss: 0.4282 - val_acc: 0.8108
Epoch 39/40
8160/8160 [==============================] - 1s - loss: 0.4031 - acc: 0.8301 - val_loss: 0.4289 - val_acc: 0.8127
Epoch 40/40
8160/8160 [==============================] - 1s - loss: 0.4024 - acc: 0.8297 - val_loss: 0.4287 - val_acc: 0.8123
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-49.h5
chunk number 49
prepare data
start training
Train on 8063 samples, validate on 2016 samples
Epoch 1/40
8063/8063 [==============================] - 1s - loss: 0.4375 - acc: 0.8109 - val_loss: 0.4284 - val_acc: 0.8160
Epoch 2/40
8063/8063 [==============================] - 1s - loss: 0.4397 - acc: 0.8078 - val_loss: 0.4275 - val_acc: 0.8180
Epoch 3/40
8063/8063 [==============================] - 1s - loss: 0.4333 - acc: 0.8140 - val_loss: 0.4312 - val_acc: 0.8155
Epoch 4/40
8063/8063 [==============================] - 1s - loss: 0.4337 - acc: 0.8096 - val_loss: 0.4285 - val_acc: 0.8155
Epoch 5/40
8063/8063 [==============================] - 1s - loss: 0.4304 - acc: 0.8111 - val_loss: 0.4273 - val_acc: 0.8175
Epoch 6/40
8063/8063 [==============================] - 1s - loss: 0.4315 - acc: 0.8085 - val_loss: 0.4271 - val_acc: 0.8219
Epoch 7/40
8063/8063 [==============================] - 1s - loss: 0.4303 - acc: 0.8105 - val_loss: 0.4293 - val_acc: 0.8165
Epoch 8/40
8063/8063 [==============================] - 1s - loss: 0.4287 - acc: 0.8106 - val_loss: 0.4298 - val_acc: 0.8180
Epoch 9/40
8063/8063 [==============================] - 1s - loss: 0.4276 - acc: 0.8125 - val_loss: 0.4264 - val_acc: 0.8160
Epoch 10/40
8063/8063 [==============================] - 1s - loss: 0.4241 - acc: 0.8150 - val_loss: 0.4249 - val_acc: 0.8150
Epoch 11/40
8063/8063 [==============================] - 1s - loss: 0.4237 - acc: 0.8178 - val_loss: 0.4249 - val_acc: 0.8175
Epoch 12/40
8063/8063 [==============================] - 1s - loss: 0.4226 - acc: 0.8188 - val_loss: 0.4253 - val_acc: 0.8194
Epoch 13/40
8063/8063 [==============================] - 1s - loss: 0.4223 - acc: 0.8181 - val_loss: 0.4255 - val_acc: 0.8170
Epoch 14/40
8063/8063 [==============================] - 1s - loss: 0.4205 - acc: 0.8190 - val_loss: 0.4263 - val_acc: 0.8165
Epoch 15/40
8063/8063 [==============================] - 1s - loss: 0.4200 - acc: 0.8187 - val_loss: 0.4258 - val_acc: 0.8189
Epoch 16/40
8063/8063 [==============================] - 1s - loss: 0.4184 - acc: 0.8181 - val_loss: 0.4268 - val_acc: 0.8165
Epoch 17/40
8063/8063 [==============================] - 1s - loss: 0.4180 - acc: 0.8195 - val_loss: 0.4263 - val_acc: 0.8155
Epoch 18/40
8063/8063 [==============================] - 1s - loss: 0.4168 - acc: 0.8203 - val_loss: 0.4252 - val_acc: 0.8175
Epoch 19/40
8063/8063 [==============================] - 1s - loss: 0.4161 - acc: 0.8212 - val_loss: 0.4248 - val_acc: 0.8180
Epoch 20/40
8063/8063 [==============================] - 1s - loss: 0.4153 - acc: 0.8214 - val_loss: 0.4252 - val_acc: 0.8150
Epoch 21/40
8063/8063 [==============================] - 1s - loss: 0.4143 - acc: 0.8224 - val_loss: 0.4256 - val_acc: 0.8160
Epoch 22/40
8063/8063 [==============================] - 1s - loss: 0.4132 - acc: 0.8223 - val_loss: 0.4264 - val_acc: 0.8170
Epoch 23/40
8063/8063 [==============================] - 1s - loss: 0.4126 - acc: 0.8215 - val_loss: 0.4260 - val_acc: 0.8175
Epoch 24/40
8063/8063 [==============================] - 1s - loss: 0.4114 - acc: 0.8228 - val_loss: 0.4253 - val_acc: 0.8145
Epoch 25/40
8063/8063 [==============================] - 1s - loss: 0.4104 - acc: 0.8229 - val_loss: 0.4251 - val_acc: 0.8120
Epoch 26/40
8063/8063 [==============================] - 1s - loss: 0.4098 - acc: 0.8240 - val_loss: 0.4254 - val_acc: 0.8140
Epoch 27/40
8063/8063 [==============================] - 1s - loss: 0.4094 - acc: 0.8245 - val_loss: 0.4249 - val_acc: 0.8140
Epoch 28/40
8063/8063 [==============================] - 1s - loss: 0.4084 - acc: 0.8254 - val_loss: 0.4248 - val_acc: 0.8175
Epoch 29/40
8063/8063 [==============================] - 1s - loss: 0.4077 - acc: 0.8238 - val_loss: 0.4251 - val_acc: 0.8180
Epoch 30/40
8063/8063 [==============================] - 1s - loss: 0.4071 - acc: 0.8244 - val_loss: 0.4252 - val_acc: 0.8180
Epoch 31/40
8063/8063 [==============================] - 1s - loss: 0.4064 - acc: 0.8244 - val_loss: 0.4253 - val_acc: 0.8185
Epoch 32/40
8063/8063 [==============================] - 1s - loss: 0.4055 - acc: 0.8243 - val_loss: 0.4255 - val_acc: 0.8160
Epoch 33/40
8063/8063 [==============================] - 1s - loss: 0.4050 - acc: 0.8251 - val_loss: 0.4256 - val_acc: 0.8180
Epoch 34/40
8063/8063 [==============================] - 1s - loss: 0.4047 - acc: 0.8253 - val_loss: 0.4260 - val_acc: 0.8165
Epoch 35/40
8063/8063 [==============================] - 1s - loss: 0.4053 - acc: 0.8255 - val_loss: 0.4272 - val_acc: 0.8199
Epoch 36/40
8063/8063 [==============================] - 1s - loss: 0.4068 - acc: 0.8239 - val_loss: 0.4264 - val_acc: 0.8194
Epoch 37/40
8063/8063 [==============================] - 1s - loss: 0.4047 - acc: 0.8240 - val_loss: 0.4256 - val_acc: 0.8204
Epoch 38/40
8063/8063 [==============================] - 1s - loss: 0.4028 - acc: 0.8291 - val_loss: 0.4260 - val_acc: 0.8180
Epoch 39/40
8063/8063 [==============================] - 1s - loss: 0.4016 - acc: 0.8286 - val_loss: 0.4272 - val_acc: 0.8185
Epoch 40/40
8063/8063 [==============================] - 1s - loss: 0.4014 - acc: 0.8271 - val_loss: 0.4288 - val_acc: 0.8185
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-50.h5
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_IPIP2D/model-final.h5
