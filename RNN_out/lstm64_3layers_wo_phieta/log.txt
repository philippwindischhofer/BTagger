chunk number 0
prepare data
start training
Train on 8136 samples, validate on 2035 samples
Epoch 1/40
8136/8136 [==============================] - 2s - loss: 0.6896 - acc: 0.6121 - val_loss: 0.6625 - val_acc: 0.6570
Epoch 2/40
8136/8136 [==============================] - 1s - loss: 0.6627 - acc: 0.6572 - val_loss: 0.6516 - val_acc: 0.6570
Epoch 3/40
8136/8136 [==============================] - 1s - loss: 0.6515 - acc: 0.6572 - val_loss: 0.6482 - val_acc: 0.6570
Epoch 4/40
8136/8136 [==============================] - 1s - loss: 0.6477 - acc: 0.6572 - val_loss: 0.6438 - val_acc: 0.6570
Epoch 5/40
8136/8136 [==============================] - 1s - loss: 0.6430 - acc: 0.6572 - val_loss: 0.6372 - val_acc: 0.6570
Epoch 6/40
8136/8136 [==============================] - 1s - loss: 0.6362 - acc: 0.6572 - val_loss: 0.6338 - val_acc: 0.6570
Epoch 7/40
8136/8136 [==============================] - 1s - loss: 0.6325 - acc: 0.6572 - val_loss: 0.6339 - val_acc: 0.6570
Epoch 8/40
8136/8136 [==============================] - 1s - loss: 0.6324 - acc: 0.6572 - val_loss: 0.6297 - val_acc: 0.6570
Epoch 9/40
8136/8136 [==============================] - 1s - loss: 0.6277 - acc: 0.6572 - val_loss: 0.6228 - val_acc: 0.6575
Epoch 10/40
8136/8136 [==============================] - 1s - loss: 0.6203 - acc: 0.6574 - val_loss: 0.6199 - val_acc: 0.6550
Epoch 11/40
8136/8136 [==============================] - 1s - loss: 0.6167 - acc: 0.6590 - val_loss: 0.6141 - val_acc: 0.6600
Epoch 12/40
8136/8136 [==============================] - 1s - loss: 0.6100 - acc: 0.6618 - val_loss: 0.6062 - val_acc: 0.6663
Epoch 13/40
8136/8136 [==============================] - 1s - loss: 0.6011 - acc: 0.6785 - val_loss: 0.6020 - val_acc: 0.6904
Epoch 14/40
8136/8136 [==============================] - 1s - loss: 0.5958 - acc: 0.6921 - val_loss: 0.5887 - val_acc: 0.6909
Epoch 15/40
8136/8136 [==============================] - 1s - loss: 0.5816 - acc: 0.7000 - val_loss: 0.5812 - val_acc: 0.7076
Epoch 16/40
8136/8136 [==============================] - 1s - loss: 0.5741 - acc: 0.7078 - val_loss: 0.5636 - val_acc: 0.7327
Epoch 17/40
8136/8136 [==============================] - 1s - loss: 0.5555 - acc: 0.7327 - val_loss: 0.5504 - val_acc: 0.7371
Epoch 18/40
8136/8136 [==============================] - 1s - loss: 0.5428 - acc: 0.7461 - val_loss: 0.5498 - val_acc: 0.7376
Epoch 19/40
8136/8136 [==============================] - 1s - loss: 0.5435 - acc: 0.7378 - val_loss: 0.5342 - val_acc: 0.7504
Epoch 20/40
8136/8136 [==============================] - 1s - loss: 0.5285 - acc: 0.7549 - val_loss: 0.5244 - val_acc: 0.7494
Epoch 21/40
8136/8136 [==============================] - 1s - loss: 0.5197 - acc: 0.7581 - val_loss: 0.5262 - val_acc: 0.7469
Epoch 22/40
8136/8136 [==============================] - 1s - loss: 0.5216 - acc: 0.7559 - val_loss: 0.5256 - val_acc: 0.7622
Epoch 23/40
8136/8136 [==============================] - 1s - loss: 0.5233 - acc: 0.7581 - val_loss: 0.5266 - val_acc: 0.7538
Epoch 24/40
8136/8136 [==============================] - 1s - loss: 0.5237 - acc: 0.7585 - val_loss: 0.5125 - val_acc: 0.7553
Epoch 25/40
8136/8136 [==============================] - 1s - loss: 0.5112 - acc: 0.7608 - val_loss: 0.5357 - val_acc: 0.7509
Epoch 26/40
8136/8136 [==============================] - 1s - loss: 0.5336 - acc: 0.7506 - val_loss: 0.5157 - val_acc: 0.7563
Epoch 27/40
8136/8136 [==============================] - 1s - loss: 0.5146 - acc: 0.7613 - val_loss: 0.5263 - val_acc: 0.7553
Epoch 28/40
8136/8136 [==============================] - 1s - loss: 0.5244 - acc: 0.7560 - val_loss: 0.5078 - val_acc: 0.7592
Epoch 29/40
8136/8136 [==============================] - 1s - loss: 0.5066 - acc: 0.7622 - val_loss: 0.5193 - val_acc: 0.7577
Epoch 30/40
8136/8136 [==============================] - 1s - loss: 0.5174 - acc: 0.7635 - val_loss: 0.5103 - val_acc: 0.7582
Epoch 31/40
8136/8136 [==============================] - 1s - loss: 0.5085 - acc: 0.7660 - val_loss: 0.5077 - val_acc: 0.7612
Epoch 32/40
8136/8136 [==============================] - 1s - loss: 0.5056 - acc: 0.7591 - val_loss: 0.5139 - val_acc: 0.7543
Epoch 33/40
8136/8136 [==============================] - 1s - loss: 0.5112 - acc: 0.7548 - val_loss: 0.5062 - val_acc: 0.7592
Epoch 34/40
8136/8136 [==============================] - 1s - loss: 0.5037 - acc: 0.7592 - val_loss: 0.5058 - val_acc: 0.7602
Epoch 35/40
8136/8136 [==============================] - 1s - loss: 0.5034 - acc: 0.7666 - val_loss: 0.5080 - val_acc: 0.7617
Epoch 36/40
8136/8136 [==============================] - 1s - loss: 0.5053 - acc: 0.7659 - val_loss: 0.4997 - val_acc: 0.7612
Epoch 37/40
8136/8136 [==============================] - 1s - loss: 0.4971 - acc: 0.7678 - val_loss: 0.5029 - val_acc: 0.7612
Epoch 38/40
8136/8136 [==============================] - 1s - loss: 0.5004 - acc: 0.7652 - val_loss: 0.4994 - val_acc: 0.7607
Epoch 39/40
8136/8136 [==============================] - 1s - loss: 0.4968 - acc: 0.7677 - val_loss: 0.4981 - val_acc: 0.7656
Epoch 40/40
8136/8136 [==============================] - 1s - loss: 0.4947 - acc: 0.7671 - val_loss: 0.5007 - val_acc: 0.7646
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-1.h5
chunk number 1
prepare data
start training
Train on 8160 samples, validate on 2041 samples
Epoch 1/40
8160/8160 [==============================] - 1s - loss: 0.5076 - acc: 0.7559 - val_loss: 0.5137 - val_acc: 0.7550
Epoch 2/40
8160/8160 [==============================] - 1s - loss: 0.5040 - acc: 0.7613 - val_loss: 0.5147 - val_acc: 0.7531
Epoch 3/40
8160/8160 [==============================] - 1s - loss: 0.5061 - acc: 0.7607 - val_loss: 0.5117 - val_acc: 0.7560
Epoch 4/40
8160/8160 [==============================] - 1s - loss: 0.5017 - acc: 0.7614 - val_loss: 0.5156 - val_acc: 0.7531
Epoch 5/40
8160/8160 [==============================] - 1s - loss: 0.5036 - acc: 0.7569 - val_loss: 0.5095 - val_acc: 0.7604
Epoch 6/40
8160/8160 [==============================] - 1s - loss: 0.4992 - acc: 0.7629 - val_loss: 0.5093 - val_acc: 0.7531
Epoch 7/40
8160/8160 [==============================] - 1s - loss: 0.5004 - acc: 0.7634 - val_loss: 0.5077 - val_acc: 0.7565
Epoch 8/40
8160/8160 [==============================] - 1s - loss: 0.4976 - acc: 0.7630 - val_loss: 0.5109 - val_acc: 0.7575
Epoch 9/40
8160/8160 [==============================] - 1s - loss: 0.4990 - acc: 0.7631 - val_loss: 0.5078 - val_acc: 0.7555
Epoch 10/40
8160/8160 [==============================] - 1s - loss: 0.4966 - acc: 0.7637 - val_loss: 0.5078 - val_acc: 0.7545
Epoch 11/40
8160/8160 [==============================] - 1s - loss: 0.4977 - acc: 0.7624 - val_loss: 0.5067 - val_acc: 0.7545
Epoch 12/40
8160/8160 [==============================] - 1s - loss: 0.4956 - acc: 0.7635 - val_loss: 0.5085 - val_acc: 0.7555
Epoch 13/40
8160/8160 [==============================] - 1s - loss: 0.4961 - acc: 0.7647 - val_loss: 0.5056 - val_acc: 0.7570
Epoch 14/40
8160/8160 [==============================] - 1s - loss: 0.4935 - acc: 0.7651 - val_loss: 0.5052 - val_acc: 0.7545
Epoch 15/40
8160/8160 [==============================] - 1s - loss: 0.4938 - acc: 0.7629 - val_loss: 0.5047 - val_acc: 0.7589
Epoch 16/40
8160/8160 [==============================] - 1s - loss: 0.4918 - acc: 0.7648 - val_loss: 0.5064 - val_acc: 0.7575
Epoch 17/40
8160/8160 [==============================] - 1s - loss: 0.4924 - acc: 0.7642 - val_loss: 0.5044 - val_acc: 0.7570
Epoch 18/40
8160/8160 [==============================] - 1s - loss: 0.4911 - acc: 0.7652 - val_loss: 0.5045 - val_acc: 0.7570
Epoch 19/40
8160/8160 [==============================] - 1s - loss: 0.4910 - acc: 0.7646 - val_loss: 0.5055 - val_acc: 0.7580
Epoch 20/40
8160/8160 [==============================] - 1s - loss: 0.4907 - acc: 0.7653 - val_loss: 0.5042 - val_acc: 0.7565
Epoch 21/40
8160/8160 [==============================] - 1s - loss: 0.4896 - acc: 0.7645 - val_loss: 0.5039 - val_acc: 0.7560
Epoch 22/40
8160/8160 [==============================] - 1s - loss: 0.4898 - acc: 0.7652 - val_loss: 0.5035 - val_acc: 0.7560
Epoch 23/40
8160/8160 [==============================] - 1s - loss: 0.4887 - acc: 0.7661 - val_loss: 0.5033 - val_acc: 0.7565
Epoch 24/40
8160/8160 [==============================] - 1s - loss: 0.4885 - acc: 0.7664 - val_loss: 0.5027 - val_acc: 0.7550
Epoch 25/40
8160/8160 [==============================] - 1s - loss: 0.4883 - acc: 0.7662 - val_loss: 0.5021 - val_acc: 0.7580
Epoch 26/40
8160/8160 [==============================] - 1s - loss: 0.4873 - acc: 0.7657 - val_loss: 0.5022 - val_acc: 0.7555
Epoch 27/40
8160/8160 [==============================] - 1s - loss: 0.4872 - acc: 0.7646 - val_loss: 0.5015 - val_acc: 0.7580
Epoch 28/40
8160/8160 [==============================] - 1s - loss: 0.4870 - acc: 0.7653 - val_loss: 0.5012 - val_acc: 0.7550
Epoch 29/40
8160/8160 [==============================] - 1s - loss: 0.4861 - acc: 0.7663 - val_loss: 0.5007 - val_acc: 0.7550
Epoch 30/40
8160/8160 [==============================] - 1s - loss: 0.4856 - acc: 0.7667 - val_loss: 0.5004 - val_acc: 0.7580
Epoch 31/40
8160/8160 [==============================] - 1s - loss: 0.4858 - acc: 0.7664 - val_loss: 0.5007 - val_acc: 0.7594
Epoch 32/40
8160/8160 [==============================] - 1s - loss: 0.4857 - acc: 0.7675 - val_loss: 0.4997 - val_acc: 0.7585
Epoch 33/40
8160/8160 [==============================] - 1s - loss: 0.4854 - acc: 0.7670 - val_loss: 0.4993 - val_acc: 0.7575
Epoch 34/40
8160/8160 [==============================] - 1s - loss: 0.4847 - acc: 0.7681 - val_loss: 0.4980 - val_acc: 0.7565
Epoch 35/40
8160/8160 [==============================] - 1s - loss: 0.4840 - acc: 0.7670 - val_loss: 0.4975 - val_acc: 0.7575
Epoch 36/40
8160/8160 [==============================] - 1s - loss: 0.4834 - acc: 0.7676 - val_loss: 0.4966 - val_acc: 0.7585
Epoch 37/40
8160/8160 [==============================] - 1s - loss: 0.4829 - acc: 0.7684 - val_loss: 0.4960 - val_acc: 0.7580
Epoch 38/40
8160/8160 [==============================] - 1s - loss: 0.4824 - acc: 0.7685 - val_loss: 0.4956 - val_acc: 0.7594
Epoch 39/40
8160/8160 [==============================] - 1s - loss: 0.4821 - acc: 0.7686 - val_loss: 0.4952 - val_acc: 0.7585
Epoch 40/40
8160/8160 [==============================] - 1s - loss: 0.4818 - acc: 0.7683 - val_loss: 0.4952 - val_acc: 0.7624
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-2.h5
chunk number 2
prepare data
start training
Train on 8182 samples, validate on 2046 samples
Epoch 1/40
8182/8182 [==============================] - 1s - loss: 0.4631 - acc: 0.7856 - val_loss: 0.4970 - val_acc: 0.7786
Epoch 2/40
8182/8182 [==============================] - 1s - loss: 0.4758 - acc: 0.7807 - val_loss: 0.4837 - val_acc: 0.7630
Epoch 3/40
8182/8182 [==============================] - 1s - loss: 0.4600 - acc: 0.7871 - val_loss: 0.4849 - val_acc: 0.7649
Epoch 4/40
8182/8182 [==============================] - 1s - loss: 0.4600 - acc: 0.7881 - val_loss: 0.4937 - val_acc: 0.7757
Epoch 5/40
8182/8182 [==============================] - 1s - loss: 0.4696 - acc: 0.7822 - val_loss: 0.4825 - val_acc: 0.7634
Epoch 6/40
8182/8182 [==============================] - 1s - loss: 0.4565 - acc: 0.7886 - val_loss: 0.4875 - val_acc: 0.7654
Epoch 7/40
8182/8182 [==============================] - 1s - loss: 0.4619 - acc: 0.7890 - val_loss: 0.4910 - val_acc: 0.7766
Epoch 8/40
8182/8182 [==============================] - 1s - loss: 0.4672 - acc: 0.7829 - val_loss: 0.4795 - val_acc: 0.7693
Epoch 9/40
8182/8182 [==============================] - 1s - loss: 0.4543 - acc: 0.7894 - val_loss: 0.4930 - val_acc: 0.7620
Epoch 10/40
8182/8182 [==============================] - 1s - loss: 0.4692 - acc: 0.7873 - val_loss: 0.4841 - val_acc: 0.7742
Epoch 11/40
8182/8182 [==============================] - 1s - loss: 0.4602 - acc: 0.7851 - val_loss: 0.4831 - val_acc: 0.7747
Epoch 12/40
8182/8182 [==============================] - 1s - loss: 0.4590 - acc: 0.7859 - val_loss: 0.4870 - val_acc: 0.7644
Epoch 13/40
8182/8182 [==============================] - 1s - loss: 0.4618 - acc: 0.7892 - val_loss: 0.4785 - val_acc: 0.7659
Epoch 14/40
8182/8182 [==============================] - 1s - loss: 0.4522 - acc: 0.7925 - val_loss: 0.4842 - val_acc: 0.7727
Epoch 15/40
8182/8182 [==============================] - 1s - loss: 0.4590 - acc: 0.7876 - val_loss: 0.4787 - val_acc: 0.7693
Epoch 16/40
8182/8182 [==============================] - 1s - loss: 0.4509 - acc: 0.7932 - val_loss: 0.4868 - val_acc: 0.7649
Epoch 17/40
8182/8182 [==============================] - 1s - loss: 0.4584 - acc: 0.7914 - val_loss: 0.4794 - val_acc: 0.7708
Epoch 18/40
8182/8182 [==============================] - 1s - loss: 0.4512 - acc: 0.7937 - val_loss: 0.4814 - val_acc: 0.7727
Epoch 19/40
8182/8182 [==============================] - 1s - loss: 0.4541 - acc: 0.7908 - val_loss: 0.4792 - val_acc: 0.7708
Epoch 20/40
8182/8182 [==============================] - 1s - loss: 0.4502 - acc: 0.7947 - val_loss: 0.4795 - val_acc: 0.7703
Epoch 21/40
8182/8182 [==============================] - 1s - loss: 0.4506 - acc: 0.7956 - val_loss: 0.4783 - val_acc: 0.7737
Epoch 22/40
8182/8182 [==============================] - 1s - loss: 0.4506 - acc: 0.7927 - val_loss: 0.4766 - val_acc: 0.7737
Epoch 23/40
8182/8182 [==============================] - 1s - loss: 0.4482 - acc: 0.7955 - val_loss: 0.4792 - val_acc: 0.7698
Epoch 24/40
8182/8182 [==============================] - 1s - loss: 0.4501 - acc: 0.7947 - val_loss: 0.4751 - val_acc: 0.7708
Epoch 25/40
8182/8182 [==============================] - 1s - loss: 0.4456 - acc: 0.7969 - val_loss: 0.4769 - val_acc: 0.7752
Epoch 26/40
8182/8182 [==============================] - 1s - loss: 0.4477 - acc: 0.7941 - val_loss: 0.4752 - val_acc: 0.7752
Epoch 27/40
8182/8182 [==============================] - 1s - loss: 0.4444 - acc: 0.7987 - val_loss: 0.4764 - val_acc: 0.7727
Epoch 28/40
8182/8182 [==============================] - 1s - loss: 0.4452 - acc: 0.7983 - val_loss: 0.4756 - val_acc: 0.7737
Epoch 29/40
8182/8182 [==============================] - 1s - loss: 0.4448 - acc: 0.7969 - val_loss: 0.4737 - val_acc: 0.7742
Epoch 30/40
8182/8182 [==============================] - 1s - loss: 0.4421 - acc: 0.7997 - val_loss: 0.4758 - val_acc: 0.7713
Epoch 31/40
8182/8182 [==============================] - 1s - loss: 0.4442 - acc: 0.7994 - val_loss: 0.4734 - val_acc: 0.7757
Epoch 32/40
8182/8182 [==============================] - 1s - loss: 0.4421 - acc: 0.7985 - val_loss: 0.4715 - val_acc: 0.7761
Epoch 33/40
8182/8182 [==============================] - 1s - loss: 0.4397 - acc: 0.8009 - val_loss: 0.4739 - val_acc: 0.7717
Epoch 34/40
8182/8182 [==============================] - 1s - loss: 0.4421 - acc: 0.8002 - val_loss: 0.4727 - val_acc: 0.7766
Epoch 35/40
8182/8182 [==============================] - 1s - loss: 0.4409 - acc: 0.7982 - val_loss: 0.4700 - val_acc: 0.7742
Epoch 36/40
8182/8182 [==============================] - 1s - loss: 0.4368 - acc: 0.8029 - val_loss: 0.4732 - val_acc: 0.7713
Epoch 37/40
8182/8182 [==============================] - 1s - loss: 0.4396 - acc: 0.8010 - val_loss: 0.4745 - val_acc: 0.7815
Epoch 38/40
8182/8182 [==============================] - 1s - loss: 0.4423 - acc: 0.7998 - val_loss: 0.4692 - val_acc: 0.7727
Epoch 39/40
8182/8182 [==============================] - 1s - loss: 0.4347 - acc: 0.8037 - val_loss: 0.4695 - val_acc: 0.7737
Epoch 40/40
8182/8182 [==============================] - 1s - loss: 0.4348 - acc: 0.8041 - val_loss: 0.4729 - val_acc: 0.7864
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-3.h5
chunk number 3
prepare data
start training
Train on 8167 samples, validate on 2042 samples
Epoch 1/40
8167/8167 [==============================] - 1s - loss: 0.4747 - acc: 0.7809 - val_loss: 0.4588 - val_acc: 0.7850
Epoch 2/40
8167/8167 [==============================] - 1s - loss: 0.4660 - acc: 0.7813 - val_loss: 0.4588 - val_acc: 0.7875
Epoch 3/40
8167/8167 [==============================] - 1s - loss: 0.4650 - acc: 0.7834 - val_loss: 0.4593 - val_acc: 0.7987
Epoch 4/40
8167/8167 [==============================] - 1s - loss: 0.4704 - acc: 0.7813 - val_loss: 0.4601 - val_acc: 0.7894
Epoch 5/40
8167/8167 [==============================] - 1s - loss: 0.4652 - acc: 0.7856 - val_loss: 0.4509 - val_acc: 0.7948
Epoch 6/40
8167/8167 [==============================] - 1s - loss: 0.4603 - acc: 0.7890 - val_loss: 0.4494 - val_acc: 0.7987
Epoch 7/40
8167/8167 [==============================] - 1s - loss: 0.4592 - acc: 0.7901 - val_loss: 0.4536 - val_acc: 0.7919
Epoch 8/40
8167/8167 [==============================] - 1s - loss: 0.4606 - acc: 0.7880 - val_loss: 0.4515 - val_acc: 0.8031
Epoch 9/40
8167/8167 [==============================] - 1s - loss: 0.4628 - acc: 0.7899 - val_loss: 0.4539 - val_acc: 0.7968
Epoch 10/40
8167/8167 [==============================] - 1s - loss: 0.4584 - acc: 0.7921 - val_loss: 0.4487 - val_acc: 0.8046
Epoch 11/40
8167/8167 [==============================] - 1s - loss: 0.4560 - acc: 0.7911 - val_loss: 0.4485 - val_acc: 0.8022
Epoch 12/40
8167/8167 [==============================] - 1s - loss: 0.4531 - acc: 0.7967 - val_loss: 0.4464 - val_acc: 0.7997
Epoch 13/40
8167/8167 [==============================] - 1s - loss: 0.4512 - acc: 0.7986 - val_loss: 0.4445 - val_acc: 0.8061
Epoch 14/40
8167/8167 [==============================] - 1s - loss: 0.4511 - acc: 0.7970 - val_loss: 0.4527 - val_acc: 0.7973
Epoch 15/40
8167/8167 [==============================] - 1s - loss: 0.4543 - acc: 0.7952 - val_loss: 0.4707 - val_acc: 0.7943
Epoch 16/40
8167/8167 [==============================] - 1s - loss: 0.4821 - acc: 0.7851 - val_loss: 0.4841 - val_acc: 0.7693
Epoch 17/40
8167/8167 [==============================] - 1s - loss: 0.4797 - acc: 0.7780 - val_loss: 0.5113 - val_acc: 0.7786
Epoch 18/40
8167/8167 [==============================] - 1s - loss: 0.5267 - acc: 0.7643 - val_loss: 0.4606 - val_acc: 0.8002
Epoch 19/40
8167/8167 [==============================] - 1s - loss: 0.4702 - acc: 0.7876 - val_loss: 0.5872 - val_acc: 0.6807
Epoch 20/40
8167/8167 [==============================] - 1s - loss: 0.5783 - acc: 0.6976 - val_loss: 0.4647 - val_acc: 0.7948
Epoch 21/40
8167/8167 [==============================] - 1s - loss: 0.4751 - acc: 0.7792 - val_loss: 0.5109 - val_acc: 0.7640
Epoch 22/40
8167/8167 [==============================] - 1s - loss: 0.5256 - acc: 0.7465 - val_loss: 0.4823 - val_acc: 0.7752
Epoch 23/40
8167/8167 [==============================] - 1s - loss: 0.4928 - acc: 0.7583 - val_loss: 0.4706 - val_acc: 0.8002
Epoch 24/40
8167/8167 [==============================] - 1s - loss: 0.4742 - acc: 0.7889 - val_loss: 0.5010 - val_acc: 0.7933
Epoch 25/40
8167/8167 [==============================] - 1s - loss: 0.5027 - acc: 0.7816 - val_loss: 0.4760 - val_acc: 0.7958
Epoch 26/40
8167/8167 [==============================] - 1s - loss: 0.4812 - acc: 0.7871 - val_loss: 0.4564 - val_acc: 0.7953
Epoch 27/40
8167/8167 [==============================] - 1s - loss: 0.4684 - acc: 0.7832 - val_loss: 0.4685 - val_acc: 0.7924
Epoch 28/40
8167/8167 [==============================] - 1s - loss: 0.4860 - acc: 0.7778 - val_loss: 0.4718 - val_acc: 0.7914
Epoch 29/40
8167/8167 [==============================] - 1s - loss: 0.4909 - acc: 0.7762 - val_loss: 0.4631 - val_acc: 0.7968
Epoch 30/40
8167/8167 [==============================] - 1s - loss: 0.4801 - acc: 0.7809 - val_loss: 0.4585 - val_acc: 0.7992
Epoch 31/40
8167/8167 [==============================] - 1s - loss: 0.4711 - acc: 0.7856 - val_loss: 0.4699 - val_acc: 0.7899
Epoch 32/40
8167/8167 [==============================] - 1s - loss: 0.4779 - acc: 0.7863 - val_loss: 0.4735 - val_acc: 0.7865
Epoch 33/40
8167/8167 [==============================] - 1s - loss: 0.4795 - acc: 0.7843 - val_loss: 0.4604 - val_acc: 0.7973
Epoch 34/40
8167/8167 [==============================] - 1s - loss: 0.4678 - acc: 0.7889 - val_loss: 0.4566 - val_acc: 0.8012
Epoch 35/40
8167/8167 [==============================] - 1s - loss: 0.4664 - acc: 0.7855 - val_loss: 0.4601 - val_acc: 0.7987
Epoch 36/40
8167/8167 [==============================] - 1s - loss: 0.4714 - acc: 0.7830 - val_loss: 0.4590 - val_acc: 0.7992
Epoch 37/40
8167/8167 [==============================] - 1s - loss: 0.4698 - acc: 0.7833 - val_loss: 0.4543 - val_acc: 0.8012
Epoch 38/40
8167/8167 [==============================] - 1s - loss: 0.4626 - acc: 0.7876 - val_loss: 0.4579 - val_acc: 0.7963
Epoch 39/40
8167/8167 [==============================] - 1s - loss: 0.4629 - acc: 0.7904 - val_loss: 0.4619 - val_acc: 0.7884
Epoch 40/40
8167/8167 [==============================] - 1s - loss: 0.4654 - acc: 0.7883 - val_loss: 0.4511 - val_acc: 0.7958
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-4.h5
chunk number 4
prepare data
start training
Train on 8048 samples, validate on 2012 samples
Epoch 1/40
8048/8048 [==============================] - 2s - loss: 0.6293 - acc: 0.6543 - val_loss: 0.5254 - val_acc: 0.7967
Epoch 2/40
8048/8048 [==============================] - 2s - loss: 0.5720 - acc: 0.7768 - val_loss: 0.6265 - val_acc: 0.7639
Epoch 3/40
8048/8048 [==============================] - 2s - loss: 0.6654 - acc: 0.7471 - val_loss: 0.6476 - val_acc: 0.7470
Epoch 4/40
8048/8048 [==============================] - 2s - loss: 0.6825 - acc: 0.7292 - val_loss: 0.6317 - val_acc: 0.7376
Epoch 5/40
8048/8048 [==============================] - 2s - loss: 0.6577 - acc: 0.7253 - val_loss: 0.6111 - val_acc: 0.7356
Epoch 6/40
8048/8048 [==============================] - 2s - loss: 0.6299 - acc: 0.7242 - val_loss: 0.6092 - val_acc: 0.7411
Epoch 7/40
8048/8048 [==============================] - 2s - loss: 0.6211 - acc: 0.7274 - val_loss: 0.6069 - val_acc: 0.7490
Epoch 8/40
8048/8048 [==============================] - 2s - loss: 0.6142 - acc: 0.7330 - val_loss: 0.5878 - val_acc: 0.7589
Epoch 9/40
8048/8048 [==============================] - 2s - loss: 0.5943 - acc: 0.7442 - val_loss: 0.5579 - val_acc: 0.7669
Epoch 10/40
8048/8048 [==============================] - 2s - loss: 0.5686 - acc: 0.7532 - val_loss: 0.5268 - val_acc: 0.7818
Epoch 11/40
8048/8048 [==============================] - 2s - loss: 0.5476 - acc: 0.7621 - val_loss: 0.5098 - val_acc: 0.7908
Epoch 12/40
8048/8048 [==============================] - 2s - loss: 0.5324 - acc: 0.7678 - val_loss: 0.5051 - val_acc: 0.7898
Epoch 13/40
8048/8048 [==============================] - 2s - loss: 0.5241 - acc: 0.7770 - val_loss: 0.5103 - val_acc: 0.7898
Epoch 14/40
8048/8048 [==============================] - 2s - loss: 0.5249 - acc: 0.7766 - val_loss: 0.5221 - val_acc: 0.7808
Epoch 15/40
8048/8048 [==============================] - 2s - loss: 0.5312 - acc: 0.7739 - val_loss: 0.5179 - val_acc: 0.7793
Epoch 16/40
8048/8048 [==============================] - 2s - loss: 0.5260 - acc: 0.7735 - val_loss: 0.5020 - val_acc: 0.7818
Epoch 17/40
8048/8048 [==============================] - 2s - loss: 0.5124 - acc: 0.7781 - val_loss: 0.4890 - val_acc: 0.7893
Epoch 18/40
8048/8048 [==============================] - 2s - loss: 0.5033 - acc: 0.7817 - val_loss: 0.4848 - val_acc: 0.7898
Epoch 19/40
8048/8048 [==============================] - 2s - loss: 0.5006 - acc: 0.7816 - val_loss: 0.4836 - val_acc: 0.7932
Epoch 20/40
8048/8048 [==============================] - 2s - loss: 0.5005 - acc: 0.7811 - val_loss: 0.4824 - val_acc: 0.7898
Epoch 21/40
8048/8048 [==============================] - 2s - loss: 0.4998 - acc: 0.7816 - val_loss: 0.4785 - val_acc: 0.7893
Epoch 22/40
8048/8048 [==============================] - 2s - loss: 0.4964 - acc: 0.7812 - val_loss: 0.4738 - val_acc: 0.7873
Epoch 23/40
8048/8048 [==============================] - 2s - loss: 0.4899 - acc: 0.7792 - val_loss: 0.4717 - val_acc: 0.7798
Epoch 24/40
8048/8048 [==============================] - 2s - loss: 0.4848 - acc: 0.7720 - val_loss: 0.4709 - val_acc: 0.7634
Epoch 25/40
8048/8048 [==============================] - 2s - loss: 0.4854 - acc: 0.7576 - val_loss: 0.4738 - val_acc: 0.7505
Epoch 26/40
8048/8048 [==============================] - 2s - loss: 0.4886 - acc: 0.7464 - val_loss: 0.4690 - val_acc: 0.7699
Epoch 27/40
8048/8048 [==============================] - 2s - loss: 0.4844 - acc: 0.7566 - val_loss: 0.4632 - val_acc: 0.7927
Epoch 28/40
8048/8048 [==============================] - 2s - loss: 0.4789 - acc: 0.7790 - val_loss: 0.4640 - val_acc: 0.7977
Epoch 29/40
8048/8048 [==============================] - 2s - loss: 0.4775 - acc: 0.7788 - val_loss: 0.4664 - val_acc: 0.7927
Epoch 30/40
8048/8048 [==============================] - 2s - loss: 0.4814 - acc: 0.7775 - val_loss: 0.4661 - val_acc: 0.7932
Epoch 31/40
8048/8048 [==============================] - 2s - loss: 0.4811 - acc: 0.7781 - val_loss: 0.4621 - val_acc: 0.7967
Epoch 32/40
8048/8048 [==============================] - 2s - loss: 0.4770 - acc: 0.7794 - val_loss: 0.4587 - val_acc: 0.7927
Epoch 33/40
8048/8048 [==============================] - 2s - loss: 0.4751 - acc: 0.7817 - val_loss: 0.4591 - val_acc: 0.7967
Epoch 34/40
8048/8048 [==============================] - 2s - loss: 0.4754 - acc: 0.7833 - val_loss: 0.4586 - val_acc: 0.7977
Epoch 35/40
8048/8048 [==============================] - 2s - loss: 0.4751 - acc: 0.7829 - val_loss: 0.4553 - val_acc: 0.7987
Epoch 36/40
8048/8048 [==============================] - 2s - loss: 0.4726 - acc: 0.7832 - val_loss: 0.4524 - val_acc: 0.8002
Epoch 37/40
8048/8048 [==============================] - 2s - loss: 0.4697 - acc: 0.7827 - val_loss: 0.4528 - val_acc: 0.7987
Epoch 38/40
8048/8048 [==============================] - 2s - loss: 0.4694 - acc: 0.7817 - val_loss: 0.4532 - val_acc: 0.7992
Epoch 39/40
8048/8048 [==============================] - 2s - loss: 0.4694 - acc: 0.7817 - val_loss: 0.4519 - val_acc: 0.8022
Epoch 40/40
8048/8048 [==============================] - 2s - loss: 0.4674 - acc: 0.7843 - val_loss: 0.4504 - val_acc: 0.8042
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-5.h5
chunk number 5
prepare data
start training
Train on 8117 samples, validate on 2030 samples
Epoch 1/40
8117/8117 [==============================] - 1s - loss: 0.4652 - acc: 0.7904 - val_loss: 0.4650 - val_acc: 0.7916
Epoch 2/40
8117/8117 [==============================] - 1s - loss: 0.4636 - acc: 0.7925 - val_loss: 0.4647 - val_acc: 0.7911
Epoch 3/40
8117/8117 [==============================] - 1s - loss: 0.4630 - acc: 0.7932 - val_loss: 0.4643 - val_acc: 0.7892
Epoch 4/40
8117/8117 [==============================] - 1s - loss: 0.4624 - acc: 0.7930 - val_loss: 0.4626 - val_acc: 0.7892
Epoch 5/40
8117/8117 [==============================] - 1s - loss: 0.4604 - acc: 0.7922 - val_loss: 0.4603 - val_acc: 0.7916
Epoch 6/40
8117/8117 [==============================] - 1s - loss: 0.4581 - acc: 0.7941 - val_loss: 0.4594 - val_acc: 0.7921
Epoch 7/40
8117/8117 [==============================] - 1s - loss: 0.4572 - acc: 0.7944 - val_loss: 0.4589 - val_acc: 0.7916
Epoch 8/40
8117/8117 [==============================] - 1s - loss: 0.4568 - acc: 0.7943 - val_loss: 0.4573 - val_acc: 0.7901
Epoch 9/40
8117/8117 [==============================] - 1s - loss: 0.4552 - acc: 0.7956 - val_loss: 0.4557 - val_acc: 0.7911
Epoch 10/40
8117/8117 [==============================] - 1s - loss: 0.4535 - acc: 0.7917 - val_loss: 0.4554 - val_acc: 0.7931
Epoch 11/40
8117/8117 [==============================] - 1s - loss: 0.4532 - acc: 0.7918 - val_loss: 0.4548 - val_acc: 0.7926
Epoch 12/40
8117/8117 [==============================] - 1s - loss: 0.4526 - acc: 0.7920 - val_loss: 0.4535 - val_acc: 0.7941
Epoch 13/40
8117/8117 [==============================] - 1s - loss: 0.4510 - acc: 0.7927 - val_loss: 0.4528 - val_acc: 0.7921
Epoch 14/40
8117/8117 [==============================] - 1s - loss: 0.4501 - acc: 0.7952 - val_loss: 0.4527 - val_acc: 0.7931
Epoch 15/40
8117/8117 [==============================] - 1s - loss: 0.4497 - acc: 0.7970 - val_loss: 0.4517 - val_acc: 0.7936
Epoch 16/40
8117/8117 [==============================] - 1s - loss: 0.4484 - acc: 0.7965 - val_loss: 0.4504 - val_acc: 0.7941
Epoch 17/40
8117/8117 [==============================] - 1s - loss: 0.4469 - acc: 0.7968 - val_loss: 0.4499 - val_acc: 0.7961
Epoch 18/40
8117/8117 [==============================] - 1s - loss: 0.4462 - acc: 0.7968 - val_loss: 0.4493 - val_acc: 0.7966
Epoch 19/40
8117/8117 [==============================] - 1s - loss: 0.4453 - acc: 0.7972 - val_loss: 0.4484 - val_acc: 0.7956
Epoch 20/40
8117/8117 [==============================] - 1s - loss: 0.4440 - acc: 0.7978 - val_loss: 0.4481 - val_acc: 0.7936
Epoch 21/40
8117/8117 [==============================] - 1s - loss: 0.4434 - acc: 0.7984 - val_loss: 0.4478 - val_acc: 0.7951
Epoch 22/40
8117/8117 [==============================] - 1s - loss: 0.4429 - acc: 0.7993 - val_loss: 0.4469 - val_acc: 0.7956
Epoch 23/40
8117/8117 [==============================] - 1s - loss: 0.4418 - acc: 0.7996 - val_loss: 0.4462 - val_acc: 0.7985
Epoch 24/40
8117/8117 [==============================] - 1s - loss: 0.4410 - acc: 0.7996 - val_loss: 0.4453 - val_acc: 0.7995
Epoch 25/40
8117/8117 [==============================] - 1s - loss: 0.4403 - acc: 0.8014 - val_loss: 0.4440 - val_acc: 0.7990
Epoch 26/40
8117/8117 [==============================] - 1s - loss: 0.4391 - acc: 0.8015 - val_loss: 0.4430 - val_acc: 0.7970
Epoch 27/40
8117/8117 [==============================] - 1s - loss: 0.4383 - acc: 0.8017 - val_loss: 0.4422 - val_acc: 0.7970
Epoch 28/40
8117/8117 [==============================] - 1s - loss: 0.4376 - acc: 0.8019 - val_loss: 0.4412 - val_acc: 0.7975
Epoch 29/40
8117/8117 [==============================] - 1s - loss: 0.4368 - acc: 0.8014 - val_loss: 0.4405 - val_acc: 0.7985
Epoch 30/40
8117/8117 [==============================] - 1s - loss: 0.4363 - acc: 0.8015 - val_loss: 0.4398 - val_acc: 0.7970
Epoch 31/40
8117/8117 [==============================] - 1s - loss: 0.4356 - acc: 0.8025 - val_loss: 0.4393 - val_acc: 0.7985
Epoch 32/40
8117/8117 [==============================] - 1s - loss: 0.4348 - acc: 0.8026 - val_loss: 0.4389 - val_acc: 0.7995
Epoch 33/40
8117/8117 [==============================] - 1s - loss: 0.4343 - acc: 0.8029 - val_loss: 0.4383 - val_acc: 0.8005
Epoch 34/40
8117/8117 [==============================] - 1s - loss: 0.4334 - acc: 0.8030 - val_loss: 0.4380 - val_acc: 0.8030
Epoch 35/40
8117/8117 [==============================] - 1s - loss: 0.4328 - acc: 0.8042 - val_loss: 0.4378 - val_acc: 0.8034
Epoch 36/40
8117/8117 [==============================] - 1s - loss: 0.4321 - acc: 0.8037 - val_loss: 0.4376 - val_acc: 0.8064
Epoch 37/40
8117/8117 [==============================] - 1s - loss: 0.4314 - acc: 0.8046 - val_loss: 0.4375 - val_acc: 0.8084
Epoch 38/40
8117/8117 [==============================] - 1s - loss: 0.4308 - acc: 0.8056 - val_loss: 0.4371 - val_acc: 0.8103
Epoch 39/40
8117/8117 [==============================] - 1s - loss: 0.4301 - acc: 0.8071 - val_loss: 0.4367 - val_acc: 0.8108
Epoch 40/40
8117/8117 [==============================] - 1s - loss: 0.4296 - acc: 0.8082 - val_loss: 0.4362 - val_acc: 0.8118
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-6.h5
chunk number 6
prepare data
start training
Train on 8182 samples, validate on 2046 samples
Epoch 1/40
8182/8182 [==============================] - 1s - loss: 0.4266 - acc: 0.8157 - val_loss: 0.4281 - val_acc: 0.8118
Epoch 2/40
8182/8182 [==============================] - 1s - loss: 0.4252 - acc: 0.8168 - val_loss: 0.4274 - val_acc: 0.8148
Epoch 3/40
8182/8182 [==============================] - 1s - loss: 0.4245 - acc: 0.8165 - val_loss: 0.4263 - val_acc: 0.8118
Epoch 4/40
8182/8182 [==============================] - 1s - loss: 0.4227 - acc: 0.8169 - val_loss: 0.4270 - val_acc: 0.8089
Epoch 5/40
8182/8182 [==============================] - 1s - loss: 0.4223 - acc: 0.8163 - val_loss: 0.4267 - val_acc: 0.8084
Epoch 6/40
8182/8182 [==============================] - 1s - loss: 0.4217 - acc: 0.8168 - val_loss: 0.4258 - val_acc: 0.8128
Epoch 7/40
8182/8182 [==============================] - 1s - loss: 0.4211 - acc: 0.8174 - val_loss: 0.4255 - val_acc: 0.8143
Epoch 8/40
8182/8182 [==============================] - 1s - loss: 0.4209 - acc: 0.8184 - val_loss: 0.4249 - val_acc: 0.8123
Epoch 9/40
8182/8182 [==============================] - 1s - loss: 0.4195 - acc: 0.8179 - val_loss: 0.4246 - val_acc: 0.8123
Epoch 10/40
8182/8182 [==============================] - 1s - loss: 0.4189 - acc: 0.8159 - val_loss: 0.4226 - val_acc: 0.8162
Epoch 11/40
8182/8182 [==============================] - 1s - loss: 0.4174 - acc: 0.8196 - val_loss: 0.4218 - val_acc: 0.8143
Epoch 12/40
8182/8182 [==============================] - 1s - loss: 0.4171 - acc: 0.8218 - val_loss: 0.4212 - val_acc: 0.8172
Epoch 13/40
8182/8182 [==============================] - 1s - loss: 0.4162 - acc: 0.8206 - val_loss: 0.4215 - val_acc: 0.8133
Epoch 14/40
8182/8182 [==============================] - 1s - loss: 0.4161 - acc: 0.8187 - val_loss: 0.4200 - val_acc: 0.8187
Epoch 15/40
8182/8182 [==============================] - 1s - loss: 0.4150 - acc: 0.8220 - val_loss: 0.4193 - val_acc: 0.8211
Epoch 16/40
8182/8182 [==============================] - 1s - loss: 0.4144 - acc: 0.8224 - val_loss: 0.4190 - val_acc: 0.8152
Epoch 17/40
8182/8182 [==============================] - 1s - loss: 0.4134 - acc: 0.8224 - val_loss: 0.4188 - val_acc: 0.8148
Epoch 18/40
8182/8182 [==============================] - 1s - loss: 0.4128 - acc: 0.8219 - val_loss: 0.4180 - val_acc: 0.8226
Epoch 19/40
8182/8182 [==============================] - 1s - loss: 0.4124 - acc: 0.8223 - val_loss: 0.4178 - val_acc: 0.8226
Epoch 20/40
8182/8182 [==============================] - 1s - loss: 0.4117 - acc: 0.8223 - val_loss: 0.4181 - val_acc: 0.8177
Epoch 21/40
8182/8182 [==============================] - 1s - loss: 0.4114 - acc: 0.8224 - val_loss: 0.4167 - val_acc: 0.8216
Epoch 22/40
8182/8182 [==============================] - 1s - loss: 0.4106 - acc: 0.8241 - val_loss: 0.4162 - val_acc: 0.8226
Epoch 23/40
8182/8182 [==============================] - 1s - loss: 0.4099 - acc: 0.8241 - val_loss: 0.4165 - val_acc: 0.8221
Epoch 24/40
8182/8182 [==============================] - 1s - loss: 0.4096 - acc: 0.8245 - val_loss: 0.4153 - val_acc: 0.8270
Epoch 25/40
8182/8182 [==============================] - 1s - loss: 0.4092 - acc: 0.8256 - val_loss: 0.4152 - val_acc: 0.8240
Epoch 26/40
8182/8182 [==============================] - 1s - loss: 0.4085 - acc: 0.8266 - val_loss: 0.4153 - val_acc: 0.8226
Epoch 27/40
8182/8182 [==============================] - 1s - loss: 0.4081 - acc: 0.8261 - val_loss: 0.4142 - val_acc: 0.8289
Epoch 28/40
8182/8182 [==============================] - 1s - loss: 0.4077 - acc: 0.8269 - val_loss: 0.4147 - val_acc: 0.8216
Epoch 29/40
8182/8182 [==============================] - 1s - loss: 0.4071 - acc: 0.8257 - val_loss: 0.4138 - val_acc: 0.8270
Epoch 30/40
8182/8182 [==============================] - 1s - loss: 0.4065 - acc: 0.8279 - val_loss: 0.4135 - val_acc: 0.8270
Epoch 31/40
8182/8182 [==============================] - 1s - loss: 0.4061 - acc: 0.8278 - val_loss: 0.4135 - val_acc: 0.8260
Epoch 32/40
8182/8182 [==============================] - 1s - loss: 0.4056 - acc: 0.8274 - val_loss: 0.4124 - val_acc: 0.8284
Epoch 33/40
8182/8182 [==============================] - 1s - loss: 0.4053 - acc: 0.8283 - val_loss: 0.4135 - val_acc: 0.8255
Epoch 34/40
8182/8182 [==============================] - 1s - loss: 0.4051 - acc: 0.8271 - val_loss: 0.4119 - val_acc: 0.8280
Epoch 35/40
8182/8182 [==============================] - 1s - loss: 0.4053 - acc: 0.8294 - val_loss: 0.4148 - val_acc: 0.8265
Epoch 36/40
8182/8182 [==============================] - 1s - loss: 0.4057 - acc: 0.8274 - val_loss: 0.4142 - val_acc: 0.8294
Epoch 37/40
8182/8182 [==============================] - 1s - loss: 0.4086 - acc: 0.8267 - val_loss: 0.4202 - val_acc: 0.8216
Epoch 38/40
8182/8182 [==============================] - 1s - loss: 0.4098 - acc: 0.8253 - val_loss: 0.4254 - val_acc: 0.8192
Epoch 39/40
8182/8182 [==============================] - 1s - loss: 0.4215 - acc: 0.8209 - val_loss: 0.4145 - val_acc: 0.8265
Epoch 40/40
8182/8182 [==============================] - 1s - loss: 0.4054 - acc: 0.8272 - val_loss: 0.4091 - val_acc: 0.8255
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-7.h5
chunk number 7
prepare data
start training
Train on 8116 samples, validate on 2030 samples
Epoch 1/40
8116/8116 [==============================] - 1s - loss: 0.4171 - acc: 0.8190 - val_loss: 0.4313 - val_acc: 0.8103
Epoch 2/40
8116/8116 [==============================] - 1s - loss: 0.4156 - acc: 0.8229 - val_loss: 0.4368 - val_acc: 0.8074
Epoch 3/40
8116/8116 [==============================] - 1s - loss: 0.4181 - acc: 0.8196 - val_loss: 0.4483 - val_acc: 0.8044
Epoch 4/40
8116/8116 [==============================] - 1s - loss: 0.4330 - acc: 0.8175 - val_loss: 0.5174 - val_acc: 0.7823
Epoch 5/40
8116/8116 [==============================] - 1s - loss: 0.4961 - acc: 0.7865 - val_loss: 0.4683 - val_acc: 0.7956
Epoch 6/40
8116/8116 [==============================] - 1s - loss: 0.4481 - acc: 0.8009 - val_loss: 0.5597 - val_acc: 0.7281
Epoch 7/40
8116/8116 [==============================] - 1s - loss: 0.5447 - acc: 0.7394 - val_loss: 0.5024 - val_acc: 0.7719
Epoch 8/40
8116/8116 [==============================] - 1s - loss: 0.4822 - acc: 0.7801 - val_loss: 0.5598 - val_acc: 0.7389
Epoch 9/40
8116/8116 [==============================] - 1s - loss: 0.5418 - acc: 0.7470 - val_loss: 0.5229 - val_acc: 0.7453
Epoch 10/40
8116/8116 [==============================] - 1s - loss: 0.5055 - acc: 0.7541 - val_loss: 0.4728 - val_acc: 0.7798
Epoch 11/40
8116/8116 [==============================] - 1s - loss: 0.4575 - acc: 0.7862 - val_loss: 0.5023 - val_acc: 0.7936
Epoch 12/40
8116/8116 [==============================] - 1s - loss: 0.4877 - acc: 0.8128 - val_loss: 0.5148 - val_acc: 0.7887
Epoch 13/40
8116/8116 [==============================] - 1s - loss: 0.4983 - acc: 0.8096 - val_loss: 0.4795 - val_acc: 0.7931
Epoch 14/40
8116/8116 [==============================] - 1s - loss: 0.4613 - acc: 0.8135 - val_loss: 0.4602 - val_acc: 0.7833
Epoch 15/40
8116/8116 [==============================] - 1s - loss: 0.4379 - acc: 0.8014 - val_loss: 0.4830 - val_acc: 0.7754
Epoch 16/40
8116/8116 [==============================] - 1s - loss: 0.4586 - acc: 0.7894 - val_loss: 0.4935 - val_acc: 0.7709
Epoch 17/40
8116/8116 [==============================] - 1s - loss: 0.4673 - acc: 0.7896 - val_loss: 0.4855 - val_acc: 0.7803
Epoch 18/40
8116/8116 [==============================] - 1s - loss: 0.4572 - acc: 0.7983 - val_loss: 0.4780 - val_acc: 0.7862
Epoch 19/40
8116/8116 [==============================] - 1s - loss: 0.4462 - acc: 0.8080 - val_loss: 0.4896 - val_acc: 0.7833
Epoch 20/40
8116/8116 [==============================] - 1s - loss: 0.4572 - acc: 0.8034 - val_loss: 0.4863 - val_acc: 0.7847
Epoch 21/40
8116/8116 [==============================] - 1s - loss: 0.4561 - acc: 0.8017 - val_loss: 0.4695 - val_acc: 0.7906
Epoch 22/40
8116/8116 [==============================] - 1s - loss: 0.4417 - acc: 0.8086 - val_loss: 0.4670 - val_acc: 0.7877
Epoch 23/40
8116/8116 [==============================] - 1s - loss: 0.4416 - acc: 0.8080 - val_loss: 0.4705 - val_acc: 0.7823
Epoch 24/40
8116/8116 [==============================] - 1s - loss: 0.4466 - acc: 0.8031 - val_loss: 0.4686 - val_acc: 0.7852
Epoch 25/40
8116/8116 [==============================] - 1s - loss: 0.4456 - acc: 0.8050 - val_loss: 0.4629 - val_acc: 0.7916
Epoch 26/40
8116/8116 [==============================] - 1s - loss: 0.4405 - acc: 0.8094 - val_loss: 0.4616 - val_acc: 0.7936
Epoch 27/40
8116/8116 [==============================] - 1s - loss: 0.4395 - acc: 0.8141 - val_loss: 0.4648 - val_acc: 0.7897
Epoch 28/40
8116/8116 [==============================] - 1s - loss: 0.4429 - acc: 0.8112 - val_loss: 0.4627 - val_acc: 0.7916
Epoch 29/40
8116/8116 [==============================] - 1s - loss: 0.4405 - acc: 0.8122 - val_loss: 0.4574 - val_acc: 0.7911
Epoch 30/40
8116/8116 [==============================] - 1s - loss: 0.4346 - acc: 0.8133 - val_loss: 0.4577 - val_acc: 0.7931
Epoch 31/40
8116/8116 [==============================] - 1s - loss: 0.4345 - acc: 0.8122 - val_loss: 0.4595 - val_acc: 0.7946
Epoch 32/40
8116/8116 [==============================] - 1s - loss: 0.4357 - acc: 0.8114 - val_loss: 0.4574 - val_acc: 0.7906
Epoch 33/40
8116/8116 [==============================] - 1s - loss: 0.4327 - acc: 0.8120 - val_loss: 0.4549 - val_acc: 0.7921
Epoch 34/40
8116/8116 [==============================] - 1s - loss: 0.4293 - acc: 0.8144 - val_loss: 0.4570 - val_acc: 0.7906
Epoch 35/40
8116/8116 [==============================] - 1s - loss: 0.4307 - acc: 0.8147 - val_loss: 0.4565 - val_acc: 0.7921
Epoch 36/40
8116/8116 [==============================] - 1s - loss: 0.4298 - acc: 0.8154 - val_loss: 0.4527 - val_acc: 0.7936
Epoch 37/40
8116/8116 [==============================] - 1s - loss: 0.4258 - acc: 0.8167 - val_loss: 0.4530 - val_acc: 0.7941
Epoch 38/40
8116/8116 [==============================] - 1s - loss: 0.4261 - acc: 0.8170 - val_loss: 0.4524 - val_acc: 0.7951
Epoch 39/40
8116/8116 [==============================] - 1s - loss: 0.4256 - acc: 0.8169 - val_loss: 0.4491 - val_acc: 0.7980
Epoch 40/40
8116/8116 [==============================] - 1s - loss: 0.4225 - acc: 0.8190 - val_loss: 0.4487 - val_acc: 0.7931
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-8.h5
chunk number 8
prepare data
start training
Train on 8086 samples, validate on 2022 samples
Epoch 1/40
8086/8086 [==============================] - 2s - loss: 0.4738 - acc: 0.7743 - val_loss: 0.4404 - val_acc: 0.8046
Epoch 2/40
8086/8086 [==============================] - 2s - loss: 0.4534 - acc: 0.7972 - val_loss: 0.4482 - val_acc: 0.8106
Epoch 3/40
8086/8086 [==============================] - 2s - loss: 0.4601 - acc: 0.8051 - val_loss: 0.4391 - val_acc: 0.8116
Epoch 4/40
8086/8086 [==============================] - 2s - loss: 0.4511 - acc: 0.8063 - val_loss: 0.4353 - val_acc: 0.8091
Epoch 5/40
8086/8086 [==============================] - 2s - loss: 0.4439 - acc: 0.8066 - val_loss: 0.4468 - val_acc: 0.8121
Epoch 6/40
8086/8086 [==============================] - 2s - loss: 0.4535 - acc: 0.8083 - val_loss: 0.4330 - val_acc: 0.8195
Epoch 7/40
8086/8086 [==============================] - 2s - loss: 0.4426 - acc: 0.8088 - val_loss: 0.4298 - val_acc: 0.8056
Epoch 8/40
8086/8086 [==============================] - 2s - loss: 0.4432 - acc: 0.7982 - val_loss: 0.4271 - val_acc: 0.8106
Epoch 9/40
8086/8086 [==============================] - 2s - loss: 0.4406 - acc: 0.8030 - val_loss: 0.4209 - val_acc: 0.8215
Epoch 10/40
8086/8086 [==============================] - 2s - loss: 0.4339 - acc: 0.8109 - val_loss: 0.4266 - val_acc: 0.8195
Epoch 11/40
8086/8086 [==============================] - 2s - loss: 0.4404 - acc: 0.8109 - val_loss: 0.4197 - val_acc: 0.8185
Epoch 12/40
8086/8086 [==============================] - 2s - loss: 0.4342 - acc: 0.8105 - val_loss: 0.4243 - val_acc: 0.8155
Epoch 13/40
8086/8086 [==============================] - 2s - loss: 0.4358 - acc: 0.8083 - val_loss: 0.4203 - val_acc: 0.8175
Epoch 14/40
8086/8086 [==============================] - 2s - loss: 0.4331 - acc: 0.8103 - val_loss: 0.4165 - val_acc: 0.8234
Epoch 15/40
8086/8086 [==============================] - 2s - loss: 0.4299 - acc: 0.8107 - val_loss: 0.4200 - val_acc: 0.8225
Epoch 16/40
8086/8086 [==============================] - 2s - loss: 0.4323 - acc: 0.8119 - val_loss: 0.4166 - val_acc: 0.8249
Epoch 17/40
8086/8086 [==============================] - 2s - loss: 0.4289 - acc: 0.8116 - val_loss: 0.4210 - val_acc: 0.8185
Epoch 18/40
8086/8086 [==============================] - 2s - loss: 0.4318 - acc: 0.8094 - val_loss: 0.4199 - val_acc: 0.8210
Epoch 19/40
8086/8086 [==============================] - 2s - loss: 0.4304 - acc: 0.8112 - val_loss: 0.4192 - val_acc: 0.8229
Epoch 20/40
8086/8086 [==============================] - 2s - loss: 0.4295 - acc: 0.8140 - val_loss: 0.4200 - val_acc: 0.8220
Epoch 21/40
8086/8086 [==============================] - 2s - loss: 0.4300 - acc: 0.8139 - val_loss: 0.4173 - val_acc: 0.8244
Epoch 22/40
8086/8086 [==============================] - 2s - loss: 0.4273 - acc: 0.8121 - val_loss: 0.4194 - val_acc: 0.8234
Epoch 23/40
8086/8086 [==============================] - 2s - loss: 0.4288 - acc: 0.8118 - val_loss: 0.4161 - val_acc: 0.8279
Epoch 24/40
8086/8086 [==============================] - 2s - loss: 0.4261 - acc: 0.8141 - val_loss: 0.4180 - val_acc: 0.8205
Epoch 25/40
8086/8086 [==============================] - 2s - loss: 0.4273 - acc: 0.8146 - val_loss: 0.4153 - val_acc: 0.8229
Epoch 26/40
8086/8086 [==============================] - 2s - loss: 0.4250 - acc: 0.8157 - val_loss: 0.4168 - val_acc: 0.8274
Epoch 27/40
8086/8086 [==============================] - 2s - loss: 0.4258 - acc: 0.8141 - val_loss: 0.4147 - val_acc: 0.8279
Epoch 28/40
8086/8086 [==============================] - 2s - loss: 0.4237 - acc: 0.8151 - val_loss: 0.4161 - val_acc: 0.8229
Epoch 29/40
8086/8086 [==============================] - 2s - loss: 0.4245 - acc: 0.8145 - val_loss: 0.4141 - val_acc: 0.8284
Epoch 30/40
8086/8086 [==============================] - 2s - loss: 0.4226 - acc: 0.8160 - val_loss: 0.4152 - val_acc: 0.8279
Epoch 31/40
8086/8086 [==============================] - 2s - loss: 0.4237 - acc: 0.8171 - val_loss: 0.4141 - val_acc: 0.8289
Epoch 32/40
8086/8086 [==============================] - 2s - loss: 0.4221 - acc: 0.8163 - val_loss: 0.4156 - val_acc: 0.8254
Epoch 33/40
8086/8086 [==============================] - 2s - loss: 0.4231 - acc: 0.8151 - val_loss: 0.4138 - val_acc: 0.8314
Epoch 34/40
8086/8086 [==============================] - 2s - loss: 0.4217 - acc: 0.8178 - val_loss: 0.4139 - val_acc: 0.8289
Epoch 35/40
8086/8086 [==============================] - 2s - loss: 0.4217 - acc: 0.8180 - val_loss: 0.4145 - val_acc: 0.8284
Epoch 36/40
8086/8086 [==============================] - 2s - loss: 0.4213 - acc: 0.8160 - val_loss: 0.4136 - val_acc: 0.8254
Epoch 37/40
8086/8086 [==============================] - 2s - loss: 0.4204 - acc: 0.8162 - val_loss: 0.4135 - val_acc: 0.8294
Epoch 38/40
8086/8086 [==============================] - 2s - loss: 0.4209 - acc: 0.8175 - val_loss: 0.4131 - val_acc: 0.8259
Epoch 39/40
8086/8086 [==============================] - 2s - loss: 0.4197 - acc: 0.8170 - val_loss: 0.4141 - val_acc: 0.8274
Epoch 40/40
8086/8086 [==============================] - 2s - loss: 0.4200 - acc: 0.8171 - val_loss: 0.4130 - val_acc: 0.8299
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-9.h5
chunk number 9
prepare data
start training
Train on 8220 samples, validate on 2055 samples
Epoch 1/40
8220/8220 [==============================] - 2s - loss: 0.4162 - acc: 0.8162 - val_loss: 0.4312 - val_acc: 0.8180
Epoch 2/40
8220/8220 [==============================] - 2s - loss: 0.4268 - acc: 0.8266 - val_loss: 0.4177 - val_acc: 0.8170
Epoch 3/40
8220/8220 [==============================] - 2s - loss: 0.4118 - acc: 0.8276 - val_loss: 0.4473 - val_acc: 0.7961
Epoch 4/40
8220/8220 [==============================] - 2s - loss: 0.4367 - acc: 0.8004 - val_loss: 0.4243 - val_acc: 0.8161
Epoch 5/40
8220/8220 [==============================] - 2s - loss: 0.4191 - acc: 0.8214 - val_loss: 0.4263 - val_acc: 0.8102
Epoch 6/40
8220/8220 [==============================] - 2s - loss: 0.4210 - acc: 0.8159 - val_loss: 0.4315 - val_acc: 0.8156
Epoch 7/40
8220/8220 [==============================] - 2s - loss: 0.4206 - acc: 0.8268 - val_loss: 0.4195 - val_acc: 0.8219
Epoch 8/40
8220/8220 [==============================] - 2s - loss: 0.4111 - acc: 0.8279 - val_loss: 0.4227 - val_acc: 0.8102
Epoch 9/40
8220/8220 [==============================] - 2s - loss: 0.4158 - acc: 0.8185 - val_loss: 0.4174 - val_acc: 0.8190
Epoch 10/40
8220/8220 [==============================] - 2s - loss: 0.4098 - acc: 0.8255 - val_loss: 0.4231 - val_acc: 0.8214
Epoch 11/40
8220/8220 [==============================] - 2s - loss: 0.4117 - acc: 0.8285 - val_loss: 0.4159 - val_acc: 0.8224
Epoch 12/40
8220/8220 [==============================] - 2s - loss: 0.4060 - acc: 0.8299 - val_loss: 0.4193 - val_acc: 0.8243
Epoch 13/40
8220/8220 [==============================] - 2s - loss: 0.4094 - acc: 0.8302 - val_loss: 0.4155 - val_acc: 0.8209
Epoch 14/40
8220/8220 [==============================] - 2s - loss: 0.4057 - acc: 0.8305 - val_loss: 0.4200 - val_acc: 0.8185
Epoch 15/40
8220/8220 [==============================] - 2s - loss: 0.4086 - acc: 0.8282 - val_loss: 0.4143 - val_acc: 0.8258
Epoch 16/40
8220/8220 [==============================] - 2s - loss: 0.4052 - acc: 0.8316 - val_loss: 0.4138 - val_acc: 0.8268
Epoch 17/40
8220/8220 [==============================] - 2s - loss: 0.4053 - acc: 0.8314 - val_loss: 0.4154 - val_acc: 0.8175
Epoch 18/40
8220/8220 [==============================] - 2s - loss: 0.4051 - acc: 0.8307 - val_loss: 0.4127 - val_acc: 0.8224
Epoch 19/40
8220/8220 [==============================] - 2s - loss: 0.4036 - acc: 0.8328 - val_loss: 0.4121 - val_acc: 0.8238
Epoch 20/40
8220/8220 [==============================] - 2s - loss: 0.4044 - acc: 0.8315 - val_loss: 0.4117 - val_acc: 0.8253
Epoch 21/40
8220/8220 [==============================] - 2s - loss: 0.4028 - acc: 0.8328 - val_loss: 0.4152 - val_acc: 0.8229
Epoch 22/40
8220/8220 [==============================] - 2s - loss: 0.4043 - acc: 0.8325 - val_loss: 0.4121 - val_acc: 0.8238
Epoch 23/40
8220/8220 [==============================] - 2s - loss: 0.4020 - acc: 0.8332 - val_loss: 0.4122 - val_acc: 0.8234
Epoch 24/40
8220/8220 [==============================] - 2s - loss: 0.4015 - acc: 0.8336 - val_loss: 0.4132 - val_acc: 0.8238
Epoch 25/40
8220/8220 [==============================] - 2s - loss: 0.4014 - acc: 0.8330 - val_loss: 0.4120 - val_acc: 0.8214
Epoch 26/40
8220/8220 [==============================] - 2s - loss: 0.4002 - acc: 0.8345 - val_loss: 0.4127 - val_acc: 0.8238
Epoch 27/40
8220/8220 [==============================] - 2s - loss: 0.4006 - acc: 0.8350 - val_loss: 0.4141 - val_acc: 0.8238
Epoch 28/40
8220/8220 [==============================] - 2s - loss: 0.4014 - acc: 0.8326 - val_loss: 0.4140 - val_acc: 0.8253
Epoch 29/40
8220/8220 [==============================] - 2s - loss: 0.4009 - acc: 0.8352 - val_loss: 0.4120 - val_acc: 0.8243
Epoch 30/40
8220/8220 [==============================] - 2s - loss: 0.3991 - acc: 0.8347 - val_loss: 0.4131 - val_acc: 0.8234
Epoch 31/40
8220/8220 [==============================] - 2s - loss: 0.4001 - acc: 0.8342 - val_loss: 0.4154 - val_acc: 0.8258
Epoch 32/40
8220/8220 [==============================] - 2s - loss: 0.4015 - acc: 0.8339 - val_loss: 0.4113 - val_acc: 0.8229
Epoch 33/40
8220/8220 [==============================] - 2s - loss: 0.3984 - acc: 0.8356 - val_loss: 0.4114 - val_acc: 0.8238
Epoch 34/40
8220/8220 [==============================] - 2s - loss: 0.3984 - acc: 0.8353 - val_loss: 0.4141 - val_acc: 0.8253
Epoch 35/40
8220/8220 [==============================] - 2s - loss: 0.4009 - acc: 0.8332 - val_loss: 0.4112 - val_acc: 0.8258
Epoch 36/40
8220/8220 [==============================] - 2s - loss: 0.3979 - acc: 0.8366 - val_loss: 0.4102 - val_acc: 0.8253
Epoch 37/40
8220/8220 [==============================] - 2s - loss: 0.3969 - acc: 0.8387 - val_loss: 0.4102 - val_acc: 0.8258
Epoch 38/40
8220/8220 [==============================] - 2s - loss: 0.3965 - acc: 0.8386 - val_loss: 0.4109 - val_acc: 0.8248
Epoch 39/40
8220/8220 [==============================] - 2s - loss: 0.3969 - acc: 0.8372 - val_loss: 0.4156 - val_acc: 0.8263
Epoch 40/40
8220/8220 [==============================] - 2s - loss: 0.4009 - acc: 0.8341 - val_loss: 0.4127 - val_acc: 0.8224
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-10.h5
chunk number 10
prepare data
start training
Train on 8132 samples, validate on 2033 samples
Epoch 1/40
8132/8132 [==============================] - 1s - loss: 0.4386 - acc: 0.8261 - val_loss: 0.4224 - val_acc: 0.8303
Epoch 2/40
8132/8132 [==============================] - 1s - loss: 0.4325 - acc: 0.8276 - val_loss: 0.4148 - val_acc: 0.8273
Epoch 3/40
8132/8132 [==============================] - 1s - loss: 0.4252 - acc: 0.8221 - val_loss: 0.4101 - val_acc: 0.8269
Epoch 4/40
8132/8132 [==============================] - 1s - loss: 0.4209 - acc: 0.8214 - val_loss: 0.4026 - val_acc: 0.8342
Epoch 5/40
8132/8132 [==============================] - 1s - loss: 0.4139 - acc: 0.8302 - val_loss: 0.4031 - val_acc: 0.8342
Epoch 6/40
8132/8132 [==============================] - 1s - loss: 0.4153 - acc: 0.8328 - val_loss: 0.4000 - val_acc: 0.8328
Epoch 7/40
8132/8132 [==============================] - 1s - loss: 0.4116 - acc: 0.8323 - val_loss: 0.4022 - val_acc: 0.8298
Epoch 8/40
8132/8132 [==============================] - 1s - loss: 0.4137 - acc: 0.8297 - val_loss: 0.3977 - val_acc: 0.8367
Epoch 9/40
8132/8132 [==============================] - 1s - loss: 0.4111 - acc: 0.8313 - val_loss: 0.3973 - val_acc: 0.8367
Epoch 10/40
8132/8132 [==============================] - 1s - loss: 0.4126 - acc: 0.8297 - val_loss: 0.3988 - val_acc: 0.8342
Epoch 11/40
8132/8132 [==============================] - 1s - loss: 0.4119 - acc: 0.8304 - val_loss: 0.3951 - val_acc: 0.8377
Epoch 12/40
8132/8132 [==============================] - 1s - loss: 0.4093 - acc: 0.8328 - val_loss: 0.3950 - val_acc: 0.8337
Epoch 13/40
8132/8132 [==============================] - 1s - loss: 0.4123 - acc: 0.8277 - val_loss: 0.3978 - val_acc: 0.8367
Epoch 14/40
8132/8132 [==============================] - 1s - loss: 0.4107 - acc: 0.8309 - val_loss: 0.3947 - val_acc: 0.8367
Epoch 15/40
8132/8132 [==============================] - 1s - loss: 0.4084 - acc: 0.8335 - val_loss: 0.3968 - val_acc: 0.8333
Epoch 16/40
8132/8132 [==============================] - 1s - loss: 0.4134 - acc: 0.8270 - val_loss: 0.4017 - val_acc: 0.8377
Epoch 17/40
8132/8132 [==============================] - 1s - loss: 0.4137 - acc: 0.8304 - val_loss: 0.3954 - val_acc: 0.8392
Epoch 18/40
8132/8132 [==============================] - 1s - loss: 0.4086 - acc: 0.8337 - val_loss: 0.3999 - val_acc: 0.8318
Epoch 19/40
8132/8132 [==============================] - 1s - loss: 0.4167 - acc: 0.8253 - val_loss: 0.4078 - val_acc: 0.8367
Epoch 20/40
8132/8132 [==============================] - 1s - loss: 0.4192 - acc: 0.8294 - val_loss: 0.3955 - val_acc: 0.8406
Epoch 21/40
8132/8132 [==============================] - 1s - loss: 0.4083 - acc: 0.8315 - val_loss: 0.4100 - val_acc: 0.8239
Epoch 22/40
8132/8132 [==============================] - 1s - loss: 0.4276 - acc: 0.8139 - val_loss: 0.4184 - val_acc: 0.8278
Epoch 23/40
8132/8132 [==============================] - 1s - loss: 0.4287 - acc: 0.8238 - val_loss: 0.4115 - val_acc: 0.8308
Epoch 24/40
8132/8132 [==============================] - 1s - loss: 0.4218 - acc: 0.8270 - val_loss: 0.4094 - val_acc: 0.8229
Epoch 25/40
8132/8132 [==============================] - 1s - loss: 0.4263 - acc: 0.8176 - val_loss: 0.3946 - val_acc: 0.8352
Epoch 26/40
8132/8132 [==============================] - 1s - loss: 0.4065 - acc: 0.8329 - val_loss: 0.4047 - val_acc: 0.8293
Epoch 27/40
8132/8132 [==============================] - 1s - loss: 0.4150 - acc: 0.8296 - val_loss: 0.3933 - val_acc: 0.8372
Epoch 28/40
8132/8132 [==============================] - 1s - loss: 0.4057 - acc: 0.8342 - val_loss: 0.4005 - val_acc: 0.8318
Epoch 29/40
8132/8132 [==============================] - 1s - loss: 0.4156 - acc: 0.8249 - val_loss: 0.3964 - val_acc: 0.8347
Epoch 30/40
8132/8132 [==============================] - 1s - loss: 0.4079 - acc: 0.8309 - val_loss: 0.4018 - val_acc: 0.8283
Epoch 31/40
8132/8132 [==============================] - 1s - loss: 0.4126 - acc: 0.8286 - val_loss: 0.3926 - val_acc: 0.8347
Epoch 32/40
8132/8132 [==============================] - 1s - loss: 0.4057 - acc: 0.8318 - val_loss: 0.3981 - val_acc: 0.8352
Epoch 33/40
8132/8132 [==============================] - 1s - loss: 0.4127 - acc: 0.8297 - val_loss: 0.3943 - val_acc: 0.8367
Epoch 34/40
8132/8132 [==============================] - 1s - loss: 0.4065 - acc: 0.8324 - val_loss: 0.3994 - val_acc: 0.8303
Epoch 35/40
8132/8132 [==============================] - 1s - loss: 0.4106 - acc: 0.8292 - val_loss: 0.3922 - val_acc: 0.8377
Epoch 36/40
8132/8132 [==============================] - 1s - loss: 0.4051 - acc: 0.8324 - val_loss: 0.3960 - val_acc: 0.8392
Epoch 37/40
8132/8132 [==============================] - 1s - loss: 0.4104 - acc: 0.8334 - val_loss: 0.3925 - val_acc: 0.8362
Epoch 38/40
8132/8132 [==============================] - 1s - loss: 0.4049 - acc: 0.8325 - val_loss: 0.3968 - val_acc: 0.8333
Epoch 39/40
8132/8132 [==============================] - 1s - loss: 0.4082 - acc: 0.8294 - val_loss: 0.3912 - val_acc: 0.8377
Epoch 40/40
8132/8132 [==============================] - 1s - loss: 0.4039 - acc: 0.8333 - val_loss: 0.3933 - val_acc: 0.8387
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-11.h5
chunk number 11
prepare data
start training
Train on 8186 samples, validate on 2047 samples
Epoch 1/40
8186/8186 [==============================] - 1s - loss: 0.4112 - acc: 0.8251 - val_loss: 0.4392 - val_acc: 0.8105
Epoch 2/40
8186/8186 [==============================] - 1s - loss: 0.4088 - acc: 0.8302 - val_loss: 0.4361 - val_acc: 0.8109
Epoch 3/40
8186/8186 [==============================] - 1s - loss: 0.4071 - acc: 0.8297 - val_loss: 0.4339 - val_acc: 0.8144
Epoch 4/40
8186/8186 [==============================] - 1s - loss: 0.4083 - acc: 0.8262 - val_loss: 0.4319 - val_acc: 0.8095
Epoch 5/40
8186/8186 [==============================] - 1s - loss: 0.4055 - acc: 0.8289 - val_loss: 0.4317 - val_acc: 0.8100
Epoch 6/40
8186/8186 [==============================] - 1s - loss: 0.4059 - acc: 0.8298 - val_loss: 0.4305 - val_acc: 0.8139
Epoch 7/40
8186/8186 [==============================] - 1s - loss: 0.4062 - acc: 0.8276 - val_loss: 0.4298 - val_acc: 0.8100
Epoch 8/40
8186/8186 [==============================] - 1s - loss: 0.4046 - acc: 0.8287 - val_loss: 0.4314 - val_acc: 0.8105
Epoch 9/40
8186/8186 [==============================] - 1s - loss: 0.4050 - acc: 0.8278 - val_loss: 0.4302 - val_acc: 0.8080
Epoch 10/40
8186/8186 [==============================] - 1s - loss: 0.4033 - acc: 0.8297 - val_loss: 0.4311 - val_acc: 0.8119
Epoch 11/40
8186/8186 [==============================] - 1s - loss: 0.4030 - acc: 0.8293 - val_loss: 0.4331 - val_acc: 0.8105
Epoch 12/40
8186/8186 [==============================] - 1s - loss: 0.4030 - acc: 0.8291 - val_loss: 0.4331 - val_acc: 0.8090
Epoch 13/40
8186/8186 [==============================] - 1s - loss: 0.4019 - acc: 0.8308 - val_loss: 0.4344 - val_acc: 0.8105
Epoch 14/40
8186/8186 [==============================] - 1s - loss: 0.4027 - acc: 0.8307 - val_loss: 0.4350 - val_acc: 0.8100
Epoch 15/40
8186/8186 [==============================] - 1s - loss: 0.4023 - acc: 0.8318 - val_loss: 0.4343 - val_acc: 0.8105
Epoch 16/40
8186/8186 [==============================] - 1s - loss: 0.4015 - acc: 0.8318 - val_loss: 0.4343 - val_acc: 0.8129
Epoch 17/40
8186/8186 [==============================] - 1s - loss: 0.4019 - acc: 0.8306 - val_loss: 0.4334 - val_acc: 0.8109
Epoch 18/40
8186/8186 [==============================] - 1s - loss: 0.4009 - acc: 0.8323 - val_loss: 0.4329 - val_acc: 0.8095
Epoch 19/40
8186/8186 [==============================] - 1s - loss: 0.4006 - acc: 0.8319 - val_loss: 0.4326 - val_acc: 0.8114
Epoch 20/40
8186/8186 [==============================] - 1s - loss: 0.4007 - acc: 0.8317 - val_loss: 0.4322 - val_acc: 0.8090
Epoch 21/40
8186/8186 [==============================] - 1s - loss: 0.4000 - acc: 0.8318 - val_loss: 0.4326 - val_acc: 0.8100
Epoch 22/40
8186/8186 [==============================] - 1s - loss: 0.4001 - acc: 0.8309 - val_loss: 0.4324 - val_acc: 0.8105
Epoch 23/40
8186/8186 [==============================] - 1s - loss: 0.3998 - acc: 0.8325 - val_loss: 0.4323 - val_acc: 0.8075
Epoch 24/40
8186/8186 [==============================] - 1s - loss: 0.3991 - acc: 0.8336 - val_loss: 0.4327 - val_acc: 0.8080
Epoch 25/40
8186/8186 [==============================] - 1s - loss: 0.3990 - acc: 0.8335 - val_loss: 0.4326 - val_acc: 0.8114
Epoch 26/40
8186/8186 [==============================] - 1s - loss: 0.3987 - acc: 0.8334 - val_loss: 0.4326 - val_acc: 0.8095
Epoch 27/40
8186/8186 [==============================] - 1s - loss: 0.3983 - acc: 0.8328 - val_loss: 0.4328 - val_acc: 0.8090
Epoch 28/40
8186/8186 [==============================] - 1s - loss: 0.3983 - acc: 0.8329 - val_loss: 0.4325 - val_acc: 0.8124
Epoch 29/40
8186/8186 [==============================] - 1s - loss: 0.3980 - acc: 0.8320 - val_loss: 0.4323 - val_acc: 0.8080
Epoch 30/40
8186/8186 [==============================] - 1s - loss: 0.3976 - acc: 0.8333 - val_loss: 0.4321 - val_acc: 0.8100
Epoch 31/40
8186/8186 [==============================] - 1s - loss: 0.3975 - acc: 0.8333 - val_loss: 0.4318 - val_acc: 0.8124
Epoch 32/40
8186/8186 [==============================] - 1s - loss: 0.3974 - acc: 0.8313 - val_loss: 0.4319 - val_acc: 0.8085
Epoch 33/40
8186/8186 [==============================] - 1s - loss: 0.3972 - acc: 0.8346 - val_loss: 0.4314 - val_acc: 0.8100
Epoch 34/40
8186/8186 [==============================] - 1s - loss: 0.3969 - acc: 0.8331 - val_loss: 0.4312 - val_acc: 0.8100
Epoch 35/40
8186/8186 [==============================] - 1s - loss: 0.3967 - acc: 0.8330 - val_loss: 0.4313 - val_acc: 0.8109
Epoch 36/40
8186/8186 [==============================] - 1s - loss: 0.3966 - acc: 0.8335 - val_loss: 0.4311 - val_acc: 0.8109
Epoch 37/40
8186/8186 [==============================] - 1s - loss: 0.3964 - acc: 0.8330 - val_loss: 0.4312 - val_acc: 0.8105
Epoch 38/40
8186/8186 [==============================] - 1s - loss: 0.3961 - acc: 0.8336 - val_loss: 0.4314 - val_acc: 0.8100
Epoch 39/40
8186/8186 [==============================] - 1s - loss: 0.3960 - acc: 0.8335 - val_loss: 0.4314 - val_acc: 0.8105
Epoch 40/40
8186/8186 [==============================] - 1s - loss: 0.3958 - acc: 0.8329 - val_loss: 0.4317 - val_acc: 0.8109
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-12.h5
chunk number 12
prepare data
start training
Train on 8205 samples, validate on 2052 samples
Epoch 1/40
8205/8205 [==============================] - 1s - loss: 0.4114 - acc: 0.8293 - val_loss: 0.4092 - val_acc: 0.8377
Epoch 2/40
8205/8205 [==============================] - 1s - loss: 0.4111 - acc: 0.8297 - val_loss: 0.4090 - val_acc: 0.8377
Epoch 3/40
8205/8205 [==============================] - 1s - loss: 0.4108 - acc: 0.8303 - val_loss: 0.4088 - val_acc: 0.8377
Epoch 4/40
8205/8205 [==============================] - 1s - loss: 0.4104 - acc: 0.8297 - val_loss: 0.4087 - val_acc: 0.8372
Epoch 5/40
8205/8205 [==============================] - 1s - loss: 0.4100 - acc: 0.8306 - val_loss: 0.4085 - val_acc: 0.8377
Epoch 6/40
8205/8205 [==============================] - 1s - loss: 0.4095 - acc: 0.8308 - val_loss: 0.4083 - val_acc: 0.8377
Epoch 7/40
8205/8205 [==============================] - 1s - loss: 0.4090 - acc: 0.8302 - val_loss: 0.4083 - val_acc: 0.8387
Epoch 8/40
8205/8205 [==============================] - 1s - loss: 0.4086 - acc: 0.8308 - val_loss: 0.4081 - val_acc: 0.8387
Epoch 9/40
8205/8205 [==============================] - 1s - loss: 0.4082 - acc: 0.8310 - val_loss: 0.4081 - val_acc: 0.8382
Epoch 10/40
8205/8205 [==============================] - 1s - loss: 0.4078 - acc: 0.8314 - val_loss: 0.4083 - val_acc: 0.8363
Epoch 11/40
8205/8205 [==============================] - 1s - loss: 0.4075 - acc: 0.8308 - val_loss: 0.4081 - val_acc: 0.8392
Epoch 12/40
8205/8205 [==============================] - 1s - loss: 0.4071 - acc: 0.8314 - val_loss: 0.4082 - val_acc: 0.8367
Epoch 13/40
8205/8205 [==============================] - 1s - loss: 0.4068 - acc: 0.8316 - val_loss: 0.4081 - val_acc: 0.8377
Epoch 14/40
8205/8205 [==============================] - 1s - loss: 0.4065 - acc: 0.8317 - val_loss: 0.4078 - val_acc: 0.8387
Epoch 15/40
8205/8205 [==============================] - 1s - loss: 0.4061 - acc: 0.8311 - val_loss: 0.4077 - val_acc: 0.8363
Epoch 16/40
8205/8205 [==============================] - 1s - loss: 0.4058 - acc: 0.8318 - val_loss: 0.4071 - val_acc: 0.8382
Epoch 17/40
8205/8205 [==============================] - 1s - loss: 0.4054 - acc: 0.8313 - val_loss: 0.4068 - val_acc: 0.8387
Epoch 18/40
8205/8205 [==============================] - 1s - loss: 0.4050 - acc: 0.8325 - val_loss: 0.4064 - val_acc: 0.8397
Epoch 19/40
8205/8205 [==============================] - 1s - loss: 0.4046 - acc: 0.8329 - val_loss: 0.4060 - val_acc: 0.8402
Epoch 20/40
8205/8205 [==============================] - 1s - loss: 0.4043 - acc: 0.8330 - val_loss: 0.4060 - val_acc: 0.8397
Epoch 21/40
8205/8205 [==============================] - 1s - loss: 0.4040 - acc: 0.8324 - val_loss: 0.4057 - val_acc: 0.8411
Epoch 22/40
8205/8205 [==============================] - 1s - loss: 0.4037 - acc: 0.8329 - val_loss: 0.4061 - val_acc: 0.8387
Epoch 23/40
8205/8205 [==============================] - 1s - loss: 0.4034 - acc: 0.8323 - val_loss: 0.4058 - val_acc: 0.8402
Epoch 24/40
8205/8205 [==============================] - 1s - loss: 0.4032 - acc: 0.8334 - val_loss: 0.4066 - val_acc: 0.8382
Epoch 25/40
8205/8205 [==============================] - 1s - loss: 0.4030 - acc: 0.8321 - val_loss: 0.4060 - val_acc: 0.8377
Epoch 26/40
8205/8205 [==============================] - 1s - loss: 0.4030 - acc: 0.8334 - val_loss: 0.4078 - val_acc: 0.8358
Epoch 27/40
8205/8205 [==============================] - 1s - loss: 0.4032 - acc: 0.8327 - val_loss: 0.4071 - val_acc: 0.8397
Epoch 28/40
8205/8205 [==============================] - 1s - loss: 0.4046 - acc: 0.8330 - val_loss: 0.4108 - val_acc: 0.8328
Epoch 29/40
8205/8205 [==============================] - 1s - loss: 0.4054 - acc: 0.8310 - val_loss: 0.4126 - val_acc: 0.8289
Epoch 30/40
8205/8205 [==============================] - 1s - loss: 0.4122 - acc: 0.8272 - val_loss: 0.4101 - val_acc: 0.8328
Epoch 31/40
8205/8205 [==============================] - 1s - loss: 0.4042 - acc: 0.8322 - val_loss: 0.4065 - val_acc: 0.8416
Epoch 32/40
8205/8205 [==============================] - 1s - loss: 0.4020 - acc: 0.8333 - val_loss: 0.4064 - val_acc: 0.8402
Epoch 33/40
8205/8205 [==============================] - 1s - loss: 0.4010 - acc: 0.8329 - val_loss: 0.4076 - val_acc: 0.8343
Epoch 34/40
8205/8205 [==============================] - 1s - loss: 0.4016 - acc: 0.8330 - val_loss: 0.4075 - val_acc: 0.8382
Epoch 35/40
8205/8205 [==============================] - 1s - loss: 0.4047 - acc: 0.8312 - val_loss: 0.4123 - val_acc: 0.8333
Epoch 36/40
8205/8205 [==============================] - 1s - loss: 0.4065 - acc: 0.8317 - val_loss: 0.4165 - val_acc: 0.8275
Epoch 37/40
8205/8205 [==============================] - 1s - loss: 0.4172 - acc: 0.8257 - val_loss: 0.4071 - val_acc: 0.8363
Epoch 38/40
8205/8205 [==============================] - 1s - loss: 0.4008 - acc: 0.8342 - val_loss: 0.4164 - val_acc: 0.8294
Epoch 39/40
8205/8205 [==============================] - 1s - loss: 0.4104 - acc: 0.8303 - val_loss: 0.4374 - val_acc: 0.8187
Epoch 40/40
8205/8205 [==============================] - 1s - loss: 0.4421 - acc: 0.8111 - val_loss: 0.4235 - val_acc: 0.8250
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-13.h5
chunk number 13
prepare data
start training
Train on 8166 samples, validate on 2042 samples
Epoch 1/40
8166/8166 [==============================] - 1s - loss: 0.4144 - acc: 0.8251 - val_loss: 0.4456 - val_acc: 0.8110
Epoch 2/40
8166/8166 [==============================] - 1s - loss: 0.4343 - acc: 0.8103 - val_loss: 0.4280 - val_acc: 0.8183
Epoch 3/40
8166/8166 [==============================] - 1s - loss: 0.4289 - acc: 0.8186 - val_loss: 0.4426 - val_acc: 0.8119
Epoch 4/40
8166/8166 [==============================] - 1s - loss: 0.4473 - acc: 0.8058 - val_loss: 0.4187 - val_acc: 0.8222
Epoch 5/40
8166/8166 [==============================] - 1s - loss: 0.4148 - acc: 0.8227 - val_loss: 0.4528 - val_acc: 0.8090
Epoch 6/40
8166/8166 [==============================] - 1s - loss: 0.4397 - acc: 0.8136 - val_loss: 0.4131 - val_acc: 0.8301
Epoch 7/40
8166/8166 [==============================] - 1s - loss: 0.4036 - acc: 0.8320 - val_loss: 0.4199 - val_acc: 0.8203
Epoch 8/40
8166/8166 [==============================] - 1s - loss: 0.4182 - acc: 0.8199 - val_loss: 0.4149 - val_acc: 0.8217
Epoch 9/40
8166/8166 [==============================] - 1s - loss: 0.4110 - acc: 0.8245 - val_loss: 0.4118 - val_acc: 0.8306
Epoch 10/40
8166/8166 [==============================] - 1s - loss: 0.4021 - acc: 0.8308 - val_loss: 0.4198 - val_acc: 0.8247
Epoch 11/40
8166/8166 [==============================] - 1s - loss: 0.4108 - acc: 0.8279 - val_loss: 0.4119 - val_acc: 0.8247
Epoch 12/40
8166/8166 [==============================] - 1s - loss: 0.4065 - acc: 0.8284 - val_loss: 0.4172 - val_acc: 0.8232
Epoch 13/40
8166/8166 [==============================] - 1s - loss: 0.4138 - acc: 0.8238 - val_loss: 0.4057 - val_acc: 0.8310
Epoch 14/40
8166/8166 [==============================] - 1s - loss: 0.3972 - acc: 0.8360 - val_loss: 0.4289 - val_acc: 0.8159
Epoch 15/40
8166/8166 [==============================] - 1s - loss: 0.4197 - acc: 0.8206 - val_loss: 0.4117 - val_acc: 0.8271
Epoch 16/40
8166/8166 [==============================] - 1s - loss: 0.4062 - acc: 0.8286 - val_loss: 0.4202 - val_acc: 0.8198
Epoch 17/40
8166/8166 [==============================] - 1s - loss: 0.4176 - acc: 0.8205 - val_loss: 0.4068 - val_acc: 0.8301
Epoch 18/40
8166/8166 [==============================] - 1s - loss: 0.3980 - acc: 0.8337 - val_loss: 0.4343 - val_acc: 0.8198
Epoch 19/40
8166/8166 [==============================] - 1s - loss: 0.4228 - acc: 0.8208 - val_loss: 0.4076 - val_acc: 0.8296
Epoch 20/40
8166/8166 [==============================] - 1s - loss: 0.3988 - acc: 0.8337 - val_loss: 0.4167 - val_acc: 0.8217
Epoch 21/40
8166/8166 [==============================] - 1s - loss: 0.4123 - acc: 0.8237 - val_loss: 0.4087 - val_acc: 0.8271
Epoch 22/40
8166/8166 [==============================] - 1s - loss: 0.4010 - acc: 0.8313 - val_loss: 0.4169 - val_acc: 0.8271
Epoch 23/40
8166/8166 [==============================] - 1s - loss: 0.4044 - acc: 0.8324 - val_loss: 0.4078 - val_acc: 0.8345
Epoch 24/40
8166/8166 [==============================] - 1s - loss: 0.3959 - acc: 0.8360 - val_loss: 0.4077 - val_acc: 0.8286
Epoch 25/40
8166/8166 [==============================] - 1s - loss: 0.4000 - acc: 0.8311 - val_loss: 0.4069 - val_acc: 0.8276
Epoch 26/40
8166/8166 [==============================] - 1s - loss: 0.3986 - acc: 0.8326 - val_loss: 0.4091 - val_acc: 0.8320
Epoch 27/40
8166/8166 [==============================] - 1s - loss: 0.3966 - acc: 0.8355 - val_loss: 0.4076 - val_acc: 0.8325
Epoch 28/40
8166/8166 [==============================] - 1s - loss: 0.3948 - acc: 0.8368 - val_loss: 0.4060 - val_acc: 0.8276
Epoch 29/40
8166/8166 [==============================] - 1s - loss: 0.3968 - acc: 0.8346 - val_loss: 0.4046 - val_acc: 0.8301
Epoch 30/40
8166/8166 [==============================] - 1s - loss: 0.3942 - acc: 0.8354 - val_loss: 0.4097 - val_acc: 0.8315
Epoch 31/40
8166/8166 [==============================] - 1s - loss: 0.3956 - acc: 0.8354 - val_loss: 0.4043 - val_acc: 0.8330
Epoch 32/40
8166/8166 [==============================] - 1s - loss: 0.3910 - acc: 0.8387 - val_loss: 0.4048 - val_acc: 0.8301
Epoch 33/40
8166/8166 [==============================] - 1s - loss: 0.3937 - acc: 0.8363 - val_loss: 0.4043 - val_acc: 0.8345
Epoch 34/40
8166/8166 [==============================] - 1s - loss: 0.3906 - acc: 0.8390 - val_loss: 0.4098 - val_acc: 0.8306
Epoch 35/40
8166/8166 [==============================] - 1s - loss: 0.3939 - acc: 0.8359 - val_loss: 0.4048 - val_acc: 0.8310
Epoch 36/40
8166/8166 [==============================] - 1s - loss: 0.3925 - acc: 0.8371 - val_loss: 0.4046 - val_acc: 0.8306
Epoch 37/40
8166/8166 [==============================] - 1s - loss: 0.3912 - acc: 0.8384 - val_loss: 0.4130 - val_acc: 0.8296
Epoch 38/40
8166/8166 [==============================] - 1s - loss: 0.3957 - acc: 0.8349 - val_loss: 0.4068 - val_acc: 0.8281
Epoch 39/40
8166/8166 [==============================] - 1s - loss: 0.3951 - acc: 0.8349 - val_loss: 0.4046 - val_acc: 0.8301
Epoch 40/40
8166/8166 [==============================] - 1s - loss: 0.3912 - acc: 0.8377 - val_loss: 0.4177 - val_acc: 0.8271
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-14.h5
chunk number 14
prepare data
start training
Train on 8130 samples, validate on 2033 samples
Epoch 1/40
8130/8130 [==============================] - 1s - loss: 0.4132 - acc: 0.8256 - val_loss: 0.4033 - val_acc: 0.8298
Epoch 2/40
8130/8130 [==============================] - 1s - loss: 0.4081 - acc: 0.8273 - val_loss: 0.4035 - val_acc: 0.8298
Epoch 3/40
8130/8130 [==============================] - 1s - loss: 0.4090 - acc: 0.8256 - val_loss: 0.4107 - val_acc: 0.8313
Epoch 4/40
8130/8130 [==============================] - 1s - loss: 0.4063 - acc: 0.8305 - val_loss: 0.4103 - val_acc: 0.8308
Epoch 5/40
8130/8130 [==============================] - 1s - loss: 0.4059 - acc: 0.8305 - val_loss: 0.4026 - val_acc: 0.8303
Epoch 6/40
8130/8130 [==============================] - 1s - loss: 0.4066 - acc: 0.8278 - val_loss: 0.4022 - val_acc: 0.8303
Epoch 7/40
8130/8130 [==============================] - 1s - loss: 0.4052 - acc: 0.8282 - val_loss: 0.4118 - val_acc: 0.8288
Epoch 8/40
8130/8130 [==============================] - 1s - loss: 0.4058 - acc: 0.8301 - val_loss: 0.4047 - val_acc: 0.8293
Epoch 9/40
8130/8130 [==============================] - 1s - loss: 0.4025 - acc: 0.8308 - val_loss: 0.4022 - val_acc: 0.8298
Epoch 10/40
8130/8130 [==============================] - 1s - loss: 0.4045 - acc: 0.8287 - val_loss: 0.4038 - val_acc: 0.8308
Epoch 11/40
8130/8130 [==============================] - 1s - loss: 0.4020 - acc: 0.8304 - val_loss: 0.4104 - val_acc: 0.8283
Epoch 12/40
8130/8130 [==============================] - 1s - loss: 0.4042 - acc: 0.8306 - val_loss: 0.4020 - val_acc: 0.8303
Epoch 13/40
8130/8130 [==============================] - 1s - loss: 0.4026 - acc: 0.8292 - val_loss: 0.4020 - val_acc: 0.8288
Epoch 14/40
8130/8130 [==============================] - 1s - loss: 0.4025 - acc: 0.8303 - val_loss: 0.4085 - val_acc: 0.8269
Epoch 15/40
8130/8130 [==============================] - 1s - loss: 0.4025 - acc: 0.8301 - val_loss: 0.4045 - val_acc: 0.8298
Epoch 16/40
8130/8130 [==============================] - 1s - loss: 0.4007 - acc: 0.8316 - val_loss: 0.4020 - val_acc: 0.8303
Epoch 17/40
8130/8130 [==============================] - 1s - loss: 0.4017 - acc: 0.8309 - val_loss: 0.4042 - val_acc: 0.8293
Epoch 18/40
8130/8130 [==============================] - 1s - loss: 0.4000 - acc: 0.8310 - val_loss: 0.4084 - val_acc: 0.8293
Epoch 19/40
8130/8130 [==============================] - 1s - loss: 0.4011 - acc: 0.8325 - val_loss: 0.4022 - val_acc: 0.8323
Epoch 20/40
8130/8130 [==============================] - 1s - loss: 0.4011 - acc: 0.8311 - val_loss: 0.4034 - val_acc: 0.8308
Epoch 21/40
8130/8130 [==============================] - 1s - loss: 0.3995 - acc: 0.8325 - val_loss: 0.4111 - val_acc: 0.8288
Epoch 22/40
8130/8130 [==============================] - 1s - loss: 0.4019 - acc: 0.8312 - val_loss: 0.4019 - val_acc: 0.8298
Epoch 23/40
8130/8130 [==============================] - 1s - loss: 0.4026 - acc: 0.8293 - val_loss: 0.4026 - val_acc: 0.8328
Epoch 24/40
8130/8130 [==============================] - 1s - loss: 0.3991 - acc: 0.8327 - val_loss: 0.4165 - val_acc: 0.8273
Epoch 25/40
8130/8130 [==============================] - 1s - loss: 0.4052 - acc: 0.8296 - val_loss: 0.4027 - val_acc: 0.8293
Epoch 26/40
8130/8130 [==============================] - 1s - loss: 0.4081 - acc: 0.8287 - val_loss: 0.4012 - val_acc: 0.8303
Epoch 27/40
8130/8130 [==============================] - 1s - loss: 0.4034 - acc: 0.8295 - val_loss: 0.4258 - val_acc: 0.8210
Epoch 28/40
8130/8130 [==============================] - 1s - loss: 0.4128 - acc: 0.8255 - val_loss: 0.4027 - val_acc: 0.8288
Epoch 29/40
8130/8130 [==============================] - 1s - loss: 0.4098 - acc: 0.8284 - val_loss: 0.4032 - val_acc: 0.8259
Epoch 30/40
8130/8130 [==============================] - 1s - loss: 0.4115 - acc: 0.8269 - val_loss: 0.4099 - val_acc: 0.8273
Epoch 31/40
8130/8130 [==============================] - 1s - loss: 0.4013 - acc: 0.8326 - val_loss: 0.4186 - val_acc: 0.8219
Epoch 32/40
8130/8130 [==============================] - 1s - loss: 0.4070 - acc: 0.8295 - val_loss: 0.4037 - val_acc: 0.8254
Epoch 33/40
8130/8130 [==============================] - 1s - loss: 0.4121 - acc: 0.8272 - val_loss: 0.4046 - val_acc: 0.8229
Epoch 34/40
8130/8130 [==============================] - 1s - loss: 0.4138 - acc: 0.8260 - val_loss: 0.4080 - val_acc: 0.8254
Epoch 35/40
8130/8130 [==============================] - 1s - loss: 0.3998 - acc: 0.8331 - val_loss: 0.4272 - val_acc: 0.8180
Epoch 36/40
8130/8130 [==============================] - 1s - loss: 0.4137 - acc: 0.8251 - val_loss: 0.4082 - val_acc: 0.8190
Epoch 37/40
8130/8130 [==============================] - 1s - loss: 0.4207 - acc: 0.8225 - val_loss: 0.4140 - val_acc: 0.8175
Epoch 38/40
8130/8130 [==============================] - 1s - loss: 0.4314 - acc: 0.8156 - val_loss: 0.4018 - val_acc: 0.8254
Epoch 39/40
8130/8130 [==============================] - 1s - loss: 0.4053 - acc: 0.8278 - val_loss: 0.4494 - val_acc: 0.8121
Epoch 40/40
8130/8130 [==============================] - 1s - loss: 0.4353 - acc: 0.8130 - val_loss: 0.4026 - val_acc: 0.8219
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-15.h5
chunk number 15
prepare data
start training
Train on 8086 samples, validate on 2022 samples
Epoch 1/40
8086/8086 [==============================] - 1s - loss: 0.4063 - acc: 0.8300 - val_loss: 0.4229 - val_acc: 0.8180
Epoch 2/40
8086/8086 [==============================] - 1s - loss: 0.4232 - acc: 0.8171 - val_loss: 0.4184 - val_acc: 0.8229
Epoch 3/40
8086/8086 [==============================] - 1s - loss: 0.4172 - acc: 0.8213 - val_loss: 0.4056 - val_acc: 0.8348
Epoch 4/40
8086/8086 [==============================] - 1s - loss: 0.3986 - acc: 0.8353 - val_loss: 0.4390 - val_acc: 0.8076
Epoch 5/40
8086/8086 [==============================] - 1s - loss: 0.4277 - acc: 0.8176 - val_loss: 0.4116 - val_acc: 0.8244
Epoch 6/40
8086/8086 [==============================] - 1s - loss: 0.4063 - acc: 0.8274 - val_loss: 0.4284 - val_acc: 0.8205
Epoch 7/40
8086/8086 [==============================] - 1s - loss: 0.4270 - acc: 0.8178 - val_loss: 0.4163 - val_acc: 0.8215
Epoch 8/40
8086/8086 [==============================] - 1s - loss: 0.4114 - acc: 0.8246 - val_loss: 0.4117 - val_acc: 0.8294
Epoch 9/40
8086/8086 [==============================] - 1s - loss: 0.4005 - acc: 0.8328 - val_loss: 0.4325 - val_acc: 0.8200
Epoch 10/40
8086/8086 [==============================] - 1s - loss: 0.4194 - acc: 0.8240 - val_loss: 0.4108 - val_acc: 0.8239
Epoch 11/40
8086/8086 [==============================] - 1s - loss: 0.4031 - acc: 0.8292 - val_loss: 0.4240 - val_acc: 0.8205
Epoch 12/40
8086/8086 [==============================] - 1s - loss: 0.4203 - acc: 0.8189 - val_loss: 0.4172 - val_acc: 0.8215
Epoch 13/40
8086/8086 [==============================] - 1s - loss: 0.4113 - acc: 0.8223 - val_loss: 0.4090 - val_acc: 0.8314
Epoch 14/40
8086/8086 [==============================] - 1s - loss: 0.3975 - acc: 0.8363 - val_loss: 0.4343 - val_acc: 0.8155
Epoch 15/40
8086/8086 [==============================] - 1s - loss: 0.4194 - acc: 0.8248 - val_loss: 0.4091 - val_acc: 0.8323
Epoch 16/40
8086/8086 [==============================] - 1s - loss: 0.3970 - acc: 0.8368 - val_loss: 0.4147 - val_acc: 0.8264
Epoch 17/40
8086/8086 [==============================] - 1s - loss: 0.4070 - acc: 0.8283 - val_loss: 0.4155 - val_acc: 0.8259
Epoch 18/40
8086/8086 [==============================] - 1s - loss: 0.4080 - acc: 0.8267 - val_loss: 0.4075 - val_acc: 0.8309
Epoch 19/40
8086/8086 [==============================] - 1s - loss: 0.3957 - acc: 0.8360 - val_loss: 0.4221 - val_acc: 0.8249
Epoch 20/40
8086/8086 [==============================] - 1s - loss: 0.4065 - acc: 0.8323 - val_loss: 0.4085 - val_acc: 0.8343
Epoch 21/40
8086/8086 [==============================] - 1s - loss: 0.3950 - acc: 0.8370 - val_loss: 0.4088 - val_acc: 0.8274
Epoch 22/40
8086/8086 [==============================] - 1s - loss: 0.3989 - acc: 0.8307 - val_loss: 0.4097 - val_acc: 0.8269
Epoch 23/40
8086/8086 [==============================] - 1s - loss: 0.4001 - acc: 0.8304 - val_loss: 0.4061 - val_acc: 0.8358
Epoch 24/40
8086/8086 [==============================] - 1s - loss: 0.3926 - acc: 0.8381 - val_loss: 0.4189 - val_acc: 0.8274
Epoch 25/40
8086/8086 [==============================] - 1s - loss: 0.4020 - acc: 0.8349 - val_loss: 0.4056 - val_acc: 0.8358
Epoch 26/40
8086/8086 [==============================] - 1s - loss: 0.3929 - acc: 0.8366 - val_loss: 0.4090 - val_acc: 0.8284
Epoch 27/40
8086/8086 [==============================] - 1s - loss: 0.3987 - acc: 0.8314 - val_loss: 0.4053 - val_acc: 0.8358
Epoch 28/40
8086/8086 [==============================] - 1s - loss: 0.3920 - acc: 0.8376 - val_loss: 0.4152 - val_acc: 0.8304
Epoch 29/40
8086/8086 [==============================] - 1s - loss: 0.3979 - acc: 0.8356 - val_loss: 0.4054 - val_acc: 0.8353
Epoch 30/40
8086/8086 [==============================] - 1s - loss: 0.3908 - acc: 0.8386 - val_loss: 0.4062 - val_acc: 0.8328
Epoch 31/40
8086/8086 [==============================] - 1s - loss: 0.3946 - acc: 0.8340 - val_loss: 0.4047 - val_acc: 0.8368
Epoch 32/40
8086/8086 [==============================] - 1s - loss: 0.3914 - acc: 0.8361 - val_loss: 0.4106 - val_acc: 0.8318
Epoch 33/40
8086/8086 [==============================] - 1s - loss: 0.3932 - acc: 0.8396 - val_loss: 0.4061 - val_acc: 0.8348
Epoch 34/40
8086/8086 [==============================] - 1s - loss: 0.3899 - acc: 0.8396 - val_loss: 0.4050 - val_acc: 0.8378
Epoch 35/40
8086/8086 [==============================] - 1s - loss: 0.3917 - acc: 0.8360 - val_loss: 0.4048 - val_acc: 0.8358
Epoch 36/40
8086/8086 [==============================] - 1s - loss: 0.3898 - acc: 0.8386 - val_loss: 0.4110 - val_acc: 0.8343
Epoch 37/40
8086/8086 [==============================] - 1s - loss: 0.3917 - acc: 0.8406 - val_loss: 0.4052 - val_acc: 0.8343
Epoch 38/40
8086/8086 [==============================] - 1s - loss: 0.3889 - acc: 0.8392 - val_loss: 0.4046 - val_acc: 0.8368
Epoch 39/40
8086/8086 [==============================] - 1s - loss: 0.3902 - acc: 0.8376 - val_loss: 0.4057 - val_acc: 0.8368
Epoch 40/40
8086/8086 [==============================] - 1s - loss: 0.3885 - acc: 0.8405 - val_loss: 0.4081 - val_acc: 0.8328
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-16.h5
chunk number 16
prepare data
start training
Train on 8113 samples, validate on 2029 samples
Epoch 1/40
8113/8113 [==============================] - 1s - loss: 0.4065 - acc: 0.8293 - val_loss: 0.3914 - val_acc: 0.8383
Epoch 2/40
8113/8113 [==============================] - 1s - loss: 0.4042 - acc: 0.8327 - val_loss: 0.3907 - val_acc: 0.8349
Epoch 3/40
8113/8113 [==============================] - 1s - loss: 0.4025 - acc: 0.8334 - val_loss: 0.3924 - val_acc: 0.8359
Epoch 4/40
8113/8113 [==============================] - 1s - loss: 0.4024 - acc: 0.8322 - val_loss: 0.3922 - val_acc: 0.8369
Epoch 5/40
8113/8113 [==============================] - 1s - loss: 0.4024 - acc: 0.8342 - val_loss: 0.3916 - val_acc: 0.8369
Epoch 6/40
8113/8113 [==============================] - 1s - loss: 0.4023 - acc: 0.8335 - val_loss: 0.3899 - val_acc: 0.8393
Epoch 7/40
8113/8113 [==============================] - 1s - loss: 0.4006 - acc: 0.8340 - val_loss: 0.3903 - val_acc: 0.8388
Epoch 8/40
8113/8113 [==============================] - 1s - loss: 0.4003 - acc: 0.8332 - val_loss: 0.3879 - val_acc: 0.8388
Epoch 9/40
8113/8113 [==============================] - 1s - loss: 0.3997 - acc: 0.8347 - val_loss: 0.3879 - val_acc: 0.8408
Epoch 10/40
8113/8113 [==============================] - 1s - loss: 0.3993 - acc: 0.8342 - val_loss: 0.3905 - val_acc: 0.8364
Epoch 11/40
8113/8113 [==============================] - 1s - loss: 0.3997 - acc: 0.8345 - val_loss: 0.3878 - val_acc: 0.8393
Epoch 12/40
8113/8113 [==============================] - 1s - loss: 0.3989 - acc: 0.8356 - val_loss: 0.3880 - val_acc: 0.8398
Epoch 13/40
8113/8113 [==============================] - 1s - loss: 0.3972 - acc: 0.8345 - val_loss: 0.3910 - val_acc: 0.8364
Epoch 14/40
8113/8113 [==============================] - 1s - loss: 0.3983 - acc: 0.8334 - val_loss: 0.3884 - val_acc: 0.8383
Epoch 15/40
8113/8113 [==============================] - 1s - loss: 0.3988 - acc: 0.8359 - val_loss: 0.3880 - val_acc: 0.8379
Epoch 16/40
8113/8113 [==============================] - 1s - loss: 0.3966 - acc: 0.8357 - val_loss: 0.3921 - val_acc: 0.8369
Epoch 17/40
8113/8113 [==============================] - 1s - loss: 0.3990 - acc: 0.8331 - val_loss: 0.3899 - val_acc: 0.8374
Epoch 18/40
8113/8113 [==============================] - 1s - loss: 0.4021 - acc: 0.8332 - val_loss: 0.3868 - val_acc: 0.8379
Epoch 19/40
8113/8113 [==============================] - 1s - loss: 0.3952 - acc: 0.8369 - val_loss: 0.3996 - val_acc: 0.8290
Epoch 20/40
8113/8113 [==============================] - 1s - loss: 0.4047 - acc: 0.8289 - val_loss: 0.4023 - val_acc: 0.8300
Epoch 21/40
8113/8113 [==============================] - 1s - loss: 0.4209 - acc: 0.8230 - val_loss: 0.3974 - val_acc: 0.8324
Epoch 22/40
8113/8113 [==============================] - 1s - loss: 0.4146 - acc: 0.8262 - val_loss: 0.4068 - val_acc: 0.8265
Epoch 23/40
8113/8113 [==============================] - 1s - loss: 0.4095 - acc: 0.8260 - val_loss: 0.3889 - val_acc: 0.8379
Epoch 24/40
8113/8113 [==============================] - 1s - loss: 0.3981 - acc: 0.8357 - val_loss: 0.3892 - val_acc: 0.8354
Epoch 25/40
8113/8113 [==============================] - 1s - loss: 0.4016 - acc: 0.8350 - val_loss: 0.3880 - val_acc: 0.8354
Epoch 26/40
8113/8113 [==============================] - 1s - loss: 0.3957 - acc: 0.8358 - val_loss: 0.3916 - val_acc: 0.8388
Epoch 27/40
8113/8113 [==============================] - 1s - loss: 0.3993 - acc: 0.8327 - val_loss: 0.3898 - val_acc: 0.8369
Epoch 28/40
8113/8113 [==============================] - 1s - loss: 0.4044 - acc: 0.8340 - val_loss: 0.3855 - val_acc: 0.8398
Epoch 29/40
8113/8113 [==============================] - 1s - loss: 0.3980 - acc: 0.8384 - val_loss: 0.4080 - val_acc: 0.8191
Epoch 30/40
8113/8113 [==============================] - 1s - loss: 0.4140 - acc: 0.8193 - val_loss: 0.3979 - val_acc: 0.8339
Epoch 31/40
8113/8113 [==============================] - 1s - loss: 0.4162 - acc: 0.8248 - val_loss: 0.3995 - val_acc: 0.8319
Epoch 32/40
8113/8113 [==============================] - 1s - loss: 0.4181 - acc: 0.8232 - val_loss: 0.3923 - val_acc: 0.8364
Epoch 33/40
8113/8113 [==============================] - 1s - loss: 0.3990 - acc: 0.8330 - val_loss: 0.4244 - val_acc: 0.8117
Epoch 34/40
8113/8113 [==============================] - 1s - loss: 0.4265 - acc: 0.8173 - val_loss: 0.3984 - val_acc: 0.8324
Epoch 35/40
8113/8113 [==============================] - 1s - loss: 0.4156 - acc: 0.8231 - val_loss: 0.4149 - val_acc: 0.8245
Epoch 36/40
8113/8113 [==============================] - 1s - loss: 0.4345 - acc: 0.8120 - val_loss: 0.3954 - val_acc: 0.8354
Epoch 37/40
8113/8113 [==============================] - 1s - loss: 0.4123 - acc: 0.8261 - val_loss: 0.4087 - val_acc: 0.8250
Epoch 38/40
8113/8113 [==============================] - 1s - loss: 0.4141 - acc: 0.8263 - val_loss: 0.3920 - val_acc: 0.8354
Epoch 39/40
8113/8113 [==============================] - 1s - loss: 0.4003 - acc: 0.8331 - val_loss: 0.3910 - val_acc: 0.8369
Epoch 40/40
8113/8113 [==============================] - 1s - loss: 0.4075 - acc: 0.8288 - val_loss: 0.3921 - val_acc: 0.8379
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-17.h5
chunk number 17
prepare data
start training
Train on 8134 samples, validate on 2034 samples
Epoch 1/40
8134/8134 [==============================] - 1s - loss: 0.4168 - acc: 0.8255 - val_loss: 0.4064 - val_acc: 0.8319
Epoch 2/40
8134/8134 [==============================] - 1s - loss: 0.4095 - acc: 0.8280 - val_loss: 0.3995 - val_acc: 0.8333
Epoch 3/40
8134/8134 [==============================] - 1s - loss: 0.4058 - acc: 0.8317 - val_loss: 0.3940 - val_acc: 0.8368
Epoch 4/40
8134/8134 [==============================] - 1s - loss: 0.4019 - acc: 0.8327 - val_loss: 0.3943 - val_acc: 0.8324
Epoch 5/40
8134/8134 [==============================] - 1s - loss: 0.4013 - acc: 0.8311 - val_loss: 0.3950 - val_acc: 0.8328
Epoch 6/40
8134/8134 [==============================] - 1s - loss: 0.4022 - acc: 0.8311 - val_loss: 0.3909 - val_acc: 0.8333
Epoch 7/40
8134/8134 [==============================] - 1s - loss: 0.4003 - acc: 0.8329 - val_loss: 0.3910 - val_acc: 0.8338
Epoch 8/40
8134/8134 [==============================] - 1s - loss: 0.4008 - acc: 0.8310 - val_loss: 0.3912 - val_acc: 0.8343
Epoch 9/40
8134/8134 [==============================] - 1s - loss: 0.3982 - acc: 0.8338 - val_loss: 0.3927 - val_acc: 0.8324
Epoch 10/40
8134/8134 [==============================] - 1s - loss: 0.3984 - acc: 0.8339 - val_loss: 0.3900 - val_acc: 0.8382
Epoch 11/40
8134/8134 [==============================] - 1s - loss: 0.3974 - acc: 0.8335 - val_loss: 0.3910 - val_acc: 0.8373
Epoch 12/40
8134/8134 [==============================] - 1s - loss: 0.3982 - acc: 0.8335 - val_loss: 0.3936 - val_acc: 0.8319
Epoch 13/40
8134/8134 [==============================] - 1s - loss: 0.3984 - acc: 0.8353 - val_loss: 0.3929 - val_acc: 0.8338
Epoch 14/40
8134/8134 [==============================] - 1s - loss: 0.3979 - acc: 0.8356 - val_loss: 0.3922 - val_acc: 0.8382
Epoch 15/40
8134/8134 [==============================] - 1s - loss: 0.3988 - acc: 0.8345 - val_loss: 0.3917 - val_acc: 0.8328
Epoch 16/40
8134/8134 [==============================] - 1s - loss: 0.3969 - acc: 0.8351 - val_loss: 0.3925 - val_acc: 0.8368
Epoch 17/40
8134/8134 [==============================] - 1s - loss: 0.3971 - acc: 0.8360 - val_loss: 0.3896 - val_acc: 0.8363
Epoch 18/40
8134/8134 [==============================] - 1s - loss: 0.3964 - acc: 0.8355 - val_loss: 0.3890 - val_acc: 0.8353
Epoch 19/40
8134/8134 [==============================] - 1s - loss: 0.3954 - acc: 0.8356 - val_loss: 0.3906 - val_acc: 0.8333
Epoch 20/40
8134/8134 [==============================] - 1s - loss: 0.3960 - acc: 0.8356 - val_loss: 0.3885 - val_acc: 0.8338
Epoch 21/40
8134/8134 [==============================] - 1s - loss: 0.3954 - acc: 0.8356 - val_loss: 0.3883 - val_acc: 0.8358
Epoch 22/40
8134/8134 [==============================] - 1s - loss: 0.3952 - acc: 0.8358 - val_loss: 0.3897 - val_acc: 0.8343
Epoch 23/40
8134/8134 [==============================] - 1s - loss: 0.3948 - acc: 0.8356 - val_loss: 0.3881 - val_acc: 0.8338
Epoch 24/40
8134/8134 [==============================] - 1s - loss: 0.3937 - acc: 0.8361 - val_loss: 0.3877 - val_acc: 0.8353
Epoch 25/40
8134/8134 [==============================] - 1s - loss: 0.3937 - acc: 0.8350 - val_loss: 0.3889 - val_acc: 0.8338
Epoch 26/40
8134/8134 [==============================] - 1s - loss: 0.3932 - acc: 0.8364 - val_loss: 0.3890 - val_acc: 0.8338
Epoch 27/40
8134/8134 [==============================] - 1s - loss: 0.3930 - acc: 0.8366 - val_loss: 0.3883 - val_acc: 0.8378
Epoch 28/40
8134/8134 [==============================] - 1s - loss: 0.3930 - acc: 0.8365 - val_loss: 0.3885 - val_acc: 0.8348
Epoch 29/40
8134/8134 [==============================] - 1s - loss: 0.3925 - acc: 0.8361 - val_loss: 0.3890 - val_acc: 0.8348
Epoch 30/40
8134/8134 [==============================] - 1s - loss: 0.3924 - acc: 0.8364 - val_loss: 0.3876 - val_acc: 0.8368
Epoch 31/40
8134/8134 [==============================] - 1s - loss: 0.3919 - acc: 0.8369 - val_loss: 0.3872 - val_acc: 0.8382
Epoch 32/40
8134/8134 [==============================] - 1s - loss: 0.3917 - acc: 0.8358 - val_loss: 0.3878 - val_acc: 0.8353
Epoch 33/40
8134/8134 [==============================] - 1s - loss: 0.3915 - acc: 0.8356 - val_loss: 0.3871 - val_acc: 0.8373
Epoch 34/40
8134/8134 [==============================] - 1s - loss: 0.3912 - acc: 0.8353 - val_loss: 0.3864 - val_acc: 0.8373
Epoch 35/40
8134/8134 [==============================] - 1s - loss: 0.3912 - acc: 0.8361 - val_loss: 0.3868 - val_acc: 0.8382
Epoch 36/40
8134/8134 [==============================] - 1s - loss: 0.3908 - acc: 0.8355 - val_loss: 0.3871 - val_acc: 0.8373
Epoch 37/40
8134/8134 [==============================] - 1s - loss: 0.3906 - acc: 0.8358 - val_loss: 0.3863 - val_acc: 0.8397
Epoch 38/40
8134/8134 [==============================] - 1s - loss: 0.3904 - acc: 0.8361 - val_loss: 0.3866 - val_acc: 0.8402
Epoch 39/40
8134/8134 [==============================] - 1s - loss: 0.3900 - acc: 0.8369 - val_loss: 0.3872 - val_acc: 0.8402
Epoch 40/40
8134/8134 [==============================] - 1s - loss: 0.3898 - acc: 0.8373 - val_loss: 0.3866 - val_acc: 0.8382
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-18.h5
chunk number 18
prepare data
start training
Train on 8221 samples, validate on 2056 samples
Epoch 1/40
8221/8221 [==============================] - 1s - loss: 0.4002 - acc: 0.8338 - val_loss: 0.4149 - val_acc: 0.8254
Epoch 2/40
8221/8221 [==============================] - 1s - loss: 0.3991 - acc: 0.8344 - val_loss: 0.4137 - val_acc: 0.8234
Epoch 3/40
8221/8221 [==============================] - 1s - loss: 0.3983 - acc: 0.8346 - val_loss: 0.4130 - val_acc: 0.8239
Epoch 4/40
8221/8221 [==============================] - 1s - loss: 0.3982 - acc: 0.8349 - val_loss: 0.4129 - val_acc: 0.8249
Epoch 5/40
8221/8221 [==============================] - 1s - loss: 0.3979 - acc: 0.8353 - val_loss: 0.4127 - val_acc: 0.8254
Epoch 6/40
8221/8221 [==============================] - 1s - loss: 0.3976 - acc: 0.8351 - val_loss: 0.4116 - val_acc: 0.8244
Epoch 7/40
8221/8221 [==============================] - 1s - loss: 0.3970 - acc: 0.8352 - val_loss: 0.4110 - val_acc: 0.8264
Epoch 8/40
8221/8221 [==============================] - 1s - loss: 0.3964 - acc: 0.8346 - val_loss: 0.4112 - val_acc: 0.8249
Epoch 9/40
8221/8221 [==============================] - 1s - loss: 0.3961 - acc: 0.8344 - val_loss: 0.4110 - val_acc: 0.8230
Epoch 10/40
8221/8221 [==============================] - 1s - loss: 0.3958 - acc: 0.8346 - val_loss: 0.4112 - val_acc: 0.8225
Epoch 11/40
8221/8221 [==============================] - 1s - loss: 0.3958 - acc: 0.8348 - val_loss: 0.4117 - val_acc: 0.8234
Epoch 12/40
8221/8221 [==============================] - 1s - loss: 0.3955 - acc: 0.8348 - val_loss: 0.4118 - val_acc: 0.8249
Epoch 13/40
8221/8221 [==============================] - 1s - loss: 0.3952 - acc: 0.8344 - val_loss: 0.4116 - val_acc: 0.8264
Epoch 14/40
8221/8221 [==============================] - 1s - loss: 0.3950 - acc: 0.8357 - val_loss: 0.4116 - val_acc: 0.8244
Epoch 15/40
8221/8221 [==============================] - 1s - loss: 0.3946 - acc: 0.8352 - val_loss: 0.4115 - val_acc: 0.8234
Epoch 16/40
8221/8221 [==============================] - 1s - loss: 0.3944 - acc: 0.8348 - val_loss: 0.4109 - val_acc: 0.8259
Epoch 17/40
8221/8221 [==============================] - 1s - loss: 0.3941 - acc: 0.8360 - val_loss: 0.4106 - val_acc: 0.8244
Epoch 18/40
8221/8221 [==============================] - 1s - loss: 0.3938 - acc: 0.8357 - val_loss: 0.4106 - val_acc: 0.8225
Epoch 19/40
8221/8221 [==============================] - 1s - loss: 0.3936 - acc: 0.8343 - val_loss: 0.4102 - val_acc: 0.8230
Epoch 20/40
8221/8221 [==============================] - 1s - loss: 0.3934 - acc: 0.8353 - val_loss: 0.4101 - val_acc: 0.8225
Epoch 21/40
8221/8221 [==============================] - 1s - loss: 0.3933 - acc: 0.8357 - val_loss: 0.4107 - val_acc: 0.8239
Epoch 22/40
8221/8221 [==============================] - 1s - loss: 0.3930 - acc: 0.8349 - val_loss: 0.4106 - val_acc: 0.8244
Epoch 23/40
8221/8221 [==============================] - 1s - loss: 0.3927 - acc: 0.8362 - val_loss: 0.4110 - val_acc: 0.8249
Epoch 24/40
8221/8221 [==============================] - 1s - loss: 0.3925 - acc: 0.8362 - val_loss: 0.4118 - val_acc: 0.8244
Epoch 25/40
8221/8221 [==============================] - 1s - loss: 0.3924 - acc: 0.8372 - val_loss: 0.4117 - val_acc: 0.8259
Epoch 26/40
8221/8221 [==============================] - 1s - loss: 0.3923 - acc: 0.8370 - val_loss: 0.4116 - val_acc: 0.8264
Epoch 27/40
8221/8221 [==============================] - 1s - loss: 0.3921 - acc: 0.8370 - val_loss: 0.4117 - val_acc: 0.8230
Epoch 28/40
8221/8221 [==============================] - 1s - loss: 0.3919 - acc: 0.8365 - val_loss: 0.4110 - val_acc: 0.8239
Epoch 29/40
8221/8221 [==============================] - 1s - loss: 0.3917 - acc: 0.8365 - val_loss: 0.4109 - val_acc: 0.8249
Epoch 30/40
8221/8221 [==============================] - 1s - loss: 0.3915 - acc: 0.8354 - val_loss: 0.4110 - val_acc: 0.8244
Epoch 31/40
8221/8221 [==============================] - 1s - loss: 0.3914 - acc: 0.8358 - val_loss: 0.4107 - val_acc: 0.8254
Epoch 32/40
8221/8221 [==============================] - 1s - loss: 0.3913 - acc: 0.8362 - val_loss: 0.4111 - val_acc: 0.8239
Epoch 33/40
8221/8221 [==============================] - 1s - loss: 0.3911 - acc: 0.8358 - val_loss: 0.4113 - val_acc: 0.8239
Epoch 34/40
8221/8221 [==============================] - 1s - loss: 0.3909 - acc: 0.8365 - val_loss: 0.4113 - val_acc: 0.8249
Epoch 35/40
8221/8221 [==============================] - 1s - loss: 0.3908 - acc: 0.8364 - val_loss: 0.4117 - val_acc: 0.8239
Epoch 36/40
8221/8221 [==============================] - 1s - loss: 0.3907 - acc: 0.8370 - val_loss: 0.4115 - val_acc: 0.8239
Epoch 37/40
8221/8221 [==============================] - 1s - loss: 0.3905 - acc: 0.8363 - val_loss: 0.4114 - val_acc: 0.8244
Epoch 38/40
8221/8221 [==============================] - 1s - loss: 0.3904 - acc: 0.8362 - val_loss: 0.4114 - val_acc: 0.8249
Epoch 39/40
8221/8221 [==============================] - 1s - loss: 0.3903 - acc: 0.8363 - val_loss: 0.4110 - val_acc: 0.8244
Epoch 40/40
8221/8221 [==============================] - 1s - loss: 0.3901 - acc: 0.8368 - val_loss: 0.4110 - val_acc: 0.8244
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-19.h5
chunk number 19
prepare data
start training
Train on 8180 samples, validate on 2046 samples
Epoch 1/40
8180/8180 [==============================] - 2s - loss: 0.4210 - acc: 0.8181 - val_loss: 0.4502 - val_acc: 0.8167
Epoch 2/40
8180/8180 [==============================] - 2s - loss: 0.4284 - acc: 0.8269 - val_loss: 0.4602 - val_acc: 0.8104
Epoch 3/40
8180/8180 [==============================] - 2s - loss: 0.4396 - acc: 0.8192 - val_loss: 0.4258 - val_acc: 0.8265
Epoch 4/40
8180/8180 [==============================] - 2s - loss: 0.4091 - acc: 0.8357 - val_loss: 0.4948 - val_acc: 0.7498
Epoch 5/40
8180/8180 [==============================] - 2s - loss: 0.4746 - acc: 0.7623 - val_loss: 0.4687 - val_acc: 0.7967
Epoch 6/40
8180/8180 [==============================] - 2s - loss: 0.4492 - acc: 0.8087 - val_loss: 0.5109 - val_acc: 0.7669
Epoch 7/40
8180/8180 [==============================] - 2s - loss: 0.4875 - acc: 0.7815 - val_loss: 0.5084 - val_acc: 0.7649
Epoch 8/40
8180/8180 [==============================] - 2s - loss: 0.4867 - acc: 0.7778 - val_loss: 0.4889 - val_acc: 0.7752
Epoch 9/40
8180/8180 [==============================] - 2s - loss: 0.4686 - acc: 0.7901 - val_loss: 0.4655 - val_acc: 0.7898
Epoch 10/40
8180/8180 [==============================] - 2s - loss: 0.4461 - acc: 0.8038 - val_loss: 0.4527 - val_acc: 0.7967
Epoch 11/40
8180/8180 [==============================] - 2s - loss: 0.4357 - acc: 0.8066 - val_loss: 0.4601 - val_acc: 0.7952
Epoch 12/40
8180/8180 [==============================] - 2s - loss: 0.4468 - acc: 0.8009 - val_loss: 0.4650 - val_acc: 0.7991
Epoch 13/40
8180/8180 [==============================] - 2s - loss: 0.4547 - acc: 0.7973 - val_loss: 0.4588 - val_acc: 0.8011
Epoch 14/40
8180/8180 [==============================] - 2s - loss: 0.4461 - acc: 0.8040 - val_loss: 0.4537 - val_acc: 0.8069
Epoch 15/40
8180/8180 [==============================] - 2s - loss: 0.4383 - acc: 0.8103 - val_loss: 0.4496 - val_acc: 0.8089
Epoch 16/40
8180/8180 [==============================] - 2s - loss: 0.4316 - acc: 0.8131 - val_loss: 0.4429 - val_acc: 0.8118
Epoch 17/40
8180/8180 [==============================] - 2s - loss: 0.4253 - acc: 0.8163 - val_loss: 0.4406 - val_acc: 0.8177
Epoch 18/40
8180/8180 [==============================] - 2s - loss: 0.4240 - acc: 0.8202 - val_loss: 0.4443 - val_acc: 0.8201
Epoch 19/40
8180/8180 [==============================] - 2s - loss: 0.4278 - acc: 0.8235 - val_loss: 0.4425 - val_acc: 0.8182
Epoch 20/40
8180/8180 [==============================] - 2s - loss: 0.4257 - acc: 0.8246 - val_loss: 0.4397 - val_acc: 0.8187
Epoch 21/40
8180/8180 [==============================] - 2s - loss: 0.4222 - acc: 0.8248 - val_loss: 0.4391 - val_acc: 0.8216
Epoch 22/40
8180/8180 [==============================] - 2s - loss: 0.4210 - acc: 0.8240 - val_loss: 0.4378 - val_acc: 0.8240
Epoch 23/40
8180/8180 [==============================] - 2s - loss: 0.4195 - acc: 0.8259 - val_loss: 0.4366 - val_acc: 0.8236
Epoch 24/40
8180/8180 [==============================] - 2s - loss: 0.4190 - acc: 0.8268 - val_loss: 0.4355 - val_acc: 0.8275
Epoch 25/40
8180/8180 [==============================] - 2s - loss: 0.4183 - acc: 0.8273 - val_loss: 0.4322 - val_acc: 0.8289
Epoch 26/40
8180/8180 [==============================] - 2s - loss: 0.4144 - acc: 0.8293 - val_loss: 0.4295 - val_acc: 0.8280
Epoch 27/40
8180/8180 [==============================] - 2s - loss: 0.4124 - acc: 0.8313 - val_loss: 0.4262 - val_acc: 0.8324
Epoch 28/40
8180/8180 [==============================] - 2s - loss: 0.4094 - acc: 0.8336 - val_loss: 0.4245 - val_acc: 0.8280
Epoch 29/40
8180/8180 [==============================] - 2s - loss: 0.4076 - acc: 0.8347 - val_loss: 0.4234 - val_acc: 0.8294
Epoch 30/40
8180/8180 [==============================] - 2s - loss: 0.4074 - acc: 0.8342 - val_loss: 0.4219 - val_acc: 0.8294
Epoch 31/40
8180/8180 [==============================] - 2s - loss: 0.4063 - acc: 0.8350 - val_loss: 0.4218 - val_acc: 0.8289
Epoch 32/40
8180/8180 [==============================] - 2s - loss: 0.4063 - acc: 0.8351 - val_loss: 0.4208 - val_acc: 0.8299
Epoch 33/40
8180/8180 [==============================] - 2s - loss: 0.4055 - acc: 0.8364 - val_loss: 0.4191 - val_acc: 0.8284
Epoch 34/40
8180/8180 [==============================] - 2s - loss: 0.4041 - acc: 0.8364 - val_loss: 0.4178 - val_acc: 0.8299
Epoch 35/40
8180/8180 [==============================] - 2s - loss: 0.4029 - acc: 0.8373 - val_loss: 0.4169 - val_acc: 0.8304
Epoch 36/40
8180/8180 [==============================] - 2s - loss: 0.4011 - acc: 0.8379 - val_loss: 0.4166 - val_acc: 0.8309
Epoch 37/40
8180/8180 [==============================] - 2s - loss: 0.4004 - acc: 0.8380 - val_loss: 0.4166 - val_acc: 0.8324
Epoch 38/40
8180/8180 [==============================] - 2s - loss: 0.3996 - acc: 0.8384 - val_loss: 0.4183 - val_acc: 0.8299
Epoch 39/40
8180/8180 [==============================] - 2s - loss: 0.3999 - acc: 0.8378 - val_loss: 0.4184 - val_acc: 0.8299
Epoch 40/40
8180/8180 [==============================] - 2s - loss: 0.3994 - acc: 0.8383 - val_loss: 0.4183 - val_acc: 0.8304
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-20.h5
chunk number 20
prepare data
start training
Train on 8123 samples, validate on 2031 samples
Epoch 1/40
8123/8123 [==============================] - 1s - loss: 0.4031 - acc: 0.8342 - val_loss: 0.4053 - val_acc: 0.8296
Epoch 2/40
8123/8123 [==============================] - 1s - loss: 0.4008 - acc: 0.8359 - val_loss: 0.4029 - val_acc: 0.8321
Epoch 3/40
8123/8123 [==============================] - 1s - loss: 0.3976 - acc: 0.8376 - val_loss: 0.4022 - val_acc: 0.8316
Epoch 4/40
8123/8123 [==============================] - 1s - loss: 0.3938 - acc: 0.8379 - val_loss: 0.4044 - val_acc: 0.8341
Epoch 5/40
8123/8123 [==============================] - 1s - loss: 0.3922 - acc: 0.8380 - val_loss: 0.4068 - val_acc: 0.8306
Epoch 6/40
8123/8123 [==============================] - 1s - loss: 0.3915 - acc: 0.8368 - val_loss: 0.4100 - val_acc: 0.8277
Epoch 7/40
8123/8123 [==============================] - 1s - loss: 0.3926 - acc: 0.8344 - val_loss: 0.4132 - val_acc: 0.8262
Epoch 8/40
8123/8123 [==============================] - 1s - loss: 0.3929 - acc: 0.8353 - val_loss: 0.4148 - val_acc: 0.8277
Epoch 9/40
8123/8123 [==============================] - 1s - loss: 0.3930 - acc: 0.8375 - val_loss: 0.4132 - val_acc: 0.8252
Epoch 10/40
8123/8123 [==============================] - 1s - loss: 0.3917 - acc: 0.8361 - val_loss: 0.4115 - val_acc: 0.8272
Epoch 11/40
8123/8123 [==============================] - 1s - loss: 0.3909 - acc: 0.8371 - val_loss: 0.4110 - val_acc: 0.8296
Epoch 12/40
8123/8123 [==============================] - 1s - loss: 0.3905 - acc: 0.8374 - val_loss: 0.4097 - val_acc: 0.8301
Epoch 13/40
8123/8123 [==============================] - 1s - loss: 0.3904 - acc: 0.8360 - val_loss: 0.4078 - val_acc: 0.8301
Epoch 14/40
8123/8123 [==============================] - 1s - loss: 0.3902 - acc: 0.8375 - val_loss: 0.4068 - val_acc: 0.8291
Epoch 15/40
8123/8123 [==============================] - 1s - loss: 0.3895 - acc: 0.8381 - val_loss: 0.4063 - val_acc: 0.8296
Epoch 16/40
8123/8123 [==============================] - 1s - loss: 0.3890 - acc: 0.8381 - val_loss: 0.4049 - val_acc: 0.8321
Epoch 17/40
8123/8123 [==============================] - 1s - loss: 0.3882 - acc: 0.8389 - val_loss: 0.4040 - val_acc: 0.8306
Epoch 18/40
8123/8123 [==============================] - 1s - loss: 0.3875 - acc: 0.8390 - val_loss: 0.4042 - val_acc: 0.8326
Epoch 19/40
8123/8123 [==============================] - 1s - loss: 0.3868 - acc: 0.8384 - val_loss: 0.4048 - val_acc: 0.8326
Epoch 20/40
8123/8123 [==============================] - 1s - loss: 0.3867 - acc: 0.8381 - val_loss: 0.4050 - val_acc: 0.8311
Epoch 21/40
8123/8123 [==============================] - 1s - loss: 0.3868 - acc: 0.8377 - val_loss: 0.4055 - val_acc: 0.8321
Epoch 22/40
8123/8123 [==============================] - 1s - loss: 0.3869 - acc: 0.8382 - val_loss: 0.4058 - val_acc: 0.8326
Epoch 23/40
8123/8123 [==============================] - 1s - loss: 0.3866 - acc: 0.8380 - val_loss: 0.4047 - val_acc: 0.8326
Epoch 24/40
8123/8123 [==============================] - 1s - loss: 0.3859 - acc: 0.8384 - val_loss: 0.4037 - val_acc: 0.8326
Epoch 25/40
8123/8123 [==============================] - 1s - loss: 0.3854 - acc: 0.8387 - val_loss: 0.4036 - val_acc: 0.8336
Epoch 26/40
8123/8123 [==============================] - 1s - loss: 0.3850 - acc: 0.8395 - val_loss: 0.4035 - val_acc: 0.8326
Epoch 27/40
8123/8123 [==============================] - 1s - loss: 0.3849 - acc: 0.8391 - val_loss: 0.4032 - val_acc: 0.8331
Epoch 28/40
8123/8123 [==============================] - 1s - loss: 0.3847 - acc: 0.8397 - val_loss: 0.4038 - val_acc: 0.8331
Epoch 29/40
8123/8123 [==============================] - 1s - loss: 0.3843 - acc: 0.8398 - val_loss: 0.4049 - val_acc: 0.8316
Epoch 30/40
8123/8123 [==============================] - 1s - loss: 0.3841 - acc: 0.8403 - val_loss: 0.4052 - val_acc: 0.8321
Epoch 31/40
8123/8123 [==============================] - 1s - loss: 0.3839 - acc: 0.8406 - val_loss: 0.4056 - val_acc: 0.8316
Epoch 32/40
8123/8123 [==============================] - 1s - loss: 0.3837 - acc: 0.8403 - val_loss: 0.4062 - val_acc: 0.8326
Epoch 33/40
8123/8123 [==============================] - 1s - loss: 0.3835 - acc: 0.8386 - val_loss: 0.4057 - val_acc: 0.8306
Epoch 34/40
8123/8123 [==============================] - 1s - loss: 0.3832 - acc: 0.8400 - val_loss: 0.4053 - val_acc: 0.8321
Epoch 35/40
8123/8123 [==============================] - 1s - loss: 0.3830 - acc: 0.8397 - val_loss: 0.4054 - val_acc: 0.8326
Epoch 36/40
8123/8123 [==============================] - 1s - loss: 0.3828 - acc: 0.8396 - val_loss: 0.4050 - val_acc: 0.8326
Epoch 37/40
8123/8123 [==============================] - 1s - loss: 0.3825 - acc: 0.8397 - val_loss: 0.4047 - val_acc: 0.8301
Epoch 38/40
8123/8123 [==============================] - 1s - loss: 0.3823 - acc: 0.8402 - val_loss: 0.4051 - val_acc: 0.8316
Epoch 39/40
8123/8123 [==============================] - 1s - loss: 0.3822 - acc: 0.8403 - val_loss: 0.4052 - val_acc: 0.8306
Epoch 40/40
8123/8123 [==============================] - 1s - loss: 0.3820 - acc: 0.8406 - val_loss: 0.4050 - val_acc: 0.8301
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-21.h5
chunk number 21
prepare data
start training
Train on 8140 samples, validate on 2036 samples
Epoch 1/40
8140/8140 [==============================] - 2s - loss: 0.4192 - acc: 0.8262 - val_loss: 0.4543 - val_acc: 0.8129
Epoch 2/40
8140/8140 [==============================] - 2s - loss: 0.4352 - acc: 0.8263 - val_loss: 0.4691 - val_acc: 0.8026
Epoch 3/40
8140/8140 [==============================] - 2s - loss: 0.4468 - acc: 0.8177 - val_loss: 0.4559 - val_acc: 0.8099
Epoch 4/40
8140/8140 [==============================] - 2s - loss: 0.4365 - acc: 0.8244 - val_loss: 0.4084 - val_acc: 0.8340
Epoch 5/40
8140/8140 [==============================] - 2s - loss: 0.4020 - acc: 0.8386 - val_loss: 0.4384 - val_acc: 0.8045
Epoch 6/40
8140/8140 [==============================] - 2s - loss: 0.4328 - acc: 0.8037 - val_loss: 0.4744 - val_acc: 0.8099
Epoch 7/40
8140/8140 [==============================] - 2s - loss: 0.4538 - acc: 0.8251 - val_loss: 0.5071 - val_acc: 0.7903
Epoch 8/40
8140/8140 [==============================] - 2s - loss: 0.4803 - acc: 0.8108 - val_loss: 0.4617 - val_acc: 0.7986
Epoch 9/40
8140/8140 [==============================] - 2s - loss: 0.4403 - acc: 0.8178 - val_loss: 0.4181 - val_acc: 0.8247
Epoch 10/40
8140/8140 [==============================] - 2s - loss: 0.4049 - acc: 0.8339 - val_loss: 0.4542 - val_acc: 0.8183
Epoch 11/40
8140/8140 [==============================] - 2s - loss: 0.4460 - acc: 0.8205 - val_loss: 0.4464 - val_acc: 0.8291
Epoch 12/40
8140/8140 [==============================] - 2s - loss: 0.4382 - acc: 0.8263 - val_loss: 0.4202 - val_acc: 0.8222
Epoch 13/40
8140/8140 [==============================] - 2s - loss: 0.4053 - acc: 0.8342 - val_loss: 0.4365 - val_acc: 0.8089
Epoch 14/40
8140/8140 [==============================] - 2s - loss: 0.4168 - acc: 0.8237 - val_loss: 0.4451 - val_acc: 0.8084
Epoch 15/40
8140/8140 [==============================] - 2s - loss: 0.4235 - acc: 0.8195 - val_loss: 0.4262 - val_acc: 0.8197
Epoch 16/40
8140/8140 [==============================] - 2s - loss: 0.4097 - acc: 0.8305 - val_loss: 0.4123 - val_acc: 0.8306
Epoch 17/40
8140/8140 [==============================] - 2s - loss: 0.4000 - acc: 0.8353 - val_loss: 0.4257 - val_acc: 0.8261
Epoch 18/40
8140/8140 [==============================] - 2s - loss: 0.4168 - acc: 0.8251 - val_loss: 0.4090 - val_acc: 0.8301
Epoch 19/40
8140/8140 [==============================] - 2s - loss: 0.3957 - acc: 0.8364 - val_loss: 0.4226 - val_acc: 0.8193
Epoch 20/40
8140/8140 [==============================] - 2s - loss: 0.4060 - acc: 0.8308 - val_loss: 0.4228 - val_acc: 0.8188
Epoch 21/40
8140/8140 [==============================] - 2s - loss: 0.4038 - acc: 0.8313 - val_loss: 0.4117 - val_acc: 0.8306
Epoch 22/40
8140/8140 [==============================] - 2s - loss: 0.3961 - acc: 0.8380 - val_loss: 0.4207 - val_acc: 0.8355
Epoch 23/40
8140/8140 [==============================] - 2s - loss: 0.4063 - acc: 0.8356 - val_loss: 0.4131 - val_acc: 0.8296
Epoch 24/40
8140/8140 [==============================] - 2s - loss: 0.3960 - acc: 0.8373 - val_loss: 0.4150 - val_acc: 0.8197
Epoch 25/40
8140/8140 [==============================] - 2s - loss: 0.3952 - acc: 0.8346 - val_loss: 0.4195 - val_acc: 0.8168
Epoch 26/40
8140/8140 [==============================] - 2s - loss: 0.3982 - acc: 0.8313 - val_loss: 0.4114 - val_acc: 0.8271
Epoch 27/40
8140/8140 [==============================] - 2s - loss: 0.3907 - acc: 0.8387 - val_loss: 0.4128 - val_acc: 0.8340
Epoch 28/40
8140/8140 [==============================] - 2s - loss: 0.3960 - acc: 0.8343 - val_loss: 0.4103 - val_acc: 0.8340
Epoch 29/40
8140/8140 [==============================] - 2s - loss: 0.3924 - acc: 0.8371 - val_loss: 0.4134 - val_acc: 0.8271
Epoch 30/40
8140/8140 [==============================] - 2s - loss: 0.3914 - acc: 0.8392 - val_loss: 0.4148 - val_acc: 0.8237
Epoch 31/40
8140/8140 [==============================] - 2s - loss: 0.3924 - acc: 0.8377 - val_loss: 0.4056 - val_acc: 0.8325
Epoch 32/40
8140/8140 [==============================] - 2s - loss: 0.3875 - acc: 0.8402 - val_loss: 0.4071 - val_acc: 0.8369
Epoch 33/40
8140/8140 [==============================] - 2s - loss: 0.3936 - acc: 0.8386 - val_loss: 0.4042 - val_acc: 0.8310
Epoch 34/40
8140/8140 [==============================] - 2s - loss: 0.3874 - acc: 0.8416 - val_loss: 0.4100 - val_acc: 0.8232
Epoch 35/40
8140/8140 [==============================] - 2s - loss: 0.3915 - acc: 0.8370 - val_loss: 0.4056 - val_acc: 0.8271
Epoch 36/40
8140/8140 [==============================] - 2s - loss: 0.3882 - acc: 0.8400 - val_loss: 0.4033 - val_acc: 0.8399
Epoch 37/40
8140/8140 [==============================] - 2s - loss: 0.3889 - acc: 0.8387 - val_loss: 0.4024 - val_acc: 0.8355
Epoch 38/40
8140/8140 [==============================] - 2s - loss: 0.3872 - acc: 0.8389 - val_loss: 0.4069 - val_acc: 0.8301
Epoch 39/40
8140/8140 [==============================] - 2s - loss: 0.3876 - acc: 0.8416 - val_loss: 0.4076 - val_acc: 0.8281
Epoch 40/40
8140/8140 [==============================] - 2s - loss: 0.3879 - acc: 0.8413 - val_loss: 0.4015 - val_acc: 0.8364
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-22.h5
chunk number 22
prepare data
start training
Train on 8040 samples, validate on 2010 samples
Epoch 1/40
8040/8040 [==============================] - 1s - loss: 0.3988 - acc: 0.8351 - val_loss: 0.3983 - val_acc: 0.8333
Epoch 2/40
8040/8040 [==============================] - 1s - loss: 0.3987 - acc: 0.8357 - val_loss: 0.3955 - val_acc: 0.8368
Epoch 3/40
8040/8040 [==============================] - 1s - loss: 0.3959 - acc: 0.8356 - val_loss: 0.3949 - val_acc: 0.8373
Epoch 4/40
8040/8040 [==============================] - 1s - loss: 0.3964 - acc: 0.8353 - val_loss: 0.3932 - val_acc: 0.8368
Epoch 5/40
8040/8040 [==============================] - 1s - loss: 0.3937 - acc: 0.8358 - val_loss: 0.3957 - val_acc: 0.8368
Epoch 6/40
8040/8040 [==============================] - 1s - loss: 0.3953 - acc: 0.8364 - val_loss: 0.3926 - val_acc: 0.8378
Epoch 7/40
8040/8040 [==============================] - 1s - loss: 0.3927 - acc: 0.8367 - val_loss: 0.3930 - val_acc: 0.8373
Epoch 8/40
8040/8040 [==============================] - 1s - loss: 0.3937 - acc: 0.8366 - val_loss: 0.3928 - val_acc: 0.8353
Epoch 9/40
8040/8040 [==============================] - 1s - loss: 0.3919 - acc: 0.8368 - val_loss: 0.3952 - val_acc: 0.8373
Epoch 10/40
8040/8040 [==============================] - 1s - loss: 0.3932 - acc: 0.8377 - val_loss: 0.3931 - val_acc: 0.8353
Epoch 11/40
8040/8040 [==============================] - 1s - loss: 0.3913 - acc: 0.8368 - val_loss: 0.3932 - val_acc: 0.8383
Epoch 12/40
8040/8040 [==============================] - 1s - loss: 0.3920 - acc: 0.8364 - val_loss: 0.3928 - val_acc: 0.8373
Epoch 13/40
8040/8040 [==============================] - 1s - loss: 0.3904 - acc: 0.8374 - val_loss: 0.3938 - val_acc: 0.8378
Epoch 14/40
8040/8040 [==============================] - 1s - loss: 0.3912 - acc: 0.8382 - val_loss: 0.3921 - val_acc: 0.8378
Epoch 15/40
8040/8040 [==============================] - 1s - loss: 0.3898 - acc: 0.8371 - val_loss: 0.3922 - val_acc: 0.8403
Epoch 16/40
8040/8040 [==============================] - 1s - loss: 0.3903 - acc: 0.8372 - val_loss: 0.3917 - val_acc: 0.8398
Epoch 17/40
8040/8040 [==============================] - 1s - loss: 0.3895 - acc: 0.8392 - val_loss: 0.3922 - val_acc: 0.8403
Epoch 18/40
8040/8040 [==============================] - 1s - loss: 0.3900 - acc: 0.8389 - val_loss: 0.3912 - val_acc: 0.8408
Epoch 19/40
8040/8040 [==============================] - 1s - loss: 0.3892 - acc: 0.8382 - val_loss: 0.3911 - val_acc: 0.8408
Epoch 20/40
8040/8040 [==============================] - 1s - loss: 0.3891 - acc: 0.8384 - val_loss: 0.3909 - val_acc: 0.8408
Epoch 21/40
8040/8040 [==============================] - 1s - loss: 0.3884 - acc: 0.8393 - val_loss: 0.3909 - val_acc: 0.8408
Epoch 22/40
8040/8040 [==============================] - 1s - loss: 0.3884 - acc: 0.8394 - val_loss: 0.3906 - val_acc: 0.8393
Epoch 23/40
8040/8040 [==============================] - 1s - loss: 0.3882 - acc: 0.8383 - val_loss: 0.3904 - val_acc: 0.8403
Epoch 24/40
8040/8040 [==============================] - 1s - loss: 0.3879 - acc: 0.8384 - val_loss: 0.3904 - val_acc: 0.8403
Epoch 25/40
8040/8040 [==============================] - 1s - loss: 0.3877 - acc: 0.8396 - val_loss: 0.3901 - val_acc: 0.8393
Epoch 26/40
8040/8040 [==============================] - 1s - loss: 0.3872 - acc: 0.8402 - val_loss: 0.3902 - val_acc: 0.8418
Epoch 27/40
8040/8040 [==============================] - 1s - loss: 0.3873 - acc: 0.8399 - val_loss: 0.3901 - val_acc: 0.8403
Epoch 28/40
8040/8040 [==============================] - 1s - loss: 0.3867 - acc: 0.8400 - val_loss: 0.3905 - val_acc: 0.8388
Epoch 29/40
8040/8040 [==============================] - 1s - loss: 0.3868 - acc: 0.8405 - val_loss: 0.3903 - val_acc: 0.8403
Epoch 30/40
8040/8040 [==============================] - 1s - loss: 0.3865 - acc: 0.8405 - val_loss: 0.3902 - val_acc: 0.8408
Epoch 31/40
8040/8040 [==============================] - 1s - loss: 0.3862 - acc: 0.8408 - val_loss: 0.3905 - val_acc: 0.8393
Epoch 32/40
8040/8040 [==============================] - 1s - loss: 0.3862 - acc: 0.8407 - val_loss: 0.3903 - val_acc: 0.8398
Epoch 33/40
8040/8040 [==============================] - 1s - loss: 0.3858 - acc: 0.8413 - val_loss: 0.3904 - val_acc: 0.8398
Epoch 34/40
8040/8040 [==============================] - 1s - loss: 0.3858 - acc: 0.8414 - val_loss: 0.3904 - val_acc: 0.8413
Epoch 35/40
8040/8040 [==============================] - 1s - loss: 0.3856 - acc: 0.8402 - val_loss: 0.3904 - val_acc: 0.8413
Epoch 36/40
8040/8040 [==============================] - 1s - loss: 0.3853 - acc: 0.8414 - val_loss: 0.3907 - val_acc: 0.8403
Epoch 37/40
8040/8040 [==============================] - 1s - loss: 0.3853 - acc: 0.8413 - val_loss: 0.3908 - val_acc: 0.8408
Epoch 38/40
8040/8040 [==============================] - 1s - loss: 0.3850 - acc: 0.8410 - val_loss: 0.3910 - val_acc: 0.8408
Epoch 39/40
8040/8040 [==============================] - 1s - loss: 0.3849 - acc: 0.8414 - val_loss: 0.3912 - val_acc: 0.8398
Epoch 40/40
8040/8040 [==============================] - 1s - loss: 0.3848 - acc: 0.8412 - val_loss: 0.3911 - val_acc: 0.8408
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-23.h5
chunk number 23
prepare data
start training
Train on 8117 samples, validate on 2030 samples
Epoch 1/40
8117/8117 [==============================] - 1s - loss: 0.3851 - acc: 0.8410 - val_loss: 0.3951 - val_acc: 0.8394
Epoch 2/40
8117/8117 [==============================] - 1s - loss: 0.3854 - acc: 0.8375 - val_loss: 0.3944 - val_acc: 0.8389
Epoch 3/40
8117/8117 [==============================] - 1s - loss: 0.3843 - acc: 0.8398 - val_loss: 0.3950 - val_acc: 0.8384
Epoch 4/40
8117/8117 [==============================] - 1s - loss: 0.3845 - acc: 0.8407 - val_loss: 0.3942 - val_acc: 0.8389
Epoch 5/40
8117/8117 [==============================] - 1s - loss: 0.3834 - acc: 0.8408 - val_loss: 0.3941 - val_acc: 0.8379
Epoch 6/40
8117/8117 [==============================] - 1s - loss: 0.3834 - acc: 0.8406 - val_loss: 0.3948 - val_acc: 0.8399
Epoch 7/40
8117/8117 [==============================] - 1s - loss: 0.3832 - acc: 0.8418 - val_loss: 0.3941 - val_acc: 0.8409
Epoch 8/40
8117/8117 [==============================] - 1s - loss: 0.3820 - acc: 0.8433 - val_loss: 0.3939 - val_acc: 0.8369
Epoch 9/40
8117/8117 [==============================] - 1s - loss: 0.3824 - acc: 0.8430 - val_loss: 0.3949 - val_acc: 0.8404
Epoch 10/40
8117/8117 [==============================] - 1s - loss: 0.3819 - acc: 0.8437 - val_loss: 0.3940 - val_acc: 0.8394
Epoch 11/40
8117/8117 [==============================] - 1s - loss: 0.3807 - acc: 0.8444 - val_loss: 0.3940 - val_acc: 0.8369
Epoch 12/40
8117/8117 [==============================] - 1s - loss: 0.3808 - acc: 0.8435 - val_loss: 0.3951 - val_acc: 0.8409
Epoch 13/40
8117/8117 [==============================] - 1s - loss: 0.3809 - acc: 0.8439 - val_loss: 0.3940 - val_acc: 0.8399
Epoch 14/40
8117/8117 [==============================] - 1s - loss: 0.3797 - acc: 0.8453 - val_loss: 0.3939 - val_acc: 0.8404
Epoch 15/40
8117/8117 [==============================] - 1s - loss: 0.3793 - acc: 0.8456 - val_loss: 0.3947 - val_acc: 0.8419
Epoch 16/40
8117/8117 [==============================] - 1s - loss: 0.3795 - acc: 0.8444 - val_loss: 0.3939 - val_acc: 0.8404
Epoch 17/40
8117/8117 [==============================] - 1s - loss: 0.3790 - acc: 0.8451 - val_loss: 0.3939 - val_acc: 0.8424
Epoch 18/40
8117/8117 [==============================] - 1s - loss: 0.3783 - acc: 0.8454 - val_loss: 0.3941 - val_acc: 0.8419
Epoch 19/40
8117/8117 [==============================] - 1s - loss: 0.3781 - acc: 0.8450 - val_loss: 0.3938 - val_acc: 0.8399
Epoch 20/40
8117/8117 [==============================] - 1s - loss: 0.3780 - acc: 0.8449 - val_loss: 0.3939 - val_acc: 0.8419
Epoch 21/40
8117/8117 [==============================] - 1s - loss: 0.3774 - acc: 0.8456 - val_loss: 0.3936 - val_acc: 0.8409
Epoch 22/40
8117/8117 [==============================] - 1s - loss: 0.3769 - acc: 0.8454 - val_loss: 0.3935 - val_acc: 0.8394
Epoch 23/40
8117/8117 [==============================] - 1s - loss: 0.3768 - acc: 0.8451 - val_loss: 0.3939 - val_acc: 0.8433
Epoch 24/40
8117/8117 [==============================] - 1s - loss: 0.3766 - acc: 0.8453 - val_loss: 0.3936 - val_acc: 0.8399
Epoch 25/40
8117/8117 [==============================] - 1s - loss: 0.3762 - acc: 0.8449 - val_loss: 0.3938 - val_acc: 0.8389
Epoch 26/40
8117/8117 [==============================] - 1s - loss: 0.3760 - acc: 0.8445 - val_loss: 0.3941 - val_acc: 0.8424
Epoch 27/40
8117/8117 [==============================] - 1s - loss: 0.3759 - acc: 0.8454 - val_loss: 0.3941 - val_acc: 0.8404
Epoch 28/40
8117/8117 [==============================] - 1s - loss: 0.3757 - acc: 0.8444 - val_loss: 0.3941 - val_acc: 0.8419
Epoch 29/40
8117/8117 [==============================] - 1s - loss: 0.3755 - acc: 0.8448 - val_loss: 0.3940 - val_acc: 0.8389
Epoch 30/40
8117/8117 [==============================] - 1s - loss: 0.3752 - acc: 0.8445 - val_loss: 0.3938 - val_acc: 0.8394
Epoch 31/40
8117/8117 [==============================] - 1s - loss: 0.3749 - acc: 0.8460 - val_loss: 0.3937 - val_acc: 0.8389
Epoch 32/40
8117/8117 [==============================] - 1s - loss: 0.3748 - acc: 0.8459 - val_loss: 0.3937 - val_acc: 0.8394
Epoch 33/40
8117/8117 [==============================] - 1s - loss: 0.3746 - acc: 0.8456 - val_loss: 0.3937 - val_acc: 0.8409
Epoch 34/40
8117/8117 [==============================] - 1s - loss: 0.3744 - acc: 0.8460 - val_loss: 0.3938 - val_acc: 0.8399
Epoch 35/40
8117/8117 [==============================] - 1s - loss: 0.3744 - acc: 0.8451 - val_loss: 0.3937 - val_acc: 0.8419
Epoch 36/40
8117/8117 [==============================] - 1s - loss: 0.3742 - acc: 0.8465 - val_loss: 0.3939 - val_acc: 0.8384
Epoch 37/40
8117/8117 [==============================] - 1s - loss: 0.3742 - acc: 0.8440 - val_loss: 0.3937 - val_acc: 0.8419
Epoch 38/40
8117/8117 [==============================] - 1s - loss: 0.3742 - acc: 0.8462 - val_loss: 0.3939 - val_acc: 0.8384
Epoch 39/40
8117/8117 [==============================] - 1s - loss: 0.3741 - acc: 0.8437 - val_loss: 0.3938 - val_acc: 0.8419
Epoch 40/40
8117/8117 [==============================] - 1s - loss: 0.3743 - acc: 0.8454 - val_loss: 0.3935 - val_acc: 0.8384
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-24.h5
chunk number 24
prepare data
start training
Train on 8140 samples, validate on 2035 samples
Epoch 1/40
8140/8140 [==============================] - 1s - loss: 0.4008 - acc: 0.8319 - val_loss: 0.3949 - val_acc: 0.8324
Epoch 2/40
8140/8140 [==============================] - 1s - loss: 0.3988 - acc: 0.8321 - val_loss: 0.3950 - val_acc: 0.8310
Epoch 3/40
8140/8140 [==============================] - 1s - loss: 0.3978 - acc: 0.8317 - val_loss: 0.3961 - val_acc: 0.8305
Epoch 4/40
8140/8140 [==============================] - 1s - loss: 0.3974 - acc: 0.8332 - val_loss: 0.3962 - val_acc: 0.8324
Epoch 5/40
8140/8140 [==============================] - 1s - loss: 0.3960 - acc: 0.8338 - val_loss: 0.3977 - val_acc: 0.8324
Epoch 6/40
8140/8140 [==============================] - 1s - loss: 0.3966 - acc: 0.8335 - val_loss: 0.3985 - val_acc: 0.8300
Epoch 7/40
8140/8140 [==============================] - 1s - loss: 0.3959 - acc: 0.8339 - val_loss: 0.3995 - val_acc: 0.8270
Epoch 8/40
8140/8140 [==============================] - 1s - loss: 0.3960 - acc: 0.8340 - val_loss: 0.3992 - val_acc: 0.8329
Epoch 9/40
8140/8140 [==============================] - 1s - loss: 0.3958 - acc: 0.8348 - val_loss: 0.3989 - val_acc: 0.8319
Epoch 10/40
8140/8140 [==============================] - 1s - loss: 0.3950 - acc: 0.8339 - val_loss: 0.3995 - val_acc: 0.8270
Epoch 11/40
8140/8140 [==============================] - 1s - loss: 0.3952 - acc: 0.8334 - val_loss: 0.3980 - val_acc: 0.8290
Epoch 12/40
8140/8140 [==============================] - 1s - loss: 0.3946 - acc: 0.8355 - val_loss: 0.3972 - val_acc: 0.8300
Epoch 13/40
8140/8140 [==============================] - 1s - loss: 0.3938 - acc: 0.8345 - val_loss: 0.3977 - val_acc: 0.8260
Epoch 14/40
8140/8140 [==============================] - 1s - loss: 0.3941 - acc: 0.8334 - val_loss: 0.3965 - val_acc: 0.8319
Epoch 15/40
8140/8140 [==============================] - 1s - loss: 0.3935 - acc: 0.8357 - val_loss: 0.3963 - val_acc: 0.8314
Epoch 16/40
8140/8140 [==============================] - 1s - loss: 0.3931 - acc: 0.8367 - val_loss: 0.3970 - val_acc: 0.8256
Epoch 17/40
8140/8140 [==============================] - 1s - loss: 0.3935 - acc: 0.8345 - val_loss: 0.3965 - val_acc: 0.8329
Epoch 18/40
8140/8140 [==============================] - 1s - loss: 0.3929 - acc: 0.8371 - val_loss: 0.3965 - val_acc: 0.8329
Epoch 19/40
8140/8140 [==============================] - 1s - loss: 0.3925 - acc: 0.8356 - val_loss: 0.3968 - val_acc: 0.8275
Epoch 20/40
8140/8140 [==============================] - 1s - loss: 0.3926 - acc: 0.8355 - val_loss: 0.3965 - val_acc: 0.8324
Epoch 21/40
8140/8140 [==============================] - 1s - loss: 0.3919 - acc: 0.8356 - val_loss: 0.3963 - val_acc: 0.8314
Epoch 22/40
8140/8140 [==============================] - 1s - loss: 0.3915 - acc: 0.8354 - val_loss: 0.3967 - val_acc: 0.8275
Epoch 23/40
8140/8140 [==============================] - 1s - loss: 0.3916 - acc: 0.8357 - val_loss: 0.3966 - val_acc: 0.8324
Epoch 24/40
8140/8140 [==============================] - 1s - loss: 0.3912 - acc: 0.8354 - val_loss: 0.3966 - val_acc: 0.8305
Epoch 25/40
8140/8140 [==============================] - 1s - loss: 0.3908 - acc: 0.8364 - val_loss: 0.3971 - val_acc: 0.8305
Epoch 26/40
8140/8140 [==============================] - 1s - loss: 0.3909 - acc: 0.8356 - val_loss: 0.3970 - val_acc: 0.8310
Epoch 27/40
8140/8140 [==============================] - 1s - loss: 0.3906 - acc: 0.8360 - val_loss: 0.3968 - val_acc: 0.8339
Epoch 28/40
8140/8140 [==============================] - 1s - loss: 0.3901 - acc: 0.8364 - val_loss: 0.3970 - val_acc: 0.8314
Epoch 29/40
8140/8140 [==============================] - 1s - loss: 0.3903 - acc: 0.8356 - val_loss: 0.3969 - val_acc: 0.8310
Epoch 30/40
8140/8140 [==============================] - 1s - loss: 0.3901 - acc: 0.8364 - val_loss: 0.3964 - val_acc: 0.8305
Epoch 31/40
8140/8140 [==============================] - 1s - loss: 0.3895 - acc: 0.8362 - val_loss: 0.3964 - val_acc: 0.8300
Epoch 32/40
8140/8140 [==============================] - 1s - loss: 0.3896 - acc: 0.8359 - val_loss: 0.3967 - val_acc: 0.8324
Epoch 33/40
8140/8140 [==============================] - 1s - loss: 0.3896 - acc: 0.8372 - val_loss: 0.3965 - val_acc: 0.8285
Epoch 34/40
8140/8140 [==============================] - 1s - loss: 0.3890 - acc: 0.8356 - val_loss: 0.3966 - val_acc: 0.8290
Epoch 35/40
8140/8140 [==============================] - 1s - loss: 0.3888 - acc: 0.8361 - val_loss: 0.3972 - val_acc: 0.8310
Epoch 36/40
8140/8140 [==============================] - 1s - loss: 0.3889 - acc: 0.8371 - val_loss: 0.3972 - val_acc: 0.8270
Epoch 37/40
8140/8140 [==============================] - 1s - loss: 0.3886 - acc: 0.8360 - val_loss: 0.3974 - val_acc: 0.8275
Epoch 38/40
8140/8140 [==============================] - 1s - loss: 0.3882 - acc: 0.8364 - val_loss: 0.3977 - val_acc: 0.8280
Epoch 39/40
8140/8140 [==============================] - 1s - loss: 0.3881 - acc: 0.8372 - val_loss: 0.3978 - val_acc: 0.8275
Epoch 40/40
8140/8140 [==============================] - 1s - loss: 0.3881 - acc: 0.8365 - val_loss: 0.3981 - val_acc: 0.8290
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-25.h5
chunk number 25
prepare data
start training
Train on 8058 samples, validate on 2015 samples
Epoch 1/40
8058/8058 [==============================] - 1s - loss: 0.3777 - acc: 0.8471 - val_loss: 0.3817 - val_acc: 0.8496
Epoch 2/40
8058/8058 [==============================] - 1s - loss: 0.3784 - acc: 0.8464 - val_loss: 0.3839 - val_acc: 0.8486
Epoch 3/40
8058/8058 [==============================] - 1s - loss: 0.3772 - acc: 0.8450 - val_loss: 0.3823 - val_acc: 0.8496
Epoch 4/40
8058/8058 [==============================] - 1s - loss: 0.3761 - acc: 0.8470 - val_loss: 0.3820 - val_acc: 0.8491
Epoch 5/40
8058/8058 [==============================] - 1s - loss: 0.3763 - acc: 0.8461 - val_loss: 0.3840 - val_acc: 0.8486
Epoch 6/40
8058/8058 [==============================] - 1s - loss: 0.3760 - acc: 0.8465 - val_loss: 0.3812 - val_acc: 0.8491
Epoch 7/40
8058/8058 [==============================] - 1s - loss: 0.3749 - acc: 0.8461 - val_loss: 0.3811 - val_acc: 0.8491
Epoch 8/40
8058/8058 [==============================] - 1s - loss: 0.3741 - acc: 0.8455 - val_loss: 0.3825 - val_acc: 0.8496
Epoch 9/40
8058/8058 [==============================] - 1s - loss: 0.3741 - acc: 0.8467 - val_loss: 0.3802 - val_acc: 0.8501
Epoch 10/40
8058/8058 [==============================] - 1s - loss: 0.3741 - acc: 0.8461 - val_loss: 0.3816 - val_acc: 0.8521
Epoch 11/40
8058/8058 [==============================] - 1s - loss: 0.3732 - acc: 0.8466 - val_loss: 0.3812 - val_acc: 0.8526
Epoch 12/40
8058/8058 [==============================] - 1s - loss: 0.3727 - acc: 0.8467 - val_loss: 0.3804 - val_acc: 0.8511
Epoch 13/40
8058/8058 [==============================] - 1s - loss: 0.3727 - acc: 0.8475 - val_loss: 0.3838 - val_acc: 0.8491
Epoch 14/40
8058/8058 [==============================] - 1s - loss: 0.3727 - acc: 0.8483 - val_loss: 0.3813 - val_acc: 0.8491
Epoch 15/40
8058/8058 [==============================] - 1s - loss: 0.3729 - acc: 0.8475 - val_loss: 0.3851 - val_acc: 0.8467
Epoch 16/40
8058/8058 [==============================] - 1s - loss: 0.3725 - acc: 0.8476 - val_loss: 0.3817 - val_acc: 0.8481
Epoch 17/40
8058/8058 [==============================] - 1s - loss: 0.3724 - acc: 0.8492 - val_loss: 0.3847 - val_acc: 0.8486
Epoch 18/40
8058/8058 [==============================] - 1s - loss: 0.3719 - acc: 0.8469 - val_loss: 0.3813 - val_acc: 0.8511
Epoch 19/40
8058/8058 [==============================] - 1s - loss: 0.3716 - acc: 0.8488 - val_loss: 0.3834 - val_acc: 0.8516
Epoch 20/40
8058/8058 [==============================] - 1s - loss: 0.3711 - acc: 0.8464 - val_loss: 0.3807 - val_acc: 0.8521
Epoch 21/40
8058/8058 [==============================] - 1s - loss: 0.3708 - acc: 0.8487 - val_loss: 0.3823 - val_acc: 0.8541
Epoch 22/40
8058/8058 [==============================] - 1s - loss: 0.3705 - acc: 0.8470 - val_loss: 0.3802 - val_acc: 0.8506
Epoch 23/40
8058/8058 [==============================] - 1s - loss: 0.3703 - acc: 0.8500 - val_loss: 0.3818 - val_acc: 0.8546
Epoch 24/40
8058/8058 [==============================] - 1s - loss: 0.3701 - acc: 0.8479 - val_loss: 0.3795 - val_acc: 0.8501
Epoch 25/40
8058/8058 [==============================] - 1s - loss: 0.3700 - acc: 0.8496 - val_loss: 0.3818 - val_acc: 0.8531
Epoch 26/40
8058/8058 [==============================] - 1s - loss: 0.3698 - acc: 0.8476 - val_loss: 0.3790 - val_acc: 0.8521
Epoch 27/40
8058/8058 [==============================] - 1s - loss: 0.3700 - acc: 0.8486 - val_loss: 0.3828 - val_acc: 0.8516
Epoch 28/40
8058/8058 [==============================] - 1s - loss: 0.3700 - acc: 0.8476 - val_loss: 0.3791 - val_acc: 0.8486
Epoch 29/40
8058/8058 [==============================] - 1s - loss: 0.3707 - acc: 0.8495 - val_loss: 0.3841 - val_acc: 0.8481
Epoch 30/40
8058/8058 [==============================] - 1s - loss: 0.3704 - acc: 0.8479 - val_loss: 0.3793 - val_acc: 0.8496
Epoch 31/40
8058/8058 [==============================] - 1s - loss: 0.3714 - acc: 0.8491 - val_loss: 0.3841 - val_acc: 0.8486
Epoch 32/40
8058/8058 [==============================] - 1s - loss: 0.3703 - acc: 0.8471 - val_loss: 0.3787 - val_acc: 0.8501
Epoch 33/40
8058/8058 [==============================] - 1s - loss: 0.3705 - acc: 0.8495 - val_loss: 0.3825 - val_acc: 0.8511
Epoch 34/40
8058/8058 [==============================] - 1s - loss: 0.3693 - acc: 0.8482 - val_loss: 0.3784 - val_acc: 0.8521
Epoch 35/40
8058/8058 [==============================] - 1s - loss: 0.3688 - acc: 0.8501 - val_loss: 0.3809 - val_acc: 0.8526
Epoch 36/40
8058/8058 [==============================] - 1s - loss: 0.3682 - acc: 0.8488 - val_loss: 0.3793 - val_acc: 0.8526
Epoch 37/40
8058/8058 [==============================] - 1s - loss: 0.3678 - acc: 0.8500 - val_loss: 0.3802 - val_acc: 0.8531
Epoch 38/40
8058/8058 [==============================] - 1s - loss: 0.3676 - acc: 0.8490 - val_loss: 0.3804 - val_acc: 0.8531
Epoch 39/40
8058/8058 [==============================] - 1s - loss: 0.3675 - acc: 0.8492 - val_loss: 0.3796 - val_acc: 0.8526
Epoch 40/40
8058/8058 [==============================] - 1s - loss: 0.3675 - acc: 0.8492 - val_loss: 0.3812 - val_acc: 0.8526
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-26.h5
chunk number 26
prepare data
start training
Train on 8100 samples, validate on 2026 samples
Epoch 1/40
8100/8100 [==============================] - 1s - loss: 0.3923 - acc: 0.8384 - val_loss: 0.4082 - val_acc: 0.8282
Epoch 2/40
8100/8100 [==============================] - 1s - loss: 0.3950 - acc: 0.8394 - val_loss: 0.4025 - val_acc: 0.8223
Epoch 3/40
8100/8100 [==============================] - 1s - loss: 0.3893 - acc: 0.8395 - val_loss: 0.4023 - val_acc: 0.8268
Epoch 4/40
8100/8100 [==============================] - 1s - loss: 0.3891 - acc: 0.8411 - val_loss: 0.4044 - val_acc: 0.8268
Epoch 5/40
8100/8100 [==============================] - 1s - loss: 0.3909 - acc: 0.8404 - val_loss: 0.4024 - val_acc: 0.8253
Epoch 6/40
8100/8100 [==============================] - 1s - loss: 0.3881 - acc: 0.8411 - val_loss: 0.4017 - val_acc: 0.8213
Epoch 7/40
8100/8100 [==============================] - 1s - loss: 0.3868 - acc: 0.8417 - val_loss: 0.4035 - val_acc: 0.8263
Epoch 8/40
8100/8100 [==============================] - 1s - loss: 0.3876 - acc: 0.8416 - val_loss: 0.4048 - val_acc: 0.8253
Epoch 9/40
8100/8100 [==============================] - 1s - loss: 0.3880 - acc: 0.8396 - val_loss: 0.4045 - val_acc: 0.8228
Epoch 10/40
8100/8100 [==============================] - 1s - loss: 0.3881 - acc: 0.8406 - val_loss: 0.4013 - val_acc: 0.8238
Epoch 11/40
8100/8100 [==============================] - 1s - loss: 0.3854 - acc: 0.8421 - val_loss: 0.4014 - val_acc: 0.8228
Epoch 12/40
8100/8100 [==============================] - 1s - loss: 0.3857 - acc: 0.8431 - val_loss: 0.4016 - val_acc: 0.8228
Epoch 13/40
8100/8100 [==============================] - 1s - loss: 0.3870 - acc: 0.8410 - val_loss: 0.4003 - val_acc: 0.8243
Epoch 14/40
8100/8100 [==============================] - 1s - loss: 0.3848 - acc: 0.8423 - val_loss: 0.4017 - val_acc: 0.8213
Epoch 15/40
8100/8100 [==============================] - 1s - loss: 0.3852 - acc: 0.8423 - val_loss: 0.4035 - val_acc: 0.8253
Epoch 16/40
8100/8100 [==============================] - 1s - loss: 0.3874 - acc: 0.8409 - val_loss: 0.4017 - val_acc: 0.8238
Epoch 17/40
8100/8100 [==============================] - 1s - loss: 0.3844 - acc: 0.8427 - val_loss: 0.4016 - val_acc: 0.8258
Epoch 18/40
8100/8100 [==============================] - 1s - loss: 0.3841 - acc: 0.8427 - val_loss: 0.4028 - val_acc: 0.8238
Epoch 19/40
8100/8100 [==============================] - 1s - loss: 0.3857 - acc: 0.8427 - val_loss: 0.4021 - val_acc: 0.8277
Epoch 20/40
8100/8100 [==============================] - 1s - loss: 0.3841 - acc: 0.8425 - val_loss: 0.4003 - val_acc: 0.8263
Epoch 21/40
8100/8100 [==============================] - 1s - loss: 0.3830 - acc: 0.8421 - val_loss: 0.4003 - val_acc: 0.8268
Epoch 22/40
8100/8100 [==============================] - 1s - loss: 0.3831 - acc: 0.8428 - val_loss: 0.4019 - val_acc: 0.8248
Epoch 23/40
8100/8100 [==============================] - 1s - loss: 0.3838 - acc: 0.8436 - val_loss: 0.4018 - val_acc: 0.8243
Epoch 24/40
8100/8100 [==============================] - 1s - loss: 0.3845 - acc: 0.8430 - val_loss: 0.4018 - val_acc: 0.8248
Epoch 25/40
8100/8100 [==============================] - 1s - loss: 0.3831 - acc: 0.8441 - val_loss: 0.4004 - val_acc: 0.8277
Epoch 26/40
8100/8100 [==============================] - 1s - loss: 0.3824 - acc: 0.8427 - val_loss: 0.4004 - val_acc: 0.8287
Epoch 27/40
8100/8100 [==============================] - 1s - loss: 0.3819 - acc: 0.8426 - val_loss: 0.4000 - val_acc: 0.8292
Epoch 28/40
8100/8100 [==============================] - 1s - loss: 0.3817 - acc: 0.8423 - val_loss: 0.3993 - val_acc: 0.8287
Epoch 29/40
8100/8100 [==============================] - 1s - loss: 0.3816 - acc: 0.8423 - val_loss: 0.4002 - val_acc: 0.8287
Epoch 30/40
8100/8100 [==============================] - 1s - loss: 0.3817 - acc: 0.8444 - val_loss: 0.3996 - val_acc: 0.8258
Epoch 31/40
8100/8100 [==============================] - 1s - loss: 0.3822 - acc: 0.8430 - val_loss: 0.4010 - val_acc: 0.8263
Epoch 32/40
8100/8100 [==============================] - 1s - loss: 0.3823 - acc: 0.8446 - val_loss: 0.4011 - val_acc: 0.8287
Epoch 33/40
8100/8100 [==============================] - 1s - loss: 0.3835 - acc: 0.8423 - val_loss: 0.4015 - val_acc: 0.8263
Epoch 34/40
8100/8100 [==============================] - 1s - loss: 0.3824 - acc: 0.8440 - val_loss: 0.4009 - val_acc: 0.8277
Epoch 35/40
8100/8100 [==============================] - 1s - loss: 0.3830 - acc: 0.8423 - val_loss: 0.4008 - val_acc: 0.8282
Epoch 36/40
8100/8100 [==============================] - 1s - loss: 0.3814 - acc: 0.8449 - val_loss: 0.3990 - val_acc: 0.8282
Epoch 37/40
8100/8100 [==============================] - 1s - loss: 0.3806 - acc: 0.8435 - val_loss: 0.3992 - val_acc: 0.8268
Epoch 38/40
8100/8100 [==============================] - 1s - loss: 0.3800 - acc: 0.8433 - val_loss: 0.3989 - val_acc: 0.8258
Epoch 39/40
8100/8100 [==============================] - 1s - loss: 0.3797 - acc: 0.8431 - val_loss: 0.3989 - val_acc: 0.8263
Epoch 40/40
8100/8100 [==============================] - 1s - loss: 0.3797 - acc: 0.8436 - val_loss: 0.3996 - val_acc: 0.8253
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-27.h5
chunk number 27
prepare data
start training
Train on 8095 samples, validate on 2024 samples
Epoch 1/40
8095/8095 [==============================] - 1s - loss: 0.3893 - acc: 0.8366 - val_loss: 0.3800 - val_acc: 0.8468
Epoch 2/40
8095/8095 [==============================] - 1s - loss: 0.3892 - acc: 0.8368 - val_loss: 0.3797 - val_acc: 0.8503
Epoch 3/40
8095/8095 [==============================] - 1s - loss: 0.3879 - acc: 0.8377 - val_loss: 0.3796 - val_acc: 0.8488
Epoch 4/40
8095/8095 [==============================] - 1s - loss: 0.3875 - acc: 0.8372 - val_loss: 0.3797 - val_acc: 0.8468
Epoch 5/40
8095/8095 [==============================] - 1s - loss: 0.3877 - acc: 0.8379 - val_loss: 0.3795 - val_acc: 0.8473
Epoch 6/40
8095/8095 [==============================] - 1s - loss: 0.3868 - acc: 0.8382 - val_loss: 0.3786 - val_acc: 0.8488
Epoch 7/40
8095/8095 [==============================] - 1s - loss: 0.3859 - acc: 0.8380 - val_loss: 0.3783 - val_acc: 0.8488
Epoch 8/40
8095/8095 [==============================] - 1s - loss: 0.3854 - acc: 0.8394 - val_loss: 0.3784 - val_acc: 0.8503
Epoch 9/40
8095/8095 [==============================] - 1s - loss: 0.3852 - acc: 0.8400 - val_loss: 0.3782 - val_acc: 0.8483
Epoch 10/40
8095/8095 [==============================] - 1s - loss: 0.3848 - acc: 0.8394 - val_loss: 0.3779 - val_acc: 0.8488
Epoch 11/40
8095/8095 [==============================] - 1s - loss: 0.3840 - acc: 0.8397 - val_loss: 0.3778 - val_acc: 0.8498
Epoch 12/40
8095/8095 [==============================] - 1s - loss: 0.3835 - acc: 0.8405 - val_loss: 0.3781 - val_acc: 0.8488
Epoch 13/40
8095/8095 [==============================] - 1s - loss: 0.3833 - acc: 0.8405 - val_loss: 0.3783 - val_acc: 0.8483
Epoch 14/40
8095/8095 [==============================] - 1s - loss: 0.3832 - acc: 0.8398 - val_loss: 0.3785 - val_acc: 0.8513
Epoch 15/40
8095/8095 [==============================] - 1s - loss: 0.3831 - acc: 0.8400 - val_loss: 0.3785 - val_acc: 0.8473
Epoch 16/40
8095/8095 [==============================] - 1s - loss: 0.3827 - acc: 0.8400 - val_loss: 0.3788 - val_acc: 0.8518
Epoch 17/40
8095/8095 [==============================] - 1s - loss: 0.3826 - acc: 0.8397 - val_loss: 0.3791 - val_acc: 0.8478
Epoch 18/40
8095/8095 [==============================] - 1s - loss: 0.3826 - acc: 0.8405 - val_loss: 0.3800 - val_acc: 0.8508
Epoch 19/40
8095/8095 [==============================] - 1s - loss: 0.3838 - acc: 0.8399 - val_loss: 0.3818 - val_acc: 0.8478
Epoch 20/40
8095/8095 [==============================] - 1s - loss: 0.3845 - acc: 0.8400 - val_loss: 0.3840 - val_acc: 0.8503
Epoch 21/40
8095/8095 [==============================] - 1s - loss: 0.3890 - acc: 0.8376 - val_loss: 0.3820 - val_acc: 0.8458
Epoch 22/40
8095/8095 [==============================] - 1s - loss: 0.3835 - acc: 0.8400 - val_loss: 0.3799 - val_acc: 0.8503
Epoch 23/40
8095/8095 [==============================] - 1s - loss: 0.3819 - acc: 0.8411 - val_loss: 0.3798 - val_acc: 0.8503
Epoch 24/40
8095/8095 [==============================] - 1s - loss: 0.3807 - acc: 0.8408 - val_loss: 0.3788 - val_acc: 0.8498
Epoch 25/40
8095/8095 [==============================] - 1s - loss: 0.3802 - acc: 0.8419 - val_loss: 0.3789 - val_acc: 0.8498
Epoch 26/40
8095/8095 [==============================] - 1s - loss: 0.3800 - acc: 0.8414 - val_loss: 0.3785 - val_acc: 0.8508
Epoch 27/40
8095/8095 [==============================] - 1s - loss: 0.3801 - acc: 0.8411 - val_loss: 0.3803 - val_acc: 0.8483
Epoch 28/40
8095/8095 [==============================] - 1s - loss: 0.3809 - acc: 0.8401 - val_loss: 0.3815 - val_acc: 0.8508
Epoch 29/40
8095/8095 [==============================] - 1s - loss: 0.3843 - acc: 0.8399 - val_loss: 0.3863 - val_acc: 0.8454
Epoch 30/40
8095/8095 [==============================] - 1s - loss: 0.3854 - acc: 0.8379 - val_loss: 0.3879 - val_acc: 0.8444
Epoch 31/40
8095/8095 [==============================] - 1s - loss: 0.3931 - acc: 0.8369 - val_loss: 0.3806 - val_acc: 0.8503
Epoch 32/40
8095/8095 [==============================] - 1s - loss: 0.3790 - acc: 0.8415 - val_loss: 0.3938 - val_acc: 0.8384
Epoch 33/40
8095/8095 [==============================] - 1s - loss: 0.3918 - acc: 0.8357 - val_loss: 0.4077 - val_acc: 0.8340
Epoch 34/40
8095/8095 [==============================] - 1s - loss: 0.4187 - acc: 0.8247 - val_loss: 0.3992 - val_acc: 0.8375
Epoch 35/40
8095/8095 [==============================] - 1s - loss: 0.4076 - acc: 0.8274 - val_loss: 0.4095 - val_acc: 0.8286
Epoch 36/40
8095/8095 [==============================] - 1s - loss: 0.4074 - acc: 0.8289 - val_loss: 0.3867 - val_acc: 0.8478
Epoch 37/40
8095/8095 [==============================] - 1s - loss: 0.3901 - acc: 0.8356 - val_loss: 0.3901 - val_acc: 0.8439
Epoch 38/40
8095/8095 [==============================] - 1s - loss: 0.3949 - acc: 0.8325 - val_loss: 0.3848 - val_acc: 0.8478
Epoch 39/40
8095/8095 [==============================] - 1s - loss: 0.3848 - acc: 0.8415 - val_loss: 0.3919 - val_acc: 0.8439
Epoch 40/40
8095/8095 [==============================] - 1s - loss: 0.3920 - acc: 0.8403 - val_loss: 0.3852 - val_acc: 0.8439
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-28.h5
chunk number 28
prepare data
start training
Train on 8135 samples, validate on 2034 samples
Epoch 1/40
8135/8135 [==============================] - 2s - loss: 0.4014 - acc: 0.8315 - val_loss: 0.4013 - val_acc: 0.8397
Epoch 2/40
8135/8135 [==============================] - 2s - loss: 0.4028 - acc: 0.8305 - val_loss: 0.3960 - val_acc: 0.8373
Epoch 3/40
8135/8135 [==============================] - 2s - loss: 0.3978 - acc: 0.8316 - val_loss: 0.3944 - val_acc: 0.8392
Epoch 4/40
8135/8135 [==============================] - 2s - loss: 0.3968 - acc: 0.8334 - val_loss: 0.3997 - val_acc: 0.8397
Epoch 5/40
8135/8135 [==============================] - 2s - loss: 0.4030 - acc: 0.8302 - val_loss: 0.3904 - val_acc: 0.8382
Epoch 6/40
8135/8135 [==============================] - 2s - loss: 0.3924 - acc: 0.8354 - val_loss: 0.4027 - val_acc: 0.8338
Epoch 7/40
8135/8135 [==============================] - 2s - loss: 0.4053 - acc: 0.8258 - val_loss: 0.4020 - val_acc: 0.8363
Epoch 8/40
8135/8135 [==============================] - 2s - loss: 0.4067 - acc: 0.8312 - val_loss: 0.3966 - val_acc: 0.8402
Epoch 9/40
8135/8135 [==============================] - 2s - loss: 0.4002 - acc: 0.8323 - val_loss: 0.3989 - val_acc: 0.8309
Epoch 10/40
8135/8135 [==============================] - 2s - loss: 0.4062 - acc: 0.8258 - val_loss: 0.3885 - val_acc: 0.8417
Epoch 11/40
8135/8135 [==============================] - 2s - loss: 0.3933 - acc: 0.8372 - val_loss: 0.3957 - val_acc: 0.8387
Epoch 12/40
8135/8135 [==============================] - 2s - loss: 0.3994 - acc: 0.8321 - val_loss: 0.3870 - val_acc: 0.8387
Epoch 13/40
8135/8135 [==============================] - 2s - loss: 0.3894 - acc: 0.8365 - val_loss: 0.4024 - val_acc: 0.8343
Epoch 14/40
8135/8135 [==============================] - 2s - loss: 0.4021 - acc: 0.8302 - val_loss: 0.4045 - val_acc: 0.8343
Epoch 15/40
8135/8135 [==============================] - 2s - loss: 0.4072 - acc: 0.8291 - val_loss: 0.3993 - val_acc: 0.8353
Epoch 16/40
8135/8135 [==============================] - 2s - loss: 0.4008 - acc: 0.8305 - val_loss: 0.4034 - val_acc: 0.8324
Epoch 17/40
8135/8135 [==============================] - 2s - loss: 0.4042 - acc: 0.8293 - val_loss: 0.3884 - val_acc: 0.8402
Epoch 18/40
8135/8135 [==============================] - 2s - loss: 0.3901 - acc: 0.8355 - val_loss: 0.3924 - val_acc: 0.8397
Epoch 19/40
8135/8135 [==============================] - 2s - loss: 0.3940 - acc: 0.8326 - val_loss: 0.3872 - val_acc: 0.8417
Epoch 20/40
8135/8135 [==============================] - 2s - loss: 0.3884 - acc: 0.8391 - val_loss: 0.3897 - val_acc: 0.8378
Epoch 21/40
8135/8135 [==============================] - 2s - loss: 0.3899 - acc: 0.8395 - val_loss: 0.3890 - val_acc: 0.8397
Epoch 22/40
8135/8135 [==============================] - 2s - loss: 0.3887 - acc: 0.8360 - val_loss: 0.3877 - val_acc: 0.8412
Epoch 23/40
8135/8135 [==============================] - 2s - loss: 0.3867 - acc: 0.8365 - val_loss: 0.3914 - val_acc: 0.8378
Epoch 24/40
8135/8135 [==============================] - 2s - loss: 0.3885 - acc: 0.8397 - val_loss: 0.3881 - val_acc: 0.8422
Epoch 25/40
8135/8135 [==============================] - 2s - loss: 0.3868 - acc: 0.8359 - val_loss: 0.3864 - val_acc: 0.8387
Epoch 26/40
8135/8135 [==============================] - 2s - loss: 0.3854 - acc: 0.8372 - val_loss: 0.3900 - val_acc: 0.8378
Epoch 27/40
8135/8135 [==============================] - 2s - loss: 0.3867 - acc: 0.8403 - val_loss: 0.3866 - val_acc: 0.8417
Epoch 28/40
8135/8135 [==============================] - 2s - loss: 0.3854 - acc: 0.8372 - val_loss: 0.3864 - val_acc: 0.8407
Epoch 29/40
8135/8135 [==============================] - 2s - loss: 0.3844 - acc: 0.8402 - val_loss: 0.3870 - val_acc: 0.8397
Epoch 30/40
8135/8135 [==============================] - 2s - loss: 0.3845 - acc: 0.8411 - val_loss: 0.3865 - val_acc: 0.8407
Epoch 31/40
8135/8135 [==============================] - 2s - loss: 0.3846 - acc: 0.8381 - val_loss: 0.3875 - val_acc: 0.8412
Epoch 32/40
8135/8135 [==============================] - 2s - loss: 0.3845 - acc: 0.8407 - val_loss: 0.3859 - val_acc: 0.8392
Epoch 33/40
8135/8135 [==============================] - 2s - loss: 0.3831 - acc: 0.8400 - val_loss: 0.3863 - val_acc: 0.8407
Epoch 34/40
8135/8135 [==============================] - 2s - loss: 0.3832 - acc: 0.8386 - val_loss: 0.3892 - val_acc: 0.8417
Epoch 35/40
8135/8135 [==============================] - 2s - loss: 0.3835 - acc: 0.8403 - val_loss: 0.3874 - val_acc: 0.8412
Epoch 36/40
8135/8135 [==============================] - 2s - loss: 0.3833 - acc: 0.8386 - val_loss: 0.3894 - val_acc: 0.8412
Epoch 37/40
8135/8135 [==============================] - 2s - loss: 0.3831 - acc: 0.8404 - val_loss: 0.3875 - val_acc: 0.8432
Epoch 38/40
8135/8135 [==============================] - 2s - loss: 0.3822 - acc: 0.8397 - val_loss: 0.3891 - val_acc: 0.8417
Epoch 39/40
8135/8135 [==============================] - 2s - loss: 0.3815 - acc: 0.8424 - val_loss: 0.3896 - val_acc: 0.8427
Epoch 40/40
8135/8135 [==============================] - 2s - loss: 0.3816 - acc: 0.8419 - val_loss: 0.3891 - val_acc: 0.8417
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-29.h5
chunk number 29
prepare data
start training
Train on 8105 samples, validate on 2027 samples
Epoch 1/40
8105/8105 [==============================] - 1s - loss: 0.4023 - acc: 0.8255 - val_loss: 0.3837 - val_acc: 0.8471
Epoch 2/40
8105/8105 [==============================] - 1s - loss: 0.3962 - acc: 0.8331 - val_loss: 0.3827 - val_acc: 0.8490
Epoch 3/40
8105/8105 [==============================] - 1s - loss: 0.3944 - acc: 0.8347 - val_loss: 0.3788 - val_acc: 0.8471
Epoch 4/40
8105/8105 [==============================] - 1s - loss: 0.3937 - acc: 0.8338 - val_loss: 0.3772 - val_acc: 0.8520
Epoch 5/40
8105/8105 [==============================] - 1s - loss: 0.3914 - acc: 0.8359 - val_loss: 0.3860 - val_acc: 0.8431
Epoch 6/40
8105/8105 [==============================] - 1s - loss: 0.3963 - acc: 0.8326 - val_loss: 0.3774 - val_acc: 0.8485
Epoch 7/40
8105/8105 [==============================] - 1s - loss: 0.3936 - acc: 0.8350 - val_loss: 0.3761 - val_acc: 0.8500
Epoch 8/40
8105/8105 [==============================] - 1s - loss: 0.3917 - acc: 0.8368 - val_loss: 0.3820 - val_acc: 0.8416
Epoch 9/40
8105/8105 [==============================] - 1s - loss: 0.3932 - acc: 0.8359 - val_loss: 0.3738 - val_acc: 0.8500
Epoch 10/40
8105/8105 [==============================] - 1s - loss: 0.3893 - acc: 0.8363 - val_loss: 0.3736 - val_acc: 0.8500
Epoch 11/40
8105/8105 [==============================] - 1s - loss: 0.3884 - acc: 0.8355 - val_loss: 0.3821 - val_acc: 0.8456
Epoch 12/40
8105/8105 [==============================] - 1s - loss: 0.3925 - acc: 0.8359 - val_loss: 0.3774 - val_acc: 0.8485
Epoch 13/40
8105/8105 [==============================] - 1s - loss: 0.3906 - acc: 0.8357 - val_loss: 0.3763 - val_acc: 0.8466
Epoch 14/40
8105/8105 [==============================] - 1s - loss: 0.3878 - acc: 0.8360 - val_loss: 0.3853 - val_acc: 0.8451
Epoch 15/40
8105/8105 [==============================] - 1s - loss: 0.3917 - acc: 0.8378 - val_loss: 0.3774 - val_acc: 0.8495
Epoch 16/40
8105/8105 [==============================] - 1s - loss: 0.3875 - acc: 0.8373 - val_loss: 0.3777 - val_acc: 0.8485
Epoch 17/40
8105/8105 [==============================] - 1s - loss: 0.3883 - acc: 0.8366 - val_loss: 0.3823 - val_acc: 0.8431
Epoch 18/40
8105/8105 [==============================] - 1s - loss: 0.3884 - acc: 0.8373 - val_loss: 0.3763 - val_acc: 0.8515
Epoch 19/40
8105/8105 [==============================] - 1s - loss: 0.3852 - acc: 0.8374 - val_loss: 0.3751 - val_acc: 0.8515
Epoch 20/40
8105/8105 [==============================] - 1s - loss: 0.3867 - acc: 0.8371 - val_loss: 0.3753 - val_acc: 0.8520
Epoch 21/40
8105/8105 [==============================] - 1s - loss: 0.3847 - acc: 0.8380 - val_loss: 0.3754 - val_acc: 0.8505
Epoch 22/40
8105/8105 [==============================] - 1s - loss: 0.3847 - acc: 0.8385 - val_loss: 0.3737 - val_acc: 0.8515
Epoch 23/40
8105/8105 [==============================] - 1s - loss: 0.3851 - acc: 0.8373 - val_loss: 0.3733 - val_acc: 0.8515
Epoch 24/40
8105/8105 [==============================] - 1s - loss: 0.3835 - acc: 0.8385 - val_loss: 0.3764 - val_acc: 0.8505
Epoch 25/40
8105/8105 [==============================] - 1s - loss: 0.3851 - acc: 0.8395 - val_loss: 0.3747 - val_acc: 0.8495
Epoch 26/40
8105/8105 [==============================] - 1s - loss: 0.3845 - acc: 0.8380 - val_loss: 0.3742 - val_acc: 0.8510
Epoch 27/40
8105/8105 [==============================] - 1s - loss: 0.3831 - acc: 0.8389 - val_loss: 0.3770 - val_acc: 0.8481
Epoch 28/40
8105/8105 [==============================] - 1s - loss: 0.3845 - acc: 0.8389 - val_loss: 0.3740 - val_acc: 0.8510
Epoch 29/40
8105/8105 [==============================] - 1s - loss: 0.3835 - acc: 0.8382 - val_loss: 0.3733 - val_acc: 0.8535
Epoch 30/40
8105/8105 [==============================] - 1s - loss: 0.3820 - acc: 0.8389 - val_loss: 0.3775 - val_acc: 0.8466
Epoch 31/40
8105/8105 [==============================] - 1s - loss: 0.3843 - acc: 0.8394 - val_loss: 0.3749 - val_acc: 0.8515
Epoch 32/40
8105/8105 [==============================] - 1s - loss: 0.3844 - acc: 0.8386 - val_loss: 0.3737 - val_acc: 0.8535
Epoch 33/40
8105/8105 [==============================] - 1s - loss: 0.3813 - acc: 0.8394 - val_loss: 0.3799 - val_acc: 0.8441
Epoch 34/40
8105/8105 [==============================] - 1s - loss: 0.3853 - acc: 0.8391 - val_loss: 0.3784 - val_acc: 0.8476
Epoch 35/40
8105/8105 [==============================] - 1s - loss: 0.3877 - acc: 0.8369 - val_loss: 0.3747 - val_acc: 0.8520
Epoch 36/40
8105/8105 [==============================] - 1s - loss: 0.3808 - acc: 0.8392 - val_loss: 0.3867 - val_acc: 0.8402
Epoch 37/40
8105/8105 [==============================] - 1s - loss: 0.3906 - acc: 0.8349 - val_loss: 0.3883 - val_acc: 0.8387
Epoch 38/40
8105/8105 [==============================] - 1s - loss: 0.3982 - acc: 0.8322 - val_loss: 0.3775 - val_acc: 0.8466
Epoch 39/40
8105/8105 [==============================] - 1s - loss: 0.3848 - acc: 0.8381 - val_loss: 0.4080 - val_acc: 0.8313
Epoch 40/40
8105/8105 [==============================] - 1s - loss: 0.4118 - acc: 0.8258 - val_loss: 0.4010 - val_acc: 0.8293
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-30.h5
chunk number 30
prepare data
start training
Train on 8185 samples, validate on 2047 samples
Epoch 1/40
8185/8185 [==============================] - 1s - loss: 0.4096 - acc: 0.8309 - val_loss: 0.4504 - val_acc: 0.8144
Epoch 2/40
8185/8185 [==============================] - 1s - loss: 0.4235 - acc: 0.8242 - val_loss: 0.4075 - val_acc: 0.8324
Epoch 3/40
8185/8185 [==============================] - 1s - loss: 0.3885 - acc: 0.8397 - val_loss: 0.4268 - val_acc: 0.8192
Epoch 4/40
8185/8185 [==============================] - 1s - loss: 0.4224 - acc: 0.8261 - val_loss: 0.3986 - val_acc: 0.8359
Epoch 5/40
8185/8185 [==============================] - 1s - loss: 0.3840 - acc: 0.8446 - val_loss: 0.4225 - val_acc: 0.8236
Epoch 6/40
8185/8185 [==============================] - 1s - loss: 0.4004 - acc: 0.8303 - val_loss: 0.4210 - val_acc: 0.8256
Epoch 7/40
8185/8185 [==============================] - 1s - loss: 0.3993 - acc: 0.8307 - val_loss: 0.3995 - val_acc: 0.8354
Epoch 8/40
8185/8185 [==============================] - 1s - loss: 0.3841 - acc: 0.8414 - val_loss: 0.4098 - val_acc: 0.8315
Epoch 9/40
8185/8185 [==============================] - 1s - loss: 0.4012 - acc: 0.8381 - val_loss: 0.3946 - val_acc: 0.8393
Epoch 10/40
8185/8185 [==============================] - 1s - loss: 0.3826 - acc: 0.8429 - val_loss: 0.4069 - val_acc: 0.8363
Epoch 11/40
8185/8185 [==============================] - 1s - loss: 0.3914 - acc: 0.8392 - val_loss: 0.4041 - val_acc: 0.8368
Epoch 12/40
8185/8185 [==============================] - 1s - loss: 0.3894 - acc: 0.8413 - val_loss: 0.3915 - val_acc: 0.8422
Epoch 13/40
8185/8185 [==============================] - 1s - loss: 0.3817 - acc: 0.8425 - val_loss: 0.4011 - val_acc: 0.8305
Epoch 14/40
8185/8185 [==============================] - 1s - loss: 0.3905 - acc: 0.8369 - val_loss: 0.3972 - val_acc: 0.8383
Epoch 15/40
8185/8185 [==============================] - 1s - loss: 0.3809 - acc: 0.8446 - val_loss: 0.4083 - val_acc: 0.8344
Epoch 16/40
8185/8185 [==============================] - 1s - loss: 0.3876 - acc: 0.8426 - val_loss: 0.3985 - val_acc: 0.8378
Epoch 17/40
8185/8185 [==============================] - 1s - loss: 0.3787 - acc: 0.8454 - val_loss: 0.4022 - val_acc: 0.8315
Epoch 18/40
8185/8185 [==============================] - 1s - loss: 0.3855 - acc: 0.8402 - val_loss: 0.3952 - val_acc: 0.8398
Epoch 19/40
8185/8185 [==============================] - 1s - loss: 0.3762 - acc: 0.8468 - val_loss: 0.4014 - val_acc: 0.8363
Epoch 20/40
8185/8185 [==============================] - 1s - loss: 0.3817 - acc: 0.8439 - val_loss: 0.3955 - val_acc: 0.8432
Epoch 21/40
8185/8185 [==============================] - 1s - loss: 0.3766 - acc: 0.8463 - val_loss: 0.3992 - val_acc: 0.8393
Epoch 22/40
8185/8185 [==============================] - 1s - loss: 0.3808 - acc: 0.8463 - val_loss: 0.3959 - val_acc: 0.8407
Epoch 23/40
8185/8185 [==============================] - 1s - loss: 0.3740 - acc: 0.8473 - val_loss: 0.4014 - val_acc: 0.8398
Epoch 24/40
8185/8185 [==============================] - 1s - loss: 0.3774 - acc: 0.8446 - val_loss: 0.3974 - val_acc: 0.8388
Epoch 25/40
8185/8185 [==============================] - 1s - loss: 0.3742 - acc: 0.8463 - val_loss: 0.3985 - val_acc: 0.8398
Epoch 26/40
8185/8185 [==============================] - 1s - loss: 0.3757 - acc: 0.8467 - val_loss: 0.3994 - val_acc: 0.8398
Epoch 27/40
8185/8185 [==============================] - 1s - loss: 0.3768 - acc: 0.8461 - val_loss: 0.3950 - val_acc: 0.8403
Epoch 28/40
8185/8185 [==============================] - 1s - loss: 0.3740 - acc: 0.8465 - val_loss: 0.3981 - val_acc: 0.8344
Epoch 29/40
8185/8185 [==============================] - 1s - loss: 0.3792 - acc: 0.8448 - val_loss: 0.3991 - val_acc: 0.8388
Epoch 30/40
8185/8185 [==============================] - 1s - loss: 0.3763 - acc: 0.8461 - val_loss: 0.3995 - val_acc: 0.8393
Epoch 31/40
8185/8185 [==============================] - 1s - loss: 0.3744 - acc: 0.8452 - val_loss: 0.4022 - val_acc: 0.8344
Epoch 32/40
8185/8185 [==============================] - 1s - loss: 0.3795 - acc: 0.8462 - val_loss: 0.4017 - val_acc: 0.8388
Epoch 33/40
8185/8185 [==============================] - 1s - loss: 0.3751 - acc: 0.8463 - val_loss: 0.3994 - val_acc: 0.8378
Epoch 34/40
8185/8185 [==============================] - 1s - loss: 0.3741 - acc: 0.8465 - val_loss: 0.3981 - val_acc: 0.8388
Epoch 35/40
8185/8185 [==============================] - 1s - loss: 0.3765 - acc: 0.8463 - val_loss: 0.3949 - val_acc: 0.8417
Epoch 36/40
8185/8185 [==============================] - 1s - loss: 0.3717 - acc: 0.8478 - val_loss: 0.3978 - val_acc: 0.8403
Epoch 37/40
8185/8185 [==============================] - 1s - loss: 0.3733 - acc: 0.8472 - val_loss: 0.3949 - val_acc: 0.8427
Epoch 38/40
8185/8185 [==============================] - 1s - loss: 0.3717 - acc: 0.8473 - val_loss: 0.3948 - val_acc: 0.8422
Epoch 39/40
8185/8185 [==============================] - 1s - loss: 0.3711 - acc: 0.8472 - val_loss: 0.3977 - val_acc: 0.8403
Epoch 40/40
8185/8185 [==============================] - 1s - loss: 0.3722 - acc: 0.8474 - val_loss: 0.3946 - val_acc: 0.8412
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-31.h5
chunk number 31
prepare data
start training
Train on 8079 samples, validate on 2020 samples
Epoch 1/40
8079/8079 [==============================] - 1s - loss: 0.3779 - acc: 0.8450 - val_loss: 0.3931 - val_acc: 0.8401
Epoch 2/40
8079/8079 [==============================] - 1s - loss: 0.3788 - acc: 0.8437 - val_loss: 0.3923 - val_acc: 0.8411
Epoch 3/40
8079/8079 [==============================] - 1s - loss: 0.3776 - acc: 0.8460 - val_loss: 0.3918 - val_acc: 0.8411
Epoch 4/40
8079/8079 [==============================] - 1s - loss: 0.3764 - acc: 0.8463 - val_loss: 0.3935 - val_acc: 0.8411
Epoch 5/40
8079/8079 [==============================] - 1s - loss: 0.3771 - acc: 0.8433 - val_loss: 0.3934 - val_acc: 0.8416
Epoch 6/40
8079/8079 [==============================] - 1s - loss: 0.3752 - acc: 0.8458 - val_loss: 0.3941 - val_acc: 0.8411
Epoch 7/40
8079/8079 [==============================] - 1s - loss: 0.3746 - acc: 0.8459 - val_loss: 0.3958 - val_acc: 0.8426
Epoch 8/40
8079/8079 [==============================] - 1s - loss: 0.3752 - acc: 0.8466 - val_loss: 0.3955 - val_acc: 0.8421
Epoch 9/40
8079/8079 [==============================] - 1s - loss: 0.3738 - acc: 0.8465 - val_loss: 0.3962 - val_acc: 0.8421
Epoch 10/40
8079/8079 [==============================] - 1s - loss: 0.3738 - acc: 0.8464 - val_loss: 0.3969 - val_acc: 0.8411
Epoch 11/40
8079/8079 [==============================] - 1s - loss: 0.3742 - acc: 0.8470 - val_loss: 0.3963 - val_acc: 0.8416
Epoch 12/40
8079/8079 [==============================] - 1s - loss: 0.3732 - acc: 0.8464 - val_loss: 0.3966 - val_acc: 0.8406
Epoch 13/40
8079/8079 [==============================] - 1s - loss: 0.3732 - acc: 0.8464 - val_loss: 0.3965 - val_acc: 0.8441
Epoch 14/40
8079/8079 [==============================] - 1s - loss: 0.3731 - acc: 0.8479 - val_loss: 0.3962 - val_acc: 0.8426
Epoch 15/40
8079/8079 [==============================] - 1s - loss: 0.3723 - acc: 0.8468 - val_loss: 0.3962 - val_acc: 0.8421
Epoch 16/40
8079/8079 [==============================] - 1s - loss: 0.3722 - acc: 0.8469 - val_loss: 0.3962 - val_acc: 0.8421
Epoch 17/40
8079/8079 [==============================] - 1s - loss: 0.3722 - acc: 0.8478 - val_loss: 0.3960 - val_acc: 0.8421
Epoch 18/40
8079/8079 [==============================] - 1s - loss: 0.3719 - acc: 0.8474 - val_loss: 0.3955 - val_acc: 0.8426
Epoch 19/40
8079/8079 [==============================] - 1s - loss: 0.3713 - acc: 0.8479 - val_loss: 0.3957 - val_acc: 0.8421
Epoch 20/40
8079/8079 [==============================] - 1s - loss: 0.3712 - acc: 0.8475 - val_loss: 0.3961 - val_acc: 0.8421
Epoch 21/40
8079/8079 [==============================] - 1s - loss: 0.3714 - acc: 0.8468 - val_loss: 0.3961 - val_acc: 0.8426
Epoch 22/40
8079/8079 [==============================] - 1s - loss: 0.3707 - acc: 0.8476 - val_loss: 0.3962 - val_acc: 0.8421
Epoch 23/40
8079/8079 [==============================] - 1s - loss: 0.3703 - acc: 0.8479 - val_loss: 0.3964 - val_acc: 0.8411
Epoch 24/40
8079/8079 [==============================] - 1s - loss: 0.3702 - acc: 0.8474 - val_loss: 0.3967 - val_acc: 0.8431
Epoch 25/40
8079/8079 [==============================] - 1s - loss: 0.3703 - acc: 0.8480 - val_loss: 0.3965 - val_acc: 0.8406
Epoch 26/40
8079/8079 [==============================] - 1s - loss: 0.3701 - acc: 0.8485 - val_loss: 0.3963 - val_acc: 0.8431
Epoch 27/40
8079/8079 [==============================] - 1s - loss: 0.3697 - acc: 0.8484 - val_loss: 0.3961 - val_acc: 0.8426
Epoch 28/40
8079/8079 [==============================] - 1s - loss: 0.3695 - acc: 0.8484 - val_loss: 0.3960 - val_acc: 0.8421
Epoch 29/40
8079/8079 [==============================] - 1s - loss: 0.3696 - acc: 0.8476 - val_loss: 0.3961 - val_acc: 0.8411
Epoch 30/40
8079/8079 [==============================] - 1s - loss: 0.3694 - acc: 0.8481 - val_loss: 0.3958 - val_acc: 0.8421
Epoch 31/40
8079/8079 [==============================] - 1s - loss: 0.3692 - acc: 0.8479 - val_loss: 0.3958 - val_acc: 0.8421
Epoch 32/40
8079/8079 [==============================] - 1s - loss: 0.3689 - acc: 0.8491 - val_loss: 0.3959 - val_acc: 0.8426
Epoch 33/40
8079/8079 [==============================] - 1s - loss: 0.3688 - acc: 0.8485 - val_loss: 0.3960 - val_acc: 0.8416
Epoch 34/40
8079/8079 [==============================] - 1s - loss: 0.3687 - acc: 0.8481 - val_loss: 0.3963 - val_acc: 0.8411
Epoch 35/40
8079/8079 [==============================] - 1s - loss: 0.3685 - acc: 0.8480 - val_loss: 0.3962 - val_acc: 0.8416
Epoch 36/40
8079/8079 [==============================] - 1s - loss: 0.3683 - acc: 0.8485 - val_loss: 0.3963 - val_acc: 0.8411
Epoch 37/40
8079/8079 [==============================] - 1s - loss: 0.3681 - acc: 0.8490 - val_loss: 0.3963 - val_acc: 0.8406
Epoch 38/40
8079/8079 [==============================] - 1s - loss: 0.3680 - acc: 0.8485 - val_loss: 0.3961 - val_acc: 0.8416
Epoch 39/40
8079/8079 [==============================] - 1s - loss: 0.3679 - acc: 0.8489 - val_loss: 0.3962 - val_acc: 0.8411
Epoch 40/40
8079/8079 [==============================] - 1s - loss: 0.3677 - acc: 0.8485 - val_loss: 0.3960 - val_acc: 0.8416
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-32.h5
chunk number 32
prepare data
start training
Train on 8130 samples, validate on 2033 samples
Epoch 1/40
8130/8130 [==============================] - 1s - loss: 0.3731 - acc: 0.8466 - val_loss: 0.3762 - val_acc: 0.8475
Epoch 2/40
8130/8130 [==============================] - 1s - loss: 0.3726 - acc: 0.8474 - val_loss: 0.3770 - val_acc: 0.8485
Epoch 3/40
8130/8130 [==============================] - 1s - loss: 0.3720 - acc: 0.8477 - val_loss: 0.3768 - val_acc: 0.8470
Epoch 4/40
8130/8130 [==============================] - 1s - loss: 0.3713 - acc: 0.8481 - val_loss: 0.3773 - val_acc: 0.8475
Epoch 5/40
8130/8130 [==============================] - 1s - loss: 0.3707 - acc: 0.8488 - val_loss: 0.3783 - val_acc: 0.8485
Epoch 6/40
8130/8130 [==============================] - 1s - loss: 0.3702 - acc: 0.8493 - val_loss: 0.3779 - val_acc: 0.8495
Epoch 7/40
8130/8130 [==============================] - 1s - loss: 0.3697 - acc: 0.8493 - val_loss: 0.3784 - val_acc: 0.8475
Epoch 8/40
8130/8130 [==============================] - 1s - loss: 0.3692 - acc: 0.8498 - val_loss: 0.3788 - val_acc: 0.8475
Epoch 9/40
8130/8130 [==============================] - 1s - loss: 0.3689 - acc: 0.8504 - val_loss: 0.3780 - val_acc: 0.8485
Epoch 10/40
8130/8130 [==============================] - 1s - loss: 0.3685 - acc: 0.8501 - val_loss: 0.3782 - val_acc: 0.8455
Epoch 11/40
8130/8130 [==============================] - 1s - loss: 0.3681 - acc: 0.8498 - val_loss: 0.3781 - val_acc: 0.8441
Epoch 12/40
8130/8130 [==============================] - 1s - loss: 0.3678 - acc: 0.8496 - val_loss: 0.3774 - val_acc: 0.8451
Epoch 13/40
8130/8130 [==============================] - 1s - loss: 0.3676 - acc: 0.8498 - val_loss: 0.3778 - val_acc: 0.8441
Epoch 14/40
8130/8130 [==============================] - 1s - loss: 0.3673 - acc: 0.8496 - val_loss: 0.3774 - val_acc: 0.8441
Epoch 15/40
8130/8130 [==============================] - 1s - loss: 0.3671 - acc: 0.8496 - val_loss: 0.3771 - val_acc: 0.8436
Epoch 16/40
8130/8130 [==============================] - 1s - loss: 0.3669 - acc: 0.8501 - val_loss: 0.3774 - val_acc: 0.8441
Epoch 17/40
8130/8130 [==============================] - 1s - loss: 0.3666 - acc: 0.8496 - val_loss: 0.3769 - val_acc: 0.8446
Epoch 18/40
8130/8130 [==============================] - 1s - loss: 0.3663 - acc: 0.8496 - val_loss: 0.3768 - val_acc: 0.8451
Epoch 19/40
8130/8130 [==============================] - 1s - loss: 0.3660 - acc: 0.8496 - val_loss: 0.3771 - val_acc: 0.8446
Epoch 20/40
8130/8130 [==============================] - 1s - loss: 0.3658 - acc: 0.8494 - val_loss: 0.3766 - val_acc: 0.8455
Epoch 21/40
8130/8130 [==============================] - 1s - loss: 0.3655 - acc: 0.8501 - val_loss: 0.3770 - val_acc: 0.8446
Epoch 22/40
8130/8130 [==============================] - 1s - loss: 0.3653 - acc: 0.8501 - val_loss: 0.3770 - val_acc: 0.8451
Epoch 23/40
8130/8130 [==============================] - 1s - loss: 0.3652 - acc: 0.8502 - val_loss: 0.3770 - val_acc: 0.8451
Epoch 24/40
8130/8130 [==============================] - 1s - loss: 0.3650 - acc: 0.8503 - val_loss: 0.3774 - val_acc: 0.8460
Epoch 25/40
8130/8130 [==============================] - 1s - loss: 0.3648 - acc: 0.8504 - val_loss: 0.3770 - val_acc: 0.8446
Epoch 26/40
8130/8130 [==============================] - 1s - loss: 0.3646 - acc: 0.8499 - val_loss: 0.3772 - val_acc: 0.8451
Epoch 27/40
8130/8130 [==============================] - 1s - loss: 0.3644 - acc: 0.8506 - val_loss: 0.3770 - val_acc: 0.8446
Epoch 28/40
8130/8130 [==============================] - 1s - loss: 0.3641 - acc: 0.8503 - val_loss: 0.3769 - val_acc: 0.8451
Epoch 29/40
8130/8130 [==============================] - 1s - loss: 0.3639 - acc: 0.8506 - val_loss: 0.3770 - val_acc: 0.8451
Epoch 30/40
8130/8130 [==============================] - 1s - loss: 0.3637 - acc: 0.8510 - val_loss: 0.3768 - val_acc: 0.8446
Epoch 31/40
8130/8130 [==============================] - 1s - loss: 0.3635 - acc: 0.8508 - val_loss: 0.3773 - val_acc: 0.8455
Epoch 32/40
8130/8130 [==============================] - 1s - loss: 0.3633 - acc: 0.8515 - val_loss: 0.3771 - val_acc: 0.8451
Epoch 33/40
8130/8130 [==============================] - 1s - loss: 0.3631 - acc: 0.8509 - val_loss: 0.3775 - val_acc: 0.8455
Epoch 34/40
8130/8130 [==============================] - 1s - loss: 0.3629 - acc: 0.8517 - val_loss: 0.3773 - val_acc: 0.8451
Epoch 35/40
8130/8130 [==============================] - 1s - loss: 0.3626 - acc: 0.8514 - val_loss: 0.3775 - val_acc: 0.8455
Epoch 36/40
8130/8130 [==============================] - 1s - loss: 0.3624 - acc: 0.8513 - val_loss: 0.3770 - val_acc: 0.8460
Epoch 37/40
8130/8130 [==============================] - 1s - loss: 0.3623 - acc: 0.8514 - val_loss: 0.3773 - val_acc: 0.8451
Epoch 38/40
8130/8130 [==============================] - 1s - loss: 0.3621 - acc: 0.8513 - val_loss: 0.3766 - val_acc: 0.8475
Epoch 39/40
8130/8130 [==============================] - 1s - loss: 0.3619 - acc: 0.8512 - val_loss: 0.3774 - val_acc: 0.8460
Epoch 40/40
8130/8130 [==============================] - 1s - loss: 0.3617 - acc: 0.8517 - val_loss: 0.3761 - val_acc: 0.8475
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-33.h5
chunk number 33
prepare data
start training
Train on 8214 samples, validate on 2054 samples
Epoch 1/40
8214/8214 [==============================] - 1s - loss: 0.3846 - acc: 0.8440 - val_loss: 0.3794 - val_acc: 0.8515
Epoch 2/40
8214/8214 [==============================] - 1s - loss: 0.3831 - acc: 0.8443 - val_loss: 0.3795 - val_acc: 0.8535
Epoch 3/40
8214/8214 [==============================] - 1s - loss: 0.3829 - acc: 0.8427 - val_loss: 0.3811 - val_acc: 0.8505
Epoch 4/40
8214/8214 [==============================] - 1s - loss: 0.3825 - acc: 0.8454 - val_loss: 0.3806 - val_acc: 0.8486
Epoch 5/40
8214/8214 [==============================] - 1s - loss: 0.3816 - acc: 0.8448 - val_loss: 0.3786 - val_acc: 0.8515
Epoch 6/40
8214/8214 [==============================] - 1s - loss: 0.3806 - acc: 0.8451 - val_loss: 0.3792 - val_acc: 0.8476
Epoch 7/40
8214/8214 [==============================] - 1s - loss: 0.3801 - acc: 0.8456 - val_loss: 0.3796 - val_acc: 0.8471
Epoch 8/40
8214/8214 [==============================] - 1s - loss: 0.3801 - acc: 0.8453 - val_loss: 0.3785 - val_acc: 0.8486
Epoch 9/40
8214/8214 [==============================] - 1s - loss: 0.3799 - acc: 0.8455 - val_loss: 0.3791 - val_acc: 0.8462
Epoch 10/40
8214/8214 [==============================] - 1s - loss: 0.3792 - acc: 0.8468 - val_loss: 0.3794 - val_acc: 0.8462
Epoch 11/40
8214/8214 [==============================] - 1s - loss: 0.3790 - acc: 0.8472 - val_loss: 0.3788 - val_acc: 0.8462
Epoch 12/40
8214/8214 [==============================] - 1s - loss: 0.3789 - acc: 0.8454 - val_loss: 0.3792 - val_acc: 0.8471
Epoch 13/40
8214/8214 [==============================] - 1s - loss: 0.3786 - acc: 0.8466 - val_loss: 0.3793 - val_acc: 0.8466
Epoch 14/40
8214/8214 [==============================] - 1s - loss: 0.3782 - acc: 0.8471 - val_loss: 0.3780 - val_acc: 0.8466
Epoch 15/40
8214/8214 [==============================] - 1s - loss: 0.3778 - acc: 0.8459 - val_loss: 0.3777 - val_acc: 0.8476
Epoch 16/40
8214/8214 [==============================] - 1s - loss: 0.3775 - acc: 0.8472 - val_loss: 0.3781 - val_acc: 0.8486
Epoch 17/40
8214/8214 [==============================] - 1s - loss: 0.3774 - acc: 0.8476 - val_loss: 0.3771 - val_acc: 0.8486
Epoch 18/40
8214/8214 [==============================] - 1s - loss: 0.3772 - acc: 0.8481 - val_loss: 0.3769 - val_acc: 0.8481
Epoch 19/40
8214/8214 [==============================] - 1s - loss: 0.3769 - acc: 0.8482 - val_loss: 0.3777 - val_acc: 0.8500
Epoch 20/40
8214/8214 [==============================] - 1s - loss: 0.3767 - acc: 0.8478 - val_loss: 0.3773 - val_acc: 0.8481
Epoch 21/40
8214/8214 [==============================] - 1s - loss: 0.3764 - acc: 0.8476 - val_loss: 0.3776 - val_acc: 0.8481
Epoch 22/40
8214/8214 [==============================] - 1s - loss: 0.3762 - acc: 0.8471 - val_loss: 0.3782 - val_acc: 0.8471
Epoch 23/40
8214/8214 [==============================] - 1s - loss: 0.3760 - acc: 0.8473 - val_loss: 0.3778 - val_acc: 0.8481
Epoch 24/40
8214/8214 [==============================] - 1s - loss: 0.3757 - acc: 0.8472 - val_loss: 0.3778 - val_acc: 0.8481
Epoch 25/40
8214/8214 [==============================] - 1s - loss: 0.3754 - acc: 0.8476 - val_loss: 0.3784 - val_acc: 0.8452
Epoch 26/40
8214/8214 [==============================] - 1s - loss: 0.3753 - acc: 0.8479 - val_loss: 0.3780 - val_acc: 0.8462
Epoch 27/40
8214/8214 [==============================] - 1s - loss: 0.3751 - acc: 0.8476 - val_loss: 0.3781 - val_acc: 0.8457
Epoch 28/40
8214/8214 [==============================] - 1s - loss: 0.3749 - acc: 0.8477 - val_loss: 0.3786 - val_acc: 0.8452
Epoch 29/40
8214/8214 [==============================] - 1s - loss: 0.3747 - acc: 0.8477 - val_loss: 0.3785 - val_acc: 0.8457
Epoch 30/40
8214/8214 [==============================] - 1s - loss: 0.3745 - acc: 0.8471 - val_loss: 0.3789 - val_acc: 0.8447
Epoch 31/40
8214/8214 [==============================] - 1s - loss: 0.3744 - acc: 0.8472 - val_loss: 0.3792 - val_acc: 0.8447
Epoch 32/40
8214/8214 [==============================] - 1s - loss: 0.3742 - acc: 0.8468 - val_loss: 0.3789 - val_acc: 0.8452
Epoch 33/40
8214/8214 [==============================] - 1s - loss: 0.3740 - acc: 0.8470 - val_loss: 0.3791 - val_acc: 0.8452
Epoch 34/40
8214/8214 [==============================] - 1s - loss: 0.3738 - acc: 0.8467 - val_loss: 0.3791 - val_acc: 0.8462
Epoch 35/40
8214/8214 [==============================] - 1s - loss: 0.3736 - acc: 0.8470 - val_loss: 0.3789 - val_acc: 0.8462
Epoch 36/40
8214/8214 [==============================] - 1s - loss: 0.3735 - acc: 0.8471 - val_loss: 0.3792 - val_acc: 0.8466
Epoch 37/40
8214/8214 [==============================] - 1s - loss: 0.3733 - acc: 0.8466 - val_loss: 0.3790 - val_acc: 0.8466
Epoch 38/40
8214/8214 [==============================] - 1s - loss: 0.3731 - acc: 0.8465 - val_loss: 0.3793 - val_acc: 0.8471
Epoch 39/40
8214/8214 [==============================] - 1s - loss: 0.3730 - acc: 0.8467 - val_loss: 0.3796 - val_acc: 0.8476
Epoch 40/40
8214/8214 [==============================] - 1s - loss: 0.3728 - acc: 0.8465 - val_loss: 0.3795 - val_acc: 0.8471
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-34.h5
chunk number 34
prepare data
start training
Train on 8084 samples, validate on 2021 samples
Epoch 1/40
8084/8084 [==============================] - 2s - loss: 0.3959 - acc: 0.8271 - val_loss: 0.4361 - val_acc: 0.8273
Epoch 2/40
8084/8084 [==============================] - 2s - loss: 0.4279 - acc: 0.8337 - val_loss: 0.4017 - val_acc: 0.8328
Epoch 3/40
8084/8084 [==============================] - 2s - loss: 0.3901 - acc: 0.8429 - val_loss: 0.5006 - val_acc: 0.7501
Epoch 4/40
8084/8084 [==============================] - 2s - loss: 0.4804 - acc: 0.7616 - val_loss: 0.4810 - val_acc: 0.7976
Epoch 5/40
8084/8084 [==============================] - 2s - loss: 0.4745 - acc: 0.7942 - val_loss: 0.5287 - val_acc: 0.7551
Epoch 6/40
8084/8084 [==============================] - 2s - loss: 0.5208 - acc: 0.7583 - val_loss: 0.5292 - val_acc: 0.7447
Epoch 7/40
8084/8084 [==============================] - 2s - loss: 0.5220 - acc: 0.7453 - val_loss: 0.5147 - val_acc: 0.7491
Epoch 8/40
8084/8084 [==============================] - 2s - loss: 0.5064 - acc: 0.7538 - val_loss: 0.4939 - val_acc: 0.7635
Epoch 9/40
8084/8084 [==============================] - 2s - loss: 0.4822 - acc: 0.7762 - val_loss: 0.4881 - val_acc: 0.7947
Epoch 10/40
8084/8084 [==============================] - 2s - loss: 0.4719 - acc: 0.8031 - val_loss: 0.4858 - val_acc: 0.7887
Epoch 11/40
8084/8084 [==============================] - 2s - loss: 0.4649 - acc: 0.8005 - val_loss: 0.4755 - val_acc: 0.7912
Epoch 12/40
8084/8084 [==============================] - 2s - loss: 0.4524 - acc: 0.8017 - val_loss: 0.4772 - val_acc: 0.7937
Epoch 13/40
8084/8084 [==============================] - 2s - loss: 0.4531 - acc: 0.8037 - val_loss: 0.4742 - val_acc: 0.7942
Epoch 14/40
8084/8084 [==============================] - 2s - loss: 0.4504 - acc: 0.8058 - val_loss: 0.4583 - val_acc: 0.7942
Epoch 15/40
8084/8084 [==============================] - 2s - loss: 0.4375 - acc: 0.8073 - val_loss: 0.4528 - val_acc: 0.7947
Epoch 16/40
8084/8084 [==============================] - 2s - loss: 0.4319 - acc: 0.8055 - val_loss: 0.4516 - val_acc: 0.8011
Epoch 17/40
8084/8084 [==============================] - 2s - loss: 0.4322 - acc: 0.8084 - val_loss: 0.4404 - val_acc: 0.8006
Epoch 18/40
8084/8084 [==============================] - 2s - loss: 0.4228 - acc: 0.8163 - val_loss: 0.4419 - val_acc: 0.8055
Epoch 19/40
8084/8084 [==============================] - 2s - loss: 0.4248 - acc: 0.8180 - val_loss: 0.4378 - val_acc: 0.8055
Epoch 20/40
8084/8084 [==============================] - 2s - loss: 0.4212 - acc: 0.8190 - val_loss: 0.4282 - val_acc: 0.8090
Epoch 21/40
8084/8084 [==============================] - 2s - loss: 0.4124 - acc: 0.8234 - val_loss: 0.4262 - val_acc: 0.8154
Epoch 22/40
8084/8084 [==============================] - 2s - loss: 0.4127 - acc: 0.8216 - val_loss: 0.4281 - val_acc: 0.8169
Epoch 23/40
8084/8084 [==============================] - 2s - loss: 0.4130 - acc: 0.8221 - val_loss: 0.4209 - val_acc: 0.8194
Epoch 24/40
8084/8084 [==============================] - 2s - loss: 0.4064 - acc: 0.8253 - val_loss: 0.4192 - val_acc: 0.8199
Epoch 25/40
8084/8084 [==============================] - 2s - loss: 0.4061 - acc: 0.8304 - val_loss: 0.4182 - val_acc: 0.8224
Epoch 26/40
8084/8084 [==============================] - 2s - loss: 0.4065 - acc: 0.8307 - val_loss: 0.4140 - val_acc: 0.8238
Epoch 27/40
8084/8084 [==============================] - 2s - loss: 0.4025 - acc: 0.8308 - val_loss: 0.4141 - val_acc: 0.8229
Epoch 28/40
8084/8084 [==============================] - 2s - loss: 0.4021 - acc: 0.8303 - val_loss: 0.4134 - val_acc: 0.8229
Epoch 29/40
8084/8084 [==============================] - 2s - loss: 0.4022 - acc: 0.8297 - val_loss: 0.4075 - val_acc: 0.8288
Epoch 30/40
8084/8084 [==============================] - 2s - loss: 0.3989 - acc: 0.8350 - val_loss: 0.4057 - val_acc: 0.8303
Epoch 31/40
8084/8084 [==============================] - 2s - loss: 0.3983 - acc: 0.8363 - val_loss: 0.4039 - val_acc: 0.8303
Epoch 32/40
8084/8084 [==============================] - 2s - loss: 0.3960 - acc: 0.8380 - val_loss: 0.4039 - val_acc: 0.8313
Epoch 33/40
8084/8084 [==============================] - 2s - loss: 0.3933 - acc: 0.8380 - val_loss: 0.4061 - val_acc: 0.8342
Epoch 34/40
8084/8084 [==============================] - 2s - loss: 0.3937 - acc: 0.8366 - val_loss: 0.4038 - val_acc: 0.8367
Epoch 35/40
8084/8084 [==============================] - 2s - loss: 0.3910 - acc: 0.8401 - val_loss: 0.4031 - val_acc: 0.8342
Epoch 36/40
8084/8084 [==============================] - 2s - loss: 0.3913 - acc: 0.8408 - val_loss: 0.4025 - val_acc: 0.8333
Epoch 37/40
8084/8084 [==============================] - 2s - loss: 0.3897 - acc: 0.8398 - val_loss: 0.4039 - val_acc: 0.8347
Epoch 38/40
8084/8084 [==============================] - 2s - loss: 0.3890 - acc: 0.8401 - val_loss: 0.4029 - val_acc: 0.8337
Epoch 39/40
8084/8084 [==============================] - 2s - loss: 0.3874 - acc: 0.8403 - val_loss: 0.4007 - val_acc: 0.8362
Epoch 40/40
8084/8084 [==============================] - 2s - loss: 0.3859 - acc: 0.8427 - val_loss: 0.4008 - val_acc: 0.8342
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-35.h5
chunk number 35
prepare data
start training
Train on 8199 samples, validate on 2050 samples
Epoch 1/40
8199/8199 [==============================] - 1s - loss: 0.4277 - acc: 0.8338 - val_loss: 0.4080 - val_acc: 0.8249
Epoch 2/40
8199/8199 [==============================] - 1s - loss: 0.3999 - acc: 0.8288 - val_loss: 0.4314 - val_acc: 0.8229
Epoch 3/40
8199/8199 [==============================] - 1s - loss: 0.4234 - acc: 0.8185 - val_loss: 0.4164 - val_acc: 0.8229
Epoch 4/40
8199/8199 [==============================] - 1s - loss: 0.4088 - acc: 0.8292 - val_loss: 0.4052 - val_acc: 0.8371
Epoch 5/40
8199/8199 [==============================] - 1s - loss: 0.3993 - acc: 0.8324 - val_loss: 0.4193 - val_acc: 0.8268
Epoch 6/40
8199/8199 [==============================] - 1s - loss: 0.4159 - acc: 0.8268 - val_loss: 0.3989 - val_acc: 0.8346
Epoch 7/40
8199/8199 [==============================] - 1s - loss: 0.3920 - acc: 0.8350 - val_loss: 0.4126 - val_acc: 0.8229
Epoch 8/40
8199/8199 [==============================] - 1s - loss: 0.4053 - acc: 0.8278 - val_loss: 0.4117 - val_acc: 0.8254
Epoch 9/40
8199/8199 [==============================] - 1s - loss: 0.4047 - acc: 0.8278 - val_loss: 0.4014 - val_acc: 0.8361
Epoch 10/40
8199/8199 [==============================] - 1s - loss: 0.3951 - acc: 0.8352 - val_loss: 0.4056 - val_acc: 0.8337
Epoch 11/40
8199/8199 [==============================] - 1s - loss: 0.4011 - acc: 0.8325 - val_loss: 0.4044 - val_acc: 0.8322
Epoch 12/40
8199/8199 [==============================] - 1s - loss: 0.3993 - acc: 0.8322 - val_loss: 0.3982 - val_acc: 0.8366
Epoch 13/40
8199/8199 [==============================] - 1s - loss: 0.3911 - acc: 0.8375 - val_loss: 0.4052 - val_acc: 0.8346
Epoch 14/40
8199/8199 [==============================] - 1s - loss: 0.3973 - acc: 0.8322 - val_loss: 0.4031 - val_acc: 0.8376
Epoch 15/40
8199/8199 [==============================] - 1s - loss: 0.3952 - acc: 0.8346 - val_loss: 0.3961 - val_acc: 0.8420
Epoch 16/40
8199/8199 [==============================] - 1s - loss: 0.3881 - acc: 0.8388 - val_loss: 0.4029 - val_acc: 0.8322
Epoch 17/40
8199/8199 [==============================] - 1s - loss: 0.3955 - acc: 0.8327 - val_loss: 0.3946 - val_acc: 0.8405
Epoch 18/40
8199/8199 [==============================] - 1s - loss: 0.3862 - acc: 0.8388 - val_loss: 0.3991 - val_acc: 0.8380
Epoch 19/40
8199/8199 [==============================] - 1s - loss: 0.3902 - acc: 0.8379 - val_loss: 0.3984 - val_acc: 0.8390
Epoch 20/40
8199/8199 [==============================] - 1s - loss: 0.3894 - acc: 0.8373 - val_loss: 0.3934 - val_acc: 0.8439
Epoch 21/40
8199/8199 [==============================] - 1s - loss: 0.3856 - acc: 0.8394 - val_loss: 0.3963 - val_acc: 0.8444
Epoch 22/40
8199/8199 [==============================] - 1s - loss: 0.3899 - acc: 0.8384 - val_loss: 0.3919 - val_acc: 0.8459
Epoch 23/40
8199/8199 [==============================] - 1s - loss: 0.3845 - acc: 0.8406 - val_loss: 0.3957 - val_acc: 0.8385
Epoch 24/40
8199/8199 [==============================] - 1s - loss: 0.3876 - acc: 0.8364 - val_loss: 0.3930 - val_acc: 0.8410
Epoch 25/40
8199/8199 [==============================] - 1s - loss: 0.3854 - acc: 0.8382 - val_loss: 0.3907 - val_acc: 0.8444
Epoch 26/40
8199/8199 [==============================] - 1s - loss: 0.3849 - acc: 0.8396 - val_loss: 0.3912 - val_acc: 0.8415
Epoch 27/40
8199/8199 [==============================] - 1s - loss: 0.3857 - acc: 0.8399 - val_loss: 0.3904 - val_acc: 0.8463
Epoch 28/40
8199/8199 [==============================] - 1s - loss: 0.3831 - acc: 0.8417 - val_loss: 0.3928 - val_acc: 0.8410
Epoch 29/40
8199/8199 [==============================] - 1s - loss: 0.3845 - acc: 0.8396 - val_loss: 0.3892 - val_acc: 0.8439
Epoch 30/40
8199/8199 [==============================] - 1s - loss: 0.3817 - acc: 0.8422 - val_loss: 0.3908 - val_acc: 0.8444
Epoch 31/40
8199/8199 [==============================] - 1s - loss: 0.3839 - acc: 0.8400 - val_loss: 0.3891 - val_acc: 0.8429
Epoch 32/40
8199/8199 [==============================] - 1s - loss: 0.3815 - acc: 0.8423 - val_loss: 0.3908 - val_acc: 0.8420
Epoch 33/40
8199/8199 [==============================] - 1s - loss: 0.3828 - acc: 0.8419 - val_loss: 0.3892 - val_acc: 0.8449
Epoch 34/40
8199/8199 [==============================] - 1s - loss: 0.3813 - acc: 0.8423 - val_loss: 0.3888 - val_acc: 0.8444
Epoch 35/40
8199/8199 [==============================] - 1s - loss: 0.3814 - acc: 0.8407 - val_loss: 0.3880 - val_acc: 0.8424
Epoch 36/40
8199/8199 [==============================] - 1s - loss: 0.3807 - acc: 0.8414 - val_loss: 0.3880 - val_acc: 0.8439
Epoch 37/40
8199/8199 [==============================] - 1s - loss: 0.3806 - acc: 0.8424 - val_loss: 0.3880 - val_acc: 0.8439
Epoch 38/40
8199/8199 [==============================] - 1s - loss: 0.3806 - acc: 0.8424 - val_loss: 0.3869 - val_acc: 0.8429
Epoch 39/40
8199/8199 [==============================] - 1s - loss: 0.3798 - acc: 0.8428 - val_loss: 0.3874 - val_acc: 0.8434
Epoch 40/40
8199/8199 [==============================] - 1s - loss: 0.3802 - acc: 0.8411 - val_loss: 0.3867 - val_acc: 0.8459
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-36.h5
chunk number 36
prepare data
start training
Train on 8126 samples, validate on 2032 samples
Epoch 1/40
8126/8126 [==============================] - 1s - loss: 0.3926 - acc: 0.8377 - val_loss: 0.3876 - val_acc: 0.8381
Epoch 2/40
8126/8126 [==============================] - 1s - loss: 0.3927 - acc: 0.8381 - val_loss: 0.3875 - val_acc: 0.8386
Epoch 3/40
8126/8126 [==============================] - 1s - loss: 0.3924 - acc: 0.8378 - val_loss: 0.3854 - val_acc: 0.8371
Epoch 4/40
8126/8126 [==============================] - 1s - loss: 0.3900 - acc: 0.8374 - val_loss: 0.3877 - val_acc: 0.8361
Epoch 5/40
8126/8126 [==============================] - 1s - loss: 0.3915 - acc: 0.8371 - val_loss: 0.3855 - val_acc: 0.8371
Epoch 6/40
8126/8126 [==============================] - 1s - loss: 0.3892 - acc: 0.8377 - val_loss: 0.3873 - val_acc: 0.8386
Epoch 7/40
8126/8126 [==============================] - 1s - loss: 0.3902 - acc: 0.8392 - val_loss: 0.3867 - val_acc: 0.8361
Epoch 8/40
8126/8126 [==============================] - 1s - loss: 0.3886 - acc: 0.8382 - val_loss: 0.3884 - val_acc: 0.8361
Epoch 9/40
8126/8126 [==============================] - 1s - loss: 0.3889 - acc: 0.8382 - val_loss: 0.3880 - val_acc: 0.8371
Epoch 10/40
8126/8126 [==============================] - 1s - loss: 0.3876 - acc: 0.8383 - val_loss: 0.3884 - val_acc: 0.8371
Epoch 11/40
8126/8126 [==============================] - 1s - loss: 0.3877 - acc: 0.8389 - val_loss: 0.3883 - val_acc: 0.8386
Epoch 12/40
8126/8126 [==============================] - 1s - loss: 0.3871 - acc: 0.8395 - val_loss: 0.3891 - val_acc: 0.8366
Epoch 13/40
8126/8126 [==============================] - 1s - loss: 0.3870 - acc: 0.8400 - val_loss: 0.3888 - val_acc: 0.8376
Epoch 14/40
8126/8126 [==============================] - 1s - loss: 0.3866 - acc: 0.8413 - val_loss: 0.3882 - val_acc: 0.8391
Epoch 15/40
8126/8126 [==============================] - 1s - loss: 0.3865 - acc: 0.8403 - val_loss: 0.3881 - val_acc: 0.8391
Epoch 16/40
8126/8126 [==============================] - 1s - loss: 0.3861 - acc: 0.8414 - val_loss: 0.3890 - val_acc: 0.8361
Epoch 17/40
8126/8126 [==============================] - 1s - loss: 0.3859 - acc: 0.8421 - val_loss: 0.3886 - val_acc: 0.8371
Epoch 18/40
8126/8126 [==============================] - 1s - loss: 0.3852 - acc: 0.8419 - val_loss: 0.3884 - val_acc: 0.8386
Epoch 19/40
8126/8126 [==============================] - 1s - loss: 0.3851 - acc: 0.8415 - val_loss: 0.3885 - val_acc: 0.8366
Epoch 20/40
8126/8126 [==============================] - 1s - loss: 0.3845 - acc: 0.8430 - val_loss: 0.3896 - val_acc: 0.8366
Epoch 21/40
8126/8126 [==============================] - 1s - loss: 0.3846 - acc: 0.8405 - val_loss: 0.3889 - val_acc: 0.8376
Epoch 22/40
8126/8126 [==============================] - 1s - loss: 0.3840 - acc: 0.8405 - val_loss: 0.3885 - val_acc: 0.8386
Epoch 23/40
8126/8126 [==============================] - 1s - loss: 0.3840 - acc: 0.8429 - val_loss: 0.3882 - val_acc: 0.8381
Epoch 24/40
8126/8126 [==============================] - 1s - loss: 0.3834 - acc: 0.8427 - val_loss: 0.3884 - val_acc: 0.8396
Epoch 25/40
8126/8126 [==============================] - 1s - loss: 0.3834 - acc: 0.8421 - val_loss: 0.3873 - val_acc: 0.8386
Epoch 26/40
8126/8126 [==============================] - 1s - loss: 0.3829 - acc: 0.8436 - val_loss: 0.3870 - val_acc: 0.8396
Epoch 27/40
8126/8126 [==============================] - 1s - loss: 0.3828 - acc: 0.8424 - val_loss: 0.3872 - val_acc: 0.8401
Epoch 28/40
8126/8126 [==============================] - 1s - loss: 0.3824 - acc: 0.8441 - val_loss: 0.3874 - val_acc: 0.8386
Epoch 29/40
8126/8126 [==============================] - 1s - loss: 0.3822 - acc: 0.8432 - val_loss: 0.3868 - val_acc: 0.8391
Epoch 30/40
8126/8126 [==============================] - 1s - loss: 0.3819 - acc: 0.8436 - val_loss: 0.3867 - val_acc: 0.8391
Epoch 31/40
8126/8126 [==============================] - 1s - loss: 0.3816 - acc: 0.8438 - val_loss: 0.3872 - val_acc: 0.8386
Epoch 32/40
8126/8126 [==============================] - 1s - loss: 0.3815 - acc: 0.8431 - val_loss: 0.3870 - val_acc: 0.8396
Epoch 33/40
8126/8126 [==============================] - 1s - loss: 0.3812 - acc: 0.8433 - val_loss: 0.3866 - val_acc: 0.8406
Epoch 34/40
8126/8126 [==============================] - 1s - loss: 0.3810 - acc: 0.8437 - val_loss: 0.3867 - val_acc: 0.8391
Epoch 35/40
8126/8126 [==============================] - 1s - loss: 0.3807 - acc: 0.8433 - val_loss: 0.3870 - val_acc: 0.8396
Epoch 36/40
8126/8126 [==============================] - 1s - loss: 0.3805 - acc: 0.8437 - val_loss: 0.3864 - val_acc: 0.8410
Epoch 37/40
8126/8126 [==============================] - 1s - loss: 0.3802 - acc: 0.8435 - val_loss: 0.3863 - val_acc: 0.8410
Epoch 38/40
8126/8126 [==============================] - 1s - loss: 0.3801 - acc: 0.8440 - val_loss: 0.3866 - val_acc: 0.8401
Epoch 39/40
8126/8126 [==============================] - 1s - loss: 0.3798 - acc: 0.8433 - val_loss: 0.3865 - val_acc: 0.8401
Epoch 40/40
8126/8126 [==============================] - 1s - loss: 0.3796 - acc: 0.8436 - val_loss: 0.3863 - val_acc: 0.8410
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-37.h5
chunk number 37
prepare data
start training
Train on 8127 samples, validate on 2032 samples
Epoch 1/40
8127/8127 [==============================] - 1s - loss: 0.3905 - acc: 0.8377 - val_loss: 0.4215 - val_acc: 0.8273
Epoch 2/40
8127/8127 [==============================] - 1s - loss: 0.3904 - acc: 0.8367 - val_loss: 0.4163 - val_acc: 0.8391
Epoch 3/40
8127/8127 [==============================] - 1s - loss: 0.3857 - acc: 0.8378 - val_loss: 0.4141 - val_acc: 0.8366
Epoch 4/40
8127/8127 [==============================] - 1s - loss: 0.3850 - acc: 0.8376 - val_loss: 0.4141 - val_acc: 0.8342
Epoch 5/40
8127/8127 [==============================] - 1s - loss: 0.3858 - acc: 0.8375 - val_loss: 0.4132 - val_acc: 0.8396
Epoch 6/40
8127/8127 [==============================] - 1s - loss: 0.3833 - acc: 0.8389 - val_loss: 0.4157 - val_acc: 0.8332
Epoch 7/40
8127/8127 [==============================] - 1s - loss: 0.3847 - acc: 0.8378 - val_loss: 0.4174 - val_acc: 0.8381
Epoch 8/40
8127/8127 [==============================] - 1s - loss: 0.3817 - acc: 0.8386 - val_loss: 0.4176 - val_acc: 0.8391
Epoch 9/40
8127/8127 [==============================] - 1s - loss: 0.3812 - acc: 0.8381 - val_loss: 0.4179 - val_acc: 0.8342
Epoch 10/40
8127/8127 [==============================] - 1s - loss: 0.3811 - acc: 0.8394 - val_loss: 0.4154 - val_acc: 0.8401
Epoch 11/40
8127/8127 [==============================] - 1s - loss: 0.3792 - acc: 0.8387 - val_loss: 0.4159 - val_acc: 0.8337
Epoch 12/40
8127/8127 [==============================] - 1s - loss: 0.3814 - acc: 0.8399 - val_loss: 0.4173 - val_acc: 0.8371
Epoch 13/40
8127/8127 [==============================] - 1s - loss: 0.3803 - acc: 0.8392 - val_loss: 0.4165 - val_acc: 0.8381
Epoch 14/40
8127/8127 [==============================] - 1s - loss: 0.3788 - acc: 0.8394 - val_loss: 0.4174 - val_acc: 0.8337
Epoch 15/40
8127/8127 [==============================] - 1s - loss: 0.3800 - acc: 0.8403 - val_loss: 0.4162 - val_acc: 0.8386
Epoch 16/40
8127/8127 [==============================] - 1s - loss: 0.3780 - acc: 0.8405 - val_loss: 0.4162 - val_acc: 0.8376
Epoch 17/40
8127/8127 [==============================] - 1s - loss: 0.3783 - acc: 0.8402 - val_loss: 0.4162 - val_acc: 0.8371
Epoch 18/40
8127/8127 [==============================] - 1s - loss: 0.3785 - acc: 0.8403 - val_loss: 0.4162 - val_acc: 0.8366
Epoch 19/40
8127/8127 [==============================] - 1s - loss: 0.3770 - acc: 0.8404 - val_loss: 0.4174 - val_acc: 0.8371
Epoch 20/40
8127/8127 [==============================] - 1s - loss: 0.3773 - acc: 0.8402 - val_loss: 0.4171 - val_acc: 0.8371
Epoch 21/40
8127/8127 [==============================] - 1s - loss: 0.3771 - acc: 0.8400 - val_loss: 0.4174 - val_acc: 0.8371
Epoch 22/40
8127/8127 [==============================] - 1s - loss: 0.3761 - acc: 0.8421 - val_loss: 0.4174 - val_acc: 0.8381
Epoch 23/40
8127/8127 [==============================] - 1s - loss: 0.3762 - acc: 0.8414 - val_loss: 0.4168 - val_acc: 0.8351
Epoch 24/40
8127/8127 [==============================] - 1s - loss: 0.3761 - acc: 0.8414 - val_loss: 0.4168 - val_acc: 0.8376
Epoch 25/40
8127/8127 [==============================] - 1s - loss: 0.3754 - acc: 0.8409 - val_loss: 0.4168 - val_acc: 0.8371
Epoch 26/40
8127/8127 [==============================] - 1s - loss: 0.3751 - acc: 0.8414 - val_loss: 0.4161 - val_acc: 0.8371
Epoch 27/40
8127/8127 [==============================] - 1s - loss: 0.3750 - acc: 0.8408 - val_loss: 0.4158 - val_acc: 0.8371
Epoch 28/40
8127/8127 [==============================] - 1s - loss: 0.3745 - acc: 0.8411 - val_loss: 0.4153 - val_acc: 0.8376
Epoch 29/40
8127/8127 [==============================] - 1s - loss: 0.3742 - acc: 0.8411 - val_loss: 0.4151 - val_acc: 0.8361
Epoch 30/40
8127/8127 [==============================] - 1s - loss: 0.3743 - acc: 0.8397 - val_loss: 0.4155 - val_acc: 0.8371
Epoch 31/40
8127/8127 [==============================] - 1s - loss: 0.3740 - acc: 0.8418 - val_loss: 0.4153 - val_acc: 0.8366
Epoch 32/40
8127/8127 [==============================] - 1s - loss: 0.3736 - acc: 0.8400 - val_loss: 0.4155 - val_acc: 0.8351
Epoch 33/40
8127/8127 [==============================] - 1s - loss: 0.3734 - acc: 0.8407 - val_loss: 0.4164 - val_acc: 0.8366
Epoch 34/40
8127/8127 [==============================] - 1s - loss: 0.3734 - acc: 0.8414 - val_loss: 0.4160 - val_acc: 0.8342
Epoch 35/40
8127/8127 [==============================] - 1s - loss: 0.3730 - acc: 0.8415 - val_loss: 0.4161 - val_acc: 0.8346
Epoch 36/40
8127/8127 [==============================] - 1s - loss: 0.3727 - acc: 0.8413 - val_loss: 0.4164 - val_acc: 0.8376
Epoch 37/40
8127/8127 [==============================] - 1s - loss: 0.3726 - acc: 0.8427 - val_loss: 0.4162 - val_acc: 0.8346
Epoch 38/40
8127/8127 [==============================] - 1s - loss: 0.3725 - acc: 0.8419 - val_loss: 0.4170 - val_acc: 0.8376
Epoch 39/40
8127/8127 [==============================] - 1s - loss: 0.3723 - acc: 0.8416 - val_loss: 0.4171 - val_acc: 0.8371
Epoch 40/40
8127/8127 [==============================] - 1s - loss: 0.3720 - acc: 0.8423 - val_loss: 0.4173 - val_acc: 0.8366
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-38.h5
chunk number 38
prepare data
start training
Train on 8040 samples, validate on 2011 samples
Epoch 1/40
8040/8040 [==============================] - 1s - loss: 0.3994 - acc: 0.8378 - val_loss: 0.4029 - val_acc: 0.8369
Epoch 2/40
8040/8040 [==============================] - 1s - loss: 0.4020 - acc: 0.8320 - val_loss: 0.3995 - val_acc: 0.8394
Epoch 3/40
8040/8040 [==============================] - 1s - loss: 0.3984 - acc: 0.8337 - val_loss: 0.4047 - val_acc: 0.8265
Epoch 4/40
8040/8040 [==============================] - 1s - loss: 0.4011 - acc: 0.8367 - val_loss: 0.3954 - val_acc: 0.8334
Epoch 5/40
8040/8040 [==============================] - 1s - loss: 0.3934 - acc: 0.8404 - val_loss: 0.3985 - val_acc: 0.8389
Epoch 6/40
8040/8040 [==============================] - 1s - loss: 0.3962 - acc: 0.8356 - val_loss: 0.3967 - val_acc: 0.8304
Epoch 7/40
8040/8040 [==============================] - 1s - loss: 0.3919 - acc: 0.8408 - val_loss: 0.3996 - val_acc: 0.8319
Epoch 8/40
8040/8040 [==============================] - 1s - loss: 0.3937 - acc: 0.8417 - val_loss: 0.3970 - val_acc: 0.8359
Epoch 9/40
8040/8040 [==============================] - 1s - loss: 0.3920 - acc: 0.8374 - val_loss: 0.3979 - val_acc: 0.8369
Epoch 10/40
8040/8040 [==============================] - 1s - loss: 0.3926 - acc: 0.8361 - val_loss: 0.3983 - val_acc: 0.8329
Epoch 11/40
8040/8040 [==============================] - 1s - loss: 0.3911 - acc: 0.8422 - val_loss: 0.3994 - val_acc: 0.8324
Epoch 12/40
8040/8040 [==============================] - 1s - loss: 0.3914 - acc: 0.8423 - val_loss: 0.3982 - val_acc: 0.8364
Epoch 13/40
8040/8040 [==============================] - 1s - loss: 0.3911 - acc: 0.8389 - val_loss: 0.3982 - val_acc: 0.8359
Epoch 14/40
8040/8040 [==============================] - 1s - loss: 0.3906 - acc: 0.8391 - val_loss: 0.3999 - val_acc: 0.8324
Epoch 15/40
8040/8040 [==============================] - 1s - loss: 0.3904 - acc: 0.8422 - val_loss: 0.3983 - val_acc: 0.8339
Epoch 16/40
8040/8040 [==============================] - 1s - loss: 0.3887 - acc: 0.8419 - val_loss: 0.3987 - val_acc: 0.8384
Epoch 17/40
8040/8040 [==============================] - 1s - loss: 0.3897 - acc: 0.8394 - val_loss: 0.3980 - val_acc: 0.8339
Epoch 18/40
8040/8040 [==============================] - 1s - loss: 0.3881 - acc: 0.8414 - val_loss: 0.3996 - val_acc: 0.8329
Epoch 19/40
8040/8040 [==============================] - 1s - loss: 0.3892 - acc: 0.8418 - val_loss: 0.3975 - val_acc: 0.8364
Epoch 20/40
8040/8040 [==============================] - 1s - loss: 0.3878 - acc: 0.8417 - val_loss: 0.3977 - val_acc: 0.8359
Epoch 21/40
8040/8040 [==============================] - 1s - loss: 0.3880 - acc: 0.8418 - val_loss: 0.3982 - val_acc: 0.8334
Epoch 22/40
8040/8040 [==============================] - 1s - loss: 0.3877 - acc: 0.8409 - val_loss: 0.3978 - val_acc: 0.8334
Epoch 23/40
8040/8040 [==============================] - 1s - loss: 0.3868 - acc: 0.8398 - val_loss: 0.3983 - val_acc: 0.8379
Epoch 24/40
8040/8040 [==============================] - 1s - loss: 0.3872 - acc: 0.8418 - val_loss: 0.3981 - val_acc: 0.8344
Epoch 25/40
8040/8040 [==============================] - 1s - loss: 0.3862 - acc: 0.8393 - val_loss: 0.3983 - val_acc: 0.8344
Epoch 26/40
8040/8040 [==============================] - 1s - loss: 0.3862 - acc: 0.8403 - val_loss: 0.3983 - val_acc: 0.8349
Epoch 27/40
8040/8040 [==============================] - 1s - loss: 0.3862 - acc: 0.8410 - val_loss: 0.3980 - val_acc: 0.8344
Epoch 28/40
8040/8040 [==============================] - 1s - loss: 0.3853 - acc: 0.8393 - val_loss: 0.3990 - val_acc: 0.8359
Epoch 29/40
8040/8040 [==============================] - 1s - loss: 0.3858 - acc: 0.8417 - val_loss: 0.3983 - val_acc: 0.8344
Epoch 30/40
8040/8040 [==============================] - 1s - loss: 0.3853 - acc: 0.8412 - val_loss: 0.3977 - val_acc: 0.8344
Epoch 31/40
8040/8040 [==============================] - 1s - loss: 0.3847 - acc: 0.8404 - val_loss: 0.3980 - val_acc: 0.8359
Epoch 32/40
8040/8040 [==============================] - 1s - loss: 0.3850 - acc: 0.8409 - val_loss: 0.3973 - val_acc: 0.8349
Epoch 33/40
8040/8040 [==============================] - 1s - loss: 0.3844 - acc: 0.8413 - val_loss: 0.3972 - val_acc: 0.8344
Epoch 34/40
8040/8040 [==============================] - 1s - loss: 0.3842 - acc: 0.8413 - val_loss: 0.3978 - val_acc: 0.8359
Epoch 35/40
8040/8040 [==============================] - 1s - loss: 0.3843 - acc: 0.8410 - val_loss: 0.3972 - val_acc: 0.8329
Epoch 36/40
8040/8040 [==============================] - 1s - loss: 0.3836 - acc: 0.8408 - val_loss: 0.3972 - val_acc: 0.8319
Epoch 37/40
8040/8040 [==============================] - 1s - loss: 0.3835 - acc: 0.8412 - val_loss: 0.3977 - val_acc: 0.8359
Epoch 38/40
8040/8040 [==============================] - 1s - loss: 0.3835 - acc: 0.8410 - val_loss: 0.3975 - val_acc: 0.8324
Epoch 39/40
8040/8040 [==============================] - 1s - loss: 0.3830 - acc: 0.8412 - val_loss: 0.3976 - val_acc: 0.8324
Epoch 40/40
8040/8040 [==============================] - 1s - loss: 0.3828 - acc: 0.8415 - val_loss: 0.3979 - val_acc: 0.8354
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-39.h5
chunk number 39
prepare data
start training
Train on 8109 samples, validate on 2028 samples
Epoch 1/40
8109/8109 [==============================] - 1s - loss: 0.3797 - acc: 0.8451 - val_loss: 0.3869 - val_acc: 0.8437
Epoch 2/40
8109/8109 [==============================] - 1s - loss: 0.3784 - acc: 0.8424 - val_loss: 0.3870 - val_acc: 0.8447
Epoch 3/40
8109/8109 [==============================] - 1s - loss: 0.3769 - acc: 0.8444 - val_loss: 0.3904 - val_acc: 0.8407
Epoch 4/40
8109/8109 [==============================] - 1s - loss: 0.3771 - acc: 0.8438 - val_loss: 0.3875 - val_acc: 0.8442
Epoch 5/40
8109/8109 [==============================] - 1s - loss: 0.3766 - acc: 0.8442 - val_loss: 0.3885 - val_acc: 0.8417
Epoch 6/40
8109/8109 [==============================] - 1s - loss: 0.3754 - acc: 0.8436 - val_loss: 0.3912 - val_acc: 0.8358
Epoch 7/40
8109/8109 [==============================] - 1s - loss: 0.3759 - acc: 0.8450 - val_loss: 0.3881 - val_acc: 0.8432
Epoch 8/40
8109/8109 [==============================] - 1s - loss: 0.3764 - acc: 0.8441 - val_loss: 0.3891 - val_acc: 0.8412
Epoch 9/40
8109/8109 [==============================] - 1s - loss: 0.3743 - acc: 0.8446 - val_loss: 0.3906 - val_acc: 0.8363
Epoch 10/40
8109/8109 [==============================] - 1s - loss: 0.3750 - acc: 0.8450 - val_loss: 0.3883 - val_acc: 0.8422
Epoch 11/40
8109/8109 [==============================] - 1s - loss: 0.3764 - acc: 0.8441 - val_loss: 0.3878 - val_acc: 0.8388
Epoch 12/40
8109/8109 [==============================] - 1s - loss: 0.3732 - acc: 0.8457 - val_loss: 0.3885 - val_acc: 0.8383
Epoch 13/40
8109/8109 [==============================] - 1s - loss: 0.3737 - acc: 0.8460 - val_loss: 0.3886 - val_acc: 0.8427
Epoch 14/40
8109/8109 [==============================] - 1s - loss: 0.3761 - acc: 0.8433 - val_loss: 0.3891 - val_acc: 0.8383
Epoch 15/40
8109/8109 [==============================] - 1s - loss: 0.3727 - acc: 0.8470 - val_loss: 0.3887 - val_acc: 0.8388
Epoch 16/40
8109/8109 [==============================] - 1s - loss: 0.3717 - acc: 0.8477 - val_loss: 0.3883 - val_acc: 0.8437
Epoch 17/40
8109/8109 [==============================] - 1s - loss: 0.3729 - acc: 0.8451 - val_loss: 0.3915 - val_acc: 0.8348
Epoch 18/40
8109/8109 [==============================] - 1s - loss: 0.3722 - acc: 0.8466 - val_loss: 0.3890 - val_acc: 0.8422
Epoch 19/40
8109/8109 [==============================] - 1s - loss: 0.3712 - acc: 0.8459 - val_loss: 0.3899 - val_acc: 0.8412
Epoch 20/40
8109/8109 [==============================] - 1s - loss: 0.3705 - acc: 0.8462 - val_loss: 0.3916 - val_acc: 0.8348
Epoch 21/40
8109/8109 [==============================] - 1s - loss: 0.3708 - acc: 0.8463 - val_loss: 0.3896 - val_acc: 0.8447
Epoch 22/40
8109/8109 [==============================] - 1s - loss: 0.3720 - acc: 0.8456 - val_loss: 0.3932 - val_acc: 0.8319
Epoch 23/40
8109/8109 [==============================] - 1s - loss: 0.3719 - acc: 0.8461 - val_loss: 0.3900 - val_acc: 0.8442
Epoch 24/40
8109/8109 [==============================] - 1s - loss: 0.3734 - acc: 0.8442 - val_loss: 0.3922 - val_acc: 0.8338
Epoch 25/40
8109/8109 [==============================] - 1s - loss: 0.3715 - acc: 0.8463 - val_loss: 0.3892 - val_acc: 0.8407
Epoch 26/40
8109/8109 [==============================] - 1s - loss: 0.3708 - acc: 0.8452 - val_loss: 0.3900 - val_acc: 0.8363
Epoch 27/40
8109/8109 [==============================] - 1s - loss: 0.3694 - acc: 0.8471 - val_loss: 0.3888 - val_acc: 0.8397
Epoch 28/40
8109/8109 [==============================] - 1s - loss: 0.3688 - acc: 0.8465 - val_loss: 0.3894 - val_acc: 0.8368
Epoch 29/40
8109/8109 [==============================] - 1s - loss: 0.3684 - acc: 0.8467 - val_loss: 0.3893 - val_acc: 0.8397
Epoch 30/40
8109/8109 [==============================] - 1s - loss: 0.3681 - acc: 0.8461 - val_loss: 0.3899 - val_acc: 0.8378
Epoch 31/40
8109/8109 [==============================] - 1s - loss: 0.3679 - acc: 0.8463 - val_loss: 0.3901 - val_acc: 0.8378
Epoch 32/40
8109/8109 [==============================] - 1s - loss: 0.3677 - acc: 0.8457 - val_loss: 0.3906 - val_acc: 0.8378
Epoch 33/40
8109/8109 [==============================] - 1s - loss: 0.3676 - acc: 0.8460 - val_loss: 0.3903 - val_acc: 0.8378
Epoch 34/40
8109/8109 [==============================] - 1s - loss: 0.3674 - acc: 0.8456 - val_loss: 0.3912 - val_acc: 0.8338
Epoch 35/40
8109/8109 [==============================] - 1s - loss: 0.3675 - acc: 0.8466 - val_loss: 0.3901 - val_acc: 0.8427
Epoch 36/40
8109/8109 [==============================] - 1s - loss: 0.3688 - acc: 0.8450 - val_loss: 0.3960 - val_acc: 0.8259
Epoch 37/40
8109/8109 [==============================] - 1s - loss: 0.3731 - acc: 0.8430 - val_loss: 0.4016 - val_acc: 0.8358
Epoch 38/40
8109/8109 [==============================] - 1s - loss: 0.3928 - acc: 0.8371 - val_loss: 0.3893 - val_acc: 0.8348
Epoch 39/40
8109/8109 [==============================] - 1s - loss: 0.3674 - acc: 0.8477 - val_loss: 0.4104 - val_acc: 0.8156
Epoch 40/40
8109/8109 [==============================] - 1s - loss: 0.3919 - acc: 0.8282 - val_loss: 0.4336 - val_acc: 0.8269
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-40.h5
chunk number 40
prepare data
start training
Train on 8199 samples, validate on 2050 samples
Epoch 1/40
8199/8199 [==============================] - 2s - loss: 0.4703 - acc: 0.8213 - val_loss: 0.4931 - val_acc: 0.7971
Epoch 2/40
8199/8199 [==============================] - 2s - loss: 0.4719 - acc: 0.8110 - val_loss: 0.4657 - val_acc: 0.8088
Epoch 3/40
8199/8199 [==============================] - 2s - loss: 0.4503 - acc: 0.8158 - val_loss: 0.4301 - val_acc: 0.8259
Epoch 4/40
8199/8199 [==============================] - 2s - loss: 0.4248 - acc: 0.8288 - val_loss: 0.4121 - val_acc: 0.8351
Epoch 5/40
8199/8199 [==============================] - 2s - loss: 0.4182 - acc: 0.8299 - val_loss: 0.4246 - val_acc: 0.8190
Epoch 6/40
8199/8199 [==============================] - 2s - loss: 0.4354 - acc: 0.8090 - val_loss: 0.4031 - val_acc: 0.8361
Epoch 7/40
8199/8199 [==============================] - 2s - loss: 0.4080 - acc: 0.8325 - val_loss: 0.4250 - val_acc: 0.8263
Epoch 8/40
8199/8199 [==============================] - 2s - loss: 0.4211 - acc: 0.8278 - val_loss: 0.4287 - val_acc: 0.8239
Epoch 9/40
8199/8199 [==============================] - 2s - loss: 0.4227 - acc: 0.8289 - val_loss: 0.4075 - val_acc: 0.8322
Epoch 10/40
8199/8199 [==============================] - 2s - loss: 0.4080 - acc: 0.8339 - val_loss: 0.3910 - val_acc: 0.8390
Epoch 11/40
8199/8199 [==============================] - 2s - loss: 0.4007 - acc: 0.8358 - val_loss: 0.4024 - val_acc: 0.8337
Epoch 12/40
8199/8199 [==============================] - 2s - loss: 0.4158 - acc: 0.8173 - val_loss: 0.3917 - val_acc: 0.8420
Epoch 13/40
8199/8199 [==============================] - 2s - loss: 0.3948 - acc: 0.8386 - val_loss: 0.4108 - val_acc: 0.8244
Epoch 14/40
8199/8199 [==============================] - 2s - loss: 0.4057 - acc: 0.8335 - val_loss: 0.4143 - val_acc: 0.8171
Epoch 15/40
8199/8199 [==============================] - 2s - loss: 0.4081 - acc: 0.8310 - val_loss: 0.3977 - val_acc: 0.8293
Epoch 16/40
8199/8199 [==============================] - 2s - loss: 0.3977 - acc: 0.8372 - val_loss: 0.3913 - val_acc: 0.8434
Epoch 17/40
8199/8199 [==============================] - 2s - loss: 0.4028 - acc: 0.8392 - val_loss: 0.3905 - val_acc: 0.8424
Epoch 18/40
8199/8199 [==============================] - 2s - loss: 0.3982 - acc: 0.8408 - val_loss: 0.3958 - val_acc: 0.8268
Epoch 19/40
8199/8199 [==============================] - 2s - loss: 0.3943 - acc: 0.8374 - val_loss: 0.4020 - val_acc: 0.8220
Epoch 20/40
8199/8199 [==============================] - 2s - loss: 0.3984 - acc: 0.8335 - val_loss: 0.3973 - val_acc: 0.8239
Epoch 21/40
8199/8199 [==============================] - 2s - loss: 0.3952 - acc: 0.8369 - val_loss: 0.3889 - val_acc: 0.8454
Epoch 22/40
8199/8199 [==============================] - 2s - loss: 0.3912 - acc: 0.8386 - val_loss: 0.3898 - val_acc: 0.8449
Epoch 23/40
8199/8199 [==============================] - 2s - loss: 0.3956 - acc: 0.8384 - val_loss: 0.3856 - val_acc: 0.8444
Epoch 24/40
8199/8199 [==============================] - 2s - loss: 0.3887 - acc: 0.8384 - val_loss: 0.3923 - val_acc: 0.8376
Epoch 25/40
8199/8199 [==============================] - 2s - loss: 0.3914 - acc: 0.8373 - val_loss: 0.3902 - val_acc: 0.8400
Epoch 26/40
8199/8199 [==============================] - 2s - loss: 0.3902 - acc: 0.8380 - val_loss: 0.3835 - val_acc: 0.8468
Epoch 27/40
8199/8199 [==============================] - 2s - loss: 0.3875 - acc: 0.8392 - val_loss: 0.3847 - val_acc: 0.8420
Epoch 28/40
8199/8199 [==============================] - 2s - loss: 0.3900 - acc: 0.8397 - val_loss: 0.3870 - val_acc: 0.8400
Epoch 29/40
8199/8199 [==============================] - 2s - loss: 0.3865 - acc: 0.8419 - val_loss: 0.3917 - val_acc: 0.8356
Epoch 30/40
8199/8199 [==============================] - 2s - loss: 0.3888 - acc: 0.8410 - val_loss: 0.3871 - val_acc: 0.8400
Epoch 31/40
8199/8199 [==============================] - 2s - loss: 0.3854 - acc: 0.8413 - val_loss: 0.3883 - val_acc: 0.8415
Epoch 32/40
8199/8199 [==============================] - 2s - loss: 0.3897 - acc: 0.8427 - val_loss: 0.3870 - val_acc: 0.8395
Epoch 33/40
8199/8199 [==============================] - 2s - loss: 0.3848 - acc: 0.8413 - val_loss: 0.3908 - val_acc: 0.8356
Epoch 34/40
8199/8199 [==============================] - 2s - loss: 0.3867 - acc: 0.8418 - val_loss: 0.3890 - val_acc: 0.8390
Epoch 35/40
8199/8199 [==============================] - 2s - loss: 0.3853 - acc: 0.8408 - val_loss: 0.3846 - val_acc: 0.8439
Epoch 36/40
8199/8199 [==============================] - 2s - loss: 0.3844 - acc: 0.8419 - val_loss: 0.3838 - val_acc: 0.8434
Epoch 37/40
8199/8199 [==============================] - 2s - loss: 0.3836 - acc: 0.8423 - val_loss: 0.3893 - val_acc: 0.8395
Epoch 38/40
8199/8199 [==============================] - 2s - loss: 0.3856 - acc: 0.8412 - val_loss: 0.3872 - val_acc: 0.8424
Epoch 39/40
8199/8199 [==============================] - 2s - loss: 0.3838 - acc: 0.8432 - val_loss: 0.3845 - val_acc: 0.8415
Epoch 40/40
8199/8199 [==============================] - 2s - loss: 0.3869 - acc: 0.8419 - val_loss: 0.3842 - val_acc: 0.8429
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-41.h5
chunk number 41
prepare data
start training
Train on 8120 samples, validate on 2031 samples
Epoch 1/40
8120/8120 [==============================] - 1s - loss: 0.3976 - acc: 0.8371 - val_loss: 0.3805 - val_acc: 0.8365
Epoch 2/40
8120/8120 [==============================] - 1s - loss: 0.3937 - acc: 0.8334 - val_loss: 0.3730 - val_acc: 0.8395
Epoch 3/40
8120/8120 [==============================] - 1s - loss: 0.3883 - acc: 0.8367 - val_loss: 0.3711 - val_acc: 0.8424
Epoch 4/40
8120/8120 [==============================] - 1s - loss: 0.3877 - acc: 0.8368 - val_loss: 0.3722 - val_acc: 0.8410
Epoch 5/40
8120/8120 [==============================] - 1s - loss: 0.3896 - acc: 0.8351 - val_loss: 0.3701 - val_acc: 0.8469
Epoch 6/40
8120/8120 [==============================] - 1s - loss: 0.3894 - acc: 0.8390 - val_loss: 0.3687 - val_acc: 0.8454
Epoch 7/40
8120/8120 [==============================] - 1s - loss: 0.3874 - acc: 0.8383 - val_loss: 0.3711 - val_acc: 0.8449
Epoch 8/40
8120/8120 [==============================] - 1s - loss: 0.3861 - acc: 0.8392 - val_loss: 0.3699 - val_acc: 0.8474
Epoch 9/40
8120/8120 [==============================] - 1s - loss: 0.3852 - acc: 0.8395 - val_loss: 0.3689 - val_acc: 0.8464
Epoch 10/40
8120/8120 [==============================] - 1s - loss: 0.3854 - acc: 0.8400 - val_loss: 0.3697 - val_acc: 0.8454
Epoch 11/40
8120/8120 [==============================] - 1s - loss: 0.3854 - acc: 0.8416 - val_loss: 0.3703 - val_acc: 0.8488
Epoch 12/40
8120/8120 [==============================] - 1s - loss: 0.3840 - acc: 0.8411 - val_loss: 0.3709 - val_acc: 0.8469
Epoch 13/40
8120/8120 [==============================] - 1s - loss: 0.3837 - acc: 0.8411 - val_loss: 0.3689 - val_acc: 0.8439
Epoch 14/40
8120/8120 [==============================] - 1s - loss: 0.3826 - acc: 0.8419 - val_loss: 0.3688 - val_acc: 0.8429
Epoch 15/40
8120/8120 [==============================] - 1s - loss: 0.3823 - acc: 0.8415 - val_loss: 0.3702 - val_acc: 0.8449
Epoch 16/40
8120/8120 [==============================] - 1s - loss: 0.3821 - acc: 0.8415 - val_loss: 0.3690 - val_acc: 0.8444
Epoch 17/40
8120/8120 [==============================] - 1s - loss: 0.3817 - acc: 0.8422 - val_loss: 0.3674 - val_acc: 0.8449
Epoch 18/40
8120/8120 [==============================] - 1s - loss: 0.3819 - acc: 0.8422 - val_loss: 0.3662 - val_acc: 0.8464
Epoch 19/40
8120/8120 [==============================] - 1s - loss: 0.3810 - acc: 0.8436 - val_loss: 0.3671 - val_acc: 0.8484
Epoch 20/40
8120/8120 [==============================] - 1s - loss: 0.3809 - acc: 0.8425 - val_loss: 0.3649 - val_acc: 0.8479
Epoch 21/40
8120/8120 [==============================] - 1s - loss: 0.3794 - acc: 0.8422 - val_loss: 0.3646 - val_acc: 0.8464
Epoch 22/40
8120/8120 [==============================] - 1s - loss: 0.3795 - acc: 0.8437 - val_loss: 0.3648 - val_acc: 0.8474
Epoch 23/40
8120/8120 [==============================] - 1s - loss: 0.3787 - acc: 0.8425 - val_loss: 0.3665 - val_acc: 0.8474
Epoch 24/40
8120/8120 [==============================] - 1s - loss: 0.3791 - acc: 0.8420 - val_loss: 0.3652 - val_acc: 0.8449
Epoch 25/40
8120/8120 [==============================] - 1s - loss: 0.3785 - acc: 0.8433 - val_loss: 0.3653 - val_acc: 0.8449
Epoch 26/40
8120/8120 [==============================] - 1s - loss: 0.3784 - acc: 0.8430 - val_loss: 0.3662 - val_acc: 0.8454
Epoch 27/40
8120/8120 [==============================] - 1s - loss: 0.3782 - acc: 0.8417 - val_loss: 0.3649 - val_acc: 0.8464
Epoch 28/40
8120/8120 [==============================] - 1s - loss: 0.3774 - acc: 0.8429 - val_loss: 0.3643 - val_acc: 0.8469
Epoch 29/40
8120/8120 [==============================] - 1s - loss: 0.3774 - acc: 0.8430 - val_loss: 0.3645 - val_acc: 0.8493
Epoch 30/40
8120/8120 [==============================] - 1s - loss: 0.3768 - acc: 0.8432 - val_loss: 0.3656 - val_acc: 0.8498
Epoch 31/40
8120/8120 [==============================] - 1s - loss: 0.3770 - acc: 0.8419 - val_loss: 0.3646 - val_acc: 0.8493
Epoch 32/40
8120/8120 [==============================] - 1s - loss: 0.3766 - acc: 0.8436 - val_loss: 0.3650 - val_acc: 0.8479
Epoch 33/40
8120/8120 [==============================] - 1s - loss: 0.3763 - acc: 0.8438 - val_loss: 0.3666 - val_acc: 0.8484
Epoch 34/40
8120/8120 [==============================] - 1s - loss: 0.3760 - acc: 0.8440 - val_loss: 0.3662 - val_acc: 0.8474
Epoch 35/40
8120/8120 [==============================] - 1s - loss: 0.3756 - acc: 0.8450 - val_loss: 0.3660 - val_acc: 0.8474
Epoch 36/40
8120/8120 [==============================] - 1s - loss: 0.3755 - acc: 0.8447 - val_loss: 0.3669 - val_acc: 0.8479
Epoch 37/40
8120/8120 [==============================] - 1s - loss: 0.3752 - acc: 0.8442 - val_loss: 0.3663 - val_acc: 0.8469
Epoch 38/40
8120/8120 [==============================] - 1s - loss: 0.3749 - acc: 0.8446 - val_loss: 0.3654 - val_acc: 0.8479
Epoch 39/40
8120/8120 [==============================] - 1s - loss: 0.3748 - acc: 0.8442 - val_loss: 0.3659 - val_acc: 0.8474
Epoch 40/40
8120/8120 [==============================] - 1s - loss: 0.3745 - acc: 0.8440 - val_loss: 0.3657 - val_acc: 0.8464
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-42.h5
chunk number 42
prepare data
start training
Train on 8105 samples, validate on 2027 samples
Epoch 1/40
8105/8105 [==============================] - 1s - loss: 0.3758 - acc: 0.8500 - val_loss: 0.3592 - val_acc: 0.8559
Epoch 2/40
8105/8105 [==============================] - 1s - loss: 0.3767 - acc: 0.8471 - val_loss: 0.3618 - val_acc: 0.8535
Epoch 3/40
8105/8105 [==============================] - 1s - loss: 0.3761 - acc: 0.8506 - val_loss: 0.3579 - val_acc: 0.8579
Epoch 4/40
8105/8105 [==============================] - 1s - loss: 0.3722 - acc: 0.8513 - val_loss: 0.3607 - val_acc: 0.8550
Epoch 5/40
8105/8105 [==============================] - 1s - loss: 0.3760 - acc: 0.8490 - val_loss: 0.3646 - val_acc: 0.8540
Epoch 6/40
8105/8105 [==============================] - 1s - loss: 0.3757 - acc: 0.8508 - val_loss: 0.3609 - val_acc: 0.8569
Epoch 7/40
8105/8105 [==============================] - 1s - loss: 0.3724 - acc: 0.8503 - val_loss: 0.3640 - val_acc: 0.8495
Epoch 8/40
8105/8105 [==============================] - 1s - loss: 0.3770 - acc: 0.8444 - val_loss: 0.3632 - val_acc: 0.8545
Epoch 9/40
8105/8105 [==============================] - 1s - loss: 0.3735 - acc: 0.8497 - val_loss: 0.3628 - val_acc: 0.8545
Epoch 10/40
8105/8105 [==============================] - 1s - loss: 0.3732 - acc: 0.8496 - val_loss: 0.3620 - val_acc: 0.8490
Epoch 11/40
8105/8105 [==============================] - 1s - loss: 0.3746 - acc: 0.8459 - val_loss: 0.3582 - val_acc: 0.8559
Epoch 12/40
8105/8105 [==============================] - 1s - loss: 0.3700 - acc: 0.8510 - val_loss: 0.3624 - val_acc: 0.8555
Epoch 13/40
8105/8105 [==============================] - 1s - loss: 0.3720 - acc: 0.8508 - val_loss: 0.3580 - val_acc: 0.8559
Epoch 14/40
8105/8105 [==============================] - 1s - loss: 0.3697 - acc: 0.8508 - val_loss: 0.3589 - val_acc: 0.8535
Epoch 15/40
8105/8105 [==============================] - 1s - loss: 0.3712 - acc: 0.8511 - val_loss: 0.3603 - val_acc: 0.8555
Epoch 16/40
8105/8105 [==============================] - 1s - loss: 0.3697 - acc: 0.8512 - val_loss: 0.3602 - val_acc: 0.8550
Epoch 17/40
8105/8105 [==============================] - 1s - loss: 0.3693 - acc: 0.8512 - val_loss: 0.3585 - val_acc: 0.8555
Epoch 18/40
8105/8105 [==============================] - 1s - loss: 0.3696 - acc: 0.8527 - val_loss: 0.3581 - val_acc: 0.8555
Epoch 19/40
8105/8105 [==============================] - 1s - loss: 0.3680 - acc: 0.8516 - val_loss: 0.3609 - val_acc: 0.8535
Epoch 20/40
8105/8105 [==============================] - 1s - loss: 0.3693 - acc: 0.8511 - val_loss: 0.3579 - val_acc: 0.8555
Epoch 21/40
8105/8105 [==============================] - 1s - loss: 0.3679 - acc: 0.8527 - val_loss: 0.3578 - val_acc: 0.8559
Epoch 22/40
8105/8105 [==============================] - 1s - loss: 0.3678 - acc: 0.8524 - val_loss: 0.3597 - val_acc: 0.8535
Epoch 23/40
8105/8105 [==============================] - 1s - loss: 0.3683 - acc: 0.8505 - val_loss: 0.3575 - val_acc: 0.8559
Epoch 24/40
8105/8105 [==============================] - 1s - loss: 0.3670 - acc: 0.8519 - val_loss: 0.3573 - val_acc: 0.8545
Epoch 25/40
8105/8105 [==============================] - 1s - loss: 0.3677 - acc: 0.8533 - val_loss: 0.3585 - val_acc: 0.8535
Epoch 26/40
8105/8105 [==============================] - 1s - loss: 0.3674 - acc: 0.8513 - val_loss: 0.3565 - val_acc: 0.8569
Epoch 27/40
8105/8105 [==============================] - 1s - loss: 0.3662 - acc: 0.8527 - val_loss: 0.3565 - val_acc: 0.8515
Epoch 28/40
8105/8105 [==============================] - 1s - loss: 0.3671 - acc: 0.8539 - val_loss: 0.3579 - val_acc: 0.8550
Epoch 29/40
8105/8105 [==============================] - 1s - loss: 0.3668 - acc: 0.8518 - val_loss: 0.3561 - val_acc: 0.8555
Epoch 30/40
8105/8105 [==============================] - 1s - loss: 0.3658 - acc: 0.8534 - val_loss: 0.3560 - val_acc: 0.8530
Epoch 31/40
8105/8105 [==============================] - 1s - loss: 0.3662 - acc: 0.8534 - val_loss: 0.3574 - val_acc: 0.8564
Epoch 32/40
8105/8105 [==============================] - 1s - loss: 0.3662 - acc: 0.8521 - val_loss: 0.3559 - val_acc: 0.8555
Epoch 33/40
8105/8105 [==============================] - 1s - loss: 0.3652 - acc: 0.8540 - val_loss: 0.3562 - val_acc: 0.8530
Epoch 34/40
8105/8105 [==============================] - 1s - loss: 0.3653 - acc: 0.8535 - val_loss: 0.3576 - val_acc: 0.8564
Epoch 35/40
8105/8105 [==============================] - 1s - loss: 0.3655 - acc: 0.8523 - val_loss: 0.3564 - val_acc: 0.8535
Epoch 36/40
8105/8105 [==============================] - 1s - loss: 0.3648 - acc: 0.8542 - val_loss: 0.3565 - val_acc: 0.8550
Epoch 37/40
8105/8105 [==============================] - 1s - loss: 0.3646 - acc: 0.8542 - val_loss: 0.3576 - val_acc: 0.8564
Epoch 38/40
8105/8105 [==============================] - 1s - loss: 0.3648 - acc: 0.8529 - val_loss: 0.3565 - val_acc: 0.8520
Epoch 39/40
8105/8105 [==============================] - 1s - loss: 0.3646 - acc: 0.8547 - val_loss: 0.3566 - val_acc: 0.8564
Epoch 40/40
8105/8105 [==============================] - 1s - loss: 0.3641 - acc: 0.8533 - val_loss: 0.3565 - val_acc: 0.8564
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-43.h5
chunk number 43
prepare data
start training
Train on 8189 samples, validate on 2048 samples
Epoch 1/40
8189/8189 [==============================] - 1s - loss: 0.3915 - acc: 0.8359 - val_loss: 0.3814 - val_acc: 0.8481
Epoch 2/40
8189/8189 [==============================] - 1s - loss: 0.3906 - acc: 0.8360 - val_loss: 0.3816 - val_acc: 0.8501
Epoch 3/40
8189/8189 [==============================] - 1s - loss: 0.3894 - acc: 0.8373 - val_loss: 0.3824 - val_acc: 0.8511
Epoch 4/40
8189/8189 [==============================] - 1s - loss: 0.3888 - acc: 0.8376 - val_loss: 0.3830 - val_acc: 0.8511
Epoch 5/40
8189/8189 [==============================] - 1s - loss: 0.3886 - acc: 0.8386 - val_loss: 0.3837 - val_acc: 0.8516
Epoch 6/40
8189/8189 [==============================] - 1s - loss: 0.3885 - acc: 0.8388 - val_loss: 0.3840 - val_acc: 0.8525
Epoch 7/40
8189/8189 [==============================] - 1s - loss: 0.3882 - acc: 0.8389 - val_loss: 0.3835 - val_acc: 0.8521
Epoch 8/40
8189/8189 [==============================] - 1s - loss: 0.3877 - acc: 0.8387 - val_loss: 0.3829 - val_acc: 0.8506
Epoch 9/40
8189/8189 [==============================] - 1s - loss: 0.3872 - acc: 0.8391 - val_loss: 0.3824 - val_acc: 0.8506
Epoch 10/40
8189/8189 [==============================] - 1s - loss: 0.3868 - acc: 0.8391 - val_loss: 0.3818 - val_acc: 0.8516
Epoch 11/40
8189/8189 [==============================] - 1s - loss: 0.3866 - acc: 0.8386 - val_loss: 0.3813 - val_acc: 0.8496
Epoch 12/40
8189/8189 [==============================] - 1s - loss: 0.3863 - acc: 0.8382 - val_loss: 0.3812 - val_acc: 0.8506
Epoch 13/40
8189/8189 [==============================] - 1s - loss: 0.3861 - acc: 0.8381 - val_loss: 0.3812 - val_acc: 0.8496
Epoch 14/40
8189/8189 [==============================] - 1s - loss: 0.3858 - acc: 0.8384 - val_loss: 0.3810 - val_acc: 0.8481
Epoch 15/40
8189/8189 [==============================] - 1s - loss: 0.3855 - acc: 0.8389 - val_loss: 0.3813 - val_acc: 0.8506
Epoch 16/40
8189/8189 [==============================] - 1s - loss: 0.3852 - acc: 0.8384 - val_loss: 0.3816 - val_acc: 0.8530
Epoch 17/40
8189/8189 [==============================] - 1s - loss: 0.3849 - acc: 0.8388 - val_loss: 0.3817 - val_acc: 0.8525
Epoch 18/40
8189/8189 [==============================] - 1s - loss: 0.3846 - acc: 0.8394 - val_loss: 0.3820 - val_acc: 0.8516
Epoch 19/40
8189/8189 [==============================] - 1s - loss: 0.3845 - acc: 0.8391 - val_loss: 0.3821 - val_acc: 0.8525
Epoch 20/40
8189/8189 [==============================] - 1s - loss: 0.3843 - acc: 0.8395 - val_loss: 0.3816 - val_acc: 0.8530
Epoch 21/40
8189/8189 [==============================] - 1s - loss: 0.3841 - acc: 0.8395 - val_loss: 0.3810 - val_acc: 0.8535
Epoch 22/40
8189/8189 [==============================] - 1s - loss: 0.3837 - acc: 0.8402 - val_loss: 0.3804 - val_acc: 0.8540
Epoch 23/40
8189/8189 [==============================] - 1s - loss: 0.3834 - acc: 0.8394 - val_loss: 0.3797 - val_acc: 0.8545
Epoch 24/40
8189/8189 [==============================] - 1s - loss: 0.3832 - acc: 0.8393 - val_loss: 0.3793 - val_acc: 0.8540
Epoch 25/40
8189/8189 [==============================] - 1s - loss: 0.3830 - acc: 0.8399 - val_loss: 0.3792 - val_acc: 0.8535
Epoch 26/40
8189/8189 [==============================] - 1s - loss: 0.3828 - acc: 0.8398 - val_loss: 0.3791 - val_acc: 0.8540
Epoch 27/40
8189/8189 [==============================] - 1s - loss: 0.3826 - acc: 0.8400 - val_loss: 0.3794 - val_acc: 0.8535
Epoch 28/40
8189/8189 [==============================] - 1s - loss: 0.3824 - acc: 0.8405 - val_loss: 0.3797 - val_acc: 0.8535
Epoch 29/40
8189/8189 [==============================] - 1s - loss: 0.3822 - acc: 0.8408 - val_loss: 0.3798 - val_acc: 0.8540
Epoch 30/40
8189/8189 [==============================] - 1s - loss: 0.3821 - acc: 0.8406 - val_loss: 0.3801 - val_acc: 0.8540
Epoch 31/40
8189/8189 [==============================] - 1s - loss: 0.3819 - acc: 0.8404 - val_loss: 0.3799 - val_acc: 0.8545
Epoch 32/40
8189/8189 [==============================] - 1s - loss: 0.3817 - acc: 0.8404 - val_loss: 0.3799 - val_acc: 0.8550
Epoch 33/40
8189/8189 [==============================] - 1s - loss: 0.3816 - acc: 0.8403 - val_loss: 0.3797 - val_acc: 0.8530
Epoch 34/40
8189/8189 [==============================] - 1s - loss: 0.3814 - acc: 0.8410 - val_loss: 0.3798 - val_acc: 0.8540
Epoch 35/40
8189/8189 [==============================] - 1s - loss: 0.3812 - acc: 0.8400 - val_loss: 0.3796 - val_acc: 0.8525
Epoch 36/40
8189/8189 [==============================] - 1s - loss: 0.3810 - acc: 0.8406 - val_loss: 0.3798 - val_acc: 0.8545
Epoch 37/40
8189/8189 [==============================] - 1s - loss: 0.3809 - acc: 0.8403 - val_loss: 0.3796 - val_acc: 0.8521
Epoch 38/40
8189/8189 [==============================] - 1s - loss: 0.3808 - acc: 0.8408 - val_loss: 0.3800 - val_acc: 0.8540
Epoch 39/40
8189/8189 [==============================] - 1s - loss: 0.3806 - acc: 0.8405 - val_loss: 0.3797 - val_acc: 0.8496
Epoch 40/40
8189/8189 [==============================] - 1s - loss: 0.3805 - acc: 0.8408 - val_loss: 0.3803 - val_acc: 0.8545
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-44.h5
chunk number 44
prepare data
start training
Train on 8052 samples, validate on 2013 samples
Epoch 1/40
8052/8052 [==============================] - 1s - loss: 0.3822 - acc: 0.8412 - val_loss: 0.3800 - val_acc: 0.8395
Epoch 2/40
8052/8052 [==============================] - 1s - loss: 0.3821 - acc: 0.8418 - val_loss: 0.3797 - val_acc: 0.8415
Epoch 3/40
8052/8052 [==============================] - 1s - loss: 0.3813 - acc: 0.8412 - val_loss: 0.3804 - val_acc: 0.8425
Epoch 4/40
8052/8052 [==============================] - 1s - loss: 0.3812 - acc: 0.8404 - val_loss: 0.3792 - val_acc: 0.8390
Epoch 5/40
8052/8052 [==============================] - 1s - loss: 0.3807 - acc: 0.8422 - val_loss: 0.3798 - val_acc: 0.8400
Epoch 6/40
8052/8052 [==============================] - 1s - loss: 0.3796 - acc: 0.8410 - val_loss: 0.3808 - val_acc: 0.8425
Epoch 7/40
8052/8052 [==============================] - 1s - loss: 0.3797 - acc: 0.8413 - val_loss: 0.3802 - val_acc: 0.8405
Epoch 8/40
8052/8052 [==============================] - 1s - loss: 0.3795 - acc: 0.8419 - val_loss: 0.3801 - val_acc: 0.8400
Epoch 9/40
8052/8052 [==============================] - 1s - loss: 0.3781 - acc: 0.8412 - val_loss: 0.3808 - val_acc: 0.8425
Epoch 10/40
8052/8052 [==============================] - 1s - loss: 0.3783 - acc: 0.8417 - val_loss: 0.3799 - val_acc: 0.8400
Epoch 11/40
8052/8052 [==============================] - 1s - loss: 0.3784 - acc: 0.8425 - val_loss: 0.3794 - val_acc: 0.8425
Epoch 12/40
8052/8052 [==============================] - 1s - loss: 0.3766 - acc: 0.8422 - val_loss: 0.3799 - val_acc: 0.8440
Epoch 13/40
8052/8052 [==============================] - 1s - loss: 0.3767 - acc: 0.8420 - val_loss: 0.3802 - val_acc: 0.8400
Epoch 14/40
8052/8052 [==============================] - 1s - loss: 0.3776 - acc: 0.8426 - val_loss: 0.3797 - val_acc: 0.8430
Epoch 15/40
8052/8052 [==============================] - 1s - loss: 0.3757 - acc: 0.8413 - val_loss: 0.3793 - val_acc: 0.8415
Epoch 16/40
8052/8052 [==============================] - 1s - loss: 0.3748 - acc: 0.8428 - val_loss: 0.3796 - val_acc: 0.8395
Epoch 17/40
8052/8052 [==============================] - 1s - loss: 0.3752 - acc: 0.8441 - val_loss: 0.3802 - val_acc: 0.8440
Epoch 18/40
8052/8052 [==============================] - 1s - loss: 0.3748 - acc: 0.8422 - val_loss: 0.3797 - val_acc: 0.8405
Epoch 19/40
8052/8052 [==============================] - 1s - loss: 0.3743 - acc: 0.8445 - val_loss: 0.3795 - val_acc: 0.8415
Epoch 20/40
8052/8052 [==============================] - 1s - loss: 0.3735 - acc: 0.8431 - val_loss: 0.3798 - val_acc: 0.8425
Epoch 21/40
8052/8052 [==============================] - 1s - loss: 0.3733 - acc: 0.8429 - val_loss: 0.3804 - val_acc: 0.8415
Epoch 22/40
8052/8052 [==============================] - 1s - loss: 0.3736 - acc: 0.8441 - val_loss: 0.3804 - val_acc: 0.8435
Epoch 23/40
8052/8052 [==============================] - 1s - loss: 0.3733 - acc: 0.8438 - val_loss: 0.3805 - val_acc: 0.8430
Epoch 24/40
8052/8052 [==============================] - 1s - loss: 0.3730 - acc: 0.8445 - val_loss: 0.3800 - val_acc: 0.8450
Epoch 25/40
8052/8052 [==============================] - 1s - loss: 0.3723 - acc: 0.8446 - val_loss: 0.3799 - val_acc: 0.8435
Epoch 26/40
8052/8052 [==============================] - 1s - loss: 0.3719 - acc: 0.8449 - val_loss: 0.3796 - val_acc: 0.8425
Epoch 27/40
8052/8052 [==============================] - 1s - loss: 0.3715 - acc: 0.8450 - val_loss: 0.3795 - val_acc: 0.8420
Epoch 28/40
8052/8052 [==============================] - 1s - loss: 0.3712 - acc: 0.8453 - val_loss: 0.3796 - val_acc: 0.8425
Epoch 29/40
8052/8052 [==============================] - 1s - loss: 0.3709 - acc: 0.8451 - val_loss: 0.3796 - val_acc: 0.8415
Epoch 30/40
8052/8052 [==============================] - 1s - loss: 0.3707 - acc: 0.8446 - val_loss: 0.3801 - val_acc: 0.8440
Epoch 31/40
8052/8052 [==============================] - 1s - loss: 0.3706 - acc: 0.8450 - val_loss: 0.3800 - val_acc: 0.8440
Epoch 32/40
8052/8052 [==============================] - 1s - loss: 0.3708 - acc: 0.8440 - val_loss: 0.3818 - val_acc: 0.8445
Epoch 33/40
8052/8052 [==============================] - 1s - loss: 0.3717 - acc: 0.8448 - val_loss: 0.3814 - val_acc: 0.8415
Epoch 34/40
8052/8052 [==============================] - 1s - loss: 0.3725 - acc: 0.8438 - val_loss: 0.3863 - val_acc: 0.8371
Epoch 35/40
8052/8052 [==============================] - 1s - loss: 0.3762 - acc: 0.8413 - val_loss: 0.3808 - val_acc: 0.8425
Epoch 36/40
8052/8052 [==============================] - 1s - loss: 0.3715 - acc: 0.8451 - val_loss: 0.3802 - val_acc: 0.8420
Epoch 37/40
8052/8052 [==============================] - 1s - loss: 0.3696 - acc: 0.8441 - val_loss: 0.3806 - val_acc: 0.8430
Epoch 38/40
8052/8052 [==============================] - 1s - loss: 0.3692 - acc: 0.8448 - val_loss: 0.3814 - val_acc: 0.8425
Epoch 39/40
8052/8052 [==============================] - 1s - loss: 0.3701 - acc: 0.8450 - val_loss: 0.3855 - val_acc: 0.8395
Epoch 40/40
8052/8052 [==============================] - 1s - loss: 0.3730 - acc: 0.8426 - val_loss: 0.3841 - val_acc: 0.8415
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-45.h5
chunk number 45
prepare data
start training
Train on 8124 samples, validate on 2032 samples
Epoch 1/40
8124/8124 [==============================] - 1s - loss: 0.3885 - acc: 0.8379 - val_loss: 0.3738 - val_acc: 0.8440
Epoch 2/40
8124/8124 [==============================] - 1s - loss: 0.3892 - acc: 0.8389 - val_loss: 0.3728 - val_acc: 0.8474
Epoch 3/40
8124/8124 [==============================] - 1s - loss: 0.3807 - acc: 0.8412 - val_loss: 0.4006 - val_acc: 0.8361
Epoch 4/40
8124/8124 [==============================] - 1s - loss: 0.3941 - acc: 0.8354 - val_loss: 0.3900 - val_acc: 0.8381
Epoch 5/40
8124/8124 [==============================] - 1s - loss: 0.4060 - acc: 0.8294 - val_loss: 0.3896 - val_acc: 0.8391
Epoch 6/40
8124/8124 [==============================] - 1s - loss: 0.4037 - acc: 0.8294 - val_loss: 0.3892 - val_acc: 0.8420
Epoch 7/40
8124/8124 [==============================] - 1s - loss: 0.3831 - acc: 0.8416 - val_loss: 0.4189 - val_acc: 0.8273
Epoch 8/40
8124/8124 [==============================] - 1s - loss: 0.4056 - acc: 0.8358 - val_loss: 0.3881 - val_acc: 0.8401
Epoch 9/40
8124/8124 [==============================] - 1s - loss: 0.3993 - acc: 0.8305 - val_loss: 0.4007 - val_acc: 0.8342
Epoch 10/40
8124/8124 [==============================] - 1s - loss: 0.4181 - acc: 0.8214 - val_loss: 0.3859 - val_acc: 0.8371
Epoch 11/40
8124/8124 [==============================] - 1s - loss: 0.3887 - acc: 0.8332 - val_loss: 0.4203 - val_acc: 0.8292
Epoch 12/40
8124/8124 [==============================] - 1s - loss: 0.4057 - acc: 0.8362 - val_loss: 0.3988 - val_acc: 0.8371
Epoch 13/40
8124/8124 [==============================] - 1s - loss: 0.3882 - acc: 0.8407 - val_loss: 0.3852 - val_acc: 0.8366
Epoch 14/40
8124/8124 [==============================] - 1s - loss: 0.3903 - acc: 0.8316 - val_loss: 0.3887 - val_acc: 0.8396
Epoch 15/40
8124/8124 [==============================] - 1s - loss: 0.3980 - acc: 0.8303 - val_loss: 0.3819 - val_acc: 0.8410
Epoch 16/40
8124/8124 [==============================] - 1s - loss: 0.3834 - acc: 0.8379 - val_loss: 0.4036 - val_acc: 0.8307
Epoch 17/40
8124/8124 [==============================] - 1s - loss: 0.3918 - acc: 0.8396 - val_loss: 0.3912 - val_acc: 0.8346
Epoch 18/40
8124/8124 [==============================] - 1s - loss: 0.3828 - acc: 0.8428 - val_loss: 0.3820 - val_acc: 0.8445
Epoch 19/40
8124/8124 [==============================] - 1s - loss: 0.3853 - acc: 0.8365 - val_loss: 0.3830 - val_acc: 0.8455
Epoch 20/40
8124/8124 [==============================] - 1s - loss: 0.3871 - acc: 0.8369 - val_loss: 0.3838 - val_acc: 0.8406
Epoch 21/40
8124/8124 [==============================] - 1s - loss: 0.3791 - acc: 0.8421 - val_loss: 0.4002 - val_acc: 0.8351
Epoch 22/40
8124/8124 [==============================] - 1s - loss: 0.3875 - acc: 0.8434 - val_loss: 0.3785 - val_acc: 0.8420
Epoch 23/40
8124/8124 [==============================] - 1s - loss: 0.3784 - acc: 0.8397 - val_loss: 0.3792 - val_acc: 0.8440
Epoch 24/40
8124/8124 [==============================] - 1s - loss: 0.3845 - acc: 0.8391 - val_loss: 0.3776 - val_acc: 0.8435
Epoch 25/40
8124/8124 [==============================] - 1s - loss: 0.3794 - acc: 0.8412 - val_loss: 0.3899 - val_acc: 0.8425
Epoch 26/40
8124/8124 [==============================] - 1s - loss: 0.3807 - acc: 0.8435 - val_loss: 0.3822 - val_acc: 0.8465
Epoch 27/40
8124/8124 [==============================] - 1s - loss: 0.3764 - acc: 0.8431 - val_loss: 0.3770 - val_acc: 0.8474
Epoch 28/40
8124/8124 [==============================] - 1s - loss: 0.3782 - acc: 0.8433 - val_loss: 0.3780 - val_acc: 0.8465
Epoch 29/40
8124/8124 [==============================] - 1s - loss: 0.3776 - acc: 0.8438 - val_loss: 0.3854 - val_acc: 0.8465
Epoch 30/40
8124/8124 [==============================] - 1s - loss: 0.3768 - acc: 0.8444 - val_loss: 0.3791 - val_acc: 0.8469
Epoch 31/40
8124/8124 [==============================] - 1s - loss: 0.3743 - acc: 0.8433 - val_loss: 0.3759 - val_acc: 0.8465
Epoch 32/40
8124/8124 [==============================] - 1s - loss: 0.3758 - acc: 0.8444 - val_loss: 0.3774 - val_acc: 0.8445
Epoch 33/40
8124/8124 [==============================] - 1s - loss: 0.3750 - acc: 0.8442 - val_loss: 0.3832 - val_acc: 0.8469
Epoch 34/40
8124/8124 [==============================] - 1s - loss: 0.3751 - acc: 0.8447 - val_loss: 0.3770 - val_acc: 0.8450
Epoch 35/40
8124/8124 [==============================] - 1s - loss: 0.3726 - acc: 0.8443 - val_loss: 0.3778 - val_acc: 0.8455
Epoch 36/40
8124/8124 [==============================] - 1s - loss: 0.3734 - acc: 0.8448 - val_loss: 0.3839 - val_acc: 0.8460
Epoch 37/40
8124/8124 [==============================] - 1s - loss: 0.3736 - acc: 0.8442 - val_loss: 0.3794 - val_acc: 0.8435
Epoch 38/40
8124/8124 [==============================] - 1s - loss: 0.3717 - acc: 0.8444 - val_loss: 0.3777 - val_acc: 0.8440
Epoch 39/40
8124/8124 [==============================] - 1s - loss: 0.3719 - acc: 0.8432 - val_loss: 0.3805 - val_acc: 0.8465
Epoch 40/40
8124/8124 [==============================] - 1s - loss: 0.3721 - acc: 0.8438 - val_loss: 0.3798 - val_acc: 0.8435
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-46.h5
chunk number 46
prepare data
start training
Train on 8172 samples, validate on 2043 samples
Epoch 1/40
8172/8172 [==============================] - 1s - loss: 0.3764 - acc: 0.8475 - val_loss: 0.3871 - val_acc: 0.8390
Epoch 2/40
8172/8172 [==============================] - 1s - loss: 0.3795 - acc: 0.8445 - val_loss: 0.3847 - val_acc: 0.8404
Epoch 3/40
8172/8172 [==============================] - 1s - loss: 0.3777 - acc: 0.8452 - val_loss: 0.3762 - val_acc: 0.8463
Epoch 4/40
8172/8172 [==============================] - 1s - loss: 0.3743 - acc: 0.8478 - val_loss: 0.3756 - val_acc: 0.8453
Epoch 5/40
8172/8172 [==============================] - 1s - loss: 0.3738 - acc: 0.8483 - val_loss: 0.3815 - val_acc: 0.8439
Epoch 6/40
8172/8172 [==============================] - 1s - loss: 0.3747 - acc: 0.8467 - val_loss: 0.3797 - val_acc: 0.8448
Epoch 7/40
8172/8172 [==============================] - 1s - loss: 0.3727 - acc: 0.8477 - val_loss: 0.3776 - val_acc: 0.8473
Epoch 8/40
8172/8172 [==============================] - 1s - loss: 0.3745 - acc: 0.8458 - val_loss: 0.3760 - val_acc: 0.8458
Epoch 9/40
8172/8172 [==============================] - 1s - loss: 0.3693 - acc: 0.8481 - val_loss: 0.3794 - val_acc: 0.8439
Epoch 10/40
8172/8172 [==============================] - 1s - loss: 0.3714 - acc: 0.8477 - val_loss: 0.3738 - val_acc: 0.8478
Epoch 11/40
8172/8172 [==============================] - 1s - loss: 0.3684 - acc: 0.8489 - val_loss: 0.3746 - val_acc: 0.8478
Epoch 12/40
8172/8172 [==============================] - 1s - loss: 0.3695 - acc: 0.8483 - val_loss: 0.3783 - val_acc: 0.8463
Epoch 13/40
8172/8172 [==============================] - 1s - loss: 0.3701 - acc: 0.8489 - val_loss: 0.3768 - val_acc: 0.8458
Epoch 14/40
8172/8172 [==============================] - 1s - loss: 0.3684 - acc: 0.8480 - val_loss: 0.3764 - val_acc: 0.8492
Epoch 15/40
8172/8172 [==============================] - 1s - loss: 0.3706 - acc: 0.8484 - val_loss: 0.3767 - val_acc: 0.8448
Epoch 16/40
8172/8172 [==============================] - 1s - loss: 0.3674 - acc: 0.8489 - val_loss: 0.3786 - val_acc: 0.8463
Epoch 17/40
8172/8172 [==============================] - 1s - loss: 0.3683 - acc: 0.8485 - val_loss: 0.3753 - val_acc: 0.8497
Epoch 18/40
8172/8172 [==============================] - 1s - loss: 0.3674 - acc: 0.8494 - val_loss: 0.3752 - val_acc: 0.8473
Epoch 19/40
8172/8172 [==============================] - 1s - loss: 0.3657 - acc: 0.8497 - val_loss: 0.3780 - val_acc: 0.8434
Epoch 20/40
8172/8172 [==============================] - 1s - loss: 0.3667 - acc: 0.8488 - val_loss: 0.3757 - val_acc: 0.8463
Epoch 21/40
8172/8172 [==============================] - 1s - loss: 0.3652 - acc: 0.8503 - val_loss: 0.3758 - val_acc: 0.8497
Epoch 22/40
8172/8172 [==============================] - 1s - loss: 0.3664 - acc: 0.8492 - val_loss: 0.3777 - val_acc: 0.8443
Epoch 23/40
8172/8172 [==============================] - 1s - loss: 0.3656 - acc: 0.8497 - val_loss: 0.3776 - val_acc: 0.8443
Epoch 24/40
8172/8172 [==============================] - 1s - loss: 0.3652 - acc: 0.8497 - val_loss: 0.3758 - val_acc: 0.8497
Epoch 25/40
8172/8172 [==============================] - 1s - loss: 0.3658 - acc: 0.8496 - val_loss: 0.3761 - val_acc: 0.8439
Epoch 26/40
8172/8172 [==============================] - 1s - loss: 0.3641 - acc: 0.8511 - val_loss: 0.3773 - val_acc: 0.8419
Epoch 27/40
8172/8172 [==============================] - 1s - loss: 0.3646 - acc: 0.8499 - val_loss: 0.3752 - val_acc: 0.8483
Epoch 28/40
8172/8172 [==============================] - 1s - loss: 0.3642 - acc: 0.8501 - val_loss: 0.3753 - val_acc: 0.8453
Epoch 29/40
8172/8172 [==============================] - 1s - loss: 0.3632 - acc: 0.8496 - val_loss: 0.3770 - val_acc: 0.8419
Epoch 30/40
8172/8172 [==============================] - 1s - loss: 0.3639 - acc: 0.8503 - val_loss: 0.3748 - val_acc: 0.8473
Epoch 31/40
8172/8172 [==============================] - 1s - loss: 0.3632 - acc: 0.8501 - val_loss: 0.3748 - val_acc: 0.8478
Epoch 32/40
8172/8172 [==============================] - 1s - loss: 0.3626 - acc: 0.8499 - val_loss: 0.3759 - val_acc: 0.8453
Epoch 33/40
8172/8172 [==============================] - 1s - loss: 0.3628 - acc: 0.8502 - val_loss: 0.3745 - val_acc: 0.8483
Epoch 34/40
8172/8172 [==============================] - 1s - loss: 0.3623 - acc: 0.8507 - val_loss: 0.3748 - val_acc: 0.8468
Epoch 35/40
8172/8172 [==============================] - 1s - loss: 0.3619 - acc: 0.8499 - val_loss: 0.3757 - val_acc: 0.8468
Epoch 36/40
8172/8172 [==============================] - 1s - loss: 0.3620 - acc: 0.8501 - val_loss: 0.3746 - val_acc: 0.8478
Epoch 37/40
8172/8172 [==============================] - 1s - loss: 0.3618 - acc: 0.8510 - val_loss: 0.3752 - val_acc: 0.8458
Epoch 38/40
8172/8172 [==============================] - 1s - loss: 0.3613 - acc: 0.8506 - val_loss: 0.3755 - val_acc: 0.8463
Epoch 39/40
8172/8172 [==============================] - 1s - loss: 0.3611 - acc: 0.8510 - val_loss: 0.3747 - val_acc: 0.8473
Epoch 40/40
8172/8172 [==============================] - 1s - loss: 0.3612 - acc: 0.8511 - val_loss: 0.3760 - val_acc: 0.8463
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-47.h5
chunk number 47
prepare data
start training
Train on 8160 samples, validate on 2040 samples
Epoch 1/40
8160/8160 [==============================] - 1s - loss: 0.3849 - acc: 0.8382 - val_loss: 0.3442 - val_acc: 0.8637
Epoch 2/40
8160/8160 [==============================] - 1s - loss: 0.3829 - acc: 0.8368 - val_loss: 0.3417 - val_acc: 0.8647
Epoch 3/40
8160/8160 [==============================] - 1s - loss: 0.3813 - acc: 0.8379 - val_loss: 0.3398 - val_acc: 0.8657
Epoch 4/40
8160/8160 [==============================] - 1s - loss: 0.3828 - acc: 0.8381 - val_loss: 0.3397 - val_acc: 0.8642
Epoch 5/40
8160/8160 [==============================] - 1s - loss: 0.3805 - acc: 0.8376 - val_loss: 0.3440 - val_acc: 0.8647
Epoch 6/40
8160/8160 [==============================] - 1s - loss: 0.3820 - acc: 0.8374 - val_loss: 0.3405 - val_acc: 0.8667
Epoch 7/40
8160/8160 [==============================] - 1s - loss: 0.3810 - acc: 0.8382 - val_loss: 0.3411 - val_acc: 0.8632
Epoch 8/40
8160/8160 [==============================] - 1s - loss: 0.3794 - acc: 0.8397 - val_loss: 0.3471 - val_acc: 0.8613
Epoch 9/40
8160/8160 [==============================] - 1s - loss: 0.3817 - acc: 0.8375 - val_loss: 0.3415 - val_acc: 0.8652
Epoch 10/40
8160/8160 [==============================] - 1s - loss: 0.3799 - acc: 0.8391 - val_loss: 0.3407 - val_acc: 0.8632
Epoch 11/40
8160/8160 [==============================] - 1s - loss: 0.3785 - acc: 0.8396 - val_loss: 0.3456 - val_acc: 0.8627
Epoch 12/40
8160/8160 [==============================] - 1s - loss: 0.3807 - acc: 0.8370 - val_loss: 0.3415 - val_acc: 0.8637
Epoch 13/40
8160/8160 [==============================] - 1s - loss: 0.3807 - acc: 0.8392 - val_loss: 0.3408 - val_acc: 0.8642
Epoch 14/40
8160/8160 [==============================] - 1s - loss: 0.3772 - acc: 0.8384 - val_loss: 0.3475 - val_acc: 0.8603
Epoch 15/40
8160/8160 [==============================] - 1s - loss: 0.3809 - acc: 0.8388 - val_loss: 0.3431 - val_acc: 0.8627
Epoch 16/40
8160/8160 [==============================] - 1s - loss: 0.3826 - acc: 0.8385 - val_loss: 0.3407 - val_acc: 0.8637
Epoch 17/40
8160/8160 [==============================] - 1s - loss: 0.3765 - acc: 0.8387 - val_loss: 0.3515 - val_acc: 0.8623
Epoch 18/40
8160/8160 [==============================] - 1s - loss: 0.3843 - acc: 0.8368 - val_loss: 0.3510 - val_acc: 0.8569
Epoch 19/40
8160/8160 [==============================] - 1s - loss: 0.3921 - acc: 0.8338 - val_loss: 0.3433 - val_acc: 0.8672
Epoch 20/40
8160/8160 [==============================] - 1s - loss: 0.3792 - acc: 0.8406 - val_loss: 0.3768 - val_acc: 0.8417
Epoch 21/40
8160/8160 [==============================] - 1s - loss: 0.4048 - acc: 0.8265 - val_loss: 0.3623 - val_acc: 0.8495
Epoch 22/40
8160/8160 [==============================] - 1s - loss: 0.4054 - acc: 0.8270 - val_loss: 0.3655 - val_acc: 0.8461
Epoch 23/40
8160/8160 [==============================] - 1s - loss: 0.4085 - acc: 0.8259 - val_loss: 0.3523 - val_acc: 0.8637
Epoch 24/40
8160/8160 [==============================] - 1s - loss: 0.3836 - acc: 0.8397 - val_loss: 0.3866 - val_acc: 0.8490
Epoch 25/40
8160/8160 [==============================] - 1s - loss: 0.4148 - acc: 0.8275 - val_loss: 0.3527 - val_acc: 0.8515
Epoch 26/40
8160/8160 [==============================] - 1s - loss: 0.3930 - acc: 0.8303 - val_loss: 0.3730 - val_acc: 0.8377
Epoch 27/40
8160/8160 [==============================] - 1s - loss: 0.4162 - acc: 0.8201 - val_loss: 0.3549 - val_acc: 0.8500
Epoch 28/40
8160/8160 [==============================] - 1s - loss: 0.3944 - acc: 0.8295 - val_loss: 0.3623 - val_acc: 0.8657
Epoch 29/40
8160/8160 [==============================] - 1s - loss: 0.3961 - acc: 0.8365 - val_loss: 0.3655 - val_acc: 0.8647
Epoch 30/40
8160/8160 [==============================] - 1s - loss: 0.3987 - acc: 0.8360 - val_loss: 0.3504 - val_acc: 0.8539
Epoch 31/40
8160/8160 [==============================] - 1s - loss: 0.3887 - acc: 0.8327 - val_loss: 0.3612 - val_acc: 0.8490
Epoch 32/40
8160/8160 [==============================] - 1s - loss: 0.4027 - acc: 0.8286 - val_loss: 0.3485 - val_acc: 0.8588
Epoch 33/40
8160/8160 [==============================] - 1s - loss: 0.3860 - acc: 0.8360 - val_loss: 0.3624 - val_acc: 0.8613
Epoch 34/40
8160/8160 [==============================] - 1s - loss: 0.3934 - acc: 0.8360 - val_loss: 0.3500 - val_acc: 0.8618
Epoch 35/40
8160/8160 [==============================] - 1s - loss: 0.3833 - acc: 0.8382 - val_loss: 0.3481 - val_acc: 0.8603
Epoch 36/40
8160/8160 [==============================] - 1s - loss: 0.3863 - acc: 0.8374 - val_loss: 0.3482 - val_acc: 0.8598
Epoch 37/40
8160/8160 [==============================] - 1s - loss: 0.3872 - acc: 0.8373 - val_loss: 0.3452 - val_acc: 0.8632
Epoch 38/40
8160/8160 [==============================] - 1s - loss: 0.3799 - acc: 0.8393 - val_loss: 0.3501 - val_acc: 0.8618
Epoch 39/40
8160/8160 [==============================] - 1s - loss: 0.3831 - acc: 0.8393 - val_loss: 0.3468 - val_acc: 0.8603
Epoch 40/40
8160/8160 [==============================] - 1s - loss: 0.3838 - acc: 0.8382 - val_loss: 0.3477 - val_acc: 0.8593
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-48.h5
chunk number 48
prepare data
start training
Train on 8160 samples, validate on 2040 samples
Epoch 1/40
8160/8160 [==============================] - 1s - loss: 0.3823 - acc: 0.8384 - val_loss: 0.3892 - val_acc: 0.8397
Epoch 2/40
8160/8160 [==============================] - 1s - loss: 0.3803 - acc: 0.8391 - val_loss: 0.3899 - val_acc: 0.8397
Epoch 3/40
8160/8160 [==============================] - 1s - loss: 0.3815 - acc: 0.8375 - val_loss: 0.3920 - val_acc: 0.8382
Epoch 4/40
8160/8160 [==============================] - 1s - loss: 0.3799 - acc: 0.8408 - val_loss: 0.3921 - val_acc: 0.8377
Epoch 5/40
8160/8160 [==============================] - 1s - loss: 0.3795 - acc: 0.8402 - val_loss: 0.3890 - val_acc: 0.8377
Epoch 6/40
8160/8160 [==============================] - 1s - loss: 0.3784 - acc: 0.8381 - val_loss: 0.3877 - val_acc: 0.8363
Epoch 7/40
8160/8160 [==============================] - 1s - loss: 0.3766 - acc: 0.8413 - val_loss: 0.3902 - val_acc: 0.8382
Epoch 8/40
8160/8160 [==============================] - 1s - loss: 0.3779 - acc: 0.8403 - val_loss: 0.3860 - val_acc: 0.8387
Epoch 9/40
8160/8160 [==============================] - 1s - loss: 0.3748 - acc: 0.8413 - val_loss: 0.3872 - val_acc: 0.8368
Epoch 10/40
8160/8160 [==============================] - 1s - loss: 0.3773 - acc: 0.8398 - val_loss: 0.3852 - val_acc: 0.8377
Epoch 11/40
8160/8160 [==============================] - 1s - loss: 0.3744 - acc: 0.8402 - val_loss: 0.3875 - val_acc: 0.8387
Epoch 12/40
8160/8160 [==============================] - 1s - loss: 0.3758 - acc: 0.8406 - val_loss: 0.3842 - val_acc: 0.8363
Epoch 13/40
8160/8160 [==============================] - 1s - loss: 0.3731 - acc: 0.8413 - val_loss: 0.3854 - val_acc: 0.8368
Epoch 14/40
8160/8160 [==============================] - 1s - loss: 0.3741 - acc: 0.8412 - val_loss: 0.3852 - val_acc: 0.8387
Epoch 15/40
8160/8160 [==============================] - 1s - loss: 0.3727 - acc: 0.8429 - val_loss: 0.3855 - val_acc: 0.8373
Epoch 16/40
8160/8160 [==============================] - 1s - loss: 0.3726 - acc: 0.8431 - val_loss: 0.3848 - val_acc: 0.8368
Epoch 17/40
8160/8160 [==============================] - 1s - loss: 0.3724 - acc: 0.8426 - val_loss: 0.3840 - val_acc: 0.8387
Epoch 18/40
8160/8160 [==============================] - 1s - loss: 0.3712 - acc: 0.8431 - val_loss: 0.3854 - val_acc: 0.8397
Epoch 19/40
8160/8160 [==============================] - 1s - loss: 0.3719 - acc: 0.8434 - val_loss: 0.3841 - val_acc: 0.8382
Epoch 20/40
8160/8160 [==============================] - 1s - loss: 0.3707 - acc: 0.8434 - val_loss: 0.3849 - val_acc: 0.8358
Epoch 21/40
8160/8160 [==============================] - 1s - loss: 0.3712 - acc: 0.8426 - val_loss: 0.3847 - val_acc: 0.8387
Epoch 22/40
8160/8160 [==============================] - 1s - loss: 0.3704 - acc: 0.8429 - val_loss: 0.3845 - val_acc: 0.8382
Epoch 23/40
8160/8160 [==============================] - 1s - loss: 0.3700 - acc: 0.8434 - val_loss: 0.3849 - val_acc: 0.8348
Epoch 24/40
8160/8160 [==============================] - 1s - loss: 0.3704 - acc: 0.8436 - val_loss: 0.3842 - val_acc: 0.8387
Epoch 25/40
8160/8160 [==============================] - 1s - loss: 0.3694 - acc: 0.8433 - val_loss: 0.3845 - val_acc: 0.8392
Epoch 26/40
8160/8160 [==============================] - 1s - loss: 0.3696 - acc: 0.8436 - val_loss: 0.3846 - val_acc: 0.8353
Epoch 27/40
8160/8160 [==============================] - 1s - loss: 0.3694 - acc: 0.8438 - val_loss: 0.3842 - val_acc: 0.8373
Epoch 28/40
8160/8160 [==============================] - 1s - loss: 0.3687 - acc: 0.8434 - val_loss: 0.3846 - val_acc: 0.8397
Epoch 29/40
8160/8160 [==============================] - 1s - loss: 0.3689 - acc: 0.8436 - val_loss: 0.3847 - val_acc: 0.8368
Epoch 30/40
8160/8160 [==============================] - 1s - loss: 0.3686 - acc: 0.8439 - val_loss: 0.3844 - val_acc: 0.8392
Epoch 31/40
8160/8160 [==============================] - 1s - loss: 0.3681 - acc: 0.8440 - val_loss: 0.3847 - val_acc: 0.8392
Epoch 32/40
8160/8160 [==============================] - 1s - loss: 0.3681 - acc: 0.8446 - val_loss: 0.3851 - val_acc: 0.8387
Epoch 33/40
8160/8160 [==============================] - 1s - loss: 0.3679 - acc: 0.8444 - val_loss: 0.3851 - val_acc: 0.8382
Epoch 34/40
8160/8160 [==============================] - 1s - loss: 0.3675 - acc: 0.8450 - val_loss: 0.3854 - val_acc: 0.8397
Epoch 35/40
8160/8160 [==============================] - 1s - loss: 0.3675 - acc: 0.8444 - val_loss: 0.3855 - val_acc: 0.8368
Epoch 36/40
8160/8160 [==============================] - 1s - loss: 0.3673 - acc: 0.8447 - val_loss: 0.3850 - val_acc: 0.8377
Epoch 37/40
8160/8160 [==============================] - 1s - loss: 0.3670 - acc: 0.8451 - val_loss: 0.3849 - val_acc: 0.8373
Epoch 38/40
8160/8160 [==============================] - 1s - loss: 0.3668 - acc: 0.8450 - val_loss: 0.3851 - val_acc: 0.8373
Epoch 39/40
8160/8160 [==============================] - 1s - loss: 0.3667 - acc: 0.8449 - val_loss: 0.3851 - val_acc: 0.8377
Epoch 40/40
8160/8160 [==============================] - 1s - loss: 0.3665 - acc: 0.8452 - val_loss: 0.3851 - val_acc: 0.8363
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-49.h5
chunk number 49
prepare data
start training
Train on 8063 samples, validate on 2016 samples
Epoch 1/40
8063/8063 [==============================] - 1s - loss: 0.3718 - acc: 0.8455 - val_loss: 0.3893 - val_acc: 0.8442
Epoch 2/40
8063/8063 [==============================] - 1s - loss: 0.3710 - acc: 0.8448 - val_loss: 0.3849 - val_acc: 0.8428
Epoch 3/40
8063/8063 [==============================] - 1s - loss: 0.3694 - acc: 0.8452 - val_loss: 0.3842 - val_acc: 0.8442
Epoch 4/40
8063/8063 [==============================] - 1s - loss: 0.3693 - acc: 0.8458 - val_loss: 0.3891 - val_acc: 0.8467
Epoch 5/40
8063/8063 [==============================] - 1s - loss: 0.3691 - acc: 0.8453 - val_loss: 0.3852 - val_acc: 0.8438
Epoch 6/40
8063/8063 [==============================] - 1s - loss: 0.3676 - acc: 0.8458 - val_loss: 0.3846 - val_acc: 0.8442
Epoch 7/40
8063/8063 [==============================] - 1s - loss: 0.3676 - acc: 0.8472 - val_loss: 0.3886 - val_acc: 0.8487
Epoch 8/40
8063/8063 [==============================] - 1s - loss: 0.3673 - acc: 0.8471 - val_loss: 0.3859 - val_acc: 0.8447
Epoch 9/40
8063/8063 [==============================] - 1s - loss: 0.3662 - acc: 0.8471 - val_loss: 0.3845 - val_acc: 0.8423
Epoch 10/40
8063/8063 [==============================] - 1s - loss: 0.3667 - acc: 0.8467 - val_loss: 0.3887 - val_acc: 0.8452
Epoch 11/40
8063/8063 [==============================] - 1s - loss: 0.3662 - acc: 0.8472 - val_loss: 0.3858 - val_acc: 0.8433
Epoch 12/40
8063/8063 [==============================] - 1s - loss: 0.3649 - acc: 0.8473 - val_loss: 0.3847 - val_acc: 0.8442
Epoch 13/40
8063/8063 [==============================] - 1s - loss: 0.3651 - acc: 0.8481 - val_loss: 0.3888 - val_acc: 0.8452
Epoch 14/40
8063/8063 [==============================] - 1s - loss: 0.3649 - acc: 0.8465 - val_loss: 0.3857 - val_acc: 0.8472
Epoch 15/40
8063/8063 [==============================] - 1s - loss: 0.3640 - acc: 0.8481 - val_loss: 0.3841 - val_acc: 0.8467
Epoch 16/40
8063/8063 [==============================] - 1s - loss: 0.3642 - acc: 0.8481 - val_loss: 0.3884 - val_acc: 0.8457
Epoch 17/40
8063/8063 [==============================] - 1s - loss: 0.3645 - acc: 0.8475 - val_loss: 0.3842 - val_acc: 0.8467
Epoch 18/40
8063/8063 [==============================] - 1s - loss: 0.3635 - acc: 0.8481 - val_loss: 0.3857 - val_acc: 0.8477
Epoch 19/40
8063/8063 [==============================] - 1s - loss: 0.3629 - acc: 0.8488 - val_loss: 0.3874 - val_acc: 0.8467
Epoch 20/40
8063/8063 [==============================] - 1s - loss: 0.3629 - acc: 0.8487 - val_loss: 0.3838 - val_acc: 0.8452
Epoch 21/40
8063/8063 [==============================] - 1s - loss: 0.3629 - acc: 0.8488 - val_loss: 0.3865 - val_acc: 0.8482
Epoch 22/40
8063/8063 [==============================] - 1s - loss: 0.3626 - acc: 0.8497 - val_loss: 0.3842 - val_acc: 0.8462
Epoch 23/40
8063/8063 [==============================] - 1s - loss: 0.3620 - acc: 0.8497 - val_loss: 0.3851 - val_acc: 0.8462
Epoch 24/40
8063/8063 [==============================] - 1s - loss: 0.3616 - acc: 0.8503 - val_loss: 0.3861 - val_acc: 0.8497
Epoch 25/40
8063/8063 [==============================] - 1s - loss: 0.3615 - acc: 0.8493 - val_loss: 0.3840 - val_acc: 0.8452
Epoch 26/40
8063/8063 [==============================] - 1s - loss: 0.3615 - acc: 0.8497 - val_loss: 0.3878 - val_acc: 0.8457
Epoch 27/40
8063/8063 [==============================] - 1s - loss: 0.3619 - acc: 0.8498 - val_loss: 0.3839 - val_acc: 0.8438
Epoch 28/40
8063/8063 [==============================] - 1s - loss: 0.3617 - acc: 0.8497 - val_loss: 0.3887 - val_acc: 0.8462
Epoch 29/40
8063/8063 [==============================] - 1s - loss: 0.3618 - acc: 0.8504 - val_loss: 0.3842 - val_acc: 0.8442
Epoch 30/40
8063/8063 [==============================] - 1s - loss: 0.3610 - acc: 0.8501 - val_loss: 0.3871 - val_acc: 0.8472
Epoch 31/40
8063/8063 [==============================] - 1s - loss: 0.3606 - acc: 0.8499 - val_loss: 0.3845 - val_acc: 0.8452
Epoch 32/40
8063/8063 [==============================] - 1s - loss: 0.3600 - acc: 0.8510 - val_loss: 0.3860 - val_acc: 0.8472
Epoch 33/40
8063/8063 [==============================] - 1s - loss: 0.3596 - acc: 0.8514 - val_loss: 0.3848 - val_acc: 0.8438
Epoch 34/40
8063/8063 [==============================] - 1s - loss: 0.3593 - acc: 0.8506 - val_loss: 0.3851 - val_acc: 0.8447
Epoch 35/40
8063/8063 [==============================] - 1s - loss: 0.3590 - acc: 0.8513 - val_loss: 0.3848 - val_acc: 0.8442
Epoch 36/40
8063/8063 [==============================] - 1s - loss: 0.3588 - acc: 0.8515 - val_loss: 0.3844 - val_acc: 0.8442
Epoch 37/40
8063/8063 [==============================] - 1s - loss: 0.3585 - acc: 0.8509 - val_loss: 0.3850 - val_acc: 0.8457
Epoch 38/40
8063/8063 [==============================] - 1s - loss: 0.3584 - acc: 0.8515 - val_loss: 0.3839 - val_acc: 0.8442
Epoch 39/40
8063/8063 [==============================] - 1s - loss: 0.3583 - acc: 0.8513 - val_loss: 0.3858 - val_acc: 0.8462
Epoch 40/40
8063/8063 [==============================] - 1s - loss: 0.3586 - acc: 0.8509 - val_loss: 0.3827 - val_acc: 0.8423
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-50.h5
saving fitted model to /users/phwindis/BTagger/RNN_out/lstm64_3layers_wo_phieta/model-final.h5
