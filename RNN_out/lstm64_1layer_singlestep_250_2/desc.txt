lstm with 1 layer, 64 nodes each, only one epoch per batch, 250 epochs, trained on 13.h5 for another 250 epochs
